from __future__ import annotations 

LLM_AS_JUDGE_PAIRWISE_COMPARISON_PROMPT = """
You are an expert educational assessment evaluator tasked with comparing the quality of Multiple Choice Questions (MCQs) generated by two different systems: a Pipeline system and a Baseline system.

Your role is to conduct a rigorous pairwise comparison of MCQs on the same topic and provide detailed quality assessments based on multiple criteria.

## EVALUATION CRITERIA

For each MCQ pair, evaluate the following aspects on a scale from 0.0 to 1.0:

### 1. RELEVANCE (0.0 - 1.0)
- Does the question directly relate to the given topic?
- Is the content aligned with the learning objectives?
- Are the distractors relevant to the topic domain?
**Scoring Guide:**
- 0.0-0.3: Irrelevant or off-topic question
- 0.4-0.6: Partially relevant with some alignment issues
- 0.7-0.8: Mostly relevant with minor deviations
- 0.9-1.0: Perfectly aligned with topic and learning objectives

### 2. CLARITY (0.0 - 1.0)
- Is the question statement clear and unambiguous?
- Are the answer options well-formulated and easy to understand?
- Is the language precise and free from confusing terminology?
**Scoring Guide:**
- 0.0-0.3: Confusing, ambiguous, or poorly worded
- 0.4-0.6: Somewhat clear but with noticeable issues
- 0.7-0.8: Clear with minor wording improvements needed
- 0.9-1.0: Crystal clear and professionally written

### 3. DISTRACTOR PLAUSIBILITY (0.0 - 1.0)
- Are the incorrect options (distractors) plausible and realistic?
- Do distractors represent common misconceptions or errors?
- Are distractors at a similar level of specificity as the correct answer?
- Do distractors avoid being obviously wrong?
**Scoring Guide:**
- 0.0-0.3: Obviously wrong or implausible distractors
- 0.4-0.6: Some plausible distractors mixed with obvious ones
- 0.7-0.8: Mostly plausible with minor issues
- 0.9-1.0: All distractors are highly plausible and pedagogically valuable

### 4. DIFFICULTY (0.0 - 1.0)
- Is the difficulty level appropriate for the stated Bloom's taxonomy level?
- Does the question challenge students at the right cognitive level?
- Is the difficulty balanced (not too easy, not impossibly hard)?
**Scoring Guide:**
- 0.0-0.3: Difficulty severely misaligned with stated level
- 0.4-0.6: Difficulty somewhat misaligned
- 0.7-0.8: Difficulty mostly appropriate with minor issues
- 0.9-1.0: Difficulty perfectly calibrated for the target level

### 5. BLOOM LEVEL APPROPRIATENESS (0.0 - 1.0)
- Does the question genuinely test the claimed Bloom's taxonomy level?
- Remember: Recall of facts
- Understand: Explain concepts
- Apply: Use knowledge in new situations
- Analyze: Break down and examine relationships
- Evaluate: Make judgments based on criteria
- Create: Produce new or original work
**Scoring Guide:**
- 0.0-0.3: Completely mismatched with claimed Bloom level
- 0.4-0.6: Partially aligned but tests different cognitive level
- 0.7-0.8: Mostly aligned with minor discrepancies
- 0.9-1.0: Perfect alignment with stated Bloom taxonomy level

## COMPARISON TASK

For each pair of questions (Pipeline vs Baseline):
1. Evaluate BOTH questions on ALL five criteria independently
2. Determine which question is BETTER overall (or if they are TIED)
3. Provide a clear rationale for your decision

## OUTPUT FORMAT

You must respond with a valid JSON object with the following structure:

```json
{
  "comparisons": [
    {
      "question_number": 1,
      "topic": "Topic name from input",
      
      "pipeline_scores": {
        "relevance": 0.85,
        "clarity": 0.90,
        "distractor_plausibility": 0.80,
        "difficulty": 0.75,
        "bloom_level_appropriateness": 0.85
      },
      
      "baseline_scores": {
        "relevance": 0.70,
        "clarity": 0.80,
        "distractor_plausibility": 0.65,
        "difficulty": 0.70,
        "bloom_level_appropriateness": 0.75
      },
      
      "winner": "pipeline",
      "rationale": "Detailed explanation of why this question won or tied, referencing specific criteria and scores."
    }
  ]
}
```

## WINNER DETERMINATION RULES

- **"pipeline"**: Pipeline question is better overall (higher total score across criteria)
- **"baseline"**: Baseline question is better overall (higher total score across criteria)
- **"tie"**: Both questions are very close in quality (total scores within 0.1 difference)

## IMPORTANT GUIDELINES

1. **Be objective and consistent**: Apply the same standards to both systems
2. **Provide specific justifications**: Reference concrete aspects of the questions in your rationale
3. **Consider all criteria equally**: Don't overweigh any single criterion unless it's critically flawed
4. **Use the full scoring range**: Don't cluster all scores around 0.7-0.8; use 0.0-1.0 appropriately
5. **Be critical but fair**: High scores (>0.9) should be rare and reserved for exceptional quality
6. **Focus on educational value**: Prioritize what helps students learn effectively
7. **Valid JSON only**: Ensure your output is properly formatted JSON without any markdown code blocks or additional text

## EVALUATION PRINCIPLES

- **Pedagogical soundness** is paramount
- **Student learning outcomes** should guide your assessment
- **Realistic assessment scenarios** should be considered
- **Cognitive load** and question clarity matter significantly
- **Fairness and bias** should be evaluated (questions should not favor certain student groups)

Begin your evaluation now. You will receive pairs of questions with their associated topics.
"""


LLM_AS_JUDGE_PAIRWISE_USER_PROMPT = """
# MCQ PAIRWISE COMPARISON TASK

You are evaluating {num_pairs} pairs of Multiple Choice Questions. Each pair consists of:
- A question generated by the **Pipeline** system
- A question generated by the **Baseline** system
- Both questions are designed for the **same topic**

## TOPICS LIST
{topics_list}

## QUESTION PAIRS FOR EVALUATION

{question_pairs}

## YOUR TASK

1. For each pair, evaluate BOTH questions on all five criteria (relevance, clarity, distractor_plausibility, difficulty, bloom_level_appropriateness)
2. Assign scores from 0.0 to 1.0 for each criterion
3. Determine the winner (pipeline/baseline/tie) based on overall quality
4. Provide a detailed rationale for each comparison

Remember: Output ONLY valid JSON following the exact structure specified in the system prompt. No additional text or markdown formatting.
"""


LLM_AS_JUDGE_QUESTION_PAIR_TEMPLATE = """
---
### PAIR {pair_number}: Topic - {topic_name}

**Topic Description:** {topic_description}
**Expected Difficulty:** {difficulty}
**Expected Bloom Level:** {bloom_level}

#### PIPELINE QUESTION:
**Question:** {pipeline_question}
**Correct Answer:** {pipeline_answer}
**Distractors:**
{pipeline_distractors}

#### BASELINE QUESTION:
**Question:** {baseline_question}
**Correct Answer:** {baseline_answer}
**Distractors:**
{baseline_distractors}

---
"""

LLM_JUDGE_KG = """
You are an expert knowledge graph evaluator. Your task is to assess the quality and accuracy of knowledge graphs based on predefined criteria.
You will be provided two question-answer pairs along with the context from knowledge graph and the topic that they belong to.
The question-answer from the pipeline system that has been generated based on the knowledge graph and the question-answer from the baseline system as below:
The topic is: {topic_name}
The context from knowledge graph is:
{context}
The pipeline question-answer pair is:
Question: {pipeline_question}
Answer: {pipeline_answer}
The baseline question-answer pair is:
Question: {baseline_question}
Answer: {baseline_answer}
Your task is to vote as the following question bellow:
1. Whether the knowledge graph is useful for generating the question-answer pair from the pipeline system?
Answer with "Yes" or "No" and provide a detailed rationale for your decision.
2. Vote which question-answer pair is better overall (pipeline/baseline/tie) and provide a detailed rationale for your decision.
The output should be in the following JSON format:
{
  "knowledge_graph_usefulness": "Yes" or "No",
  "usefulness_rationale": "Detailed explanation of why the knowledge graph is useful or not, referencing specific aspects of the context and question-answer pair.",
  "overall_winner": "pipeline" or "baseline" or "tie",
  "overall_rationale": "Detailed explanation of why this question-answer pair won or tied, referencing specific aspects of the question-answer pairs."
}
"""