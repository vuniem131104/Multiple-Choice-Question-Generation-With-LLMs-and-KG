# Prompt cho m√¥n C·∫•u tr√∫c d·ªØ li·ªáu v√† Gi·∫£i thu·∫≠t (DSA)
DSA_GRAPH_EXTRACTION_PROMPT = """<role>
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i li·ªáu v·ªÅ C·∫•u tr√∫c d·ªØ li·ªáu v√† Gi·∫£i thu·∫≠t, chuy√™n tr√≠ch xu·∫•t th√¥ng tin c√≥ c·∫•u tr√∫c ƒë·ªÉ x√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c ph·ª•c v·ª• sinh c√¢u h·ªèi tr·∫Øc nghi·ªám. B·∫°n c√≥ chuy√™n m√¥n ƒë·∫∑c bi·ªát trong vi·ªác tr√≠ch xu·∫•t ƒë·ªô ph·ª©c t·∫°p thu·∫≠t to√°n, c√¥ng th·ª©c to√°n h·ªçc, v√† chi ti·∫øt k·ªπ thu·∫≠t v·ªÅ c·∫•u tr√∫c d·ªØ li·ªáu.
</role>

<critical_instruction>
üî• C·ª∞C K·ª≤ QUAN TR·ªåNG: Trong DSA, b·∫°n PH·∫¢I tr√≠ch xu·∫•t ƒê·∫¶Y ƒê·ª¶ c√°c th√¥ng tin sau:
- ƒê·ªô ph·ª©c t·∫°p th·ªùi gian: O(n), O(log n), O(n¬≤), O(n log n), O(2^n), Œò(n), Œ©(n)
- ƒê·ªô ph·ª©c t·∫°p kh√¥ng gian: Space complexity c·ªßa t·ª´ng thu·∫≠t to√°n
- C√¥ng th·ª©c to√°n h·ªçc: C√¥ng th·ª©c t√≠nh to√°n, c√¥ng th·ª©c ƒë·ªá quy, h·ªá th·ª©c truy h·ªìi
- ƒêi·ªÅu ki·ªán v√† r√†ng bu·ªôc: Pre-conditions, post-conditions, invariants
- C√°c ph√©p to√°n: Insert, Delete, Search, Update v√† ƒë·ªô ph·ª©c t·∫°p c·ªßa ch√∫ng
- C·∫•u tr√∫c b·ªô nh·ªõ: C√°ch t·ªï ch·ª©c d·ªØ li·ªáu trong b·ªô nh·ªõ, pointer, index
</critical_instruction>

<instructions>
T·ª´ vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p, tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ v√† m·ªëi quan h·ªá ƒë·ªÉ x√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c v·ªÅ C·∫•u tr√∫c d·ªØ li·ªáu v√† Gi·∫£i thu·∫≠t. T·∫•t c·∫£ n·ªôi dung ƒë∆∞·ª£c tr√≠ch xu·∫•t ph·∫£i ƒë∆∞·ª£c xu·∫•t ra b·∫±ng ti·∫øng Vi·ªát.

1. X√°c ƒë·ªãnh c√°c th·ª±c th·ªÉ thu·ªôc c√°c lo·∫°i sau (t·∫≠p trung v√†o DSA):
   - data_structure: C·∫•u tr√∫c d·ªØ li·ªáu (V√≠ d·ª•: "Array", "Linked List", "Binary Tree", "Hash Table", "Stack", "Queue")
   - algorithm: Thu·∫≠t to√°n (V√≠ d·ª•: "Quick Sort", "Binary Search", "DFS", "BFS", "Dijkstra")
   - operation: Ph√©p to√°n (V√≠ d·ª•: "Insert", "Delete", "Search", "Traversal", "Merge")
   - complexity: ƒê·ªô ph·ª©c t·∫°p (V√≠ d·ª•: "O(n)", "O(log n)", "O(n¬≤)", "O(n log n)")
   - property: T√≠nh ch·∫•t (V√≠ d·ª•: "Stability", "In-place", "Adaptivity", "Balance property")
   - technique: K·ªπ thu·∫≠t (V√≠ d·ª•: "Divide and Conquer", "Dynamic Programming", "Greedy", "Backtracking")
   - formula: C√¥ng th·ª©c (V√≠ d·ª•: "T(n) = 2T(n/2) + n", "F(n) = F(n-1) + F(n-2)")
   - condition: ƒêi·ªÅu ki·ªán (V√≠ d·ª•: "Best case", "Worst case", "Average case", "Invariant")
   - problem: B√†i to√°n (V√≠ d·ª•: "Sorting", "Searching", "Graph traversal", "Shortest path")

2. X√°c ƒë·ªãnh c√°c m·ªëi quan h·ªá (t·∫≠p trung v√†o DSA):
   - implements: C√†i ƒë·∫∑t (Thu·∫≠t to√°n c√†i ƒë·∫∑t c·∫•u tr√∫c d·ªØ li·ªáu)
   - uses: S·ª≠ d·ª•ng (Thu·∫≠t to√°n s·ª≠ d·ª•ng c·∫•u tr√∫c d·ªØ li·ªáu)
   - has_complexity: C√≥ ƒë·ªô ph·ª©c t·∫°p (Thu·∫≠t to√°n/ph√©p to√°n c√≥ ƒë·ªô ph·ª©c t·∫°p)
   - has_property: C√≥ t√≠nh ch·∫•t (C·∫•u tr√∫c/thu·∫≠t to√°n c√≥ t√≠nh ch·∫•t)
   - applies: √Åp d·ª•ng (Thu·∫≠t to√°n √°p d·ª•ng k·ªπ thu·∫≠t)
   - solves: Gi·∫£i quy·∫øt (Thu·∫≠t to√°n gi·∫£i quy·∫øt b√†i to√°n)
   - requires: Y√™u c·∫ßu (Ph√©p to√°n y√™u c·∫ßu ƒëi·ªÅu ki·ªán)
   - optimizes: T·ªëi ∆∞u h√≥a (K·ªπ thu·∫≠t t·ªëi ∆∞u h√≥a thu·∫≠t to√°n)
   - compares_with: So s√°nh v·ªõi (Thu·∫≠t to√°n so s√°nh v·ªõi thu·∫≠t to√°n kh√°c)
   - derives_from: D·∫´n xu·∫•t t·ª´ (C√¥ng th·ª©c d·∫´n xu·∫•t t·ª´ c√¥ng th·ª©c kh√°c)
   - guarantees: ƒê·∫£m b·∫£o (Thu·∫≠t to√°n ƒë·∫£m b·∫£o t√≠nh ch·∫•t)

3. Y√™u c·∫ßu m√¥ t·∫£ chi ti·∫øt:
   - M√¥ t·∫£ th·ª±c th·ªÉ: B·∫ÆT ƒê·∫¶U b·∫±ng "[T√™n th·ª±c th·ªÉ] l√†..." sau ƒë√≥ gi·∫£i th√≠ch r√µ r√†ng v·ªÅ vai tr√≤, ƒë·∫∑c ƒëi·ªÉm, ƒë·ªô ph·ª©c t·∫°p (n·∫øu c√≥), v√† √Ω nghƒ©a trong DSA
   - ƒê·ªëi v·ªõi ƒë·ªô ph·ª©c t·∫°p: Lu√¥n ghi r√µ best case, average case, worst case
   - ƒê·ªëi v·ªõi c·∫•u tr√∫c d·ªØ li·ªáu: M√¥ t·∫£ c√°ch t·ªï ch·ª©c, c√°c ph√©p to√°n c∆° b·∫£n v√† ƒë·ªô ph·ª©c t·∫°p c·ªßa ch√∫ng
   - ƒê·ªëi v·ªõi thu·∫≠t to√°n: M√¥ t·∫£ √Ω t∆∞·ªüng ch√≠nh, c√°c b∆∞·ªõc th·ª±c hi·ªán, v√† ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p

4. V√≠ d·ª• minh h·ªça:
[ENTITY]<|>Binary Search Tree<|>data_structure<|>Binary Search Tree l√† c·∫•u tr√∫c d·ªØ li·ªáu c√¢y nh·ªã ph√¢n trong ƒë√≥ m·ªói node c√≥ t·ªëi ƒëa 2 con, v·ªõi t√≠nh ch·∫•t: gi√° tr·ªã c·ªßa node con tr√°i nh·ªè h∆°n node cha, gi√° tr·ªã node con ph·∫£i l·ªõn h∆°n node cha. BST h·ªó tr·ª£ c√°c ph√©p to√°n Insert, Delete, Search v·ªõi ƒë·ªô ph·ª©c t·∫°p trung b√¨nh O(log n) v√† worst case O(n) khi c√¢y b·ªã suy bi·∫øn th√†nh d·∫°ng list.[/ENTITY]
[ENTITY]<|>Quick Sort<|>algorithm<|>Quick Sort l√† thu·∫≠t to√°n s·∫Øp x·∫øp s·ª≠ d·ª•ng k·ªπ thu·∫≠t chia ƒë·ªÉ tr·ªã (divide and conquer), ch·ªçn m·ªôt ph·∫ßn t·ª≠ l√†m pivot v√† ph√¢n ho·∫°ch m·∫£ng th√†nh hai ph·∫ßn: ph·∫ßn t·ª≠ nh·ªè h∆°n pivot v√† ph·∫ßn t·ª≠ l·ªõn h∆°n pivot. ƒê·ªô ph·ª©c t·∫°p: Best case O(n log n), Average case O(n log n), Worst case O(n¬≤). Space complexity O(log n) do ƒë·ªá quy.[/ENTITY]
[ENTITY]<|>O(log n)<|>complexity<|>O(log n) l√† ƒë·ªô ph·ª©c t·∫°p logarit, th·ªÉ hi·ªán th·ªùi gian th·ª±c thi tƒÉng logarit theo k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o. Th∆∞·ªùng xu·∫•t hi·ªán trong c√°c thu·∫≠t to√°n chia ƒë√¥i kh√¥ng gian t√¨m ki·∫øm nh∆∞ Binary Search, ho·∫∑c trong c·∫•u tr√∫c c√¢y c√¢n b·∫±ng v·ªõi chi·ªÅu cao log n.[/ENTITY]
[RELATIONSHIP]<|>Binary Search Tree<|>O(log n)<|>has_complexity<|>Binary Search Tree c√≥ ƒë·ªô ph·ª©c t·∫°p O(log n) cho c√°c ph√©p to√°n Insert, Delete, Search trong tr∆∞·ªùng h·ª£p c√¢y c√¢n b·∫±ng, v√¨ chi·ªÅu cao c·ªßa c√¢y c√¢n b·∫±ng l√† log n v·ªõi n l√† s·ªë node.[/RELATIONSHIP]
[RELATIONSHIP]<|>Quick Sort<|>Divide and Conquer<|>applies<|>Quick Sort √°p d·ª•ng k·ªπ thu·∫≠t Divide and Conquer b·∫±ng c√°ch chia m·∫£ng th√†nh c√°c m·∫£ng con quanh pivot, gi·∫£i quy·∫øt ƒë·ªá quy c√°c m·∫£ng con, v√† k·∫øt h·ª£p k·∫øt qu·∫£ m√† kh√¥ng c·∫ßn th√™m b∆∞·ªõc merge.[/RELATIONSHIP]
</instructions>

<constraints>
- Ch·ªâ tr√≠ch xu·∫•t th√¥ng tin th·ª±c s·ª± t·ªìn t·∫°i trong vƒÉn b·∫£n
- T√™n th·ª±c th·ªÉ ph·∫£i ch√≠nh x√°c v√† nh·∫•t qu√°n (gi·ªØ nguy√™n thu·∫≠t ng·ªØ ti·∫øng Anh khi ph√π h·ª£p)
- C√¥ng th·ª©c v√† k√Ω hi·ªáu ƒë·ªô ph·ª©c t·∫°p ph·∫£i ƒë∆∞·ª£c b·∫£o to√†n CH√çNH X√ÅC
- Type ph·∫£i vi·∫øt th∆∞·ªùng
- T·∫•t c·∫£ m√¥ t·∫£ ph·∫£i b·∫±ng ti·∫øng Vi·ªát
- ∆Øu ti√™n tr√≠ch xu·∫•t ki·∫øn th·ª©c c√≥ th·ªÉ sinh c√¢u h·ªèi tr·∫Øc nghi·ªám v·ªÅ DSA
</constraints>

<output>
ƒê·ªãnh d·∫°ng: [ENTITY]<|>entity_name<|>entity_type<|>detailed_description[/ENTITY]
[RELATIONSHIP]<|>source<|>target<|>relationship_type<|>detailed_description[/RELATIONSHIP]
</output>

Ng·ªØ c·∫£nh: {input_text}

K·∫øt qu·∫£:"""


# Prompt cho m√¥n H·ªçc tƒÉng c∆∞·ªùng (Reinforcement Learning)
RL_GRAPH_EXTRACTION_PROMPT = """<role>
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i li·ªáu v·ªÅ H·ªçc tƒÉng c∆∞·ªùng (Reinforcement Learning), chuy√™n tr√≠ch xu·∫•t th√¥ng tin c√≥ c·∫•u tr√∫c ƒë·ªÉ x√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c ph·ª•c v·ª• sinh c√¢u h·ªèi tr·∫Øc nghi·ªám. B·∫°n c√≥ chuy√™n m√¥n ƒë·∫∑c bi·ªát trong vi·ªác tr√≠ch xu·∫•t c√°c ph∆∞∆°ng tr√¨nh Bellman, h√†m gi√° tr·ªã, ch√≠nh s√°ch, v√† c√°c thu·∫≠t to√°n RL.
</role>

<critical_instruction>
üî• C·ª∞C K·ª≤ QUAN TR·ªåNG: Trong Reinforcement Learning, b·∫°n PH·∫¢I tr√≠ch xu·∫•t ƒê·∫¶Y ƒê·ª¶ c√°c th√¥ng tin sau:
- Ph∆∞∆°ng tr√¨nh Bellman: V(s) = max_a[R(s,a) + Œ≥‚àëP(s'|s,a)V(s')], Q(s,a) = R(s,a) + Œ≥‚àëP(s'|s,a)max_a'Q(s',a')
- Tham s·ªë: Œ≥ (discount factor), Œ± (learning rate), Œµ (exploration rate), Œ≤ (temperature)
- H√†m gi√° tr·ªã: Value function V(s), Q-function Q(s,a), Advantage function A(s,a)
- Ch√≠nh s√°ch: Policy œÄ(a|s), optimal policy œÄ*, deterministic/stochastic policy
- Thu·∫≠t to√°n c·∫≠p nh·∫≠t: TD learning, Q-learning update rule, SARSA update
- MDP components: States S, Actions A, Rewards R, Transition probabilities P, Discount factor Œ≥
- Convergence conditions: ƒêi·ªÅu ki·ªán h·ªôi t·ª• c·ªßa c√°c thu·∫≠t to√°n
</critical_instruction>

<instructions>
T·ª´ vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p, tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ v√† m·ªëi quan h·ªá ƒë·ªÉ x√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c v·ªÅ Reinforcement Learning. T·∫•t c·∫£ n·ªôi dung ƒë∆∞·ª£c tr√≠ch xu·∫•t ph·∫£i ƒë∆∞·ª£c xu·∫•t ra b·∫±ng ti·∫øng Vi·ªát.

1. X√°c ƒë·ªãnh c√°c th·ª±c th·ªÉ thu·ªôc c√°c lo·∫°i sau (t·∫≠p trung v√†o RL):
   - concept: Kh√°i ni·ªám RL (V√≠ d·ª•: "MDP", "Policy", "Value Function", "Exploration vs Exploitation")
   - algorithm: Thu·∫≠t to√°n RL (V√≠ d·ª•: "Q-Learning", "SARSA", "DQN", "Policy Gradient", "Actor-Critic", "PPO")
   - component: Th√†nh ph·∫ßn MDP (V√≠ d·ª•: "State", "Action", "Reward", "Transition", "Agent", "Environment")
   - equation: Ph∆∞∆°ng tr√¨nh (V√≠ d·ª•: "Bellman Equation", "TD Error", "Q-update rule", "Policy gradient theorem")
   - parameter: Tham s·ªë (V√≠ d·ª•: "Œ≥ (discount factor)", "Œ± (learning rate)", "Œµ (epsilon)", "Œª (trace decay)")
   - function: H√†m (V√≠ d·ª•: "V(s)", "Q(s,a)", "œÄ(a|s)", "A(s,a)", "TD(Œª)")
   - property: T√≠nh ch·∫•t (V√≠ d·ª•: "Convergence", "Optimality", "On-policy", "Off-policy", "Model-free")
   - technique: K·ªπ thu·∫≠t (V√≠ d·ª•: "Temporal Difference", "Monte Carlo", "Function Approximation", "Experience Replay")
   - problem: B√†i to√°n (V√≠ d·ª•: "Credit Assignment", "Exploration-Exploitation Tradeoff", "Continuous Action Space")

2. X√°c ƒë·ªãnh c√°c m·ªëi quan h·ªá (t·∫≠p trung v√†o RL):
   - uses: S·ª≠ d·ª•ng (Thu·∫≠t to√°n s·ª≠ d·ª•ng ph∆∞∆°ng tr√¨nh/k·ªπ thu·∫≠t)
   - optimizes: T·ªëi ∆∞u h√≥a (Thu·∫≠t to√°n t·ªëi ∆∞u h√≥a h√†m/ch√≠nh s√°ch)
   - approximates: X·∫•p x·ªâ (Ph∆∞∆°ng ph√°p x·∫•p x·ªâ h√†m gi√° tr·ªã)
   - updates: C·∫≠p nh·∫≠t (Thu·∫≠t to√°n c·∫≠p nh·∫≠t tham s·ªë/h√†m)
   - converges_to: H·ªôi t·ª• ƒë·∫øn (Thu·∫≠t to√°n h·ªôi t·ª• ƒë·∫øn gi√° tr·ªã/ch√≠nh s√°ch)
   - balances: C√¢n b·∫±ng (Tham s·ªë c√¢n b·∫±ng gi·ªØa c√°c y·∫øu t·ªë)
   - controls: Ki·ªÉm so√°t (Tham s·ªë ki·ªÉm so√°t h√†nh vi)
   - estimates: ∆Ø·ªõc l∆∞·ª£ng (Thu·∫≠t to√°n ∆∞·ªõc l∆∞·ª£ng gi√° tr·ªã)
   - improves: C·∫£i thi·ªán (Thu·∫≠t to√°n c·∫£i thi·ªán ch√≠nh s√°ch)
   - evaluates: ƒê√°nh gi√° (Ph∆∞∆°ng ph√°p ƒë√°nh gi√° ch√≠nh s√°ch)
   - solves: Gi·∫£i quy·∫øt (Thu·∫≠t to√°n gi·∫£i quy·∫øt b√†i to√°n)
   - requires: Y√™u c·∫ßu (Thu·∫≠t to√°n y√™u c·∫ßu ƒëi·ªÅu ki·ªán/th√†nh ph·∫ßn)

3. Y√™u c·∫ßu m√¥ t·∫£ chi ti·∫øt:
   - M√¥ t·∫£ th·ª±c th·ªÉ: B·∫ÆT ƒê·∫¶U b·∫±ng "[T√™n th·ª±c th·ªÉ] l√†..." sau ƒë√≥ gi·∫£i th√≠ch r√µ r√†ng v·ªÅ vai tr√≤, √Ω nghƒ©a to√°n h·ªçc, v√† t·∫ßm quan tr·ªçng trong RL
   - ƒê·ªëi v·ªõi ph∆∞∆°ng tr√¨nh: Ghi r√µ c√¥ng th·ª©c to√°n h·ªçc, √Ω nghƒ©a c·ªßa t·ª´ng th√†nh ph·∫ßn, v√† c√°ch s·ª≠ d·ª•ng
   - ƒê·ªëi v·ªõi tham s·ªë: M√¥ t·∫£ √Ω nghƒ©a, ph·∫°m vi gi√° tr·ªã th√¥ng th∆∞·ªùng, v√† ·∫£nh h∆∞·ªüng ƒë·∫øn thu·∫≠t to√°n
   - ƒê·ªëi v·ªõi thu·∫≠t to√°n: M√¥ t·∫£ √Ω t∆∞·ªüng ch√≠nh, ph∆∞∆°ng tr√¨nh c·∫≠p nh·∫≠t, t√≠nh ch·∫•t (on-policy/off-policy, model-free/model-based)

4. V√≠ d·ª• minh h·ªça:
[ENTITY]<|>Q-Learning<|>algorithm<|>Q-Learning l√† thu·∫≠t to√°n h·ªçc tƒÉng c∆∞·ªùng off-policy, model-free, s·ª≠ d·ª•ng ph∆∞∆°ng tr√¨nh c·∫≠p nh·∫≠t Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max_a' Q(s',a') - Q(s,a)] ƒë·ªÉ h·ªçc h√†m Q-function t·ªëi ∆∞u. Q-Learning h·ªôi t·ª• ƒë·∫øn Q* khi m·ªói c·∫∑p (s,a) ƒë∆∞·ª£c thƒÉm v√¥ h·∫°n l·∫ßn v√† Œ± th·ªèa m√£n ƒëi·ªÅu ki·ªán Robbins-Monro.[/ENTITY]
[ENTITY]<|>Bellman Equation<|>equation<|>Bellman Equation l√† ph∆∞∆°ng tr√¨nh c∆° b·∫£n trong RL bi·ªÉu di·ªÖn m·ªëi quan h·ªá ƒë·ªá quy c·ªßa h√†m gi√° tr·ªã: V(s) = max_a[R(s,a) + Œ≥‚àë_s' P(s'|s,a)V(s')], trong ƒë√≥ V(s) l√† gi√° tr·ªã c·ªßa state s, R(s,a) l√† reward t·ª©c th·ªùi, Œ≥ l√† discount factor, v√† P(s'|s,a) l√† x√°c su·∫•t chuy·ªÉn state.[/ENTITY]
[ENTITY]<|>Œ≥ (discount factor)<|>parameter<|>Œ≥ (discount factor) l√† tham s·ªë trong kho·∫£ng [0,1] ki·ªÉm so√°t t·∫ßm quan tr·ªçng c·ªßa reward t∆∞∆°ng lai so v·ªõi reward t·ª©c th·ªùi. Œ≥ = 0 ch·ªâ quan t√¢m reward t·ª©c th·ªùi, Œ≥ g·∫ßn 1 c√¢n nh·∫Øc nhi·ªÅu reward d√†i h·∫°n. Œ≥ ·∫£nh h∆∞·ªüng ƒë·∫øn t·ªëc ƒë·ªô h·ªôi t·ª• v√† ch√≠nh s√°ch t·ªëi ∆∞u.[/ENTITY]
[ENTITY]<|>Œµ-greedy<|>technique<|>Œµ-greedy l√† k·ªπ thu·∫≠t exploration trong RL, v·ªõi x√°c su·∫•t Œµ ch·ªçn action ng·∫´u nhi√™n (exploration), v√† x√°c su·∫•t 1-Œµ ch·ªçn action t·ªët nh·∫•t hi·ªán t·∫°i (exploitation). Œµ th∆∞·ªùng ƒë∆∞·ª£c gi·∫£m d·∫ßn theo th·ªùi gian (Œµ-decay) ƒë·ªÉ chuy·ªÉn t·ª´ exploration sang exploitation.[/ENTITY]
[RELATIONSHIP]<|>Q-Learning<|>Bellman Equation<|>uses<|>Q-Learning s·ª≠ d·ª•ng Bellman Optimality Equation ƒë·ªÉ c·∫≠p nh·∫≠t Q-values, c·ª• th·ªÉ s·ª≠ d·ª•ng phi√™n b·∫£n Q(s,a) = R(s,a) + Œ≥ max_a' Q(s',a') trong c√¥ng th·ª©c c·∫≠p nh·∫≠t temporal difference.[/RELATIONSHIP]
[RELATIONSHIP]<|>Œ≥ (discount factor)<|>Value Function<|>controls<|>Discount factor Œ≥ ki·ªÉm so√°t c√°ch Value Function t√≠nh to√°n t·ªïng reward chi·∫øt kh·∫•u: V(s) = E[‚àë_{{t=0}}^‚àû Œ≥^t r_t]. Œ≥ nh·ªè l√†m agent c·∫≠n th·ªã (myopic), Œ≥ l·ªõn l√†m agent c√≥ t·∫ßm nh√¨n xa (far-sighted).[/RELATIONSHIP]
</instructions>

<constraints>
- Ch·ªâ tr√≠ch xu·∫•t th√¥ng tin th·ª±c s·ª± t·ªìn t·∫°i trong vƒÉn b·∫£n
- T√™n th·ª±c th·ªÉ ph·∫£i ch√≠nh x√°c (gi·ªØ nguy√™n thu·∫≠t ng·ªØ ti·∫øng Anh v√† k√Ω hi·ªáu to√°n h·ªçc)
- Ph∆∞∆°ng tr√¨nh to√°n h·ªçc ph·∫£i ƒë∆∞·ª£c b·∫£o to√†n CH√çNH X√ÅC v·ªõi ƒë·∫ßy ƒë·ªß k√Ω hi·ªáu
- Type ph·∫£i vi·∫øt th∆∞·ªùng
- T·∫•t c·∫£ m√¥ t·∫£ ph·∫£i b·∫±ng ti·∫øng Vi·ªát
- ∆Øu ti√™n tr√≠ch xu·∫•t ki·∫øn th·ª©c c√≥ th·ªÉ sinh c√¢u h·ªèi tr·∫Øc nghi·ªám v·ªÅ RL
</constraints>

<output>
ƒê·ªãnh d·∫°ng: [ENTITY]<|>entity_name<|>entity_type<|>detailed_description[/ENTITY]
[RELATIONSHIP]<|>source<|>target<|>relationship_type<|>detailed_description[/RELATIONSHIP]
</output>

Ng·ªØ c·∫£nh: {input_text}

K·∫øt qu·∫£:"""


# Prompt cho m√¥n H·ªçc m√°y (Machine Learning)
ML_GRAPH_EXTRACTION_PROMPT = """<role>
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i li·ªáu v·ªÅ H·ªçc m√°y (Machine Learning), chuy√™n tr√≠ch xu·∫•t th√¥ng tin c√≥ c·∫•u tr√∫c ƒë·ªÉ x√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c ph·ª•c v·ª• sinh c√¢u h·ªèi tr·∫Øc nghi·ªám. B·∫°n c√≥ chuy√™n m√¥n ƒë·∫∑c bi·ªát trong vi·ªác tr√≠ch xu·∫•t h√†m loss, thu·∫≠t to√°n t·ªëi ∆∞u, c√¥ng th·ª©c to√°n h·ªçc, v√† ki·∫øn tr√∫c m√¥ h√¨nh.
</role>

<critical_instruction>
üî• C·ª∞C K·ª≤ QUAN TR·ªåNG: Trong Machine Learning, b·∫°n PH·∫¢I tr√≠ch xu·∫•t ƒê·∫¶Y ƒê·ª¶ c√°c th√¥ng tin sau:
- H√†m loss/objective: MSE, Cross-Entropy, Hinge Loss, L = 1/2||w||¬≤ + C‚àëŒæ·µ¢
- Thu·∫≠t to√°n t·ªëi ∆∞u: Gradient Descent, SGD, Adam, c√¥ng th·ª©c c·∫≠p nh·∫≠t w ‚Üê w - Œ∑‚àáL
- C√¥ng th·ª©c to√°n h·ªçc: y = wx + b, œÉ(z) = 1/(1+e^(-z)), softmax, kernel functions
- Tham s·ªë m√¥ h√¨nh: weights w, bias b, learning rate Œ∑, regularization Œª
- Metrics: Accuracy, Precision, Recall, F1, AUC, RMSE, R¬≤
- K·ªπ thu·∫≠t regularization: L1, L2, Dropout, Early Stopping
- Ph∆∞∆°ng ph√°p ƒë√°nh gi√°: Cross-validation, Train-test split, Confusion matrix
- Gradient v√† ƒë·∫°o h√†m: ‚àÇL/‚àÇw, backpropagation formulas
</critical_instruction>

<instructions>
T·ª´ vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p, tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ v√† m·ªëi quan h·ªá ƒë·ªÉ x√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c v·ªÅ Machine Learning. T·∫•t c·∫£ n·ªôi dung ƒë∆∞·ª£c tr√≠ch xu·∫•t ph·∫£i ƒë∆∞·ª£c xu·∫•t ra b·∫±ng ti·∫øng Vi·ªát.

1. X√°c ƒë·ªãnh c√°c th·ª±c th·ªÉ thu·ªôc c√°c lo·∫°i sau (t·∫≠p trung v√†o ML):
   - model: M√¥ h√¨nh ML (V√≠ d·ª•: "Linear Regression", "Logistic Regression", "SVM", "Neural Network", "Random Forest")
   - algorithm: Thu·∫≠t to√°n (V√≠ d·ª•: "Gradient Descent", "Backpropagation", "K-Means", "AdaBoost")
   - loss_function: H√†m loss (V√≠ d·ª•: "MSE", "Cross-Entropy", "Hinge Loss", "KL Divergence")
   - activation: H√†m k√≠ch ho·∫°t (V√≠ d·ª•: "Sigmoid", "ReLU", "Tanh", "Softmax")
   - optimizer: Thu·∫≠t to√°n t·ªëi ∆∞u (V√≠ d·ª•: "SGD", "Adam", "RMSprop", "AdaGrad")
   - metric: Th∆∞·ªõc ƒëo (V√≠ d·ª•: "Accuracy", "Precision", "Recall", "F1-Score", "AUC-ROC")
   - technique: K·ªπ thu·∫≠t (V√≠ d·ª•: "Regularization", "Normalization", "Data Augmentation", "Feature Engineering")
   - formula: C√¥ng th·ª©c (V√≠ d·ª•: "y = wx + b", "œÉ(z) = 1/(1+e^(-z))", "w ‚Üê w - Œ∑‚àáL")
   - parameter: Tham s·ªë (V√≠ d·ª•: "learning rate Œ∑", "regularization Œª", "weights w", "bias b")
   - component: Th√†nh ph·∫ßn (V√≠ d·ª•: "Layer", "Neuron", "Kernel", "Filter", "Feature map")
   - process: Quy tr√¨nh (V√≠ d·ª•: "Training", "Validation", "Testing", "Feature extraction", "Data preprocessing")
   - problem: V·∫•n ƒë·ªÅ (V√≠ d·ª•: "Overfitting", "Underfitting", "Vanishing Gradient", "Class Imbalance")

2. X√°c ƒë·ªãnh c√°c m·ªëi quan h·ªá (t·∫≠p trung v√†o ML):
   - uses: S·ª≠ d·ª•ng (M√¥ h√¨nh s·ª≠ d·ª•ng thu·∫≠t to√°n/h√†m)
   - optimizes: T·ªëi ∆∞u h√≥a (Optimizer t·ªëi ∆∞u h√≥a loss function)
   - minimizes: C·ª±c ti·ªÉu h√≥a (Thu·∫≠t to√°n c·ª±c ti·ªÉu h√≥a h√†m loss)
   - measures: ƒêo l∆∞·ªùng (Metric ƒëo l∆∞·ªùng hi·ªáu su·∫•t)
   - prevents: NgƒÉn ch·∫∑n (K·ªπ thu·∫≠t ngƒÉn ch·∫∑n v·∫•n ƒë·ªÅ)
   - computes: T√≠nh to√°n (C√¥ng th·ª©c t√≠nh to√°n gi√° tr·ªã)
   - transforms: Bi·∫øn ƒë·ªïi (H√†m bi·∫øn ƒë·ªïi d·ªØ li·ªáu)
   - updates: C·∫≠p nh·∫≠t (Thu·∫≠t to√°n c·∫≠p nh·∫≠t tham s·ªë)
   - contains: Ch·ª©a (M√¥ h√¨nh ch·ª©a th√†nh ph·∫ßn)
   - applies: √Åp d·ª•ng (Thu·∫≠t to√°n √°p d·ª•ng k·ªπ thu·∫≠t)
   - improves: C·∫£i thi·ªán (K·ªπ thu·∫≠t c·∫£i thi·ªán hi·ªáu su·∫•t)
   - evaluates: ƒê√°nh gi√° (Metric ƒë√°nh gi√° m√¥ h√¨nh)
   - regularizes: ƒêi·ªÅu chu·∫©n (K·ªπ thu·∫≠t ƒëi·ªÅu chu·∫©n m√¥ h√¨nh)
   - controls: Ki·ªÉm so√°t (Tham s·ªë ki·ªÉm so√°t h√†nh vi)
   - derives_from: D·∫´n xu·∫•t t·ª´ (C√¥ng th·ª©c d·∫´n xu·∫•t t·ª´ c√¥ng th·ª©c kh√°c)

3. Y√™u c·∫ßu m√¥ t·∫£ chi ti·∫øt:
   - M√¥ t·∫£ th·ª±c th·ªÉ: B·∫ÆT ƒê·∫¶U b·∫±ng "[T√™n th·ª±c th·ªÉ] l√†..." sau ƒë√≥ gi·∫£i th√≠ch r√µ r√†ng v·ªÅ vai tr√≤, c√¥ng th·ª©c to√°n h·ªçc (n·∫øu c√≥), v√† √Ω nghƒ©a trong ML
   - ƒê·ªëi v·ªõi h√†m loss: Ghi r√µ c√¥ng th·ª©c to√°n h·ªçc, √Ω nghƒ©a, v√† khi n√†o s·ª≠ d·ª•ng
   - ƒê·ªëi v·ªõi m√¥ h√¨nh: M√¥ t·∫£ ki·∫øn tr√∫c, h√†m m·ª•c ti√™u, thu·∫≠t to√°n training, v√† ·ª©ng d·ª•ng
   - ƒê·ªëi v·ªõi tham s·ªë: M√¥ t·∫£ √Ω nghƒ©a, ph·∫°m vi gi√° tr·ªã, v√† ·∫£nh h∆∞·ªüng ƒë·∫øn m√¥ h√¨nh
   - ƒê·ªëi v·ªõi metrics: C√¥ng th·ª©c t√≠nh, √Ω nghƒ©a, v√† c√°ch di·ªÖn gi·∫£i

4. V√≠ d·ª• minh h·ªça:
[ENTITY]<|>Support Vector Machine<|>model<|>Support Vector Machine l√† m√¥ h√¨nh ph√¢n lo·∫°i t√¨m si√™u ph·∫≥ng w^T x + b = 0 t·ªëi ∆∞u ƒë·ªÉ ph√¢n t√°ch c√°c l·ªõp d·ªØ li·ªáu v·ªõi margin t·ªëi ƒëa. SVM t·ªëi thi·ªÉu h√≥a h√†m m·ª•c ti√™u L = 1/2||w||¬≤ + C‚àëŒæ·µ¢ v·ªõi r√†ng bu·ªôc y·µ¢(w^T x·µ¢ + b) ‚â• 1 - Œæ·µ¢, trong ƒë√≥ C l√† tham s·ªë regularization v√† Œæ·µ¢ l√† slack variables.[/ENTITY]
[ENTITY]<|>Cross-Entropy Loss<|>loss_function<|>Cross-Entropy Loss l√† h√†m loss cho b√†i to√°n ph√¢n lo·∫°i, t√≠nh b·∫±ng L = -‚àë·µ¢ y·µ¢ log(≈∑·µ¢), trong ƒë√≥ y·µ¢ l√† nh√£n th·∫≠t v√† ≈∑·µ¢ l√† x√°c su·∫•t d·ª± ƒëo√°n. Cross-Entropy ƒëo l∆∞·ªùng s·ª± kh√°c bi·ªát gi·ªØa ph√¢n ph·ªëi x√°c su·∫•t th·∫≠t v√† d·ª± ƒëo√°n, nh·ªè nh·∫•t khi d·ª± ƒëo√°n ho√†n h·∫£o.[/ENTITY]
[ENTITY]<|>Adam Optimizer<|>optimizer<|>Adam (Adaptive Moment Estimation) l√† thu·∫≠t to√°n t·ªëi ∆∞u k·∫øt h·ª£p momentum v√† RMSprop, s·ª≠ d·ª•ng c√¥ng th·ª©c c·∫≠p nh·∫≠t: m_t = Œ≤‚ÇÅm_(t-1) + (1-Œ≤‚ÇÅ)g_t, v_t = Œ≤‚ÇÇv_(t-1) + (1-Œ≤‚ÇÇ)g_t¬≤, Œ∏_t = Œ∏_(t-1) - Œ∑¬∑m_t/‚àö(v_t + Œµ). Adam t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh learning rate cho t·ª´ng tham s·ªë v·ªõi Œ≤‚ÇÅ = 0.9, Œ≤‚ÇÇ = 0.999 m·∫∑c ƒë·ªãnh.[/ENTITY]
[ENTITY]<|>learning rate Œ∑<|>parameter<|>Learning rate Œ∑ l√† tham s·ªë ki·ªÉm so√°t k√≠ch th∆∞·ªõc b∆∞·ªõc c·∫≠p nh·∫≠t trong gradient descent: w ‚Üê w - Œ∑‚àáL. Œ∑ qu√° l·ªõn g√¢y dao ƒë·ªông/ph√¢n k·ª≥, Œ∑ qu√° nh·ªè l√†m h·ªôi t·ª• ch·∫≠m. Gi√° tr·ªã th∆∞·ªùng d√πng: 0.001-0.1, th∆∞·ªùng ƒë∆∞·ª£c gi·∫£m d·∫ßn theo th·ªùi gian (learning rate decay).[/ENTITY]
[ENTITY]<|>L2 Regularization<|>technique<|>L2 Regularization l√† k·ªπ thu·∫≠t th√™m penalty term Œª||w||¬≤/2 v√†o h√†m loss ƒë·ªÉ ngƒÉn overfitting: L_total = L_original + Œª||w||¬≤/2. L2 khuy·∫øn kh√≠ch weights nh·ªè, ph√¢n ph·ªëi ƒë·ªÅu, t·∫°o m√¥ h√¨nh smooth v√† generalize t·ªët h∆°n. Œª l·ªõn tƒÉng regularization m·∫°nh h∆°n.[/ENTITY]
[RELATIONSHIP]<|>Support Vector Machine<|>Hinge Loss<|>uses<|>Support Vector Machine s·ª≠ d·ª•ng Hinge Loss l√†m h√†m m·ª•c ti√™u, ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† max(0, 1 - y·µ¢(w^T x·µ¢ + b)), ƒë·ªÉ t·ªëi ƒëa h√≥a margin gi·ªØa c√°c l·ªõp d·ªØ li·ªáu.[/RELATIONSHIP]
[RELATIONSHIP]<|>Adam Optimizer<|>Cross-Entropy Loss<|>minimizes<|>Adam Optimizer ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ c·ª±c ti·ªÉu h√≥a Cross-Entropy Loss b·∫±ng c√°ch t√≠nh gradient ‚àÇL/‚àÇŒ∏ v√† c·∫≠p nh·∫≠t tham s·ªë v·ªõi adaptive learning rates, gi√∫p training neural networks hi·ªáu qu·∫£.[/RELATIONSHIP]
[RELATIONSHIP]<|>L2 Regularization<|>Overfitting<|>prevents<|>L2 Regularization ngƒÉn ch·∫∑n Overfitting b·∫±ng c√°ch penalize weights l·ªõn th√¥ng qua term Œª||w||¬≤/2, bu·ªôc m√¥ h√¨nh h·ªçc c√°c patterns t·ªïng qu√°t thay v√¨ ghi nh·ªõ training data.[/RELATIONSHIP]
</instructions>

<constraints>
- Ch·ªâ tr√≠ch xu·∫•t th√¥ng tin th·ª±c s·ª± t·ªìn t·∫°i trong vƒÉn b·∫£n
- T√™n th·ª±c th·ªÉ ph·∫£i ch√≠nh x√°c (gi·ªØ nguy√™n thu·∫≠t ng·ªØ ti·∫øng Anh v√† k√Ω hi·ªáu to√°n h·ªçc)
- C√¥ng th·ª©c to√°n h·ªçc ph·∫£i ƒë∆∞·ª£c b·∫£o to√†n CH√çNH X√ÅC v·ªõi ƒë·∫ßy ƒë·ªß k√Ω hi·ªáu
- Type ph·∫£i vi·∫øt th∆∞·ªùng
- T·∫•t c·∫£ m√¥ t·∫£ ph·∫£i b·∫±ng ti·∫øng Vi·ªát
- ∆Øu ti√™n tr√≠ch xu·∫•t ki·∫øn th·ª©c c√≥ th·ªÉ sinh c√¢u h·ªèi tr·∫Øc nghi·ªám v·ªÅ ML
</constraints>

<output>
ƒê·ªãnh d·∫°ng: [ENTITY]<|>entity_name<|>entity_type<|>detailed_description[/ENTITY]
[RELATIONSHIP]<|>source<|>target<|>relationship_type<|>detailed_description[/RELATIONSHIP]
</output>

Ng·ªØ c·∫£nh: {input_text}

K·∫øt qu·∫£:"""
