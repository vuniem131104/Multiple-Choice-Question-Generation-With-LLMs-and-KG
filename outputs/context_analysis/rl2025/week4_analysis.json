[
    {
        "topic_description": "Khái niệm tối ưu hóa hàm giá trị của một MDP mà không cần mô hình rõ ràng. Học sinh cần hiểu về các kỹ thuật lấy mẫu từ kinh nghiệm và ứng dụng cho các bài toán như quản lý danh mục đầu tư và cờ vây. Nội dung này thuộc nền tảng tuần 4 qua khái niệm Kiểm soát không mô hình.",
        "output_with_context": {
            "question": "Trong phương pháp Monte Carlo, giá trị của trạng thái V^π(s) được ước lượng bằng cách nào?",
            "answer": "Bằng cách lấy trung bình các return thực tế nhận được từ nhiều episode.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\nDynamic Programming là nền tảng quan trọng trong Reinforcement Learning. Mặc dù có hạn chế về yêu cầu biết mô hình và curse of dimensionality, DP cung cấp:\n\n- **Framework lý thuyết**: Hiểu rõ về optimal value functions và policies\n- **Nền tảng thuật toán**: Cơ sở cho các phương pháp model-free\n- **Công cụ thực tế**: Giải quyết được nhiều bài toán thực tế\n\nCác khái niệm từ DP (đặc biệt là GPI và Bellman equations) sẽ xuất hiện xuyên suốt trong các phương pháp học tăng cường tiếp theo, từ Temporal-Difference Learning đến Deep Q-Networks.\n\n---\n\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Learning\n\n#### 1.1. Định nghĩa\nModel-Free Learning là các phương pháp học tăng cường không yêu cầu biết trước mô hình môi trường (hàm chuyển trạng thái P và hàm phần thưởng R). Agent học trực tiếp từ kinh nghiệm tương tác với môi trường.\n\n#### 1.2. So sánh Model-Based vs Model-Free\n\n| Đặc điểm | Model-Based | Model-Free |\n|----------|-------------|------------|\n| Yêu cầu mô hình | Cần biết P và R | Không cần |\n| Học từ | Phương trình Bellman | Kinh nghiệm thực tế |\n| Ưu điểm | Hiệu quả dữ liệu | Linh hoạt, thực tế |\n| Nhược điểm | Khó có mô hình chính xác | Cần nhiều dữ liệu |\n| Ví dụ | DP, Model-Based RL | MC, TD, Q-Learning |\n\n#### 1.3. Bài toán Prediction\n**Mục tiêu**: Đánh giá một chính sách π cho trước\n- Input: Chính sách π\n- Output: Hàm giá trị V^π(s)\n- Không cần biết mô hình môi trường\n\n### 2. Monte Carlo Methods - Phương Pháp Monte Carlo\n\n#### 2.1. Ý tưởng cơ bản\nMonte Carlo (MC) ước lượng giá trị của trạng thái bằng cách lấy trung bình các return thực tế nhận được từ nhiều episodes.\n\n**Nguyên lý**:\n```\nV^π(s) = E_π[G_t | S_t = s]\n       ≈ average of returns từ trạng thái s\n```\n\n#### 2.2. Episode và Return\n\n**Episode**: Một chuỗi hoàn chỉnh từ trạng thái ban đầu đến kết thúc\n```\nS_0, A_0, R_1, S_1, A_1, R_2, ..., S_T\n```\n\n**Return**: Tổng phần thưởng chiết khấu\n```\nG_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... + γ^{T-t-1}R_T\n```\n\n#### 2.3. First-Visit Monte Carlo\n\n**Thuật toán**:\n```\nKhởi tạo:\n    V(s) = 0, ∀s\n    Returns(s) = danh sách rỗng, ∀s\n\nVới mỗi episode:\n    Tạo một episode tuân theo π: S_0, A_0, R_1, ..., S_T\n    G = 0\n    \n    Với mỗi bước t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        \n\n**Các khái niệm quan trọng:**\n- Optimal value functions là các hàm giá trị biểu diễn giá trị tối đa có thể đạt được từ một trạng thái hoặc cặp trạng thái-hành động, tuân theo một chính sách tối ưu. Chúng là mục tiêu chính trong nhiều bài toán Reinforcement Learning.\n\n**Mối quan hệ:**\n- Dynamic Programming cung cấp một framework lý thuyết để hiểu rõ về các hàm giá trị tối ưu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Markov Decision Process (MDP) là một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự, nơi một agent tương tác với môi trường để tối đa hóa tổng phần thưởng chiết khấu. Trong MDP, kết quả của các quyết định có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường (Reinforcement Learning) và được định nghĩa bởi các thành phần: tập trạng thái S, tập hành động A, hàm phần thưởng R, xác suất chuyển trạng thái P, và discount factor γ.\n\n**Mối quan hệ:**\n- Mô hình không chính xác là một thách thức khi các giả định về P và R trong Markov Decision Processes không được biết trước hoặc không chính xác.\n- Markov Decision Processes có thành phần là Reward (Phần thưởng), là tín hiệu phản hồi số từ môi trường.\n- Markov Decision Processes có thành phần là γ (discount factor), kiểm soát tầm quan trọng của phần thưởng tương lai.\n- Môi trường không hoàn toàn quan sát được là một giới hạn của mô hình Markov Decision Processes chuẩn, dẫn đến việc sử dụng POMDP.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- Markov Decision Process (MDP) là một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự, nơi một agent tương tác với môi trường để tối đa hóa tổng phần thưởng chiết khấu. Trong MDP, kết quả của các quyết định có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường (Reinforcement Learning) và được định nghĩa bởi các thành phần: tập trạng thái S, tập hành động A, hàm phần thưởng R, xác suất chuyển trạng thái P, và discount factor γ.\n- Finite MDP là một loại Quá trình Quyết định Markov trong đó tập hợp các trạng thái (S) và tập hợp các hành động (A) đều hữu hạn. Điều này làm cho chúng dễ tính toán và phân tích hơn, cho phép sử dụng các phương pháp giải chính xác như Value Iteration và Policy Iteration.\n\n**Mối quan hệ:**\n- Mô hình không chính xác là một thách thức khi các giả định về P và R trong Markov Decision Processes không được biết trước hoặc không chính xác.\n- Markov Decision Processes có thành phần là γ (discount factor), kiểm soát tầm quan trọng của phần thưởng tương lai.\n- Môi trường không hoàn toàn quan sát được là một giới hạn của mô hình Markov Decision Processes chuẩn, dẫn đến việc sử dụng POMDP.\n- Finite MDP có tính chất là tập hành động A là hữu hạn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n#### 7.1. Game Playing\n\n**TD-Gammon** (1992):\n- Backgammon AI\n- Sử dụng TD(λ) với neural network\n- Self-play\n- Đạt world-champion level\n\n**AlphaGo Zero**:\n- Sử dụng TD-style updates\n- Self-play + MCTS\n- Không cần human knowledge\n\n#### 7.2. Robot Navigation\n\n**Task**: Robot tìm đường trong môi trường chưa biết\n- State: Vị trí robot\n- Action: Di chuyển\n- Reward: -1 mỗi bước, +100 khi đến đích\n\n**Ưu điểm TD**:\n- Học online trong quá trình điều hướng\n- Không cần đợi đến đích mới cập nhật\n- Adapt với môi trường thay đổi\n\n#### 7.3. Resource Management\n\n**Elevator Control**:\n- State: Vị trí thang máy, yêu cầu chờ\n- Action: Lên/xuống/đứng yên\n- Reward: -1 × tổng thời gian chờ\n\n**TD Learning**:\n- Học value function cho mỗi trạng thái\n- Online learning từ hoạt động hàng ngày\n- Cải thiện liên tục\n\n### 8. Phân tích lý thuyết\n\n#### 8.1. Tốc độ hội tụ\n\n**Monte Carlo**:\n```\nV_k(s) → V^π(s) với rate O(1/√k)\nk: số episodes\n```\n\n**TD(0)**:\n```\nV_k(s) → V^π(s) nhanh hơn trong thực tế\nKhông có bound lý thuyết chặt chẽ\n```\n\n**Thực nghiệm**: TD thường nhanh hơn MC 2-10 lần\n\n#### 8.2. Điều kiện hội tụ\n\n**Robbins-Monro conditions** cho learning rate α_t:\n```\nΣ_{t=1}^∞ α_t = ∞     (đảm bảo hội tụ)\nΣ_{t=1}^∞ α_t² < ∞    (đảm bảo variance hội tụ về 0)\n```\n\n**Ví dụ**:\n- α_t = 1/t: Thỏa mãn\n- α_t = 0.01: Không thỏa mãn điều kiện 1, nhưng practical\n\n#### 8.3. Bias-Variance Tradeoff\n\n**Monte Carlo**:\n- Bias = 0 (ước lượng không chệch)\n- Variance cao (phụ thuộc vào toàn bộ trajectory)\n\n**TD(0)**:\n- Bias > 0 (phụ thuộc vào V hiện tại)\n- Variance thấp (chỉ phụ thuộc 1 bước)\n\n**n-Step TD**: Cân bằng\n```\nBias giảm khi n tăng\nVariance tăng khi n tăng\n```\n\n### 9. So sánh tổng hợp\n\n#### 9.1. Bảng so sánh đầy đủ\n\n| Tiêu chí | MC | TD(0) | TD(λ) | DP |\n|----------|----|----|-------|-----|\n| Model-free | ✓ | ✓ | ✓ | ✗ |\n| Bootstrap | ✗ | ✓ | ✓ | ✓ |\n| Online | ✗ | ✓ | ✓ | ✓ |\n| Episodic only | ✓ | ✗ | ✗ | ✗ |\n| Bias | Low | High | Medium | Low |\n| Variance | High | Low | Medium | Low |\n| Convergence | Slow | Fast | Medium | Fastest |\n\n#### 9.2. Khi nào dùng phương pháp nào?\n\n**Monte Carlo**:\n- Môi trường không Markov\n- Cần ước lượng unbiased\n- Episodic tasks ngắn\n\n**TD(0)**:\n- Môi trường Markov\n- Continuing tasks\n\n**Các khái niệm quan trọng:**\n- Monte Carlo (MC) là một nhóm các phương pháp model-free, không bootstrap trong Reinforcement Learning, học trực tiếp từ kinh nghiệm mà không yêu cầu biết mô hình môi trường. MC ước lượng giá trị của trạng thái hoặc cặp trạng thái-hành động, hoặc học chính sách, bằng cách lấy trung bình các return (tổng phần thưởng chiết khấu thực tế) nhận được từ nhiều episode hoàn chỉnh (từ trạng thái ban đầu đến trạng thái kết thúc). Việc học chỉ diễn ra sau khi một episode kết thúc. MC có bias thấp nhưng variance cao, chỉ phù hợp với các tác vụ episodic và có thể hội tụ đến local optimum hoặc global optimum với xấp xỉ hàm tuyến tính.\n\n**Mối quan hệ:**\n- Monte Carlo là thuật toán model-free."
        },
        "output_without_context": {
            "question": "Tối ưu hóa hàm giá trị của một MDP mà không cần mô hình rõ ràng thuộc lĩnh vực nào?",
            "answer": "Kiểm soát không mô hình"
        }
    },
    {
        "topic_description": "Học sinh cần nhận diện và mô tả các khái niệm Học theo chính sách, bao gồm việc học hỏi từ chính các trải nghiệm của chính sách đang học. Nội dung liên quan đến các thuật ngữ và phương pháp được triển khai ở tuần 4 và ứng dụng của nó từ tuần 2.",
        "output_with_context": {
            "question": "Chính sách nào trong Học Tăng cường học từ chính trải nghiệm của chính nó và cải thiện dựa trên chính sách đó?",
            "answer": "On-Policy Learning",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n**Định nghĩa**: Một dãy chính sách {π_t} thỏa mãn GLIE nếu:\n1. Mọi cặp (s,a) được visit vô hạn lần\n2. Chính sách hội tụ đến greedy policy\n\n**Ví dụ GLIE**:\n```\nε_t = 1/t\n```\n\n**Định lý**: GLIE Monte Carlo Control hội tụ đến π*\n\n#### 2.6. Ví dụ: Blackjack với MC Control\n\n**Setup**:\n- State: (player_sum, dealer_card, usable_ace)\n- Action: Hit hoặc Stick\n- Reward: +1 (win), -1 (lose), 0 (draw)\n\n**Kết quả**:\n```\nChính sách học được:\n- Stick khi player_sum ≥ 20\n- Hit khi player_sum < 12\n- Phức tạp hơn ở vùng 12-19 (phụ thuộc dealer card)\n```\n\n### 3. On-Policy vs Off-Policy Learning\n\n#### 3.1. Định nghĩa\n\n**On-Policy**:\n- Học về chính sách π từ experience generated bởi π\n- Evaluate và improve cùng một chính sách\n- Ví dụ: SARSA, Monte Carlo Control\n\n**Off-Policy**:\n- Học về chính sách π (target) từ experience của μ (behavior)\n- π ≠ μ\n- Ví dụ: Q-Learning, Importance Sampling\n\n#### 3.2. So sánh\n\n| Đặc điểm | On-Policy | Off-Policy |\n|----------|-----------|------------|\n| Chính sách | Một chính sách | Hai chính sách |\n| Sample efficiency | Thấp hơn | Cao hơn |\n| Variance | Thấp | Cao |\n| Converge | Ổn định | Có thể diverge |\n| Use old data | Không | Có thể |\n| Learn optimal | Không (nếu ε-greedy) | Có |\n\n### 4. SARSA - On-Policy TD Control\n\n#### 4.1. Ý tưởng\nÁp dụng TD(0) cho Q(s,a) thay vì V(s)\n\n**TD Update cho Q**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n```\n\n**Tên gọi**: SARSA = (S, A, R, S', A')\n\n#### 4.2. Thuật toán SARSA\n\n```\nKhởi tạo Q(s,a) arbitrarily, ∀s ∈ S, a ∈ A\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    Chọn A từ S theo ε-greedy(Q)\n    \n    Lặp với mỗi bước:\n        Thực hiện A, quan sát R, S'\n        Chọn A' từ S' theo ε-greedy(Q)\n        \n        Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n        \n        S ← S'; A ← A'\n    cho đến S là terminal\n```\n\n#### 4.3. Tính chất của SARSA\n\n**Hội tụ**:\n- GLIE schedule + Robbins-Monro conditions → Q → Q*\n- Trong thực tế, dùng ε nhỏ cố định hoặc decay\n\n**On-Policy**:\n- Học về chính sách ε-greedy đang dùng\n- Safe: Tính đến exploration trong học\n\n#### 4.4. Ví dụ: Windy Gridworld\n\n**Mô tả**:\n- Grid 7×10 với \"wind\" ở một số cột\n- Wind đẩy agent lên 1-2 ô\n- Start: (3,0), Goal: (3,7)\n- Actions: 4 hướng\n\n**Kết quả SARSA**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- On-Policy Learning là một phương pháp học tăng cường trong đó agent học về chính sách π từ kinh nghiệm được tạo ra bởi chính sách π đó, nghĩa là chính sách được sử dụng để tạo dữ liệu (behavior policy) cũng chính là chính sách đang được cải thiện (target policy), với các ví dụ điển hình như SARSA và Monte Carlo Control. Ngược lại, Off-Policy Learning là một phương pháp học tăng cường mà agent học về một chính sách π (target policy) từ kinh nghiệm được tạo ra bởi một chính sách khác μ (behavior policy), trong đó π ≠ μ, cho phép agent học về chính sách tối ưu trong khi vẫn thực hiện các hành động khám phá theo một chính sách khác, với các ví dụ điển hình là Q-Learning và Importance Sampling.\n\n**Mối quan hệ:**\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n- Off-Policy Learning học về chính sách π (target policy).\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Policy Iteration luôn hội tụ đến chính sách tối ưu π*.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Policy Iteration luôn hội tụ đến chính sách tối ưu π*.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nMột quá trình được gọi là có tính chất Markov nếu:\n```\nP[Sₜ₊₁ | Sₜ] = P[Sₜ₊₁ | S₁, S₂, ..., Sₜ]\n```\n\n#### 3.2. Ý nghĩa\n- Trạng thái hiện tại chứa đầy đủ thông tin để dự đoán tương lai\n- Không cần phải nhớ toàn bộ lịch sử\n- Đơn giản hóa việc tính toán và phân tích\n\n### 4. Chính sách (Policy)\n\n#### 4.1. Định nghĩa\nChính sách π là một hàm ánh xạ từ trạng thái đến hành động:\n- **Chính sách xác định (Deterministic)**: π(s) = a\n- **Chính sách ngẫu nhiên (Stochastic)**: π(a|s) = P[Aₜ = a | Sₜ = s]\n\n#### 4.2. Phân loại\n- **Chính sách tĩnh (Stationary)**: Không thay đổi theo thời gian\n- **Chính sách động (Non-stationary)**: Thay đổi theo thời gian\n\n### 5. Hàm giá trị (Value Functions)\n\n#### 5.1. Hàm giá trị trạng thái (State-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s và tuân theo chính sách π\n\n```\nVᵖ(s) = Eᵖ[Gₜ | Sₜ = s]\n      = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s]\n```\n\n#### 5.2. Hàm giá trị hành động (Action-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s, thực hiện hành động a, và sau đó tuân theo chính sách π\n\n```\nQᵖ(s,a) = Eᵖ[Gₜ | Sₜ = s, Aₜ = a]\n        = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s, Aₜ = a]\n```\n\n#### 5.3. Mối quan hệ giữa V và Q\n```\nVᵖ(s) = Σₐ π(a|s)Qᵖ(s,a)\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')\n```\n\n### 6. Phương trình Bellman\n\n#### 6.1. Phương trình Bellman cho Vᵖ\n```\nVᵖ(s) = Σₐ π(a|s)[R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')]\n```\n\n**Giải thích**:\n- Giá trị của một trạng thái bằng phần thưởng tức thời cộng với giá trị chiết khấu của trạng thái tiếp theo\n- Đây là một phương trình đệ quy\n\n#### 6.2. Phương trình Bellman cho Qᵖ\n```\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) Σₐ' π(a'|s')Qᵖ(s',a')\n```\n\n#### 6.3. Phương trình Bellman tối ưu\n```\n\n**Các khái niệm quan trọng:**\n- Deterministic Policy (Chính sách xác định) là một loại chính sách trong Reinforcement Learning (RL) mà tại mỗi trạng thái 's', chính sách π(s) (hoặc μ(s;θ)) sẽ luôn chọn một hành động 'a' cụ thể và duy nhất. Công thức biểu diễn là π(s) = a, trong đó θ là các tham số của chính sách. Các chính sách này thường được suy ra từ các phương pháp dựa trên giá trị (Value-Based Methods), ví dụ như π(s) = argmax_a Q(s,a), và thường hiệu quả hơn về mặt mẫu (sample efficient) trong các không gian hành động liên tục.\n- Non-stationary Policy (Chính sách động) là một chính sách thay đổi theo thời gian, nghĩa là hành vi của agent có thể điều chỉnh theo các giai đoạn khác nhau của quá trình học.\n- Policy (Chính sách) π là một hàm ánh xạ từ trạng thái đến hành động, định nghĩa hành vi của agent. Chính sách có thể là xác định (deterministic) hoặc ngẫu nhiên (stochastic).\n\n**Mối quan hệ:**\n- Policy có thể là Deterministic Policy, nơi một hành động cụ thể được chọn cho mỗi trạng thái.\n- Policy có thể là Non-stationary Policy, thay đổi theo thời gian."
        },
        "output_without_context": {
            "question": "Học theo chính sách trong Học Tăng cường chủ yếu liên quan đến việc học hỏi từ điều gì?",
            "answer": "Các trải nghiệm của chính các chính sách đang học."
        }
    },
    {
        "topic_description": "Khái niệm về thăm dò ε-Greedy cho phép học sinh phân tích cách thức chọn lựa hành động tham lam và việc thăm dò hành động ngẫu nhiên. Học sinh sẽ áp dụng công thức để tính toán xác suất của các hành động trong không gian hành động. Nội dung kết nối từ tuần 4 và phương pháp Lập trình động ở tuần 2.",
        "output_with_context": {
            "question": "Trong phương pháp ε-greedy, xác suất chọn hành động ngẫu nhiên là gì?",
            "answer": "Xác suất chọn hành động ngẫu nhiên là ε.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n**Định nghĩa**: Một dãy chính sách {π_t} thỏa mãn GLIE nếu:\n1. Mọi cặp (s,a) được visit vô hạn lần\n2. Chính sách hội tụ đến greedy policy\n\n**Ví dụ GLIE**:\n```\nε_t = 1/t\n```\n\n**Định lý**: GLIE Monte Carlo Control hội tụ đến π*\n\n#### 2.6. Ví dụ: Blackjack với MC Control\n\n**Setup**:\n- State: (player_sum, dealer_card, usable_ace)\n- Action: Hit hoặc Stick\n- Reward: +1 (win), -1 (lose), 0 (draw)\n\n**Kết quả**:\n```\nChính sách học được:\n- Stick khi player_sum ≥ 20\n- Hit khi player_sum < 12\n- Phức tạp hơn ở vùng 12-19 (phụ thuộc dealer card)\n```\n\n### 3. On-Policy vs Off-Policy Learning\n\n#### 3.1. Định nghĩa\n\n**On-Policy**:\n- Học về chính sách π từ experience generated bởi π\n- Evaluate và improve cùng một chính sách\n- Ví dụ: SARSA, Monte Carlo Control\n\n**Off-Policy**:\n- Học về chính sách π (target) từ experience của μ (behavior)\n- π ≠ μ\n- Ví dụ: Q-Learning, Importance Sampling\n\n#### 3.2. So sánh\n\n| Đặc điểm | On-Policy | Off-Policy |\n|----------|-----------|------------|\n| Chính sách | Một chính sách | Hai chính sách |\n| Sample efficiency | Thấp hơn | Cao hơn |\n| Variance | Thấp | Cao |\n| Converge | Ổn định | Có thể diverge |\n| Use old data | Không | Có thể |\n| Learn optimal | Không (nếu ε-greedy) | Có |\n\n### 4. SARSA - On-Policy TD Control\n\n#### 4.1. Ý tưởng\nÁp dụng TD(0) cho Q(s,a) thay vì V(s)\n\n**TD Update cho Q**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n```\n\n**Tên gọi**: SARSA = (S, A, R, S', A')\n\n#### 4.2. Thuật toán SARSA\n\n```\nKhởi tạo Q(s,a) arbitrarily, ∀s ∈ S, a ∈ A\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    Chọn A từ S theo ε-greedy(Q)\n    \n    Lặp với mỗi bước:\n        Thực hiện A, quan sát R, S'\n        Chọn A' từ S' theo ε-greedy(Q)\n        \n        Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n        \n        S ← S'; A ← A'\n    cho đến S là terminal\n```\n\n#### 4.3. Tính chất của SARSA\n\n**Hội tụ**:\n- GLIE schedule + Robbins-Monro conditions → Q → Q*\n- Trong thực tế, dùng ε nhỏ cố định hoặc decay\n\n**On-Policy**:\n- Học về chính sách ε-greedy đang dùng\n- Safe: Tính đến exploration trong học\n\n#### 4.4. Ví dụ: Windy Gridworld\n\n**Mô tả**:\n- Grid 7×10 với \"wind\" ở một số cột\n- Wind đẩy agent lên 1-2 ô\n- Start: (3,0), Goal: (3,7)\n- Actions: 4 hướng\n\n**Kết quả SARSA**:\n\n**Các khái niệm quan trọng:**\n- ε-greedy(Q) là một chính sách hành động được sử dụng để chọn hành động dựa trên hàm Q-function hiện tại. Với xác suất ε, nó chọn một hành động ngẫu nhiên từ tất cả các hành động có thể. Với xác suất 1-ε, nó chọn hành động có giá trị Q cao nhất (tham lam) cho trạng thái hiện tại. Chính sách này cân bằng giữa khám phá và khai thác.\n\n**Mối quan hệ:**\n- SARSA chọn hành động A từ S và A' từ S' theo chính sách ε-greedy(Q).\n- Tham số ε kiểm soát hành vi của chính sách ε-greedy(Q) bằng cách xác định xác suất khám phá.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n    Evaluate all agents\n    Replace worst với copies of best\n    Mutate hyperparameters\n```\n\n**Benefits**: Automatic hyperparameter tuning + diversity\n\n#### 9.2. Quality Diversity (QD)\n\n**Goal**: Find diverse set of good solutions\n\n**MAP-Elites**:\n```\nGrid of cells, mỗi cell = behavior niche\nStore best solution in mỗi cell\nExplore để fill all cells\n```\n\n**Applications**: Robotics, game playing\n\n### 10. Exploration trong Deep RL\n\n#### 10.1. Noisy Networks\n\n**Idea**: Add noise to network weights\n\n**NoisyNet**:\n```\ny = (μ^w + σ^w ⊙ ε^w) x + (μ^b + σ^b ⊙ ε^b)\n\nμ, σ: Learned parameters\nε: Random noise\n```\n\n**Benefits**:\n- Automatic exploration\n- State-dependent noise\n- No epsilon scheduling\n\n#### 10.2. Parameter Space Noise\n\n**Add noise to policy parameters**:\n```\nθ_noisy = θ + N(0, σ²I)\n```\n\n**Adaptive noise**:\n```\nAdjust σ based on action space distance\n```\n\n#### 10.3. Bootstrapped DQN\n\n**Multiple heads** cho uncertainty:\n```\nQ_1(s,a), Q_2(s,a), ..., Q_K(s,a)\n\nSample head k uniformly\nUse Q_k for exploration\n```\n\n**Effect**: Deep exploration (multi-step exploration)\n\n### 11. Practical Guidelines\n\n#### 11.1. Choosing Exploration Strategy\n\n**Simple tasks**:\n- ε-greedy với decay\n- Optimistic initialization\n\n**Complex tasks**:\n- UCB / Thompson Sampling\n- Curiosity-driven\n\n**Sparse rewards**:\n- HER\n- Intrinsic motivation\n- Count-based\n\n**Continuous control**:\n- Gaussian noise\n- Parameter space noise\n- NoisyNet\n\n#### 11.2. Debugging Exploration\n\n**Metrics to track**:\n```\n- State/action coverage\n- Entropy của policy\n- Intrinsic rewards\n- Episode diversity\n```\n\n**Signs of poor exploration**:\n- Low state coverage\n- Policy becomes deterministic too early\n- Training plateaus early\n\n#### 11.3. Hyperparameter Tuning\n\n**ε-greedy**:\n- Start: 1.0\n- End: 0.01-0.1\n- Decay: Linear hoặc exponential\n\n**Curiosity coefficient β**:\n- Start small: 0.01-0.1\n- Tune based on reward scale\n\n**UCB constant c**:\n- Typical: 1.0-2.0\n- Higher = more exploration\n\n### 12. Code Examples\n\n#### 12.1. ε-Greedy với Decay\n```python\nclass EpsilonGreedy:\n    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, \n                 epsilon_decay=0.995):\n        self.epsilon = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.epsilon_decay = epsilon_decay\n    \n    def select_action(self, q_values):\n        if np.random.random() < self.epsilon:\n            return np.random.randint(len(q_values))\n        else:\n            return np.argmax(q_values)\n    \n    def decay(self):\n        self.epsilon = max(self.epsilon_end, \n                          self.epsilon * self.epsilon_decay)\n```\n\n#### 12.2. UCB\n```python\nclass UCB:\n    def __init__(self, num_actions, c=2.0):\n        self.Q = np.zeros(num_actions)\n        self.N = np.zeros(num_actions)\n        self.c = c\n        self.t = 0\n    \n    def select_action(self):\n        self.t += 1\n        \n        # Avoid division by zero\n        ucb_values = np.where(\n            self.N > 0,\n            self.Q + self.c * np.sqrt(np.log(self.t) / self.N),\n            np.inf\n        )\n        \n        return np.argmax(ucb_values)\n    \n    def update(self, action, reward):\n        self.N[action] += 1\n        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n```\n\n#### 12.3. Curiosity-Driven (Simplified)\n```python\nclass CuriosityModule(nn.Module):\n    def __init__(self, state_dim, action_dim, feature_dim=128):\n        super().__init__()\n        # Inverse model\n        self.inverse = nn.Sequential(\n            nn.Linear(state_dim * 2, 256),\n            nn.ReLU(),\n\n**Các khái niệm quan trọng:**\n- EpsilonGreedy là một lớp triển khai chiến lược ε-greedy. Nó có các tham số epsilon_start, epsilon_end, và epsilon_decay để quản lý quá trình giảm epsilon. Phương thức select_action chọn hành động ngẫu nhiên với xác suất epsilon hoặc hành động có Q-value cao nhất với xác suất 1-epsilon. Phương thức decay giảm giá trị epsilon theo thời gian.\n\n**Mối quan hệ:**\n- Lớp EpsilonGreedy sử dụng epsilon_decay để giảm giá trị epsilon theo thời gian.\n- Lớp EpsilonGreedy sử dụng tham số epsilon để quyết định hành động."
        },
        "output_without_context": {
            "question": "Trong phương pháp ε-Greedy, xác suất để chọn hành động tham lam là bao nhiêu khi ε = 0.1?",
            "answer": "0.9"
        }
    },
    {
        "topic_description": "Khái niệm học Q là một phương pháp kiểm soát ngoài chính sách. Học sinh cần hiểu về cách thức cập nhật hàm giá trị Q(S, A) và ứng dụng của nó thông qua các ví dụ thực tế như Cliff Walking. Chủ đề này liên quan đến tuần 4 và cần áp dụng kiến thức từ tuần 3 về các phương pháp kiểm soát.",
        "output_with_context": {
            "question": "Trong phương pháp học Q, hàm giá trị Q(S, A) được cập nhật dựa trên công thức nào?",
            "answer": "Q(S, A) = Q(S, A) + α[R + γ max Q(S', A') - Q(S, A)]",
            "rag_context": null
        },
        "output_without_context": {
            "question": "Trong phương pháp học Q, hàm giá trị Q(S, A) được cập nhật dựa trên yếu tố nào sau đây?",
            "answer": "Phần thưởng nhận được và giá trị Q của trạng thái tiếp theo."
        }
    },
    {
        "topic_description": "Khái niệm về sự khác biệt giữa hai phương pháp học là Monte-Carlo và Temporal-Difference (TD). Học sinh sẽ so sánh các ưu nhược điểm của từng phương pháp trong dự đoán không mô hình. Qua đó, kết nối các khái niệm từ tuần 3 đến tuần 4.",
        "output_with_context": {
            "question": "So sánh ưu điểm của phương pháp học Monte-Carlo và Temporal-Difference (TD) trong việc dự đoán không mô hình. Phương pháp nào thường hội tụ nhanh hơn trong thực tế?",
            "answer": "Temporal-Difference (TD) thường hội tụ nhanh hơn Monte-Carlo trong thực tế.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n        Nếu S_t xuất hiện lần đầu tiên trong episode:\n            Thêm G vào Returns(S_t)\n            V(S_t) = average(Returns(S_t))\n```\n\n**Đặc điểm**:\n- Chỉ cập nhật cho lần xuất hiện đầu tiên của trạng thái\n- Không có bias\n- Đảm bảo hội tụ đến V^π(s) khi số episodes → ∞\n\n#### 2.4. Every-Visit Monte Carlo\n\n**Khác biệt**: Cập nhật cho mọi lần xuất hiện của trạng thái trong episode\n\n```\nVới mỗi bước t = T-1, T-2, ..., 0:\n    G = γG + R_{t+1}\n    Thêm G vào Returns(S_t)\n    V(S_t) = average(Returns(S_t))\n```\n\n**So sánh**:\n- Every-visit có variance thấp hơn\n- Cả hai đều hội tụ đến V^π(s)\n- Every-visit thường được sử dụng nhiều hơn\n\n#### 2.5. Incremental Mean Update\n\n**Vấn đề**: Lưu trữ tất cả returns không hiệu quả\n\n**Giải pháp**: Cập nhật incremental\n```\nCông thức tổng quát:\n    μ_k = μ_{k-1} + (1/k)(x_k - μ_{k-1})\n\nÁp dụng cho MC:\n    N(s) = N(s) + 1\n    V(s) = V(s) + (1/N(s))(G - V(s))\n    \nHoặc dùng learning rate α cố định:\n    V(s) = V(s) + α(G - V(s))\n```\n\n**Lợi ích**:\n- Tiết kiệm bộ nhớ\n- Cập nhật online\n- Quên dần các ước lượng cũ (với α cố định)\n\n#### 2.6. Ví dụ: Blackjack\n\n**Mô tả bài toán**:\n- **Trạng thái**: (tổng bài của người chơi, bài úp của dealer, có ace không)\n- **Hành động**: Hit (rút thêm) hoặc Stick (dừng)\n- **Phần thưởng**: +1 thắng, -1 thua, 0 hòa\n- **Chính sách**: Stick nếu tổng ≥ 20, ngược lại Hit\n\n**Ứng dụng MC**:\n1. Chơi nhiều games theo chính sách\n2. Ghi lại returns cho mỗi trạng thái\n3. Tính V^π bằng trung bình returns\n\n**Kết quả**:\n```\nV^π(20, ACE, usable_ace) ≈ 0.8  (rất tốt)\nV^π(12, 2, no_ace) ≈ -0.3       (tệ)\n```\n\n### 3. Temporal-Difference Learning (TD)\n\n#### 3.1. Giới thiệu\nTD Learning kết hợp ý tưởng của MC và DP:\n- Như MC: Học từ kinh nghiệm, không cần mô hình\n- Như DP: Bootstrap từ ước lượng hiện tại, không cần episode hoàn chỉnh\n\n#### 3.2. TD(0) - Temporal Difference cơ bản\n\n**TD Update Rule**:\n```\nV(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]\n```\n\n**Các thành phần**:\n- **TD Target**: R_{t+1} + γV(S_{t+1})\n- **TD Error**: δ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\n- **Update**: V(S_t) ← V(S_t) + α × δ_t\n\n#### 3.3. Thuật toán TD(0)\n\n```\n\n**Các khái niệm quan trọng:**\n- TD(0) (Temporal Difference learning với λ=0) là thuật toán Temporal-Difference Learning cơ bản nhất, một phương pháp học tăng cường model-free, online và bootstrap. Nó học từng bước một từ kinh nghiệm mà không cần đợi đến cuối episode, phù hợp với các tác vụ liên tục. TD(0) cập nhật hàm giá trị V(s) của trạng thái hiện tại (S_t) dựa trên phần thưởng tức thời (R_{t+1}) và ước lượng giá trị của trạng thái tiếp theo (V(S_{t+1})). Công thức cập nhật là V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]. Thuật toán này có bias cao nhưng variance thấp, thường hội tụ nhanh hơn Monte Carlo trong thực tế. SARSA là một ứng dụng của TD(0) cho Q-function.\n- Temporal-Difference Learning (TD) là một loại thuật toán học tăng cường kết hợp ý tưởng từ Monte Carlo và Dynamic Programming. Giống như Monte Carlo, TD học từ kinh nghiệm mà không cần mô hình môi trường. Giống như Dynamic Programming, TD bootstrap từ các ước lượng giá trị hiện tại của các trạng thái tiếp theo, cho phép cập nhật giá trị mà không cần chờ đến cuối episode.\n\n**Mối quan hệ:**\n- TD(0) hội tụ đến V^π(s) nhanh hơn Monte Carlo trong thực tế.\n- TD(0) thường hội tụ nhanh hơn Monte Carlo trong thực tế.\n- Temporal-Difference Learning (TD) kết hợp ý tưởng bootstrap từ ước lượng hiện tại của Dynamic Programming, cho phép cập nhật mà không cần chờ đến cuối episode.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nKhởi tạo V(s) arbitrarily, ∀s ∈ S\n\nVới mỗi episode:\n    Khởi tạo S\n    \n    Lặp cho mỗi bước của episode:\n        A ← hành động từ S theo π\n        Thực hiện A, quan sát R, S'\n        \n        V(S) ← V(S) + α[R + γV(S') - V(S)]\n        \n        S ← S'\n    cho đến khi S là terminal\n```\n\n#### 3.4. So sánh MC vs TD(0)\n\n| Tiêu chí | Monte Carlo | TD(0) |\n|----------|-------------|-------|\n| Cập nhật | Cuối episode | Sau mỗi bước |\n| Bootstrap | Không | Có |\n| Target | G_t (actual return) | R + γV(S') |\n| Bias | Unbiased | Biased |\n| Variance | High variance | Low variance |\n| Hội tụ | Chậm | Nhanh hơn |\n| Môi trường | Cần episodic | Cả episodic và continuing |\n\n#### 3.5. Ưu điểm của TD\n\n✅ **Học online**: Cập nhật sau mỗi bước, không cần chờ episode kết thúc\n✅ **Continuing tasks**: Hoạt động với tasks không có điểm kết thúc\n✅ **Lower variance**: Bootstrap giảm variance so với MC\n✅ **Học nhanh hơn**: Thường hội tụ nhanh hơn MC trong thực tế\n✅ **Hiệu quả dữ liệu**: Sử dụng thông tin từ ước lượng hiện tại\n\n#### 3.6. Ví dụ minh họa: Random Walk\n\n**Setup**:\n```\nStates: [A] [B] [C] [D] [E]\nStart: C\nTerminal: Left of A hoặc Right of E\nReward: 0 mọi nơi, +1 khi đến Right of E\nAction: Left hoặc Right (random với p=0.5)\n```\n\n**True values**:\n```\nV^π(A) = 1/6, V^π(B) = 2/6, V^π(C) = 3/6,\nV^π(D) = 4/6, V^π(E) = 5/6\n```\n\n**So sánh MC vs TD**:\n- TD hội tụ nhanh hơn với cùng số episodes\n- TD ít sensitive với khởi tạo\n- MC có RMS error cao hơn trong giai đoạn đầu\n\n### 4. Batch Methods và Certainty Equivalence\n\n#### 4.1. Batch Learning\n**Ý tưởng**: Lặp lại huấn luyện trên cùng một batch experience cho đến hội tụ\n\n```\nCho trước batch experience: \n    {(S₁, A₁, R₁, S'₁), (S₂, A₂, R₂, S'₂), ...}\n\nLặp cho đến hội tụ:\n    Với mỗi experience (S, A, R, S'):\n        Cập nhật V(S) theo MC hoặc TD\n```\n\n#### 4.2. Certainty Equivalence\n\n**MC**: Tìm V^π minimize mean-squared error với observed returns\n```\nV^π = argmin_V Σ_episodes Σ_t (G_t - V(S_t))²\n```\n\n**TD**: Tìm V^π thỏa mãn phương trình Bellman cho MDP ước lượng\n```\nV^π(s) = E[R + γV^π(S') | s]\n```\n(ước lượng từ experience)\n\n**Kết quả**:\n- TD tận dụng cấu trúc Markov\n- MC chỉ minimize error, không exploit Markov property\n- TD thường hiệu quả hơn trong môi trường Markov\n\n#### 4.3. Ví dụ: AB Example\n\n**Experience**:\n```\nA, 0, B, 0\nB, 1 (terminal)\nB, 1 (terminal)\n\n**Các khái niệm quan trọng:**\n- Temporal Difference (TD(0)) là một thuật toán học tăng cường model-free, on-policy để ước lượng hàm giá trị V(s). Nó cập nhật hàm giá trị sau mỗi bước thời gian, sử dụng ước lượng của chính nó (bootstrap) để tính toán target. Công thức cập nhật là V(S) ← V(S) + α[R + γV(S') - V(S)]. TD(0) có ưu điểm là học online, hoạt động với continuing tasks, có variance thấp hơn và hội tụ nhanh hơn Monte Carlo.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n#### 7.1. Game Playing\n\n**TD-Gammon** (1992):\n- Backgammon AI\n- Sử dụng TD(λ) với neural network\n- Self-play\n- Đạt world-champion level\n\n**AlphaGo Zero**:\n- Sử dụng TD-style updates\n- Self-play + MCTS\n- Không cần human knowledge\n\n#### 7.2. Robot Navigation\n\n**Task**: Robot tìm đường trong môi trường chưa biết\n- State: Vị trí robot\n- Action: Di chuyển\n- Reward: -1 mỗi bước, +100 khi đến đích\n\n**Ưu điểm TD**:\n- Học online trong quá trình điều hướng\n- Không cần đợi đến đích mới cập nhật\n- Adapt với môi trường thay đổi\n\n#### 7.3. Resource Management\n\n**Elevator Control**:\n- State: Vị trí thang máy, yêu cầu chờ\n- Action: Lên/xuống/đứng yên\n- Reward: -1 × tổng thời gian chờ\n\n**TD Learning**:\n- Học value function cho mỗi trạng thái\n- Online learning từ hoạt động hàng ngày\n- Cải thiện liên tục\n\n### 8. Phân tích lý thuyết\n\n#### 8.1. Tốc độ hội tụ\n\n**Monte Carlo**:\n```\nV_k(s) → V^π(s) với rate O(1/√k)\nk: số episodes\n```\n\n**TD(0)**:\n```\nV_k(s) → V^π(s) nhanh hơn trong thực tế\nKhông có bound lý thuyết chặt chẽ\n```\n\n**Thực nghiệm**: TD thường nhanh hơn MC 2-10 lần\n\n#### 8.2. Điều kiện hội tụ\n\n**Robbins-Monro conditions** cho learning rate α_t:\n```\nΣ_{t=1}^∞ α_t = ∞     (đảm bảo hội tụ)\nΣ_{t=1}^∞ α_t² < ∞    (đảm bảo variance hội tụ về 0)\n```\n\n**Ví dụ**:\n- α_t = 1/t: Thỏa mãn\n- α_t = 0.01: Không thỏa mãn điều kiện 1, nhưng practical\n\n#### 8.3. Bias-Variance Tradeoff\n\n**Monte Carlo**:\n- Bias = 0 (ước lượng không chệch)\n- Variance cao (phụ thuộc vào toàn bộ trajectory)\n\n**TD(0)**:\n- Bias > 0 (phụ thuộc vào V hiện tại)\n- Variance thấp (chỉ phụ thuộc 1 bước)\n\n**n-Step TD**: Cân bằng\n```\nBias giảm khi n tăng\nVariance tăng khi n tăng\n```\n\n### 9. So sánh tổng hợp\n\n#### 9.1. Bảng so sánh đầy đủ\n\n| Tiêu chí | MC | TD(0) | TD(λ) | DP |\n|----------|----|----|-------|-----|\n| Model-free | ✓ | ✓ | ✓ | ✗ |\n| Bootstrap | ✗ | ✓ | ✓ | ✓ |\n| Online | ✗ | ✓ | ✓ | ✓ |\n| Episodic only | ✓ | ✗ | ✗ | ✗ |\n| Bias | Low | High | Medium | Low |\n| Variance | High | Low | Medium | Low |\n| Convergence | Slow | Fast | Medium | Fastest |\n\n#### 9.2. Khi nào dùng phương pháp nào?\n\n**Monte Carlo**:\n- Môi trường không Markov\n- Cần ước lượng unbiased\n- Episodic tasks ngắn\n\n**TD(0)**:\n- Môi trường Markov\n- Continuing tasks\n\n**Các khái niệm quan trọng:**\n- TD(0) (Temporal Difference learning với λ=0) là thuật toán Temporal-Difference Learning cơ bản nhất, một phương pháp học tăng cường model-free, online và bootstrap. Nó học từng bước một từ kinh nghiệm mà không cần đợi đến cuối episode, phù hợp với các tác vụ liên tục. TD(0) cập nhật hàm giá trị V(s) của trạng thái hiện tại (S_t) dựa trên phần thưởng tức thời (R_{t+1}) và ước lượng giá trị của trạng thái tiếp theo (V(S_{t+1})). Công thức cập nhật là V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]. Thuật toán này có bias cao nhưng variance thấp, thường hội tụ nhanh hơn Monte Carlo trong thực tế. SARSA là một ứng dụng của TD(0) cho Q-function.\n- Markov property (tính chất Markov) là một tính chất của môi trường trong Học tăng cường (Reinforcement Learning), trong đó trạng thái hiện tại chứa tất cả thông tin cần thiết để dự đoán trạng thái và phần thưởng tiếp theo, độc lập với lịch sử các trạng thái và hành động trước đó. Điều này có nghĩa là trạng thái tiếp theo chỉ phụ thuộc vào trạng thái và hành động hiện tại. Các thuật toán Temporal Difference (TD), bao gồm TD(0), tận dụng cấu trúc Markov này và thường hiệu quả hơn trong môi trường Markov, trong khi Monte Carlo (MC) thì không.\n\n**Mối quan hệ:**\n- TD(0) hội tụ đến V^π(s) nhanh hơn Monte Carlo trong thực tế.\n- TD(0) thường hiệu quả hơn trong môi trường Markov vì nó dựa vào tính chất Markov để cập nhật giá trị trạng thái dựa trên trạng thái tiếp theo.\n- TD(0) thường hội tụ nhanh hơn Monte Carlo trong thực tế.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n- Cần học nhanh\n- Online learning\n\n**TD(λ)**:\n- Khi cần cân bằng bias-variance\n- Credit assignment phức tạp\n- Eligibility traces quan trọng\n\n### 10. Code Implementation\n\n#### 10.1. Monte Carlo First-Visit\n```python\ndef monte_carlo_prediction(env, policy, num_episodes, gamma=0.99):\n    V = defaultdict(float)\n    returns = defaultdict(list)\n    \n    for _ in range(num_episodes):\n        episode = generate_episode(env, policy)\n        G = 0\n        visited = set()\n        \n        # Duyệt ngược từ cuối episode\n        for t in range(len(episode)-1, -1, -1):\n            state, action, reward = episode[t]\n            G = gamma * G + reward\n            \n            if state not in visited:\n                returns[state].append(G)\n                V[state] = np.mean(returns[state])\n                visited.add(state)\n    \n    return V\n```\n\n#### 10.2. TD(0)\n```python\ndef td_prediction(env, policy, num_episodes, alpha=0.1, gamma=0.99):\n    V = defaultdict(float)\n    \n    for _ in range(num_episodes):\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # TD update\n            td_target = reward + gamma * V[next_state]\n            td_error = td_target - V[state]\n            V[state] += alpha * td_error\n            \n            state = next_state\n    \n    return V\n```\n\n#### 10.3. TD(λ) with Eligibility Traces\n```python\ndef td_lambda_prediction(env, policy, num_episodes, \n                        alpha=0.1, gamma=0.99, lambda_=0.9):\n    V = defaultdict(float)\n    \n    for _ in range(num_episodes):\n        E = defaultdict(float)  # Eligibility traces\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # TD error\n            delta = reward + gamma * V[next_state] - V[state]\n            \n            # Update eligibility trace\n            E[state] += 1\n            \n            # Update all states\n            for s in E:\n                V[s] += alpha * delta * E[s]\n                E[s] *= gamma * lambda_\n            \n            state = next_state\n    \n    return V\n```\n\n### 11. Bài tập thực hành\n\n#### 11.1. Bài tập cơ bản\n1. Implement First-Visit MC cho Blackjack\n2. So sánh MC vs TD(0) trên Random Walk\n3. Visualize learning curves với different α\n\n#### 11.2. Bài tập nâng cao\n1. Implement n-step TD với n = 1, 3, 5, 10\n2. Compare TD(λ) với λ = 0, 0.5, 0.9, 1.0\n3. Analyze bias-variance tradeoff empirically\n\n#### 11.3. Dự án\n1. Build Tic-Tac-Toe AI với TD learning\n2. Robot navigation trong gridworld phức tạp\n3. Stock price prediction với TD methods\n\n### 12. Kết luận\n\nModel-Free Prediction giải quyết vấn đề quan trọng: **Đánh giá chính sách mà không cần biết mô hình môi trường**.\n\n**Các phương pháp chính**:\n- **Monte Carlo**: Đơn giản, unbiased, nhưng high variance\n- **TD(0)**: Efficient, online, nhưng biased\n- **TD(λ)**: Cân bằng tốt nhất, flexible\n\n**Key insights**:\n1. Bootstrap (TD) vs Full returns (MC) là tradeoff cơ bản\n2. TD thường hiệu quả hơn trong môi trường Markov\n3. Eligibility traces (TD(λ)) cung cấp spectrum liên tục\n4. Lựa chọn phương pháp phụ thuộc vào đặc điểm bài toán\n\n\n**Các khái niệm quan trọng:**\n- TD(0) (Temporal Difference learning với λ=0) là thuật toán Temporal-Difference Learning cơ bản nhất, một phương pháp học tăng cường model-free, online và bootstrap. Nó học từng bước một từ kinh nghiệm mà không cần đợi đến cuối episode, phù hợp với các tác vụ liên tục. TD(0) cập nhật hàm giá trị V(s) của trạng thái hiện tại (S_t) dựa trên phần thưởng tức thời (R_{t+1}) và ước lượng giá trị của trạng thái tiếp theo (V(S_{t+1})). Công thức cập nhật là V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]. Thuật toán này có bias cao nhưng variance thấp, thường hội tụ nhanh hơn Monte Carlo trong thực tế. SARSA là một ứng dụng của TD(0) cho Q-function.\n\n**Mối quan hệ:**\n- TD(0) hội tụ đến V^π(s) nhanh hơn Monte Carlo trong thực tế.\n- TD(0) thường hội tụ nhanh hơn Monte Carlo trong thực tế."
        },
        "output_without_context": {
            "question": "So với phương pháp học Monte-Carlo, phương pháp học Temporal-Difference (TD) có ưu điểm gì nổi bật trong việc dự đoán không mô hình?",
            "answer": "Temporal-Difference (TD) cho phép cập nhật giá trị ngay lập tức sau mỗi bước mà không cần chờ đến khi hoàn thành một tập hợp các bước."
        }
    },
    {
        "topic_description": "Học sinh sẽ mô tả thuật toán Sarsa và cách nó cập nhật giá trị Q dựa trên các hành động. Liên kết giữa các khái niệm được học ở tuần 3 và ứng dụng trong tuần 4 để giải quyết các bài toán không đầy đủ thông tin.",
        "output_with_context": {
            "question": "Trong thuật toán SARSA, công thức cập nhật giá trị Q cho cặp trạng thái-hành động (S, A) là gì?",
            "answer": "Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n#### 11.2. Khi nào dùng thuật toán nào?\n\n**Monte Carlo Control**:\n- Episodic tasks\n- Model-free\n- Simple implementation\n\n**SARSA**:\n- Safe exploration quan trọng\n- Online learning\n- On-policy preferred\n\n**Q-Learning**:\n- Learn optimal policy\n- Can reuse data\n- Most popular choice\n\n**Expected SARSA**:\n- Reduce variance\n- Small action space\n- More stable than SARSA\n\n**Double Q-Learning**:\n- Overestimation is problem\n- Critical applications\n- Better performance often\n\n### 12. Code Implementation\n\n#### 12.1. SARSA\n```python\ndef sarsa(env, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1):\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n    \n    for episode in range(num_episodes):\n        state = env.reset()\n        action = epsilon_greedy_policy(Q, state, epsilon)\n        \n        done = False\n        while not done:\n            next_state, reward, done, _ = env.step(action)\n            next_action = epsilon_greedy_policy(Q, next_state, epsilon)\n            \n            # SARSA update\n            td_target = reward + gamma * Q[next_state][next_action]\n            td_error = td_target - Q[state][action]\n            Q[state][action] += alpha * td_error\n            \n            state = next_state\n            action = next_action\n    \n    return Q\n\ndef epsilon_greedy_policy(Q, state, epsilon):\n    if np.random.random() < epsilon:\n        return np.random.randint(len(Q[state]))\n    else:\n        return np.argmax(Q[state])\n```\n\n#### 12.2. Q-Learning\n```python\ndef q_learning(env, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1):\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n    \n    for episode in range(num_episodes):\n        state = env.reset()\n        \n        done = False\n        while not done:\n            # Behavior policy: ε-greedy\n            action = epsilon_greedy_policy(Q, state, epsilon)\n            next_state, reward, done, _ = env.step(action)\n            \n            # Q-Learning update (target policy: greedy)\n            td_target = reward + gamma * np.max(Q[next_state])\n            td_error = td_target - Q[state][action]\n            Q[state][action] += alpha * td_error\n            \n            state = next_state\n    \n    return Q\n```\n\n#### 12.3. Double Q-Learning\n```python\ndef double_q_learning(env, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1):\n    Q1 = defaultdict(lambda: np.zeros(env.action_space.n))\n    Q2 = defaultdict(lambda: np.zeros(env.action_space.n))\n    \n    for episode in range(num_episodes):\n        state = env.reset()\n        \n        done = False\n        while not done:\n            # Action selection based on Q1 + Q2\n            Q_combined = Q1[state] + Q2[state]\n            action = epsilon_greedy_action(Q_combined, epsilon)\n            \n            next_state, reward, done, _ = env.step(action)\n            \n            # Randomly update Q1 or Q2\n            if np.random.random() < 0.5:\n                # Update Q1\n                best_action = np.argmax(Q1[next_state])\n                td_target = reward + gamma * Q2[next_state][best_action]\n                Q1[state][action] += alpha * (td_target - Q1[state][action])\n            else:\n                # Update Q2\n                best_action = np.argmax(Q2[next_state])\n                td_target = reward + gamma * Q1[next_state][best_action]\n                Q2[state][action] += alpha * (td_target - Q2[state][action])\n            \n            state = next_state\n    \n    return Q1, Q2\n```\n\n#### 12.4. Expected SARSA\n```python\ndef expected_sarsa(env, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1):\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n    \n    for episode in range(num_episodes):\n        state = env.reset()\n        \n        done = False\n        while not done:\n            action = epsilon_greedy_policy(Q, state, epsilon)\n            next_state, reward, done, _ = env.step(action)\n            \n            # Calculate expected Q value\n\n**Các khái niệm quan trọng:**\n- SARSA (State-Action-Reward-State-Action) là một thuật toán học tăng cường (reinforcement learning) on-policy, model-free, sử dụng phương pháp Temporal Difference (TD) để học hàm Q-function. Nó cập nhật Q-value dựa trên bộ 5 (State, Action, Reward, Next State, Next Action) và học chính sách đang được thực hiện (behavior policy), nghĩa là hành động A' được chọn bởi chính sách hiện tại. Phương trình cập nhật của SARSA là Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]. SARSA được coi là \"an toàn\" vì nó học chính sách mà nó đang sử dụng để hành động, phù hợp với các môi trường có chi phí cao cho các hành động không tối ưu và khi \"safe exploration\" là quan trọng, tính đến rủi ro của việc khám phá. SARSA có phương sai cao hơn Expected SARSA do lấy mẫu A'.\n\nExpected SARSA là một biến thể on-policy, model-free của SARSA. Thay vì sử dụng Q-value của hành động tiếp theo được chọn, nó sử dụng giá trị kỳ vọng của Q-value ở trạng thái tiếp theo, được tính bằng cách lấy trung bình có trọng số của tất cả các hành động có thể có theo chính sách hiện tại. Điều này giúp giảm phương sai và làm cho thuật toán ổn định hơn SARSA truyền thống, đặc biệt hiệu quả với không gian hành động nhỏ. Phương trình cập nhật của Expected SARSA là Q(s,a) ← Q(s,a) + α[reward + γ * expected_q - Q(s,a)].\n\n**Mối quan hệ:**\n- Expected SARSA sử dụng Learning Rate (α) để kiểm soát kích thước bước cập nhật của Q-values: Q[state][action] += alpha * td_error.\n- Expected SARSA sử dụng Discount Factor (γ) để tính toán td_target: td_target = reward + gamma * expected_q, chiết khấu giá trị kỳ vọng của trạng thái tiếp theo.\n- Expected SARSA cập nhật Q-function bằng cách sử dụng công thức Q[state][action] += alpha * td_error, trong đó td_error được tính từ td_target và Q[state][action] hiện tại.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n            policy_probs = epsilon_greedy_probs(Q[next_state], epsilon)\n            expected_q = np.sum(policy_probs * Q[next_state])\n            \n            # Expected SARSA update\n            td_target = reward + gamma * expected_q\n            td_error = td_target - Q[state][action]\n            Q[state][action] += alpha * td_error\n            \n            state = next_state\n    \n    return Q\n\ndef epsilon_greedy_probs(q_values, epsilon):\n    probs = np.ones(len(q_values)) * epsilon / len(q_values)\n    best_action = np.argmax(q_values)\n    probs[best_action] += (1.0 - epsilon)\n    return probs\n```\n\n### 13. Hyperparameter Tuning\n\n#### 13.1. Learning Rate (α)\n- **Quá cao**: Không ổn định, oscillate\n- **Quá thấp**: Học chậm\n- **Typical**: 0.01 - 0.5\n- **Adaptive**: α_t = 1/t hoặc decay schedule\n\n#### 13.2. Exploration Rate (ε)\n- **Strategies**:\n  - Constant: ε = 0.1\n  - Decay: ε_t = ε_0 / t\n  - Exponential: ε_t = ε_min + (ε_max - ε_min) × e^(-decay×t)\n- **Typical**: Start 1.0, end 0.01\n\n#### 13.3. Discount Factor (γ)\n- **γ → 0**: Myopic (chỉ quan tâm immediate reward)\n- **γ → 1**: Farsighted (quan tâm long-term)\n- **Typical**: 0.9 - 0.99\n\n### 14. Bài tập thực hành\n\n#### 14.1. Bài tập cơ bản\n1. Implement SARSA và Q-Learning cho GridWorld\n2. Compare performance on Cliff Walking\n3. Visualize Q-values và learned policies\n\n#### 14.2. Bài tập nâng cao\n1. Implement Double Q-Learning và compare với Q-Learning\n2. Experiment với different ε-decay strategies\n3. Compare n-step methods với n = 1, 3, 5\n\n#### 14.3. Dự án\n1. Build Taxi environment solver với Q-Learning\n2. Implement SARSA(λ) với eligibility traces\n3. Create game AI (Tic-Tac-Toe hoặc Connect Four)\n\n### 15. Kết luận\n\nModel-Free Control cho phép agent tìm chính sách tối ưu mà không cần biết mô hình môi trường - một breakthrough quan trọng trong RL.\n\n**Key Takeaways**:\n\n1. **Q-Function là chìa khóa**: Cho phép policy improvement mà không cần mô hình\n2. **Exploration-Exploitation Tradeoff**: ε-greedy là giải pháp đơn giản và hiệu quả\n3. **On-Policy vs Off-Policy**: Trade-off giữa stability và optimality\n4. **SARSA vs Q-Learning**: Safe vs Optimal\n5. **Variance Reduction**: Expected SARSA, Double Q-Learning cải thiện stability\n\n**Chuẩn bị cho phần tiếp theo**:\n- Tabular methods không scale với large state spaces\n- Cần **Function Approximation** để xử lý continuous states\n- Deep Q-Networks (DQN) sẽ kết hợp Q-Learning với Deep Learning!\n\n---\n\n\n**Các khái niệm quan trọng:**\n- SARSA (State-Action-Reward-State-Action) là một thuật toán học tăng cường (reinforcement learning) on-policy, model-free, sử dụng phương pháp Temporal Difference (TD) để học hàm Q-function. Nó cập nhật Q-value dựa trên bộ 5 (State, Action, Reward, Next State, Next Action) và học chính sách đang được thực hiện (behavior policy), nghĩa là hành động A' được chọn bởi chính sách hiện tại. Phương trình cập nhật của SARSA là Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]. SARSA được coi là \"an toàn\" vì nó học chính sách mà nó đang sử dụng để hành động, phù hợp với các môi trường có chi phí cao cho các hành động không tối ưu và khi \"safe exploration\" là quan trọng, tính đến rủi ro của việc khám phá. SARSA có phương sai cao hơn Expected SARSA do lấy mẫu A'.\n\nExpected SARSA là một biến thể on-policy, model-free của SARSA. Thay vì sử dụng Q-value của hành động tiếp theo được chọn, nó sử dụng giá trị kỳ vọng của Q-value ở trạng thái tiếp theo, được tính bằng cách lấy trung bình có trọng số của tất cả các hành động có thể có theo chính sách hiện tại. Điều này giúp giảm phương sai và làm cho thuật toán ổn định hơn SARSA truyền thống, đặc biệt hiệu quả với không gian hành động nhỏ. Phương trình cập nhật của Expected SARSA là Q(s,a) ← Q(s,a) + α[reward + γ * expected_q - Q(s,a)].\n\n**Mối quan hệ:**\n- Expected SARSA sử dụng Learning Rate (α) để kiểm soát kích thước bước cập nhật của Q-values: Q[state][action] += alpha * td_error.\n- Expected SARSA sử dụng Discount Factor (γ) để tính toán td_target: td_target = reward + gamma * expected_q, chiết khấu giá trị kỳ vọng của trạng thái tiếp theo.\n- Expected SARSA cập nhật Q-function bằng cách sử dụng công thức Q[state][action] += alpha * td_error, trong đó td_error được tính từ td_target và Q[state][action] hiện tại.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n- Học được đường đi tối ưu sau ~170 episodes\n- Tận dụng wind để di chuyển nhanh hơn\n- An toàn hơn Q-learning (tính đến exploration)\n\n### 5. Q-Learning - Off-Policy TD Control\n\n#### 5.1. Ý tưởng chính\nHọc về chính sách tham lam (greedy) trong khi hành động theo ε-greedy\n\n**Q-Learning Update**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\n                                            ↑ Greedy!\n```\n\n#### 5.2. Thuật toán Q-Learning\n\n```\nKhởi tạo Q(s,a) arbitrarily, ∀s,a\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        Chọn A từ S theo ε-greedy(Q)  ← Behavior policy\n        Thực hiện A, quan sát R, S'\n        \n        Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n                                   ↑ Target policy (greedy)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 5.3. So sánh SARSA vs Q-Learning\n\n**SARSA Update**:\n```\nQ(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n                          ↑ A' theo ε-greedy\n```\n\n**Q-Learning Update**:\n```\nQ(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n                          ↑ Greedy choice\n```\n\n**Khác biệt**:\n- SARSA: Conservative, tính đến exploration risk\n- Q-Learning: Optimistic, học optimal ignoring exploration\n\n#### 5.4. Cliff Walking Example\n\n**Setup**:\n```\n[S] [ ] [ ] ... [ ] [G]\n[ ] [ ] [ ] ... [ ] [ ]\n[C] [C] [C] ... [C] [ ]\n```\n- S: Start, G: Goal, C: Cliff (reward = -100)\n- Normal step: reward = -1\n\n**Kết quả**:\n- **Q-Learning**: Học đường ngắn nhất (sát cliff) nhưng thường rơi xuống khi explore\n- **SARSA**: Học đường an toàn (xa cliff) vì tính đến khả năng explore\n- **Performance**: SARSA tốt hơn trong training, Q-Learning tốt hơn nếu greedy\n\n#### 5.5. Tính chất Q-Learning\n\n**Hội tụ**:\n- Đảm bảo hội tụ đến Q* với:\n  - Mọi (s,a) được visit vô hạn lần\n  - Learning rate thỏa mãn Robbins-Monro\n- Không phụ thuộc vào behavior policy!\n\n**Ưu điểm**:\n✅ Học optimal policy\n✅ Có thể tái sử dụng old experience\n✅ Học từ demonstrations\n✅ Simple và popular\n\n**Nhược điểm**:\n❌ Có thể không ổn định\n❌ Overestimation bias\n❌ Higher variance\n\n### 6. Expected SARSA\n\n#### 6.1. Ý tưởng\nThay vì sample A', lấy expectation theo policy\n\n**Update Rule**:\n```\nQ(S,A) ← Q(S,A) + α[R + γ Σ_a π(a|S')Q(S',a) - Q(S,A)]\n                          ↑ Expected value\n```\n\n**Với ε-greedy**:\n```\nQ(S,A) ← Q(S,A) + α[R + γ E_π[Q(S',·)] - Q(S,A)]\n\nE_π[Q(S',·)] = Σ_a π(a|S')Q(S',a)\n\n**Các khái niệm quan trọng:**\n- Phương trình cập nhật SARSA là công thức chính để điều chỉnh giá trị Q của cặp trạng thái-hành động hiện tại (S_t, A_t) trong thuật toán SARSA. Công thức này là: `Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]` hoặc `Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])`. Nó dựa trên phần thưởng tức thời R_{t+1} (hoặc R) và giá trị Q của cặp trạng thái-hành động tiếp theo (S_{t+1}, A_{t+1}) (hoặc S', A'), trong đó A' (hoặc next_action) là hành động thực sự được chọn bởi chính sách hành vi hiện tại (ví dụ: ε-greedy). Phương trình này sử dụng lỗi TD để điều chỉnh Q-value hiện tại và là một dạng của Bellman Equation được áp dụng cho Q-function trong bối cảnh TD learning, với α là learning rate và γ là discount factor.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n             = (1-ε)max_a Q(S',a) + ε · average_a Q(S',a)\n```\n\n#### 6.2. Thuật toán Expected SARSA\n\n```\nKhởi tạo Q(s,a) arbitrarily\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        Chọn A từ S theo π (ví dụ: ε-greedy)\n        Thực hiện A, quan sát R, S'\n        \n        expected_q = Σ_a π(a|S')Q(S',a)\n        Q(S,A) ← Q(S,A) + α[R + γ·expected_q - Q(S,A)]\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 6.3. Đặc điểm\n\n**Ưu điểm**:\n- Lower variance than SARSA (no sampling A')\n- Có thể off-policy nếu behavior ≠ target\n- Computational cost tăng nhưng thường đáng giá\n\n**So sánh**:\n- SARSA: Q(S,A) ← ... + γQ(S',A')  [sample]\n- Expected SARSA: Q(S,A) ← ... + γE[Q(S',·)]  [expectation]\n- Q-Learning: Q(S,A) ← ... + γmax_a Q(S',a)  [max]\n\n### 7. Double Q-Learning\n\n#### 7.1. Maximization Bias Problem\n\n**Vấn đề**:\n```\nQ(s,a) = R(s,a) + γ max_a' Q(s',a')\n              ↑ Overestimate!\n\nE[max(X₁, X₂)] ≥ max(E[X₁], E[X₂])\n```\n\n**Hậu quả**: Q-Learning thường overestimate values\n\n#### 7.2. Giải pháp: Double Q-Learning\n\n**Ý tưởng**: Duy trì 2 Q-functions: Q₁ và Q₂\n\n**Update**:\n```\nVới xác suất 0.5:\n    A* = argmax_a Q₁(S',a)\n    Q₁(S,A) ← Q₁(S,A) + α[R + γQ₂(S',A*) - Q₁(S,A)]\n                                    ↑ Dùng Q₂ để estimate\nNgược lại:\n    A* = argmax_a Q₂(S',a)\n    Q₂(S,A) ← Q₂(S,A) + α[R + γQ₁(S',A*) - Q₂(S,A)]\n                                    ↑ Dùng Q₁ để estimate\n```\n\n#### 7.3. Thuật toán Double Q-Learning\n\n```\nKhởi tạo Q₁(s,a) và Q₂(s,a) arbitrarily, ∀s,a\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        Chọn A từ S theo ε-greedy(Q₁ + Q₂)\n        Thực hiện A, quan sát R, S'\n        \n        Với xác suất 0.5:\n            A* = argmax_a Q₁(S',a)\n            Q₁(S,A) ← Q₁(S,A) + α[R + γQ₂(S',A*) - Q₁(S,A)]\n        Ngược lại:\n            A* = argmax_a Q₂(S',a)\n            Q₂(S,A) ← Q₂(S,A) + α[R + γQ₁(S',A*) - Q₂(S,A)]\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 7.4. Lợi ích\n- Giảm overestimation bias\n- Cải thiện stability và performance\n- Nền tảng cho Double DQN (Deep RL)\n\n### 8. n-Step Methods và Eligibility Traces\n\n#### 8.1. n-Step SARSA\n\n**n-Step Return**:\n```\nG_t^(n) = R_{t+1} + γR_{t+2} + ... + γ^{n-1}R_{t+n} + γ^n Q(S_{t+n}, A_{t+n})\n```\n\n**Update**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[G_t^(n) - Q(S_t, A_t)]\n\n**Các khái niệm quan trọng:**\n- n-Step SARSA là một thuật toán học tăng cường thuộc nhóm n-Step Methods, mở rộng SARSA bằng cách sử dụng n-step return thay vì 1-step return. Nó cập nhật Q-function dựa trên tổng phần thưởng chiết khấu trong 'n' bước tiếp theo, cộng với giá trị Q-function ước tính tại trạng thái và hành động sau 'n' bước. Công thức cập nhật là Q(S_t, A_t) ← Q(S_t, A_t) + α[G_t^(n) - Q(S_t, A_t)], trong đó G_t^(n) là n-step return.\n\n**Mối quan hệ:**\n- n-Step SARSA cập nhật Q-function Q(S_t, A_t) bằng cách sử dụng công thức Q(S_t, A_t) ← Q(S_t, A_t) + α[G_t^(n) - Q(S_t, A_t)].\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n**Features**:\n- **Separate features**: φ(s, a) cho mỗi cặp (s,a)\n- **State-action features**: Kết hợp φ_s và φ_a\n\n#### 5.2. Semi-gradient SARSA\n\n**Update Rule**:\n```\nw ← w + α[R + γQ̂(S', A'; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n```\n\n**Thuật toán**:\n```\nKhởi tạo w arbitrarily\nThiết lập ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    Chọn A từ S theo ε-greedy(Q̂(S, ·; w))\n    \n    Lặp với mỗi bước:\n        Thực hiện A, quan sát R, S'\n        Chọn A' từ S' theo ε-greedy(Q̂(S', ·; w))\n        \n        w ← w + α[R + γQ̂(S', A'; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n        \n        S ← S'; A ← A'\n    cho đến S là terminal\n```\n\n#### 5.3. Semi-gradient Q-Learning\n\n**Update Rule**:\n```\nw ← w + α[R + γ max_a Q̂(S', a; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n```\n\n**Thuật toán**:\n```\nKhởi tạo w arbitrarily\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        Chọn A từ S theo ε-greedy(Q̂(S, ·; w))\n        Thực hiện A, quan sát R, S'\n        \n        w ← w + α[R + γ max_a Q̂(S', a; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 5.4. Ví dụ: Mountain Car\n\n**Problem Setup**:\n- State: (position, velocity)\n- Action: {left, right, no-op}\n- Reward: -1 per step\n- Goal: Reach top of hill\n\n**Features**: Tile coding với multiple tilings\n```\nφ(s, a) = tile_coding(position, velocity, action)\n```\n\n**Results với Semi-gradient SARSA**:\n- Converge sau ~500 episodes\n- Learned policy: Build momentum bằng oscillation\n\n### 6. Convergence và Stability\n\n#### 6.1. Convergence Guarantees\n\n**Monte Carlo**:\n- ✅ Converge đến local optimum\n- ✅ Với linear FA: converge đến global optimum\n\n**TD với Linear FA**:\n- ✅ On-policy: Converge đến near-optimal\n- ❌ Off-policy: Có thể diverge (deadly triad)\n\n**Semi-gradient Methods**:\n- Không follow true gradient\n- Có thể không converge\n- Nhưng thường work well trong thực tế\n\n#### 6.2. Deadly Triad\n\n**Ba yếu tố gây divergence**:\n1. **Function Approximation**: Không phải tabular\n2. **Bootstrapping**: TD-style updates\n3. **Off-policy**: Behavior ≠ target policy\n\n**Khi có cả 3**: Divergence có thể xảy ra\n\n**Giải pháp**:\n- Loại bỏ một trong ba yếu tố\n- Sử dụng gradient TD methods\n- Importance sampling\n- Experience replay với target network (DQN)\n\n#### 6.3. Baird's Counterexample\n\n**Setup**: Simple MDP với linear FA và off-policy TD\n\n**Kết quả**: Parameters diverge đến infinity!\n\n**Ý nghĩa**: Cần cẩn thận với off-policy + FA + bootstrapping\n\n### 7. Batch Methods\n\n#### 7.1. Experience Replay\n\n**Ý tưởng**: Lưu trữ và replay experiences\n```\n\n**Các khái niệm quan trọng:**\n- Semi-gradient SARSA và Semi-gradient Q-Learning là các thuật toán học tăng cường model-free sử dụng phương pháp xấp xỉ hàm giá trị (tuyến tính hoặc phi tuyến tính) để cập nhật trọng số `w` của hàm `Q̂(S, A; w)`. Cả hai thuật toán đều được gọi là \"semi-gradient\" vì chúng chỉ tính gradient đối với tham số `w` của `Q̂(S, A; w)` mà không tính gradient đối với giá trị Q ước tính ở trạng thái tiếp theo. Semi-gradient SARSA là một thuật toán on-policy, cập nhật `w` dựa trên kinh nghiệm `(S, A, R, S', A')` theo công thức `w ← w + α[R + γQ̂(S', A'; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)`. Ngược lại, Semi-gradient Q-Learning là một thuật toán off-policy, cập nhật `w` dựa trên kinh nghiệm `(S, A, R, S')` và giá trị Q tối đa ở trạng thái tiếp theo theo công thức `w ← w + α[R + γ max_a Q̂(S', a; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)`.\n- A' là ký hiệu cho hành động tiếp theo (Next Action) mà agent sẽ thực hiện từ trạng thái S'. Trong thuật toán SARSA, A' được chọn từ trạng thái S' theo chính sách hiện tại (thường là ε-greedy) và được sử dụng để cập nhật Q-value của cặp (S, A).\n\n**Mối quan hệ:**\n- Semi-gradient SARSA tối ưu hóa hàm Q̂(S, A; w) bằng cách cập nhật trọng số w để giảm lỗi Temporal Difference."
        },
        "output_without_context": {
            "question": "Trong thuật toán Sarsa, giá trị Q được cập nhật dựa trên hành động nào sau đây?",
            "answer": "Hành động tiếp theo được thực hiện trong trạng thái tiếp theo."
        }
    },
    {
        "topic_description": "Học sinh sẽ hiểu quá trình lặp lại giữa đánh giá chính sách và cải thiện chính sách trong môi trường không mô hình. Chủ đề này nền tảng cho các khái niệm học tập hiệu quả trong tuần 4 và liên quan đến kiến thức từ tuần 2 về lập trình động.",
        "output_with_context": {
            "question": "Công thức nào được sử dụng trong Policy Evaluation để tính toán hàm giá trị trạng thái V^π(s) cho một chính sách π đã cho?",
            "answer": "V^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- Policy Improvement Theorem phát biểu rằng nếu Q-value của hành động được chọn bởi chính sách mới π' tại trạng thái s, dưới chính sách cũ π (tức là Q^π(s, π'(s))), lớn hơn hoặc bằng V-value của trạng thái s dưới chính sách cũ π (tức là V^π(s)), thì chính sách mới π' sẽ tốt hơn hoặc bằng chính sách cũ π. Điều này có nghĩa là hàm giá trị của chính sách mới π' sẽ lớn hơn hoặc bằng hàm giá trị của chính sách cũ π cho mọi trạng thái s.\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n- Policy Improvement là bước thứ hai trong thuật toán Policy Iteration, nhằm tạo ra một chính sách mới π' tốt hơn hoặc bằng chính sách hiện tại π. Quá trình này dựa trên hàm giá trị V(s) (hoặc V^π(s)) đã được đánh giá từ bước Policy Evaluation. Mục tiêu là cập nhật chính sách hiện tại để trở nên tham lam hơn (greedy), chọn hành động tối ưu tại mỗi trạng thái. Cụ thể, đối với mỗi trạng thái, chính sách mới chọn hành động (a) tối đa hóa Q-value, được tính bằng sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Kỹ thuật này đảm bảo rằng các hành động được chọn mang lại giá trị cao nhất theo hàm giá trị hiện tại.\n\n**Mối quan hệ:**\n- Generalized Policy Iteration (GPI) bao gồm bước Policy Evaluation để đánh giá chính sách hiện tại.\n- Generalized Policy Iteration (GPI) bao gồm bước Policy Improvement để cải thiện chính sách dựa trên đánh giá.\n- Policy Improvement cải thiện chính sách π dựa trên hàm giá trị đã được đánh giá.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n#### 13.1. Approximate Dynamic Programming\n- Sử dụng function approximation\n- Neural networks để biểu diễn V hoặc π\n- Trade-off giữa accuracy và scalability\n\n#### 13.2. Model-Free Methods\n- Không cần biết P và R\n- Học từ experience\n- Temporal-Difference Learning, Q-Learning (phần sau)\n\n#### 13.3. Deep Reinforcement Learning\n- Kết hợp DP với deep learning\n- DQN, Actor-Critic, PPO\n- Giải quyết được bài toán phức tạp\n\n### 14. Code Implementation - Ví dụ Python\n\n#### 14.1. Policy Iteration\n```python\ndef policy_iteration(env, gamma=0.9, theta=1e-6):\n    # Khởi tạo\n    V = np.zeros(env.num_states)\n    policy = np.zeros(env.num_states, dtype=int)\n    \n    while True:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.num_states):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([p * (r + gamma * V[s_]) \n                           for p, s_, r in env.transitions(s, a)])\n                delta = max(delta, abs(v - V[s]))\n            if delta < theta:\n                break\n        \n        # Policy Improvement\n        policy_stable = True\n        for s in range(env.num_states):\n            old_action = policy[s]\n            # Tìm hành động tốt nhất\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            policy[s] = np.argmax(action_values)\n            \n            if old_action != policy[s]:\n                policy_stable = False\n        \n        if policy_stable:\n            return V, policy\n```\n\n#### 14.2. Value Iteration\n```python\ndef value_iteration(env, gamma=0.9, theta=1e-6):\n    V = np.zeros(env.num_states)\n    \n    while True:\n        delta = 0\n        for s in range(env.num_states):\n            v = V[s]\n            # Bellman optimality update\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            V[s] = max(action_values)\n            delta = max(delta, abs(v - V[s]))\n        \n        if delta < theta:\n            break\n    \n    # Trích xuất chính sách\n    policy = np.zeros(env.num_states, dtype=int)\n    for s in range(env.num_states):\n        action_values = []\n        for a in range(env.num_actions):\n            q_value = sum([p * (r + gamma * V[s_]) \n                          for p, s_, r in env.transitions(s, a)])\n            action_values.append(q_value)\n        policy[s] = np.argmax(action_values)\n    \n    return V, policy\n```\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement policy evaluation cho Gridworld\n2. So sánh tốc độ hội tụ của in-place và two-array DP\n3. Visualize quá trình hội tụ của value iteration\n\n#### 15.2. Bài tập nâng cao\n1. Giải Gambler's Problem với các giá trị p khác nhau\n2. Implement prioritized sweeping\n3. Modified policy iteration với k=1, k=3, k=∞\n\n#### 15.3. Dự án\n1. Xây dựng AI cho game 2048 bằng DP\n2. Tối ưu hóa việc sạc pin cho robot\n3. Quản lý danh mục đầu tư bằng MDP\n\n### 16. Kết luận\n\n\n**Các khái niệm quan trọng:**\n- Policy Improvement là bước thứ hai trong thuật toán Policy Iteration, nhằm tạo ra một chính sách mới π' tốt hơn hoặc bằng chính sách hiện tại π. Quá trình này dựa trên hàm giá trị V(s) (hoặc V^π(s)) đã được đánh giá từ bước Policy Evaluation. Mục tiêu là cập nhật chính sách hiện tại để trở nên tham lam hơn (greedy), chọn hành động tối ưu tại mỗi trạng thái. Cụ thể, đối với mỗi trạng thái, chính sách mới chọn hành động (a) tối đa hóa Q-value, được tính bằng sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Kỹ thuật này đảm bảo rằng các hành động được chọn mang lại giá trị cao nhất theo hàm giá trị hiện tại.\n\n**Mối quan hệ:**\n- Generalized Policy Iteration (GPI) bao gồm bước Policy Improvement để cải thiện chính sách dựa trên đánh giá.\n- Policy Improvement cải thiện chính sách π dựa trên hàm giá trị đã được đánh giá.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Khi cần chính sách tốt sớm trong quá trình\n- Không gian hành động nhỏ\n- Có thể dừng sớm khi chính sách đủ tốt\n\n**Value Iteration**:\n- Không gian hành động lớn\n- Cần giải nhanh\n- Chỉ quan tâm đến chính sách cuối cùng\n\n### 7. Asynchronous Dynamic Programming\n\n#### 7.1. Vấn đề của Synchronous DP\n- Cập nhật tất cả trạng thái mỗi iteration\n- Không hiệu quả với không gian trạng thái lớn\n- Lãng phí tính toán cho trạng thái ít quan trọng\n\n#### 7.2. In-Place Dynamic Programming\n\n**Ý tưởng**: Sử dụng giá trị mới ngay khi tính được\n```\nVới mỗi s ∈ S:\n    V(s) = max_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n```\n(không cần lưu bản sao cũ của V)\n\n**Lợi ích**: Hội tụ nhanh hơn, tiết kiệm bộ nhớ\n\n#### 7.3. Prioritized Sweeping\n\n**Ý tưởng**: Ưu tiên cập nhật các trạng thái quan trọng\n```\n1. Khởi tạo hàng đợi ưu tiên Q\n2. Với mỗi s, tính:\n   priority = |max_a[R(s,a) + γΣP(s'|s,a)V(s')] - V(s)|\n3. Chọn s có priority cao nhất, cập nhật V(s)\n4. Cập nhật priority cho các trạng thái tiền nhiệm\n```\n\n**Lợi ích**: Tập trung vào vùng quan trọng của không gian trạng thái\n\n#### 7.4. Real-Time Dynamic Programming\n\n**Đặc điểm**:\n- Chỉ cập nhật các trạng thái mà agent thực sự gặp phải\n- Phù hợp với bài toán có không gian trạng thái rất lớn\n- Học trong quá trình tương tác\n\n### 8. Generalized Policy Iteration (GPI)\n\n#### 8.1. Khái niệm\nGPI là framework chung cho nhiều thuật toán RL:\n```\n        Evaluation\n       ↗          ↘\n    Policy    ←→    Value\n       ↖          ↙\n       Improvement\n```\n\n#### 8.2. Đặc điểm\n- Evaluation: Làm cho V nhất quán với π\n- Improvement: Làm cho π tham lam theo V\n- Hai quá trình cạnh tranh và hợp tác\n- Cuối cùng hội tụ đến optimal\n\n#### 8.3. Các biến thể\n- **Policy Iteration**: Evaluation hoàn toàn trước improvement\n- **Value Iteration**: 1 bước evaluation rồi improvement\n- **Asynchronous DP**: Cập nhật không đồng bộ\n- **Temporal-Difference Learning**: GPI với mẫu (phần sau)\n\n### 9. Efficiency of Dynamic Programming\n\n#### 9.1. Độ phức tạp tính toán\n\n**Worst case**:\n- Số states: n = |S|\n- Số actions: k = |A|\n- Độ phức tạp: O(n² × k) mỗi iteration\n\n**So sánh với các phương pháp khác**:\n- Linear Programming: O(n³) nhưng đảm bảo polynomial\n- Policy Search: Exponential trong worst case\n- DP: Polynomial, hiệu quả với medium-sized problems\n\n#### 9.2. Curse of Dimensionality\n\n**Vấn đề**:\n\n**Các khái niệm quan trọng:**\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n- Generalized Policy Iteration (GPI) là một khuôn khổ chung cho các thuật toán học tăng cường, bao gồm hai quá trình tương tác và cạnh tranh: Policy Evaluation (đánh giá chính sách) và Policy Improvement (cải thiện chính sách). Policy Evaluation làm cho hàm giá trị V nhất quán với chính sách hiện tại π, trong khi Policy Improvement làm cho chính sách π trở nên tham lam hơn dựa trên hàm giá trị V. Hai quá trình này lặp đi lặp lại cho đến khi chính sách và hàm giá trị hội tụ về chính sách tối ưu và hàm giá trị tối ưu.\n\n**Mối quan hệ:**\n- Generalized Policy Iteration (GPI) bao gồm bước Policy Evaluation để đánh giá chính sách hiện tại.\n- Generalized Policy Iteration (GPI) bao gồm bước Policy Improvement để cải thiện chính sách dựa trên đánh giá.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n\n**Mối quan hệ:**\n- Generalized Policy Iteration (GPI) bao gồm bước Policy Evaluation để đánh giá chính sách hiện tại."
        },
        "output_without_context": {
            "question": "Quá trình nào liên quan đến việc lặp lại giữa đánh giá chính sách và cải thiện chính sách trong môi trường không mô hình?",
            "answer": "Lặp lại chính sách tổng quát"
        }
    },
    {
        "topic_description": "Học sinh cần mô tả sự kết hợp giữa Dấu vết đủ điều kiện và các lợi nhuận trong học n-bước, hiểu rằng chúng phù hợp từ cả góc nhìn tiến về phía trước và lùi về phía sau. Chủ đề này trải rộng kiến thức từ tuần 4 tới những phương pháp học khác nhau trong tuần 3.",
        "output_with_context": {
            "question": "Mối quan hệ giữa Dấu vết đủ điều kiện và học n-bước có thể được mô tả như thế nào trong thuật toán n-Step SARSA, đặc biệt là trong việc cập nhật hàm giá trị Q?",
            "answer": "Q(S_t, A_t) ← Q(S_t, A_t) + α[G_t^(n) - Q(S_t, A_t)]",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n             = (1-ε)max_a Q(S',a) + ε · average_a Q(S',a)\n```\n\n#### 6.2. Thuật toán Expected SARSA\n\n```\nKhởi tạo Q(s,a) arbitrarily\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        Chọn A từ S theo π (ví dụ: ε-greedy)\n        Thực hiện A, quan sát R, S'\n        \n        expected_q = Σ_a π(a|S')Q(S',a)\n        Q(S,A) ← Q(S,A) + α[R + γ·expected_q - Q(S,A)]\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 6.3. Đặc điểm\n\n**Ưu điểm**:\n- Lower variance than SARSA (no sampling A')\n- Có thể off-policy nếu behavior ≠ target\n- Computational cost tăng nhưng thường đáng giá\n\n**So sánh**:\n- SARSA: Q(S,A) ← ... + γQ(S',A')  [sample]\n- Expected SARSA: Q(S,A) ← ... + γE[Q(S',·)]  [expectation]\n- Q-Learning: Q(S,A) ← ... + γmax_a Q(S',a)  [max]\n\n### 7. Double Q-Learning\n\n#### 7.1. Maximization Bias Problem\n\n**Vấn đề**:\n```\nQ(s,a) = R(s,a) + γ max_a' Q(s',a')\n              ↑ Overestimate!\n\nE[max(X₁, X₂)] ≥ max(E[X₁], E[X₂])\n```\n\n**Hậu quả**: Q-Learning thường overestimate values\n\n#### 7.2. Giải pháp: Double Q-Learning\n\n**Ý tưởng**: Duy trì 2 Q-functions: Q₁ và Q₂\n\n**Update**:\n```\nVới xác suất 0.5:\n    A* = argmax_a Q₁(S',a)\n    Q₁(S,A) ← Q₁(S,A) + α[R + γQ₂(S',A*) - Q₁(S,A)]\n                                    ↑ Dùng Q₂ để estimate\nNgược lại:\n    A* = argmax_a Q₂(S',a)\n    Q₂(S,A) ← Q₂(S,A) + α[R + γQ₁(S',A*) - Q₂(S,A)]\n                                    ↑ Dùng Q₁ để estimate\n```\n\n#### 7.3. Thuật toán Double Q-Learning\n\n```\nKhởi tạo Q₁(s,a) và Q₂(s,a) arbitrarily, ∀s,a\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        Chọn A từ S theo ε-greedy(Q₁ + Q₂)\n        Thực hiện A, quan sát R, S'\n        \n        Với xác suất 0.5:\n            A* = argmax_a Q₁(S',a)\n            Q₁(S,A) ← Q₁(S,A) + α[R + γQ₂(S',A*) - Q₁(S,A)]\n        Ngược lại:\n            A* = argmax_a Q₂(S',a)\n            Q₂(S,A) ← Q₂(S,A) + α[R + γQ₁(S',A*) - Q₂(S,A)]\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 7.4. Lợi ích\n- Giảm overestimation bias\n- Cải thiện stability và performance\n- Nền tảng cho Double DQN (Deep RL)\n\n### 8. n-Step Methods và Eligibility Traces\n\n#### 8.1. n-Step SARSA\n\n**n-Step Return**:\n```\nG_t^(n) = R_{t+1} + γR_{t+2} + ... + γ^{n-1}R_{t+n} + γ^n Q(S_{t+n}, A_{t+n})\n```\n\n**Update**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[G_t^(n) - Q(S_t, A_t)]\n\n**Các khái niệm quan trọng:**\n- n-Step Methods là một loại kỹ thuật trong Học tăng cường (Reinforcement Learning) kết hợp các ý tưởng từ Monte Carlo và Temporal Difference (TD) learning. Thay vì chỉ sử dụng phần thưởng tức thời và giá trị của trạng thái tiếp theo (như 1-step TD), hoặc toàn bộ chuỗi phần thưởng (như Monte Carlo), n-Step Methods xem xét tổng phần thưởng trong 'n' bước trong tương lai và giá trị của trạng thái sau 'n' bước để cập nhật hàm giá trị (Q-function hoặc V-function). Kỹ thuật này giúp cân bằng giữa phương sai thấp của TD và độ chính xác cao hơn của Monte Carlo, đồng thời cân bằng giữa phương sai và độ lệch.\n\n**Mối quan hệ:**\n- n-Step Methods kết hợp ý tưởng từ Monte Carlo bằng cách xem xét nhiều bước phần thưởng trong tương lai.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Lựa Chọn Đặc Trưng & Tối Ưu Hóa Mô Hình\n- CV cho stable estimate\n\n**1. K-Fold Cross-Validation:**\n\n**Thuật toán:**\n1. Chia data thành k folds\n2. For i = 1 to k:\n   - Use fold i làm validation\n   - Use k-1 folds còn lại làm training\n   - Train và evaluate\n3. Average metrics across k folds\n\n**Chọn k:**\n- k=5: Standard, good balance\n- k=10: More stable, more computational\n- Larger k: Less bias, more variance, more expensive\n\n**Ưu điểm:**\n- Sử dụng toàn bộ data\n- Stable estimate\n- Reduce variance\n\n**Nhược điểm:**\n- k lần training (expensive)\n- Có thể chậm\n\n**2. Stratified K-Fold:**\n\n**Nguyên lý:**\n- Maintain class distribution trong mỗi fold\n- Each fold representative\n\n**Khi nào dùng:**\n- Imbalanced datasets\n- Classification tasks\n- Đảm bảo mỗi fold có đủ samples mỗi class\n\n**Ưu điểm:**\n- Fair evaluation với imbalanced data\n- Consistent class proportions\n\n**3. Leave-One-Out (LOO):**\n\n**Nguyên lý:**\n- k = n (n = số samples)\n- Mỗi sample là một fold\n\n**Ưu điểm:**\n- Maximum data cho training\n- No randomness\n- Deterministic\n\n**Nhược điểm:**\n- Rất chậm (n iterations)\n- High variance\n- Chỉ khả thi với small datasets (< 1000)\n\n**Khi nào dùng:**\n- Very small datasets\n- Need maximum training data\n- Computational resources available\n\n**4. Time Series Cross-Validation:**\n\n**Nguyên lý:**\n- Respect temporal order\n- Train on past, validate on future\n- No data leakage from future\n\n**Expanding Window:**\n```\nFold 1: Train [1:100] → Test [101:120]\nFold 2: Train [1:120] → Test [121:140]\nFold 3: Train [1:140] → Test [141:160]\n```\n\n**Rolling Window:**\n```\nFold 1: Train [1:100] → Test [101:120]\nFold 2: Train [21:120] → Test [121:140]\nFold 3: Train [41:140] → Test [141:160]\n```\n\n**Quan trọng:**\n- **KHÔNG shuffle data**\n- Maintain temporal order\n- Avoid look-ahead bias\n\n**5. Nested Cross-Validation:**\n\n**Nguyên lý:**\n- Outer loop: Model evaluation\n- Inner loop: Hyperparameter tuning\n- Prevents overfitting in parameter selection\n\n**Structure:**\n```\nOuter CV (5-fold):\n  For each outer fold:\n    Inner CV (5-fold):\n      Hyperparameter tuning\n    Train with best params\n    Evaluate on outer fold\n```\n\n**Ưu điểm:**\n- Unbiased performance estimate\n- Proper hyperparameter tuning\n- Gold standard\n\n**Nhược điểm:**\n- Very expensive (k_outer × k_inner trainings)\n- Overkill cho simple problems\n\n**Khi nào dùng:**\n- Need unbiased estimate\n- Publishing results\n- Critical applications\n- Have computational resources\n\n### Learning Curves (Đường Cong Học)\n\nPhân tích hiệu suất mô hình vs kích thước training set.\n\n**Vẽ gì:**\n- X-axis: Training set size\n- Y-axis: Error (hoặc Score)\n- Two curves: Training error & Validation error\n\n**Chẩn Đoán:**\n\n**1. High Bias (Underfitting):**\n```\nTraining error: Cao\nValidation error: Cao\nGap: Nhỏ\nBoth plateau at high error\n```\n**Dấu hiệu:**\n- Cả hai curves plateau\n- Performance kém ngay cả với nhiều data\n- Thêm data không giúp\n\n**Giải pháp:**\n- Increase model complexity\n- Add features\n- Reduce regularization\n- Try complex model\n\n**2. High Variance (Overfitting):**\n```\nTraining error: Thấp\nValidation error: Cao\nGap: Lớn\nGap doesn't close with more data\n\n**Các khái niệm quan trọng:**\n- Learning Curves (Đường cong học) là biểu đồ phân tích hiệu suất của mô hình (thường là lỗi hoặc điểm số) so với kích thước của tập huấn luyện. Biểu đồ này thường có hai đường: đường lỗi huấn luyện (Training error) và đường lỗi validation (Validation error). Learning Curves giúp chẩn đoán các vấn đề như High Bias (Underfitting) và High Variance (Overfitting) của mô hình.\n\n**Mối quan hệ:**\n- Learning Curves chẩn đoán vấn đề High Bias (Underfitting) khi cả training error và validation error đều cao và có khoảng cách nhỏ, cả hai đường đều đạt đến một mức cao và không cải thiện khi thêm dữ liệu.\n- Learning Curves chẩn đoán vấn đề High Variance (Overfitting) khi training error thấp, validation error cao và có một khoảng cách lớn giữa hai đường, khoảng cách này không thu hẹp khi thêm dữ liệu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n    Failure: β_a ← β_a + 1\n```\n\n**Properties**:\n- Optimal regret bounds\n- Often better than UCB trong practice\n\n### 5. Count-Based Exploration\n\n#### 5.1. Exploration Bonuses\n\n**Idea**: Add bonus reward for visiting rare states\n\n**Intrinsic Motivation**:\n```\nr_total = r_extrinsic + β · r_intrinsic\n\nr_intrinsic = 1/√N(s)  hoặc  1/√N(s,a)\n```\n\n**Effect**: Encourage visiting under-explored regions\n\n#### 5.2. Pseudo-Count Methods\n\n**For large state spaces**: Cannot count exactly\n\n**Density Models**:\n```\nρ(s): Estimate density/frequency of state s\nPseudo-count: N(s) ∝ ρ(s) / (1 - ρ(s))\nBonus: r_intrinsic = β/√N(s)\n```\n\n**Examples**:\n- Context Tree Switching (CTS)\n- Neural density models\n\n### 6. Curiosity-Driven Exploration\n\n#### 6.1. Intrinsic Curiosity Module (ICM)\n\n**Components**:\n```\n1. Forward Model: Predict next state feature\n   ŝ_{t+1} = f(s_t, a_t)\n\n2. Inverse Model: Predict action from states\n   â_t = g(s_t, s_{t+1})\n\n3. Intrinsic Reward: Prediction error\n   r_intrinsic = ||ŝ_{t+1} - s_{t+1}||²\n```\n\n**Intuition**: States that are hard to predict are \"interesting\"\n\n#### 6.2. Random Network Distillation (RND)\n\n**Setup**:\n```\nFixed random network: f_target(s)\nLearned predictor: f_pred(s; θ)\n```\n\n**Intrinsic Reward**:\n```\nr_intrinsic = ||f_target(s) - f_pred(s; θ)||²\n```\n\n**Properties**:\n- Novel states have high prediction error\n- Visited states have low error\n- Non-stationary targets avoided\n\n#### 6.3. Never Give Up (NGU)\n\n**Combines**:\n- Episodic novelty (memory-based)\n- Life-long novelty (RND-based)\n\n**Two-timescale curiosity**:\n```\nr_episodic: Within episode novelty\nr_lifelong: Across episodes novelty\n```\n\n### 7. Information-Theoretic Exploration\n\n#### 7.1. Information Gain\n\n**Maximum Information Gain**:\n```\nMaximize: I(Θ; O | a) = H(Θ) - H(Θ|O,a)\nΘ: Parameters/state of world\nO: Observations\n```\n\n**Intuition**: Choose actions that reveal most information\n\n#### 7.2. Entropy Maximization\n\n**Maximum Entropy RL** (covered in SAC):\n```\nπ* = argmax_π E[Σ_t (R_t + α H(π(·|S_t)))]\n```\n\n**Benefits**:\n- Natural exploration\n- Robust policies\n- Multiple solutions\n\n#### 7.3. Empowerment\n\n**Definition**: Mutual information between actions và future states\n```\nEmpowerment = I(A_t; S_{t+k} | S_t)\n```\n\n**Intuition**: Maximize control over future\n\n### 8. Goal-Driven Exploration\n\n#### 8.1. Hindsight Experience Replay (HER)\n\n**Problem**: Sparse rewards → most episodes fail → little learning\n\n**Idea**: Learn from failures by relabeling goals\n```\nOriginal: Goal = g, achieved = g', reward = 0 (failure)\nHER: Goal = g', achieved = g', reward = 0 (success!)\n```\n\n**Algorithm**:\n```\nStore transition (s, a, r, s', g) vào replay buffer\n\nAdditionally store:\n    (s, a, r', s', g') where g' = achieved_goal(s')\n    r' = reward(s', a, g')\n```\n\n**Effect**: Every trajectory teaches something\n\n#### 8.2. Curriculum Learning\n\n**Progressive Difficulty**:\n```\nEasy tasks → Medium tasks → Hard tasks\n```\n\n**Automatic Curriculum**:\n- Track success rates\n- Adjust task distribution\n- Focus on \"frontier\" of capability\n\n### 9. Multi-Agent Exploration\n\n#### 9.1. Population-Based Training\n\n**Idea**: Train population of agents với different hyperparameters\n\n**Process**:\n```\nPopulation: {Agent_1, ..., Agent_N}\n\nPeriodically:\n\n**Các khái niệm quan trọng:**\n- N(s) là số lần trạng thái s đã được thăm, và N(s,a) là số lần cặp trạng thái-hành động (s,a) đã được thăm. Trong Count-Based Exploration, cả N(s) và N(s,a) đều được sử dụng để tính toán phần thưởng nội tại, nhằm khuyến khích agent thăm các trạng thái và khám phá các cặp trạng thái-hành động ít được thăm (có N(s) hoặc N(s,a) thấp).\n\n**Mối quan hệ:**\n- r_intrinsic trong Count-Based Exploration có thể được tính dựa trên N(s,a) theo công thức 1/√N(s,a).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**Nguyên lý:**\n- Anomalies are few và different\n- Easier to isolate than normal points\n- Random partitioning\n\n**Path length:**\n- Normal points: Longer paths\n- Anomalies: Shorter paths\n\n**Thuật toán:**\n1. Build ensemble of isolation trees\n2. Each tree: Random splits\n3. Compute average path length\n4. Shorter paths → Higher anomaly score\n\n**Ưu điểm:**\n- Fast, scalable\n- High-dimensional data\n- Không cần distribution assumption\n\n**Nhược điểm:**\n- Không giải thích tại sao anomaly\n- Random (need multiple trees)\n\n**3. One-Class SVM:**\n\n**Nguyên lý:**\n- Learn boundary around normal data\n- Separate normal from origin\n- Points outside boundary = anomalies\n\n**Objective:**\nMaximize margin from origin to hyperplane\n\n**Ưu điểm:**\n- Kernel trick for non-linear\n- Theoretical foundation\n- Good cho high-dim\n\n**Nhược điểm:**\n- Expensive với large data\n- Sensitive to ν parameter\n- Cần feature scaling\n\n**4. Local Outlier Factor (LOF):**\n\n**Nguyên lý:**\n- Compare local density với neighbors\n- Anomaly có density thấp hơn neighbors\n\n**LOF Score:**\n- ~1: Normal\n- <<1: Denser than neighbors (inlier)\n- >>1: Less dense (outlier)\n\n**Ưu điểm:**\n- Local anomalies\n- Varying densities\n\n**Nhược điểm:**\n- Expensive (O(N²))\n- Sensitive to k\n\n**5. Autoencoders:**\n\n**Nguyên lý:**\n- Neural network học reconstruct normal data\n- Anomalies have high reconstruction error\n\n**Reconstruction error:**\n$$Error = ||x - \\hat{x}||^2$$\n\n- Threshold: Mean + k × std\n- Points above threshold = anomalies\n\n**Ưu điểm:**\n- Non-linear patterns\n- High-dimensional\n- Deep representations\n\n**Nhược điểm:**\n- Need training data (mostly normal)\n- Computationally expensive\n- Hyperparameter tuning\n\n**Ứng dụng:**\n- Fraud detection\n- Network intrusion\n- Manufacturing defects\n- Medical diagnosis\n- System monitoring\n\n### Association Rule Learning\n\nKhám phá mối quan hệ giữa các biến.\n\n**Market Basket Analysis:**\nTìm sản phẩm thường được mua cùng nhau.\n\n**Terminology:**\n- **Itemset:** Tập hợp items {Milk, Bread}\n- **Transaction:** Một lần mua hàng\n- **Rule:** A → B (If buy A, then buy B)\n\n**Metrics:**\n\n**1. Support:**\n$$Support(A) = \frac{\text{Transactions containing A}}{\text{Total transactions}}$$\n\n- Tỷ lệ transactions có itemset\n- Popular items có support cao\n\n**2. Confidence:**\n$$Confidence(A \rightarrow B) = \frac{Support(A \\cup B)}{Support(A)} = P(B|A)$$\n\n- Tỷ lệ transactions có B trong số có A\n- Strength của rule\n\n**3. Lift:**\n$$Lift(A \rightarrow B) = \frac{Confidence(A \rightarrow B)}{Support(B)} = \frac{P(A \\cap B)}{P(A)P(B)}$$\n\n- Measure of association\n- Lift = 1: Independent\n- Lift > 1: Positive correlation\n- Lift < 1: Negative correlation\n\n**Ví dụ:**\n- Rule: {Milk} → {Bread}\n- Support({Milk, Bread}) = 0.3 (30% transactions)\n- Confidence = 0.6 (60% người mua Milk cũng mua Bread)\n- Lift = 1.2 (Buying Milk tăng 20% khả năng mua Bread)\n\n**Algorithms:**\n\n**1. Apriori Algorithm:**\n\n**Nguyên lý:**\n- If itemset frequent → all subsets frequent\n- If itemset infrequent → all supersets infrequent\n\n**Steps:**\n1. Find frequent 1-itemsets (support ≥ min_support)\n2. Generate candidate k-itemsets from frequent (k-1)-itemsets\n3. Prune candidates using apriori property\n4. Count support và find frequent k-itemsets\n\n**Các khái niệm quan trọng:**\n- Rule (luật kết hợp) là một biểu thức có dạng A → B, có nghĩa là \"Nếu mua A, thì có khả năng mua B\". Nó mô tả mối quan hệ giữa hai itemset.\n\n**Mối quan hệ:**\n- Apriori Algorithm tạo ra các Rule (luật kết hợp) từ các itemset thường xuyên đã tìm được."
        },
        "output_without_context": {
            "question": "Mối quan hệ giữa Dấu vết đủ điều kiện và học n-bước có thể được phân tích từ góc độ nào để hiểu rõ hơn về lợi nhuận trong quá trình học?",
            "answer": "Cả góc nhìn tiến về phía trước và lùi về phía sau."
        }
    }
]