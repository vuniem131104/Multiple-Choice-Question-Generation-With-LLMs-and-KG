[
    {
        "topic_description": "Chủ đề này khám phá khái niệm lập trình động (Dynamic Programming - DP), nơi bao gồm các thuộc tính cần thiết để áp dụng DP như cấu trúc con tối ưu và bài toán con chồng chéo. Học sinh sẽ được kiểm tra khả năng định nghĩa và đưa ra ví dụ về ứng dụng của DP trong các bài toán thực tế.",
        "output_with_context": {
            "question": "Dynamic Programming (DP) là gì?",
            "answer": "Dynamic Programming là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Dynamic Programming (DP) là một tập hợp các kỹ thuật và một nền tảng lý thuyết quan trọng trong Reinforcement Learning (RL). DP giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng. Trong RL, DP cung cấp một framework lý thuyết để hiểu về các hàm giá trị tối ưu và chính sách tối ưu, đồng thời là cơ sở thuật toán cho các phương pháp model-free. DP được sử dụng để giải quyết các bài toán Markov Decision Process (MDP) khi mô hình môi trường (P và R) được biết hoàn toàn (model-based), sử dụng phương trình Bellman để tính toán hàm giá trị. Các thuật toán như Policy Iteration và Value Iteration thuộc nhóm này. DP đảm bảo tìm ra chính sách tối ưu nhưng bị hạn chế bởi \"curse of dimensionality\" và yêu cầu biết mô hình môi trường.\n- Overlapping Subproblems là một điều kiện áp dụng Dynamic Programming, nghĩa là các bài toán con được giải đi giải lại nhiều lần, cho phép lưu trữ và tái sử dụng kết quả để tăng hiệu quả.\n- Optimal Substructure là một điều kiện áp dụng Dynamic Programming, nghĩa là nghiệm tối ưu của bài toán lớn có thể được xây dựng từ nghiệm tối ưu của các bài toán con.\n- Dynamic Programming (Quy hoạch động) là một kỹ thuật giải thuật bằng cách chia bài toán lớn thành các bài toán con chồng chéo và lưu trữ kết quả của các bài toán con để tránh tính toán lại. Trong Floyd-Warshall, nó được sử dụng để tính toán đường đi ngắn nhất qua các tập hợp đỉnh trung gian tăng dần.\n\n**Mối quan hệ:**\n- Dynamic Programming cung cấp một framework lý thuyết để hiểu rõ về các chính sách tối ưu.\n- Dynamic Programming cung cấp một framework lý thuyết để hiểu rõ về các hàm giá trị tối ưu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Ví dụ: Bàn cờ vây 19×19 có ~10^170 trạng thái\n\n**Giải pháp**:\n- Function approximation (phần sau)\n- Sampling-based methods\n- Hierarchical RL\n\n### 10. Ví dụ thực tế: Car Rental Problem\n\n#### 10.1. Mô tả bài toán\n**Jack's Car Rental**:\n- 2 địa điểm cho thuê xe\n- Mỗi đêm có thể di chuyển tối đa 5 xe giữa 2 địa điểm\n- Chi phí di chuyển: $2/xe\n- Thu nhập thuê xe: $10/xe\n- Số xe được thuê/trả theo phân phối Poisson\n\n#### 10.2. Biểu diễn MDP\n```\nState: (n1, n2) - số xe tại mỗi địa điểm (0-20)\nAction: -5 đến +5 (số xe di chuyển từ địa điểm 1 đến 2)\nReward: 10 × (số xe thuê được) - 2 × |action|\nTransition: Theo phân phối Poisson\n```\n\n#### 10.3. Giải bằng Policy Iteration\n\n**Chính sách khởi tạo**: Không di chuyển xe\n**Sau iteration 1**: Di chuyển xe từ địa điểm thừa sang thiếu\n**Hội tụ**: ~4-5 iterations\n**Chính sách tối ưu**: Cân bằng số xe giữa 2 địa điểm\n\n#### 10.4. Kết quả\n```\nV*(10,10) ≈ $500\nChiến lược: Luôn giữ ~10 xe mỗi địa điểm\n```\n\n### 11. Ví dụ thực tế: Inventory Management\n\n#### 11.1. Bài toán quản lý kho\n\n**Setup**:\n- Tồn kho từ 0 đến MAX_INVENTORY\n- Mỗi kỳ: Quyết định đặt hàng bao nhiêu\n- Chi phí đặt hàng + chi phí lưu kho\n- Doanh thu từ bán hàng\n- Mất khách nếu hết hàng\n\n#### 11.2. MDP Formulation\n```\nState: Mức tồn kho hiện tại\nAction: Số lượng đặt hàng\nReward: Revenue - Ordering_cost - Holding_cost - Stockout_penalty\nTransition: Stochastic demand\n```\n\n#### 11.3. Giải pháp\n- Sử dụng Value Iteration\n- Tìm chính sách (s,S): Đặt hàng đến mức S khi tồn kho ≤ s\n- Optimize (s,S) parameters\n\n### 12. Ưu điểm và hạn chế của Dynamic Programming\n\n#### 12.1. Ưu điểm\n✅ **Đảm bảo tối ưu**: Hội tụ đến chính sách tối ưu π*\n✅ **Cơ sở lý thuyết vững chắc**: Dựa trên phương trình Bellman\n✅ **Hiệu quả với medium-sized problems**: Polynomial complexity\n✅ **Framework cho nhiều thuật toán khác**: Nền tảng cho TD, Q-learning\n\n#### 12.2. Hạn chế\n❌ **Yêu cầu biết hoàn toàn mô hình**: Cần biết P và R\n❌ **Curse of dimensionality**: Không mở rộng cho không gian lớn\n❌ **Cần duyệt tất cả trạng thái**: Không practical với continuous states\n❌ **Computation cost**: Tốn nhiều tính toán mỗi iteration\n\n### 13. Mở rộng và cải tiến\n\n\n**Các khái niệm quan trọng:**\n- Dynamic Programming (DP) là một tập hợp các kỹ thuật và một nền tảng lý thuyết quan trọng trong Reinforcement Learning (RL). DP giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng. Trong RL, DP cung cấp một framework lý thuyết để hiểu về các hàm giá trị tối ưu và chính sách tối ưu, đồng thời là cơ sở thuật toán cho các phương pháp model-free. DP được sử dụng để giải quyết các bài toán Markov Decision Process (MDP) khi mô hình môi trường (P và R) được biết hoàn toàn (model-based), sử dụng phương trình Bellman để tính toán hàm giá trị. Các thuật toán như Policy Iteration và Value Iteration thuộc nhóm này. DP đảm bảo tìm ra chính sách tối ưu nhưng bị hạn chế bởi \"curse of dimensionality\" và yêu cầu biết mô hình môi trường.\n- Dynamic Programming (Quy hoạch động) là một kỹ thuật giải thuật bằng cách chia bài toán lớn thành các bài toán con chồng chéo và lưu trữ kết quả của các bài toán con để tránh tính toán lại. Trong Floyd-Warshall, nó được sử dụng để tính toán đường đi ngắn nhất qua các tập hợp đỉnh trung gian tăng dần.\n\n**Mối quan hệ:**\n- Dynamic Programming cung cấp một framework lý thuyết để hiểu rõ về các chính sách tối ưu.\n- Dynamic Programming cung cấp một framework lý thuyết để hiểu rõ về các hàm giá trị tối ưu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\nDynamic Programming là nền tảng quan trọng trong Reinforcement Learning. Mặc dù có hạn chế về yêu cầu biết mô hình và curse of dimensionality, DP cung cấp:\n\n- **Framework lý thuyết**: Hiểu rõ về optimal value functions và policies\n- **Nền tảng thuật toán**: Cơ sở cho các phương pháp model-free\n- **Công cụ thực tế**: Giải quyết được nhiều bài toán thực tế\n\nCác khái niệm từ DP (đặc biệt là GPI và Bellman equations) sẽ xuất hiện xuyên suốt trong các phương pháp học tăng cường tiếp theo, từ Temporal-Difference Learning đến Deep Q-Networks.\n\n---\n\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Learning\n\n#### 1.1. Định nghĩa\nModel-Free Learning là các phương pháp học tăng cường không yêu cầu biết trước mô hình môi trường (hàm chuyển trạng thái P và hàm phần thưởng R). Agent học trực tiếp từ kinh nghiệm tương tác với môi trường.\n\n#### 1.2. So sánh Model-Based vs Model-Free\n\n| Đặc điểm | Model-Based | Model-Free |\n|----------|-------------|------------|\n| Yêu cầu mô hình | Cần biết P và R | Không cần |\n| Học từ | Phương trình Bellman | Kinh nghiệm thực tế |\n| Ưu điểm | Hiệu quả dữ liệu | Linh hoạt, thực tế |\n| Nhược điểm | Khó có mô hình chính xác | Cần nhiều dữ liệu |\n| Ví dụ | DP, Model-Based RL | MC, TD, Q-Learning |\n\n#### 1.3. Bài toán Prediction\n**Mục tiêu**: Đánh giá một chính sách π cho trước\n- Input: Chính sách π\n- Output: Hàm giá trị V^π(s)\n- Không cần biết mô hình môi trường\n\n### 2. Monte Carlo Methods - Phương Pháp Monte Carlo\n\n#### 2.1. Ý tưởng cơ bản\nMonte Carlo (MC) ước lượng giá trị của trạng thái bằng cách lấy trung bình các return thực tế nhận được từ nhiều episodes.\n\n**Nguyên lý**:\n```\nV^π(s) = E_π[G_t | S_t = s]\n       ≈ average of returns từ trạng thái s\n```\n\n#### 2.2. Episode và Return\n\n**Episode**: Một chuỗi hoàn chỉnh từ trạng thái ban đầu đến kết thúc\n```\nS_0, A_0, R_1, S_1, A_1, R_2, ..., S_T\n```\n\n**Return**: Tổng phần thưởng chiết khấu\n```\nG_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... + γ^{T-t-1}R_T\n```\n\n#### 2.3. First-Visit Monte Carlo\n\n**Thuật toán**:\n```\nKhởi tạo:\n    V(s) = 0, ∀s\n    Returns(s) = danh sách rỗng, ∀s\n\nVới mỗi episode:\n    Tạo một episode tuân theo π: S_0, A_0, R_1, ..., S_T\n    G = 0\n    \n    Với mỗi bước t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        \n\n**Các khái niệm quan trọng:**\n- Dynamic Programming (DP) là một tập hợp các kỹ thuật và một nền tảng lý thuyết quan trọng trong Reinforcement Learning (RL). DP giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng. Trong RL, DP cung cấp một framework lý thuyết để hiểu về các hàm giá trị tối ưu và chính sách tối ưu, đồng thời là cơ sở thuật toán cho các phương pháp model-free. DP được sử dụng để giải quyết các bài toán Markov Decision Process (MDP) khi mô hình môi trường (P và R) được biết hoàn toàn (model-based), sử dụng phương trình Bellman để tính toán hàm giá trị. Các thuật toán như Policy Iteration và Value Iteration thuộc nhóm này. DP đảm bảo tìm ra chính sách tối ưu nhưng bị hạn chế bởi \"curse of dimensionality\" và yêu cầu biết mô hình môi trường.\n- Dynamic Programming (Quy hoạch động) là một kỹ thuật giải thuật bằng cách chia bài toán lớn thành các bài toán con chồng chéo và lưu trữ kết quả của các bài toán con để tránh tính toán lại. Trong Floyd-Warshall, nó được sử dụng để tính toán đường đi ngắn nhất qua các tập hợp đỉnh trung gian tăng dần.\n\n**Mối quan hệ:**\n- Dynamic Programming cung cấp một framework lý thuyết để hiểu rõ về các chính sách tối ưu."
        },
        "output_without_context": {
            "question": "Lập trình động (Dynamic Programming) yêu cầu hai thuộc tính chính nào để áp dụng hiệu quả?",
            "answer": "Cấu trúc con tối ưu và bài toán con chồng chéo."
        }
    },
    {
        "topic_description": "Chủ đề này tập trung vào quy trình đánh giá một chính sách đã cho thông qua phương pháp đánh giá chính sách lặp (Iterative Policy Evaluation). Học sinh sẽ được yêu cầu hiểu công thức và cách áp dụng sao lưu kỳ vọng Bellman để cập nhật các giá trị trạng thái. Ví dụ minh họa thực tế sẽ giúp kiểm tra khả năng áp dụng.",
        "output_with_context": {
            "question": "Công thức nào được sử dụng trong phương pháp đánh giá chính sách lặp để cập nhật giá trị trạng thái V(s)?",
            "answer": "V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n- Policy Evaluation là quá trình ước lượng hàm giá trị (V^π hoặc Q^π) cho một chính sách π đã cho. Trong Model-Free Control, Policy Evaluation phải được thực hiện mà không cần biết mô hình môi trường, thường thông qua các phương pháp như Monte Carlo hoặc Temporal Difference.\n\n**Mối quan hệ:**\n- Policy Iteration sử dụng Policy Evaluation để tính toán hàm giá trị V^π cho chính sách hiện tại trong mỗi lần lặp.\n- Policy Iteration sử dụng bước Policy Evaluation để đánh giá chính sách hiện tại.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n- Policy Evaluation là quá trình ước lượng hàm giá trị (V^π hoặc Q^π) cho một chính sách π đã cho. Trong Model-Free Control, Policy Evaluation phải được thực hiện mà không cần biết mô hình môi trường, thường thông qua các phương pháp như Monte Carlo hoặc Temporal Difference.\n\n**Mối quan hệ:**\n- Policy Iteration sử dụng Policy Evaluation để tính toán hàm giá trị V^π cho chính sách hiện tại trong mỗi lần lặp.\n- Policy Iteration sử dụng bước Policy Evaluation để đánh giá chính sách hiện tại.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n#### 13.1. Approximate Dynamic Programming\n- Sử dụng function approximation\n- Neural networks để biểu diễn V hoặc π\n- Trade-off giữa accuracy và scalability\n\n#### 13.2. Model-Free Methods\n- Không cần biết P và R\n- Học từ experience\n- Temporal-Difference Learning, Q-Learning (phần sau)\n\n#### 13.3. Deep Reinforcement Learning\n- Kết hợp DP với deep learning\n- DQN, Actor-Critic, PPO\n- Giải quyết được bài toán phức tạp\n\n### 14. Code Implementation - Ví dụ Python\n\n#### 14.1. Policy Iteration\n```python\ndef policy_iteration(env, gamma=0.9, theta=1e-6):\n    # Khởi tạo\n    V = np.zeros(env.num_states)\n    policy = np.zeros(env.num_states, dtype=int)\n    \n    while True:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.num_states):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([p * (r + gamma * V[s_]) \n                           for p, s_, r in env.transitions(s, a)])\n                delta = max(delta, abs(v - V[s]))\n            if delta < theta:\n                break\n        \n        # Policy Improvement\n        policy_stable = True\n        for s in range(env.num_states):\n            old_action = policy[s]\n            # Tìm hành động tốt nhất\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            policy[s] = np.argmax(action_values)\n            \n            if old_action != policy[s]:\n                policy_stable = False\n        \n        if policy_stable:\n            return V, policy\n```\n\n#### 14.2. Value Iteration\n```python\ndef value_iteration(env, gamma=0.9, theta=1e-6):\n    V = np.zeros(env.num_states)\n    \n    while True:\n        delta = 0\n        for s in range(env.num_states):\n            v = V[s]\n            # Bellman optimality update\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            V[s] = max(action_values)\n            delta = max(delta, abs(v - V[s]))\n        \n        if delta < theta:\n            break\n    \n    # Trích xuất chính sách\n    policy = np.zeros(env.num_states, dtype=int)\n    for s in range(env.num_states):\n        action_values = []\n        for a in range(env.num_actions):\n            q_value = sum([p * (r + gamma * V[s_]) \n                          for p, s_, r in env.transitions(s, a)])\n            action_values.append(q_value)\n        policy[s] = np.argmax(action_values)\n    \n    return V, policy\n```\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement policy evaluation cho Gridworld\n2. So sánh tốc độ hội tụ của in-place và two-array DP\n3. Visualize quá trình hội tụ của value iteration\n\n#### 15.2. Bài tập nâng cao\n1. Giải Gambler's Problem với các giá trị p khác nhau\n2. Implement prioritized sweeping\n3. Modified policy iteration với k=1, k=3, k=∞\n\n#### 15.3. Dự án\n1. Xây dựng AI cho game 2048 bằng DP\n2. Tối ưu hóa việc sạc pin cho robot\n3. Quản lý danh mục đầu tư bằng MDP\n\n### 16. Kết luận\n\n\n**Các khái niệm quan trọng:**\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n- Policy Evaluation là quá trình ước lượng hàm giá trị (V^π hoặc Q^π) cho một chính sách π đã cho. Trong Model-Free Control, Policy Evaluation phải được thực hiện mà không cần biết mô hình môi trường, thường thông qua các phương pháp như Monte Carlo hoặc Temporal Difference.\n\n**Mối quan hệ:**\n- Policy Iteration sử dụng bước Policy Evaluation để đánh giá chính sách hiện tại."
        },
        "output_without_context": {
            "question": "Trong phương pháp đánh giá chính sách lặp, công thức nào được sử dụng để cập nhật giá trị trạng thái?",
            "answer": "Công thức sao lưu kỳ vọng Bellman."
        }
    },
    {
        "topic_description": "Chủ đề này tập trung vào việc lặp lại giữa đánh giá và cải thiện chính sách trong lập trình động. Học sinh sẽ phải chứng minh khả năng mô tả quy trình này, cùng với các công thức liên quan và cách hội tụ đến chính sách tối ưu.",
        "output_with_context": {
            "question": "Trong quy trình lặp chính sách, công thức nào được sử dụng để cải thiện chính sách mới π' từ chính sách hiện tại π dựa trên hàm giá trị V^π?",
            "answer": "π'(s) = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- Policy Improvement Theorem phát biểu rằng nếu Q-value của hành động được chọn bởi chính sách mới π' tại trạng thái s, dưới chính sách cũ π (tức là Q^π(s, π'(s))), lớn hơn hoặc bằng V-value của trạng thái s dưới chính sách cũ π (tức là V^π(s)), thì chính sách mới π' sẽ tốt hơn hoặc bằng chính sách cũ π. Điều này có nghĩa là hàm giá trị của chính sách mới π' sẽ lớn hơn hoặc bằng hàm giá trị của chính sách cũ π cho mọi trạng thái s.\n- Policy Improvement là bước thứ hai trong thuật toán Policy Iteration, nhằm tạo ra một chính sách mới π' tốt hơn hoặc bằng chính sách hiện tại π. Quá trình này dựa trên hàm giá trị V(s) (hoặc V^π(s)) đã được đánh giá từ bước Policy Evaluation. Mục tiêu là cập nhật chính sách hiện tại để trở nên tham lam hơn (greedy), chọn hành động tối ưu tại mỗi trạng thái. Cụ thể, đối với mỗi trạng thái, chính sách mới chọn hành động (a) tối đa hóa Q-value, được tính bằng sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Kỹ thuật này đảm bảo rằng các hành động được chọn mang lại giá trị cao nhất theo hàm giá trị hiện tại.\n\n**Mối quan hệ:**\n- Policy Iteration sử dụng Policy Improvement để tạo ra một chính sách mới π' tốt hơn dựa trên hàm giá trị V^π đã được đánh giá.\n- Policy Improvement cải thiện chính sách π dựa trên hàm giá trị đã được đánh giá.\n- Policy Iteration sử dụng bước Policy Improvement để cải thiện chính sách.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n- Iterative Policy Evaluation cập nhật giá trị của V^π(s) trong mỗi lần lặp cho đến khi hội tụ.\n- Policy Iteration sử dụng Policy Improvement để tạo ra một chính sách mới, tham lam hơn dựa trên Vπ.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n\n**Mối quan hệ:**\n- Iterative Policy Evaluation cập nhật giá trị của V^π(s) trong mỗi lần lặp cho đến khi hội tụ.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Ví dụ: Bàn cờ vây 19×19 có ~10^170 trạng thái\n\n**Giải pháp**:\n- Function approximation (phần sau)\n- Sampling-based methods\n- Hierarchical RL\n\n### 10. Ví dụ thực tế: Car Rental Problem\n\n#### 10.1. Mô tả bài toán\n**Jack's Car Rental**:\n- 2 địa điểm cho thuê xe\n- Mỗi đêm có thể di chuyển tối đa 5 xe giữa 2 địa điểm\n- Chi phí di chuyển: $2/xe\n- Thu nhập thuê xe: $10/xe\n- Số xe được thuê/trả theo phân phối Poisson\n\n#### 10.2. Biểu diễn MDP\n```\nState: (n1, n2) - số xe tại mỗi địa điểm (0-20)\nAction: -5 đến +5 (số xe di chuyển từ địa điểm 1 đến 2)\nReward: 10 × (số xe thuê được) - 2 × |action|\nTransition: Theo phân phối Poisson\n```\n\n#### 10.3. Giải bằng Policy Iteration\n\n**Chính sách khởi tạo**: Không di chuyển xe\n**Sau iteration 1**: Di chuyển xe từ địa điểm thừa sang thiếu\n**Hội tụ**: ~4-5 iterations\n**Chính sách tối ưu**: Cân bằng số xe giữa 2 địa điểm\n\n#### 10.4. Kết quả\n```\nV*(10,10) ≈ $500\nChiến lược: Luôn giữ ~10 xe mỗi địa điểm\n```\n\n### 11. Ví dụ thực tế: Inventory Management\n\n#### 11.1. Bài toán quản lý kho\n\n**Setup**:\n- Tồn kho từ 0 đến MAX_INVENTORY\n- Mỗi kỳ: Quyết định đặt hàng bao nhiêu\n- Chi phí đặt hàng + chi phí lưu kho\n- Doanh thu từ bán hàng\n- Mất khách nếu hết hàng\n\n#### 11.2. MDP Formulation\n```\nState: Mức tồn kho hiện tại\nAction: Số lượng đặt hàng\nReward: Revenue - Ordering_cost - Holding_cost - Stockout_penalty\nTransition: Stochastic demand\n```\n\n#### 11.3. Giải pháp\n- Sử dụng Value Iteration\n- Tìm chính sách (s,S): Đặt hàng đến mức S khi tồn kho ≤ s\n- Optimize (s,S) parameters\n\n### 12. Ưu điểm và hạn chế của Dynamic Programming\n\n#### 12.1. Ưu điểm\n✅ **Đảm bảo tối ưu**: Hội tụ đến chính sách tối ưu π*\n✅ **Cơ sở lý thuyết vững chắc**: Dựa trên phương trình Bellman\n✅ **Hiệu quả với medium-sized problems**: Polynomial complexity\n✅ **Framework cho nhiều thuật toán khác**: Nền tảng cho TD, Q-learning\n\n#### 12.2. Hạn chế\n❌ **Yêu cầu biết hoàn toàn mô hình**: Cần biết P và R\n❌ **Curse of dimensionality**: Không mở rộng cho không gian lớn\n❌ **Cần duyệt tất cả trạng thái**: Không practical với continuous states\n❌ **Computation cost**: Tốn nhiều tính toán mỗi iteration\n\n### 13. Mở rộng và cải tiến\n\n\n**Các khái niệm quan trọng:**\n- Optimality là tính chất đảm bảo rằng một thuật toán hoặc chính sách đạt được kết quả tốt nhất có thể (tối đa hóa tổng Reward chiết khấu). Dynamic Programming (Policy Iteration, Value Iteration) có ưu điểm là đảm bảo hội tụ đến chính sách tối ưu π*.\n\n**Mối quan hệ:**\n- Dynamic Programming có ưu điểm là đảm bảo tối ưu, hội tụ đến chính sách tối ưu π*."
        },
        "output_without_context": {
            "question": "Trong quy trình lặp chính sách, bước nào là cần thiết để cải thiện chính sách hiện tại nhằm đạt được chính sách tối ưu?",
            "answer": "Đánh giá và điều chỉnh chính sách dựa trên phản hồi từ môi trường."
        }
    },
    {
        "topic_description": "Chủ đề này khám phá nguyên tắc tối ưu trong MDPs, trong đó mỗi chính sách tối ưu có thể phân tách thành hai thành phần. Học sinh sẽ phải nhận diện và phân tích tính chất này cũng như ứng dụng của nó trong việc đạt được giá trị tối ưu từ một trạng thái nhất định.",
        "output_with_context": {
            "question": "Chính sách tối ưu π* trong MDP được xác định như thế nào dựa trên hàm giá trị Q*?",
            "answer": "π*(s) = argmaxₐ Q*(s,a)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- Linear Programming là một kỹ thuật giải MDP bằng cách biểu diễn bài toán dưới dạng một bài toán quy hoạch tuyến tính. Sau đó, bài toán có thể được giải bằng các bộ giải quy hoạch tuyến tính (LP solver) tiêu chuẩn để tìm ra chính sách tối ưu.\n- Optimal Policy (Chính sách tối ưu) là một chính sách π* mà khi được tuân theo, sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng từ mọi trạng thái. Nó đảm bảo rằng hàm giá trị của chính sách tối ưu Vᵖ*(s) lớn hơn hoặc bằng hàm giá trị của bất kỳ chính sách nào khác Vᵖ(s) cho mọi trạng thái s.\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- Markov Decision Process (MDP) là một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự, nơi một agent tương tác với môi trường để tối đa hóa tổng phần thưởng chiết khấu. Trong MDP, kết quả của các quyết định có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường (Reinforcement Learning) và được định nghĩa bởi các thành phần: tập trạng thái S, tập hành động A, hàm phần thưởng R, xác suất chuyển trạng thái P, và discount factor γ.\n\n**Mối quan hệ:**\n- Linear Programming giải quyết bài toán tìm chính sách tối ưu π* bằng cách chuyển đổi nó thành một bài toán quy hoạch tuyến tính.\n- Markov Decision Processes có thành phần là State (Trạng thái), mô tả tình hình hiện tại của môi trường.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Markov Decision Process (MDP) là một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự, nơi một agent tương tác với môi trường để tối đa hóa tổng phần thưởng chiết khấu. Trong MDP, kết quả của các quyết định có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường (Reinforcement Learning) và được định nghĩa bởi các thành phần: tập trạng thái S, tập hành động A, hàm phần thưởng R, xác suất chuyển trạng thái P, và discount factor γ.\n- Optimal Substructure là một điều kiện áp dụng Dynamic Programming, nghĩa là nghiệm tối ưu của bài toán lớn có thể được xây dựng từ nghiệm tối ưu của các bài toán con.\n\n**Mối quan hệ:**\n- Markov Decision Processes có thành phần là State (Trạng thái), mô tả tình hình hiện tại của môi trường.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Ví dụ: Bàn cờ vây 19×19 có ~10^170 trạng thái\n\n**Giải pháp**:\n- Function approximation (phần sau)\n- Sampling-based methods\n- Hierarchical RL\n\n### 10. Ví dụ thực tế: Car Rental Problem\n\n#### 10.1. Mô tả bài toán\n**Jack's Car Rental**:\n- 2 địa điểm cho thuê xe\n- Mỗi đêm có thể di chuyển tối đa 5 xe giữa 2 địa điểm\n- Chi phí di chuyển: $2/xe\n- Thu nhập thuê xe: $10/xe\n- Số xe được thuê/trả theo phân phối Poisson\n\n#### 10.2. Biểu diễn MDP\n```\nState: (n1, n2) - số xe tại mỗi địa điểm (0-20)\nAction: -5 đến +5 (số xe di chuyển từ địa điểm 1 đến 2)\nReward: 10 × (số xe thuê được) - 2 × |action|\nTransition: Theo phân phối Poisson\n```\n\n#### 10.3. Giải bằng Policy Iteration\n\n**Chính sách khởi tạo**: Không di chuyển xe\n**Sau iteration 1**: Di chuyển xe từ địa điểm thừa sang thiếu\n**Hội tụ**: ~4-5 iterations\n**Chính sách tối ưu**: Cân bằng số xe giữa 2 địa điểm\n\n#### 10.4. Kết quả\n```\nV*(10,10) ≈ $500\nChiến lược: Luôn giữ ~10 xe mỗi địa điểm\n```\n\n### 11. Ví dụ thực tế: Inventory Management\n\n#### 11.1. Bài toán quản lý kho\n\n**Setup**:\n- Tồn kho từ 0 đến MAX_INVENTORY\n- Mỗi kỳ: Quyết định đặt hàng bao nhiêu\n- Chi phí đặt hàng + chi phí lưu kho\n- Doanh thu từ bán hàng\n- Mất khách nếu hết hàng\n\n#### 11.2. MDP Formulation\n```\nState: Mức tồn kho hiện tại\nAction: Số lượng đặt hàng\nReward: Revenue - Ordering_cost - Holding_cost - Stockout_penalty\nTransition: Stochastic demand\n```\n\n#### 11.3. Giải pháp\n- Sử dụng Value Iteration\n- Tìm chính sách (s,S): Đặt hàng đến mức S khi tồn kho ≤ s\n- Optimize (s,S) parameters\n\n### 12. Ưu điểm và hạn chế của Dynamic Programming\n\n#### 12.1. Ưu điểm\n✅ **Đảm bảo tối ưu**: Hội tụ đến chính sách tối ưu π*\n✅ **Cơ sở lý thuyết vững chắc**: Dựa trên phương trình Bellman\n✅ **Hiệu quả với medium-sized problems**: Polynomial complexity\n✅ **Framework cho nhiều thuật toán khác**: Nền tảng cho TD, Q-learning\n\n#### 12.2. Hạn chế\n❌ **Yêu cầu biết hoàn toàn mô hình**: Cần biết P và R\n❌ **Curse of dimensionality**: Không mở rộng cho không gian lớn\n❌ **Cần duyệt tất cả trạng thái**: Không practical với continuous states\n❌ **Computation cost**: Tốn nhiều tính toán mỗi iteration\n\n### 13. Mở rộng và cải tiến\n\n\n**Các khái niệm quan trọng:**\n- Optimality là tính chất đảm bảo rằng một thuật toán hoặc chính sách đạt được kết quả tốt nhất có thể (tối đa hóa tổng Reward chiết khấu). Dynamic Programming (Policy Iteration, Value Iteration) có ưu điểm là đảm bảo hội tụ đến chính sách tối ưu π*.\n\n**Mối quan hệ:**\n- Dynamic Programming có ưu điểm là đảm bảo tối ưu, hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Linear Programming giải quyết bài toán tìm chính sách tối ưu π* bằng cách chuyển đổi nó thành một bài toán quy hoạch tuyến tính.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*."
        },
        "output_without_context": {
            "question": "Trong nguyên tắc tối ưu của MDPs, mỗi chính sách tối ưu có thể được phân tách thành những thành phần nào?",
            "answer": "Giá trị tối ưu từ trạng thái và chính sách tối ưu."
        }
    },
    {
        "topic_description": "Chủ đề này tập trung vào vấn đề tìm chính sách tối ưu thông qua phương pháp lặp giá trị (Value Iteration). Học sinh sẽ phải hiểu và áp dụng công thức để tìm giá trị tối ưu, cũng như sự khác biệt giữa lặp giá trị và lặp chính sách. Các ví dụ thực tế sẽ được sử dụng để kiểm tra khả năng giải quyết vấn đề phức tạp.",
        "output_with_context": {
            "question": "Trong phương pháp lặp giá trị, công thức nào được sử dụng để cập nhật hàm giá trị V(s) của mỗi trạng thái?",
            "answer": "V[s] = max(action_values) với q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Ví dụ: Bàn cờ vây 19×19 có ~10^170 trạng thái\n\n**Giải pháp**:\n- Function approximation (phần sau)\n- Sampling-based methods\n- Hierarchical RL\n\n### 10. Ví dụ thực tế: Car Rental Problem\n\n#### 10.1. Mô tả bài toán\n**Jack's Car Rental**:\n- 2 địa điểm cho thuê xe\n- Mỗi đêm có thể di chuyển tối đa 5 xe giữa 2 địa điểm\n- Chi phí di chuyển: $2/xe\n- Thu nhập thuê xe: $10/xe\n- Số xe được thuê/trả theo phân phối Poisson\n\n#### 10.2. Biểu diễn MDP\n```\nState: (n1, n2) - số xe tại mỗi địa điểm (0-20)\nAction: -5 đến +5 (số xe di chuyển từ địa điểm 1 đến 2)\nReward: 10 × (số xe thuê được) - 2 × |action|\nTransition: Theo phân phối Poisson\n```\n\n#### 10.3. Giải bằng Policy Iteration\n\n**Chính sách khởi tạo**: Không di chuyển xe\n**Sau iteration 1**: Di chuyển xe từ địa điểm thừa sang thiếu\n**Hội tụ**: ~4-5 iterations\n**Chính sách tối ưu**: Cân bằng số xe giữa 2 địa điểm\n\n#### 10.4. Kết quả\n```\nV*(10,10) ≈ $500\nChiến lược: Luôn giữ ~10 xe mỗi địa điểm\n```\n\n### 11. Ví dụ thực tế: Inventory Management\n\n#### 11.1. Bài toán quản lý kho\n\n**Setup**:\n- Tồn kho từ 0 đến MAX_INVENTORY\n- Mỗi kỳ: Quyết định đặt hàng bao nhiêu\n- Chi phí đặt hàng + chi phí lưu kho\n- Doanh thu từ bán hàng\n- Mất khách nếu hết hàng\n\n#### 11.2. MDP Formulation\n```\nState: Mức tồn kho hiện tại\nAction: Số lượng đặt hàng\nReward: Revenue - Ordering_cost - Holding_cost - Stockout_penalty\nTransition: Stochastic demand\n```\n\n#### 11.3. Giải pháp\n- Sử dụng Value Iteration\n- Tìm chính sách (s,S): Đặt hàng đến mức S khi tồn kho ≤ s\n- Optimize (s,S) parameters\n\n### 12. Ưu điểm và hạn chế của Dynamic Programming\n\n#### 12.1. Ưu điểm\n✅ **Đảm bảo tối ưu**: Hội tụ đến chính sách tối ưu π*\n✅ **Cơ sở lý thuyết vững chắc**: Dựa trên phương trình Bellman\n✅ **Hiệu quả với medium-sized problems**: Polynomial complexity\n✅ **Framework cho nhiều thuật toán khác**: Nền tảng cho TD, Q-learning\n\n#### 12.2. Hạn chế\n❌ **Yêu cầu biết hoàn toàn mô hình**: Cần biết P và R\n❌ **Curse of dimensionality**: Không mở rộng cho không gian lớn\n❌ **Cần duyệt tất cả trạng thái**: Không practical với continuous states\n❌ **Computation cost**: Tốn nhiều tính toán mỗi iteration\n\n### 13. Mở rộng và cải tiến\n\n\n**Các khái niệm quan trọng:**\n- Value Iteration và Policy Iteration là hai thuật toán quy hoạch động (Dynamic Programming) được sử dụng để tìm hàm giá trị tối ưu V* và chính sách tối ưu π* trong một Markov Decision Process (MDP) khi mô hình môi trường được biết.\n\n**Value Iteration (Lặp giá trị)**\nValue Iteration là một thuật toán tập trung vào việc tìm hàm giá trị tối ưu V*(s). Nó hoạt động bằng cách lặp đi lặp lại việc cập nhật hàm giá trị V(s) của mỗi trạng thái dựa trên phương trình Bellman Optimality Equation: `V[s] = max(action_values)` với `q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`. Quá trình này tiếp tục cho đến khi V(s) hội tụ về V*(s). Sau khi V*(s) hội tụ, chính sách tối ưu π* được trích xuất từ V*. Value Iteration đảm bảo hội tụ đến V* và π*, tìm được nghiệm tối ưu cho các MDP hữu hạn. Nó phù hợp khi không gian hành động lớn, cần giải nhanh và chỉ quan tâm đến chính sách cuối cùng, thực hiện một bước evaluation và một bước improvement trong mỗi lần lặp. Ví dụ ứng dụng là tìm chính sách (s,S) trong Quản lý tồn kho.\n\n**Policy Iteration (Lặp chính sách)**\nPolicy Iteration là một thuật toán tập trung vào việc tìm chính sách tối ưu π*. Nó bao gồm hai bước lặp đi lặp lại:\n1.  **Policy Evaluation (Đánh giá chính sách):** Tính toán hàm giá trị Vπ cho chính sách hiện tại π cho đến khi hội tụ, sử dụng công thức `V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`.\n2.  **Policy Improvement (Cải thiện chính sách):** Tạo ra một chính sách mới π' tham lam dựa trên Vπ đã tính, sử dụng công thức `policy[s] = np.argmax(action_values)` với `q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`.\nQuá trình này lặp lại cho đến khi chính sách không còn được cải thiện, tức là đã đạt được chính sách tối ưu và hàm giá trị tối ưu. Policy Iteration là một phần của framework Generalized Policy Iteration (GPI) và đảm bảo hội tụ đến chính sách tối ưu π*. So với Value Iteration, Policy Iteration thường có ít tổng số lần lặp hơn nhưng mỗi lần lặp lại tốn nhiều thời gian hơn do bước evaluation đầy đủ, và thường hội tụ nhanh hơn.\n\n**Mối quan hệ:**\n- Value Iteration được sử dụng để giải quyết bài toán Gambler's Problem bằng cách tìm ra chính sách đặt cược tối ưu.\n- Value Iteration được sử dụng để giải quyết bài toán Inventory Management và tìm ra chính sách (s,S) tối ưu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- Optimal Policy (Chính sách tối ưu) là một chính sách π* mà khi được tuân theo, sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng từ mọi trạng thái. Nó đảm bảo rằng hàm giá trị của chính sách tối ưu Vᵖ*(s) lớn hơn hoặc bằng hàm giá trị của bất kỳ chính sách nào khác Vᵖ(s) cho mọi trạng thái s.\n- Value Iteration và Policy Iteration là hai thuật toán quy hoạch động (Dynamic Programming) được sử dụng để tìm hàm giá trị tối ưu V* và chính sách tối ưu π* trong một Markov Decision Process (MDP) khi mô hình môi trường được biết.\n\n**Value Iteration (Lặp giá trị)**\nValue Iteration là một thuật toán tập trung vào việc tìm hàm giá trị tối ưu V*(s). Nó hoạt động bằng cách lặp đi lặp lại việc cập nhật hàm giá trị V(s) của mỗi trạng thái dựa trên phương trình Bellman Optimality Equation: `V[s] = max(action_values)` với `q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`. Quá trình này tiếp tục cho đến khi V(s) hội tụ về V*(s). Sau khi V*(s) hội tụ, chính sách tối ưu π* được trích xuất từ V*. Value Iteration đảm bảo hội tụ đến V* và π*, tìm được nghiệm tối ưu cho các MDP hữu hạn. Nó phù hợp khi không gian hành động lớn, cần giải nhanh và chỉ quan tâm đến chính sách cuối cùng, thực hiện một bước evaluation và một bước improvement trong mỗi lần lặp. Ví dụ ứng dụng là tìm chính sách (s,S) trong Quản lý tồn kho.\n\n**Policy Iteration (Lặp chính sách)**\nPolicy Iteration là một thuật toán tập trung vào việc tìm chính sách tối ưu π*. Nó bao gồm hai bước lặp đi lặp lại:\n1.  **Policy Evaluation (Đánh giá chính sách):** Tính toán hàm giá trị Vπ cho chính sách hiện tại π cho đến khi hội tụ, sử dụng công thức `V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`.\n2.  **Policy Improvement (Cải thiện chính sách):** Tạo ra một chính sách mới π' tham lam dựa trên Vπ đã tính, sử dụng công thức `policy[s] = np.argmax(action_values)` với `q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`.\nQuá trình này lặp lại cho đến khi chính sách không còn được cải thiện, tức là đã đạt được chính sách tối ưu và hàm giá trị tối ưu. Policy Iteration là một phần của framework Generalized Policy Iteration (GPI) và đảm bảo hội tụ đến chính sách tối ưu π*. So với Value Iteration, Policy Iteration thường có ít tổng số lần lặp hơn nhưng mỗi lần lặp lại tốn nhiều thời gian hơn do bước evaluation đầy đủ, và thường hội tụ nhanh hơn.\n\n**Mối quan hệ:**\n- Value Iteration được sử dụng để giải quyết bài toán Gambler's Problem bằng cách tìm ra chính sách đặt cược tối ưu.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n- Value Iteration được sử dụng để giải quyết bài toán Inventory Management và tìm ra chính sách (s,S) tối ưu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n#### 13.1. Approximate Dynamic Programming\n- Sử dụng function approximation\n- Neural networks để biểu diễn V hoặc π\n- Trade-off giữa accuracy và scalability\n\n#### 13.2. Model-Free Methods\n- Không cần biết P và R\n- Học từ experience\n- Temporal-Difference Learning, Q-Learning (phần sau)\n\n#### 13.3. Deep Reinforcement Learning\n- Kết hợp DP với deep learning\n- DQN, Actor-Critic, PPO\n- Giải quyết được bài toán phức tạp\n\n### 14. Code Implementation - Ví dụ Python\n\n#### 14.1. Policy Iteration\n```python\ndef policy_iteration(env, gamma=0.9, theta=1e-6):\n    # Khởi tạo\n    V = np.zeros(env.num_states)\n    policy = np.zeros(env.num_states, dtype=int)\n    \n    while True:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.num_states):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([p * (r + gamma * V[s_]) \n                           for p, s_, r in env.transitions(s, a)])\n                delta = max(delta, abs(v - V[s]))\n            if delta < theta:\n                break\n        \n        # Policy Improvement\n        policy_stable = True\n        for s in range(env.num_states):\n            old_action = policy[s]\n            # Tìm hành động tốt nhất\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            policy[s] = np.argmax(action_values)\n            \n            if old_action != policy[s]:\n                policy_stable = False\n        \n        if policy_stable:\n            return V, policy\n```\n\n#### 14.2. Value Iteration\n```python\ndef value_iteration(env, gamma=0.9, theta=1e-6):\n    V = np.zeros(env.num_states)\n    \n    while True:\n        delta = 0\n        for s in range(env.num_states):\n            v = V[s]\n            # Bellman optimality update\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            V[s] = max(action_values)\n            delta = max(delta, abs(v - V[s]))\n        \n        if delta < theta:\n            break\n    \n    # Trích xuất chính sách\n    policy = np.zeros(env.num_states, dtype=int)\n    for s in range(env.num_states):\n        action_values = []\n        for a in range(env.num_actions):\n            q_value = sum([p * (r + gamma * V[s_]) \n                          for p, s_, r in env.transitions(s, a)])\n            action_values.append(q_value)\n        policy[s] = np.argmax(action_values)\n    \n    return V, policy\n```\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement policy evaluation cho Gridworld\n2. So sánh tốc độ hội tụ của in-place và two-array DP\n3. Visualize quá trình hội tụ của value iteration\n\n#### 15.2. Bài tập nâng cao\n1. Giải Gambler's Problem với các giá trị p khác nhau\n2. Implement prioritized sweeping\n3. Modified policy iteration với k=1, k=3, k=∞\n\n#### 15.3. Dự án\n1. Xây dựng AI cho game 2048 bằng DP\n2. Tối ưu hóa việc sạc pin cho robot\n3. Quản lý danh mục đầu tư bằng MDP\n\n### 16. Kết luận\n\n\n**Các khái niệm quan trọng:**\n- Value Iteration và Policy Iteration là hai thuật toán quy hoạch động (Dynamic Programming) được sử dụng để tìm hàm giá trị tối ưu V* và chính sách tối ưu π* trong một Markov Decision Process (MDP) khi mô hình môi trường được biết.\n\n**Value Iteration (Lặp giá trị)**\nValue Iteration là một thuật toán tập trung vào việc tìm hàm giá trị tối ưu V*(s). Nó hoạt động bằng cách lặp đi lặp lại việc cập nhật hàm giá trị V(s) của mỗi trạng thái dựa trên phương trình Bellman Optimality Equation: `V[s] = max(action_values)` với `q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`. Quá trình này tiếp tục cho đến khi V(s) hội tụ về V*(s). Sau khi V*(s) hội tụ, chính sách tối ưu π* được trích xuất từ V*. Value Iteration đảm bảo hội tụ đến V* và π*, tìm được nghiệm tối ưu cho các MDP hữu hạn. Nó phù hợp khi không gian hành động lớn, cần giải nhanh và chỉ quan tâm đến chính sách cuối cùng, thực hiện một bước evaluation và một bước improvement trong mỗi lần lặp. Ví dụ ứng dụng là tìm chính sách (s,S) trong Quản lý tồn kho.\n\n**Policy Iteration (Lặp chính sách)**\nPolicy Iteration là một thuật toán tập trung vào việc tìm chính sách tối ưu π*. Nó bao gồm hai bước lặp đi lặp lại:\n1.  **Policy Evaluation (Đánh giá chính sách):** Tính toán hàm giá trị Vπ cho chính sách hiện tại π cho đến khi hội tụ, sử dụng công thức `V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`.\n2.  **Policy Improvement (Cải thiện chính sách):** Tạo ra một chính sách mới π' tham lam dựa trên Vπ đã tính, sử dụng công thức `policy[s] = np.argmax(action_values)` với `q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`.\nQuá trình này lặp lại cho đến khi chính sách không còn được cải thiện, tức là đã đạt được chính sách tối ưu và hàm giá trị tối ưu. Policy Iteration là một phần của framework Generalized Policy Iteration (GPI) và đảm bảo hội tụ đến chính sách tối ưu π*. So với Value Iteration, Policy Iteration thường có ít tổng số lần lặp hơn nhưng mỗi lần lặp lại tốn nhiều thời gian hơn do bước evaluation đầy đủ, và thường hội tụ nhanh hơn.\n\n**Mối quan hệ:**\n- Value Iteration được sử dụng để giải quyết bài toán Gambler's Problem bằng cách tìm ra chính sách đặt cược tối ưu.\n- Value Iteration được sử dụng để giải quyết bài toán Inventory Management và tìm ra chính sách (s,S) tối ưu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n\n**Mối quan hệ:**\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*."
        },
        "output_without_context": {
            "question": "Trong phương pháp lặp giá trị, điều gì là yếu tố chính để xác định chính sách tối ưu trong một bài toán quyết định?",
            "answer": "Giá trị tối ưu của trạng thái."
        }
    },
    {
        "topic_description": "Chủ đề liên tuần này tích hợp khái niệm MDPs từ tuần trước với phương pháp lập trình động hiện tại. Học sinh sẽ được yêu cầu so sánh và phân tích mối liên hệ giữa MDP và DP, cách DP có thể được áp dụng cho MDPs và ví dụ cụ thể điển hình.",
        "output_with_context": {
            "question": "So sánh mối quan hệ giữa Markov Decision Processes (MDPs) và Dynamic Programming (DP) trong việc giải quyết bài toán tối ưu hóa, đặc biệt là cách DP có thể được áp dụng cho MDPs?",
            "answer": "Dynamic Programming được sử dụng để tính toán chính sách tối ưu cho MDPs thông qua các phương pháp như Value Iteration và Policy Iteration.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- Finite MDP là một loại Quá trình Quyết định Markov trong đó tập hợp các trạng thái (S) và tập hợp các hành động (A) đều hữu hạn. Điều này làm cho chúng dễ tính toán và phân tích hơn, cho phép sử dụng các phương pháp giải chính xác như Value Iteration và Policy Iteration.\n- Infinite MDP là một loại Quá trình Quyết định Markov trong đó tập hợp các trạng thái (S) hoặc tập hợp các hành động (A) (hoặc cả hai) là vô hạn. Đối với các MDP này, cần phải sử dụng các kỹ thuật xấp xỉ, chẳng hạn như xấp xỉ hàm giá trị, để giải quyết bài toán.\n- Continuing MDP là một loại Quá trình Quyết định Markov không có điểm kết thúc rõ ràng và chạy vô hạn. Các ví dụ bao gồm hệ thống kiểm soát nhiệt độ hoặc các tác vụ điều khiển liên tục, nơi tác nhân phải hoạt động liên tục mà không có một \"kết thúc\" cụ thể.\n\n**Mối quan hệ:**\n- Infinite MDP có tính chất là tập trạng thái S là vô hạn hoặc tập hành động A là vô hạn.\n- Finite MDP có tính chất là tập trạng thái S là hữu hạn.\n- Infinite MDP có tính chất là tập hành động A là vô hạn hoặc tập trạng thái S là vô hạn.\n- Finite MDP có tính chất là tập hành động A là hữu hạn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n#### 13.1. Approximate Dynamic Programming\n- Sử dụng function approximation\n- Neural networks để biểu diễn V hoặc π\n- Trade-off giữa accuracy và scalability\n\n#### 13.2. Model-Free Methods\n- Không cần biết P và R\n- Học từ experience\n- Temporal-Difference Learning, Q-Learning (phần sau)\n\n#### 13.3. Deep Reinforcement Learning\n- Kết hợp DP với deep learning\n- DQN, Actor-Critic, PPO\n- Giải quyết được bài toán phức tạp\n\n### 14. Code Implementation - Ví dụ Python\n\n#### 14.1. Policy Iteration\n```python\ndef policy_iteration(env, gamma=0.9, theta=1e-6):\n    # Khởi tạo\n    V = np.zeros(env.num_states)\n    policy = np.zeros(env.num_states, dtype=int)\n    \n    while True:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.num_states):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([p * (r + gamma * V[s_]) \n                           for p, s_, r in env.transitions(s, a)])\n                delta = max(delta, abs(v - V[s]))\n            if delta < theta:\n                break\n        \n        # Policy Improvement\n        policy_stable = True\n        for s in range(env.num_states):\n            old_action = policy[s]\n            # Tìm hành động tốt nhất\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            policy[s] = np.argmax(action_values)\n            \n            if old_action != policy[s]:\n                policy_stable = False\n        \n        if policy_stable:\n            return V, policy\n```\n\n#### 14.2. Value Iteration\n```python\ndef value_iteration(env, gamma=0.9, theta=1e-6):\n    V = np.zeros(env.num_states)\n    \n    while True:\n        delta = 0\n        for s in range(env.num_states):\n            v = V[s]\n            # Bellman optimality update\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            V[s] = max(action_values)\n            delta = max(delta, abs(v - V[s]))\n        \n        if delta < theta:\n            break\n    \n    # Trích xuất chính sách\n    policy = np.zeros(env.num_states, dtype=int)\n    for s in range(env.num_states):\n        action_values = []\n        for a in range(env.num_actions):\n            q_value = sum([p * (r + gamma * V[s_]) \n                          for p, s_, r in env.transitions(s, a)])\n            action_values.append(q_value)\n        policy[s] = np.argmax(action_values)\n    \n    return V, policy\n```\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement policy evaluation cho Gridworld\n2. So sánh tốc độ hội tụ của in-place và two-array DP\n3. Visualize quá trình hội tụ của value iteration\n\n#### 15.2. Bài tập nâng cao\n1. Giải Gambler's Problem với các giá trị p khác nhau\n2. Implement prioritized sweeping\n3. Modified policy iteration với k=1, k=3, k=∞\n\n#### 15.3. Dự án\n1. Xây dựng AI cho game 2048 bằng DP\n2. Tối ưu hóa việc sạc pin cho robot\n3. Quản lý danh mục đầu tư bằng MDP\n\n### 16. Kết luận\n\n\n**Các khái niệm quan trọng:**\n- Planning by Dynamic Programming (Lập kế hoạch bằng Quy hoạch động) là một phương pháp trong Reinforcement Learning sử dụng các thuật toán quy hoạch động để giải quyết các bài toán MDP khi mô hình môi trường (xác suất chuyển trạng thái P và phần thưởng R) được biết. Các thuật toán như Policy Iteration và Value Iteration thuộc nhóm này, chúng tính toán hàm giá trị và chính sách tối ưu bằng cách lặp đi lặp lại các phép tính dựa trên Bellman Equation.\n- Quản lý danh mục đầu tư bằng MDP là một ứng dụng của Reinforcement Learning trong lĩnh vực tài chính. Bài toán này mô hình hóa quá trình ra quyết định đầu tư như một Markov Decision Process, nơi agent (nhà đầu tư) học cách phân bổ tài sản vào các loại hình đầu tư khác nhau để tối đa hóa lợi nhuận hoặc giảm thiểu rủi ro theo thời gian.\n\n**Mối quan hệ:**\n- Deep Reinforcement Learning kết hợp các ý tưởng của Planning by Dynamic Programming với deep learning.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Khi cần chính sách tốt sớm trong quá trình\n- Không gian hành động nhỏ\n- Có thể dừng sớm khi chính sách đủ tốt\n\n**Value Iteration**:\n- Không gian hành động lớn\n- Cần giải nhanh\n- Chỉ quan tâm đến chính sách cuối cùng\n\n### 7. Asynchronous Dynamic Programming\n\n#### 7.1. Vấn đề của Synchronous DP\n- Cập nhật tất cả trạng thái mỗi iteration\n- Không hiệu quả với không gian trạng thái lớn\n- Lãng phí tính toán cho trạng thái ít quan trọng\n\n#### 7.2. In-Place Dynamic Programming\n\n**Ý tưởng**: Sử dụng giá trị mới ngay khi tính được\n```\nVới mỗi s ∈ S:\n    V(s) = max_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n```\n(không cần lưu bản sao cũ của V)\n\n**Lợi ích**: Hội tụ nhanh hơn, tiết kiệm bộ nhớ\n\n#### 7.3. Prioritized Sweeping\n\n**Ý tưởng**: Ưu tiên cập nhật các trạng thái quan trọng\n```\n1. Khởi tạo hàng đợi ưu tiên Q\n2. Với mỗi s, tính:\n   priority = |max_a[R(s,a) + γΣP(s'|s,a)V(s')] - V(s)|\n3. Chọn s có priority cao nhất, cập nhật V(s)\n4. Cập nhật priority cho các trạng thái tiền nhiệm\n```\n\n**Lợi ích**: Tập trung vào vùng quan trọng của không gian trạng thái\n\n#### 7.4. Real-Time Dynamic Programming\n\n**Đặc điểm**:\n- Chỉ cập nhật các trạng thái mà agent thực sự gặp phải\n- Phù hợp với bài toán có không gian trạng thái rất lớn\n- Học trong quá trình tương tác\n\n### 8. Generalized Policy Iteration (GPI)\n\n#### 8.1. Khái niệm\nGPI là framework chung cho nhiều thuật toán RL:\n```\n        Evaluation\n       ↗          ↘\n    Policy    ←→    Value\n       ↖          ↙\n       Improvement\n```\n\n#### 8.2. Đặc điểm\n- Evaluation: Làm cho V nhất quán với π\n- Improvement: Làm cho π tham lam theo V\n- Hai quá trình cạnh tranh và hợp tác\n- Cuối cùng hội tụ đến optimal\n\n#### 8.3. Các biến thể\n- **Policy Iteration**: Evaluation hoàn toàn trước improvement\n- **Value Iteration**: 1 bước evaluation rồi improvement\n- **Asynchronous DP**: Cập nhật không đồng bộ\n- **Temporal-Difference Learning**: GPI với mẫu (phần sau)\n\n### 9. Efficiency of Dynamic Programming\n\n#### 9.1. Độ phức tạp tính toán\n\n**Worst case**:\n- Số states: n = |S|\n- Số actions: k = |A|\n- Độ phức tạp: O(n² × k) mỗi iteration\n\n**So sánh với các phương pháp khác**:\n- Linear Programming: O(n³) nhưng đảm bảo polynomial\n- Policy Search: Exponential trong worst case\n- DP: Polynomial, hiệu quả với medium-sized problems\n\n#### 9.2. Curse of Dimensionality\n\n**Vấn đề**:\n\n**Các khái niệm quan trọng:**\n- Synchronous DP là một phương pháp quy hoạch động truyền thống, trong đó tất cả các trạng thái trong không gian trạng thái S đều được cập nhật đồng bộ trong mỗi lần lặp. Phương pháp này không hiệu quả với không gian trạng thái lớn vì lãng phí tính toán cho các trạng thái ít quan trọng và yêu cầu lưu trữ bản sao cũ của hàm giá trị để thực hiện cập nhật.\n- Asynchronous Dynamic Programming là một nhóm các kỹ thuật trong quy hoạch động nhằm giải quyết vấn đề không hiệu quả của Synchronous DP khi không gian trạng thái lớn. Thay vì cập nhật tất cả các trạng thái trong mỗi iteration, các phương pháp Asynchronous DP chỉ cập nhật một tập hợp con các trạng thái, giúp tiết kiệm tính toán và có thể hội tụ nhanh hơn. Các biến thể bao gồm In-Place Dynamic Programming, Prioritized Sweeping và Real-Time Dynamic Programming.\n\n**Mối quan hệ:**\n- Asynchronous Dynamic Programming giải quyết các vấn đề về hiệu quả và lãng phí tính toán của Synchronous DP khi không gian trạng thái lớn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n\n### 1. Giới thiệu về MDP\n\nQuá trình Quyết định Markov (Markov Decision Process - MDP) là một framework toán học để mô hình hóa việc ra quyết định trong các tình huống mà kết quả có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường.\n\n### 2. Các thành phần cơ bản của MDP\n\nMột MDP được định nghĩa bởi bộ năm phần tử (S, A, P, R, γ):\n\n#### 2.1. Tập trạng thái (State Space - S)\n- **Định nghĩa**: Tập hợp tất cả các trạng thái có thể mà agent có thể gặp trong môi trường\n- **Ký hiệu**: S = {s₁, s₂, ..., sₙ}\n- **Ví dụ**: Trong game cờ vua, mỗi trạng thái là một cấu hình cụ thể của bàn cờ\n\n#### 2.2. Tập hành động (Action Space - A)\n- **Định nghĩa**: Tập hợp tất cả các hành động mà agent có thể thực hiện\n- **Ký hiệu**: A = {a₁, a₂, ..., aₘ}\n- **Phân loại**:\n  - Không gian hành động rời rạc: Số lượng hành động hữu hạn\n  - Không gian hành động liên tục: Hành động có thể nhận giá trị trong một khoảng liên tục\n\n#### 2.3. Hàm chuyển trạng thái (State Transition Function - P)\n- **Định nghĩa**: Xác suất chuyển từ trạng thái s sang trạng thái s' khi thực hiện hành động a\n- **Công thức**: P(s'|s,a) = P[Sₜ₊₁ = s' | Sₜ = s, Aₜ = a]\n- **Tính chất Markov**: Trạng thái tương lai chỉ phụ thuộc vào trạng thái hiện tại, không phụ thuộc vào lịch sử\n\n#### 2.4. Hàm phần thưởng (Reward Function - R)\n- **Định nghĩa**: Phần thưởng nhận được khi thực hiện hành động a từ trạng thái s\n- **Công thức**: R(s,a) hoặc R(s,a,s')\n- **Mục đích**: Định hướng agent học hành vi tối ưu\n\n#### 2.5. Hệ số chiết khấu (Discount Factor - γ)\n- **Định nghĩa**: Hệ số để cân bằng giữa phần thưởng tức thời và phần thưởng dài hạn\n- **Giá trị**: 0 ≤ γ ≤ 1\n- **Ý nghĩa**:\n  - γ = 0: Chỉ quan tâm đến phần thưởng tức thời\n  - γ = 1: Phần thưởng tương lai có giá trị bằng phần thưởng hiện tại\n  - 0 < γ < 1: Phần thưởng tương lai bị chiết khấu\n\n### 3. Tính chất Markov\n\n#### 3.1. Định nghĩa\n\n**Các khái niệm quan trọng:**\n- Markov Decision Process (MDP) là một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự, nơi một agent tương tác với môi trường để tối đa hóa tổng phần thưởng chiết khấu. Trong MDP, kết quả của các quyết định có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường (Reinforcement Learning) và được định nghĩa bởi các thành phần: tập trạng thái S, tập hành động A, hàm phần thưởng R, xác suất chuyển trạng thái P, và discount factor γ.\n\n**Mối quan hệ:**\n- Một Markov Decision Process yêu cầu hàm phần thưởng R để định hướng agent học hành vi tối ưu."
        },
        "output_without_context": {
            "question": "So sánh mối liên hệ giữa MDP và DP trong việc giải quyết các bài toán tối ưu hóa, và cho biết DP có thể được áp dụng cho MDPs như thế nào?",
            "answer": "DP được sử dụng để giải quyết MDPs thông qua việc tối ưu hóa các quyết định trong các trạng thái khác nhau."
        }
    },
    {
        "topic_description": "Chủ đề này phân tích định lý ánh xạ co (Contraction Mapping Theorem) và sự hội tụ của các phương pháp đánh giá chính sách. Học sinh sẽ cần hiểu công thức của định lý này và áp dụng nó vào quá trình hội tụ của lặp giá trị và đánh giá chính sách, từ đó đánh giá tác động của nó trong học tăng cường.",
        "output_with_context": {
            "question": "Trong ánh xạ co, điều kiện nào đảm bảo rằng một chính sách mới π' sẽ tốt hơn hoặc bằng chính sách cũ π trong quá trình cải thiện chính sách?",
            "answer": "Nếu với mọi trạng thái s, Q^π(s, π'(s)) ≥ V^π(s) thì chính sách π' tốt hơn hoặc bằng π.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- Convergence là tính chất của một thuật toán học tăng cường khi các ước lượng của hàm giá trị hoặc chính sách của nó ổn định và tiến đến một giá trị hoặc chính sách tối ưu (hoặc gần tối ưu) sau một số lượng lớn các bước cập nhật. Điều kiện hội tụ thường phụ thuộc vào các tham số như learning rate và việc thăm dò không gian trạng thái/hành động.\n- Convergence là tính chất của một thuật toán khi nó đạt đến một trạng thái ổn định, nơi các giá trị hoặc chính sách không còn thay đổi đáng kể giữa các lần lặp. Policy Iteration luôn hội tụ đến chính sách tối ưu π*. Điều kiện hội tụ trong Policy Evaluation được kiểm tra bằng cách so sánh sự thay đổi lớn nhất Δ với một ngưỡng θ.\n- Convergence là tính chất hội tụ của một thuật toán, chỉ ra rằng thuật toán sẽ đạt được một kết quả ổn định hoặc tối ưu sau một số lần lặp nhất định. Trong Value Iteration, thuật toán hội tụ khi sự thay đổi của hàm giá trị V(s) giữa các lần lặp nhỏ hơn một ngưỡng θ. Tốc độ hội tụ phụ thuộc vào γ và không phụ thuộc vào chính sách khởi tạo.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Mỗi lần evaluation tốn nhiều thời gian\n- Không hiệu quả với không gian trạng thái lớn\n\n#### 4.3. Modified Policy Iteration\n\n**Ý tưởng**: Không cần evaluation hoàn toàn, chỉ cần k bước:\n```\nLặp k lần:\n    V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n```\n\n**Lợi ích**: Cân bằng giữa tốc độ hội tụ và độ chính xác\n\n### 5. Value Iteration - Lặp Giá Trị\n\n#### 5.1. Ý tưởng chính\nKết hợp policy evaluation và improvement thành một bước:\n```\nV(s) = max_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n```\n\n#### 5.2. Thuật toán Value Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n\n2. Lặp cho đến khi hội tụ:\n   Δ = 0\n   Với mỗi s ∈ S:\n      v = V(s)\n      V(s) = max_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n      Δ = max(Δ, |v - V(s)|)\n   \n   Nếu Δ < θ: dừng\n\n3. Trích xuất chính sách:\n   π(s) = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n```\n\n#### 5.3. Phân tích thuật toán\n\n**Độ phức tạp thời gian**:\n- Mỗi iteration: O(|S|² × |A|)\n- Số iterations: O(log(1/(θ(1-γ))))\n\n**Tốc độ hội tụ**:\n- Phụ thuộc vào γ: γ càng nhỏ, hội tụ càng nhanh\n- Không phụ thuộc vào chính sách khởi tạo\n\n#### 5.4. Ví dụ: Gambler's Problem\n\n**Mô tả bài toán**:\n- Người chơi có $0-100\n- Mỗi lần đặt cược $stake\n- Thắng với xác suất p, được 2×stake\n- Thua: mất stake\n- Mục tiêu: Đạt $100\n\n**Biểu diễn MDP**:\n```\nS = {0, 1, 2, ..., 100}\nA(s) = {0, 1, ..., min(s, 100-s)}\nR(s=100) = 1, R(other) = 0\nγ = 1\n```\n\n**Kết quả**:\n- Với p = 0.4: Đặt cược toàn bộ khi có ít tiền\n- Với p = 0.55: Chiến lược bảo thủ hơn\n\n### 6. So sánh Policy Iteration vs Value Iteration\n\n#### 6.1. Bảng so sánh\n\n| Tiêu chí | Policy Iteration | Value Iteration |\n|----------|------------------|-----------------|\n| Số bước mỗi iteration | Nhiều (evaluation đầy đủ) | 1 bước |\n| Số iterations tổng | Ít (thường < 10) | Nhiều hơn |\n| Thời gian mỗi iteration | Lâu | Nhanh |\n| Tổng thời gian | Phụ thuộc bài toán | Phụ thuộc bài toán |\n| Chính sách trung gian | Có thể sử dụng | Không có |\n\n#### 6.2. Khi nào dùng phương pháp nào?\n\n**Policy Iteration**:\n\n**Các khái niệm quan trọng:**\n- Convergence là tính chất của một thuật toán học tăng cường khi các ước lượng của hàm giá trị hoặc chính sách của nó ổn định và tiến đến một giá trị hoặc chính sách tối ưu (hoặc gần tối ưu) sau một số lượng lớn các bước cập nhật. Điều kiện hội tụ thường phụ thuộc vào các tham số như learning rate và việc thăm dò không gian trạng thái/hành động.\n- Convergence là tính chất của một thuật toán khi nó đạt đến một trạng thái ổn định, nơi các giá trị hoặc chính sách không còn thay đổi đáng kể giữa các lần lặp. Policy Iteration luôn hội tụ đến chính sách tối ưu π*. Điều kiện hội tụ trong Policy Evaluation được kiểm tra bằng cách so sánh sự thay đổi lớn nhất Δ với một ngưỡng θ.\n- Convergence là tính chất hội tụ của một thuật toán, chỉ ra rằng thuật toán sẽ đạt được một kết quả ổn định hoặc tối ưu sau một số lần lặp nhất định. Trong Value Iteration, thuật toán hội tụ khi sự thay đổi của hàm giá trị V(s) giữa các lần lặp nhỏ hơn một ngưỡng θ. Tốc độ hội tụ phụ thuộc vào γ và không phụ thuộc vào chính sách khởi tạo.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n**Features**:\n- **Separate features**: φ(s, a) cho mỗi cặp (s,a)\n- **State-action features**: Kết hợp φ_s và φ_a\n\n#### 5.2. Semi-gradient SARSA\n\n**Update Rule**:\n```\nw ← w + α[R + γQ̂(S', A'; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n```\n\n**Thuật toán**:\n```\nKhởi tạo w arbitrarily\nThiết lập ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    Chọn A từ S theo ε-greedy(Q̂(S, ·; w))\n    \n    Lặp với mỗi bước:\n        Thực hiện A, quan sát R, S'\n        Chọn A' từ S' theo ε-greedy(Q̂(S', ·; w))\n        \n        w ← w + α[R + γQ̂(S', A'; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n        \n        S ← S'; A ← A'\n    cho đến S là terminal\n```\n\n#### 5.3. Semi-gradient Q-Learning\n\n**Update Rule**:\n```\nw ← w + α[R + γ max_a Q̂(S', a; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n```\n\n**Thuật toán**:\n```\nKhởi tạo w arbitrarily\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        Chọn A từ S theo ε-greedy(Q̂(S, ·; w))\n        Thực hiện A, quan sát R, S'\n        \n        w ← w + α[R + γ max_a Q̂(S', a; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 5.4. Ví dụ: Mountain Car\n\n**Problem Setup**:\n- State: (position, velocity)\n- Action: {left, right, no-op}\n- Reward: -1 per step\n- Goal: Reach top of hill\n\n**Features**: Tile coding với multiple tilings\n```\nφ(s, a) = tile_coding(position, velocity, action)\n```\n\n**Results với Semi-gradient SARSA**:\n- Converge sau ~500 episodes\n- Learned policy: Build momentum bằng oscillation\n\n### 6. Convergence và Stability\n\n#### 6.1. Convergence Guarantees\n\n**Monte Carlo**:\n- ✅ Converge đến local optimum\n- ✅ Với linear FA: converge đến global optimum\n\n**TD với Linear FA**:\n- ✅ On-policy: Converge đến near-optimal\n- ❌ Off-policy: Có thể diverge (deadly triad)\n\n**Semi-gradient Methods**:\n- Không follow true gradient\n- Có thể không converge\n- Nhưng thường work well trong thực tế\n\n#### 6.2. Deadly Triad\n\n**Ba yếu tố gây divergence**:\n1. **Function Approximation**: Không phải tabular\n2. **Bootstrapping**: TD-style updates\n3. **Off-policy**: Behavior ≠ target policy\n\n**Khi có cả 3**: Divergence có thể xảy ra\n\n**Giải pháp**:\n- Loại bỏ một trong ba yếu tố\n- Sử dụng gradient TD methods\n- Importance sampling\n- Experience replay với target network (DQN)\n\n#### 6.3. Baird's Counterexample\n\n**Setup**: Simple MDP với linear FA và off-policy TD\n\n**Kết quả**: Parameters diverge đến infinity!\n\n**Ý nghĩa**: Cần cẩn thận với off-policy + FA + bootstrapping\n\n### 7. Batch Methods\n\n#### 7.1. Experience Replay\n\n**Ý tưởng**: Lưu trữ và replay experiences\n```\n\n**Các khái niệm quan trọng:**\n- Convergence là tính chất của một thuật toán học tăng cường khi các ước lượng của hàm giá trị hoặc chính sách của nó ổn định và tiến đến một giá trị hoặc chính sách tối ưu (hoặc gần tối ưu) sau một số lượng lớn các bước cập nhật. Điều kiện hội tụ thường phụ thuộc vào các tham số như learning rate và việc thăm dò không gian trạng thái/hành động.\n- Convergence là tính chất của một thuật toán khi nó đạt đến một trạng thái ổn định, nơi các giá trị hoặc chính sách không còn thay đổi đáng kể giữa các lần lặp. Policy Iteration luôn hội tụ đến chính sách tối ưu π*. Điều kiện hội tụ trong Policy Evaluation được kiểm tra bằng cách so sánh sự thay đổi lớn nhất Δ với một ngưỡng θ.\n- Convergence là tính chất hội tụ của một thuật toán, chỉ ra rằng thuật toán sẽ đạt được một kết quả ổn định hoặc tối ưu sau một số lần lặp nhất định. Trong Value Iteration, thuật toán hội tụ khi sự thay đổi của hàm giá trị V(s) giữa các lần lặp nhỏ hơn một ngưỡng θ. Tốc độ hội tụ phụ thuộc vào γ và không phụ thuộc vào chính sách khởi tạo.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nKhởi tạo V(s) arbitrarily, ∀s ∈ S\n\nVới mỗi episode:\n    Khởi tạo S\n    \n    Lặp cho mỗi bước của episode:\n        A ← hành động từ S theo π\n        Thực hiện A, quan sát R, S'\n        \n        V(S) ← V(S) + α[R + γV(S') - V(S)]\n        \n        S ← S'\n    cho đến khi S là terminal\n```\n\n#### 3.4. So sánh MC vs TD(0)\n\n| Tiêu chí | Monte Carlo | TD(0) |\n|----------|-------------|-------|\n| Cập nhật | Cuối episode | Sau mỗi bước |\n| Bootstrap | Không | Có |\n| Target | G_t (actual return) | R + γV(S') |\n| Bias | Unbiased | Biased |\n| Variance | High variance | Low variance |\n| Hội tụ | Chậm | Nhanh hơn |\n| Môi trường | Cần episodic | Cả episodic và continuing |\n\n#### 3.5. Ưu điểm của TD\n\n✅ **Học online**: Cập nhật sau mỗi bước, không cần chờ episode kết thúc\n✅ **Continuing tasks**: Hoạt động với tasks không có điểm kết thúc\n✅ **Lower variance**: Bootstrap giảm variance so với MC\n✅ **Học nhanh hơn**: Thường hội tụ nhanh hơn MC trong thực tế\n✅ **Hiệu quả dữ liệu**: Sử dụng thông tin từ ước lượng hiện tại\n\n#### 3.6. Ví dụ minh họa: Random Walk\n\n**Setup**:\n```\nStates: [A] [B] [C] [D] [E]\nStart: C\nTerminal: Left of A hoặc Right of E\nReward: 0 mọi nơi, +1 khi đến Right of E\nAction: Left hoặc Right (random với p=0.5)\n```\n\n**True values**:\n```\nV^π(A) = 1/6, V^π(B) = 2/6, V^π(C) = 3/6,\nV^π(D) = 4/6, V^π(E) = 5/6\n```\n\n**So sánh MC vs TD**:\n- TD hội tụ nhanh hơn với cùng số episodes\n- TD ít sensitive với khởi tạo\n- MC có RMS error cao hơn trong giai đoạn đầu\n\n### 4. Batch Methods và Certainty Equivalence\n\n#### 4.1. Batch Learning\n**Ý tưởng**: Lặp lại huấn luyện trên cùng một batch experience cho đến hội tụ\n\n```\nCho trước batch experience: \n    {(S₁, A₁, R₁, S'₁), (S₂, A₂, R₂, S'₂), ...}\n\nLặp cho đến hội tụ:\n    Với mỗi experience (S, A, R, S'):\n        Cập nhật V(S) theo MC hoặc TD\n```\n\n#### 4.2. Certainty Equivalence\n\n**MC**: Tìm V^π minimize mean-squared error với observed returns\n```\nV^π = argmin_V Σ_episodes Σ_t (G_t - V(S_t))²\n```\n\n**TD**: Tìm V^π thỏa mãn phương trình Bellman cho MDP ước lượng\n```\nV^π(s) = E[R + γV^π(S') | s]\n```\n(ước lượng từ experience)\n\n**Kết quả**:\n- TD tận dụng cấu trúc Markov\n- MC chỉ minimize error, không exploit Markov property\n- TD thường hiệu quả hơn trong môi trường Markov\n\n#### 4.3. Ví dụ: AB Example\n\n**Experience**:\n```\nA, 0, B, 0\nB, 1 (terminal)\nB, 1 (terminal)\n\n**Các khái niệm quan trọng:**\n- Hội tụ là quá trình một thuật toán học tăng cường đạt đến một trạng thái ổn định, nơi các ước lượng của hàm giá trị hoặc chính sách không còn thay đổi đáng kể nữa, hoặc đạt đến giá trị tối ưu. TD(0) thường hội tụ nhanh hơn Monte Carlo."
        },
        "output_without_context": {
            "question": "Định lý ánh xạ co (Contraction Mapping Theorem) khẳng định rằng trong một không gian metric, nếu một ánh xạ là co, thì nó có một điểm cố định duy nhất. Tác động của định lý này trong học tăng cường là gì?",
            "answer": "Nó đảm bảo rằng các phương pháp đánh giá chính sách sẽ hội tụ đến giá trị tối ưu."
        }
    },
    {
        "topic_description": "Chủ đề này kiểm tra sự khác biệt giữa lập trình động đồng bộ và không đồng bộ, cũng như ứng dụng cho các bài toán thực tế. Học sinh sẽ được yêu cầu đưa ra định nghĩa, phân tích ưu và nhược điểm, và các kịch bản chuyển giao cho mỗi loại.",
        "output_with_context": {
            "question": "Sự khác biệt chính giữa lập trình động đồng bộ và không đồng bộ là gì?",
            "answer": "Lập trình động đồng bộ cập nhật tất cả các trạng thái trong mỗi lần lặp, trong khi lập trình động không đồng bộ chỉ cập nhật một tập hợp con các trạng thái.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Khi cần chính sách tốt sớm trong quá trình\n- Không gian hành động nhỏ\n- Có thể dừng sớm khi chính sách đủ tốt\n\n**Value Iteration**:\n- Không gian hành động lớn\n- Cần giải nhanh\n- Chỉ quan tâm đến chính sách cuối cùng\n\n### 7. Asynchronous Dynamic Programming\n\n#### 7.1. Vấn đề của Synchronous DP\n- Cập nhật tất cả trạng thái mỗi iteration\n- Không hiệu quả với không gian trạng thái lớn\n- Lãng phí tính toán cho trạng thái ít quan trọng\n\n#### 7.2. In-Place Dynamic Programming\n\n**Ý tưởng**: Sử dụng giá trị mới ngay khi tính được\n```\nVới mỗi s ∈ S:\n    V(s) = max_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n```\n(không cần lưu bản sao cũ của V)\n\n**Lợi ích**: Hội tụ nhanh hơn, tiết kiệm bộ nhớ\n\n#### 7.3. Prioritized Sweeping\n\n**Ý tưởng**: Ưu tiên cập nhật các trạng thái quan trọng\n```\n1. Khởi tạo hàng đợi ưu tiên Q\n2. Với mỗi s, tính:\n   priority = |max_a[R(s,a) + γΣP(s'|s,a)V(s')] - V(s)|\n3. Chọn s có priority cao nhất, cập nhật V(s)\n4. Cập nhật priority cho các trạng thái tiền nhiệm\n```\n\n**Lợi ích**: Tập trung vào vùng quan trọng của không gian trạng thái\n\n#### 7.4. Real-Time Dynamic Programming\n\n**Đặc điểm**:\n- Chỉ cập nhật các trạng thái mà agent thực sự gặp phải\n- Phù hợp với bài toán có không gian trạng thái rất lớn\n- Học trong quá trình tương tác\n\n### 8. Generalized Policy Iteration (GPI)\n\n#### 8.1. Khái niệm\nGPI là framework chung cho nhiều thuật toán RL:\n```\n        Evaluation\n       ↗          ↘\n    Policy    ←→    Value\n       ↖          ↙\n       Improvement\n```\n\n#### 8.2. Đặc điểm\n- Evaluation: Làm cho V nhất quán với π\n- Improvement: Làm cho π tham lam theo V\n- Hai quá trình cạnh tranh và hợp tác\n- Cuối cùng hội tụ đến optimal\n\n#### 8.3. Các biến thể\n- **Policy Iteration**: Evaluation hoàn toàn trước improvement\n- **Value Iteration**: 1 bước evaluation rồi improvement\n- **Asynchronous DP**: Cập nhật không đồng bộ\n- **Temporal-Difference Learning**: GPI với mẫu (phần sau)\n\n### 9. Efficiency of Dynamic Programming\n\n#### 9.1. Độ phức tạp tính toán\n\n**Worst case**:\n- Số states: n = |S|\n- Số actions: k = |A|\n- Độ phức tạp: O(n² × k) mỗi iteration\n\n**So sánh với các phương pháp khác**:\n- Linear Programming: O(n³) nhưng đảm bảo polynomial\n- Policy Search: Exponential trong worst case\n- DP: Polynomial, hiệu quả với medium-sized problems\n\n#### 9.2. Curse of Dimensionality\n\n**Vấn đề**:\n\n**Các khái niệm quan trọng:**\n- Real-Time Dynamic Programming là một kỹ thuật trong Asynchronous Dynamic Programming, đặc biệt phù hợp với các bài toán có không gian trạng thái rất lớn. Thay vì cập nhật tất cả các trạng thái, nó chỉ cập nhật các trạng thái mà agent thực sự gặp phải trong quá trình tương tác với môi trường. Điều này cho phép học trong quá trình thực thi và tập trung tài nguyên tính toán vào các phần liên quan của không gian trạng thái.\n- Synchronous DP là một phương pháp quy hoạch động truyền thống, trong đó tất cả các trạng thái trong không gian trạng thái S đều được cập nhật đồng bộ trong mỗi lần lặp. Phương pháp này không hiệu quả với không gian trạng thái lớn vì lãng phí tính toán cho các trạng thái ít quan trọng và yêu cầu lưu trữ bản sao cũ của hàm giá trị để thực hiện cập nhật.\n- Asynchronous Dynamic Programming là một nhóm các kỹ thuật trong quy hoạch động nhằm giải quyết vấn đề không hiệu quả của Synchronous DP khi không gian trạng thái lớn. Thay vì cập nhật tất cả các trạng thái trong mỗi iteration, các phương pháp Asynchronous DP chỉ cập nhật một tập hợp con các trạng thái, giúp tiết kiệm tính toán và có thể hội tụ nhanh hơn. Các biến thể bao gồm In-Place Dynamic Programming, Prioritized Sweeping và Real-Time Dynamic Programming.\n\n**Mối quan hệ:**\n- Asynchronous Dynamic Programming giải quyết các vấn đề về hiệu quả và lãng phí tính toán của Synchronous DP khi không gian trạng thái lớn.\n- Real-Time Dynamic Programming giải quyết vấn đề Curse of Dimensionality bằng cách chỉ cập nhật các trạng thái mà agent thực sự gặp phải, phù hợp với không gian trạng thái rất lớn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Overlapping Subproblems là một điều kiện áp dụng Dynamic Programming, nghĩa là các bài toán con được giải đi giải lại nhiều lần, cho phép lưu trữ và tái sử dụng kết quả để tăng hiệu quả.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nMột quá trình được gọi là có tính chất Markov nếu:\n```\nP[Sₜ₊₁ | Sₜ] = P[Sₜ₊₁ | S₁, S₂, ..., Sₜ]\n```\n\n#### 3.2. Ý nghĩa\n- Trạng thái hiện tại chứa đầy đủ thông tin để dự đoán tương lai\n- Không cần phải nhớ toàn bộ lịch sử\n- Đơn giản hóa việc tính toán và phân tích\n\n### 4. Chính sách (Policy)\n\n#### 4.1. Định nghĩa\nChính sách π là một hàm ánh xạ từ trạng thái đến hành động:\n- **Chính sách xác định (Deterministic)**: π(s) = a\n- **Chính sách ngẫu nhiên (Stochastic)**: π(a|s) = P[Aₜ = a | Sₜ = s]\n\n#### 4.2. Phân loại\n- **Chính sách tĩnh (Stationary)**: Không thay đổi theo thời gian\n- **Chính sách động (Non-stationary)**: Thay đổi theo thời gian\n\n### 5. Hàm giá trị (Value Functions)\n\n#### 5.1. Hàm giá trị trạng thái (State-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s và tuân theo chính sách π\n\n```\nVᵖ(s) = Eᵖ[Gₜ | Sₜ = s]\n      = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s]\n```\n\n#### 5.2. Hàm giá trị hành động (Action-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s, thực hiện hành động a, và sau đó tuân theo chính sách π\n\n```\nQᵖ(s,a) = Eᵖ[Gₜ | Sₜ = s, Aₜ = a]\n        = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s, Aₜ = a]\n```\n\n#### 5.3. Mối quan hệ giữa V và Q\n```\nVᵖ(s) = Σₐ π(a|s)Qᵖ(s,a)\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')\n```\n\n### 6. Phương trình Bellman\n\n#### 6.1. Phương trình Bellman cho Vᵖ\n```\nVᵖ(s) = Σₐ π(a|s)[R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')]\n```\n\n**Giải thích**:\n- Giá trị của một trạng thái bằng phần thưởng tức thời cộng với giá trị chiết khấu của trạng thái tiếp theo\n- Đây là một phương trình đệ quy\n\n#### 6.2. Phương trình Bellman cho Qᵖ\n```\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) Σₐ' π(a'|s')Qᵖ(s',a')\n```\n\n#### 6.3. Phương trình Bellman tối ưu\n```\n\n**Các khái niệm quan trọng:**\n- Non-stationary Policy (Chính sách động) là một chính sách thay đổi theo thời gian, nghĩa là hành vi của agent có thể điều chỉnh theo các giai đoạn khác nhau của quá trình học.\n\n**Mối quan hệ:**\n- Policy có thể là Non-stationary Policy, thay đổi theo thời gian."
        },
        "output_without_context": {
            "question": "Sự khác biệt chính giữa lập trình động đồng bộ và không đồng bộ là gì?",
            "answer": "Lập trình động đồng bộ thực hiện các tác vụ theo thứ tự, trong khi lập trình động không đồng bộ cho phép thực hiện các tác vụ song song mà không chờ đợi kết quả."
        }
    }
]