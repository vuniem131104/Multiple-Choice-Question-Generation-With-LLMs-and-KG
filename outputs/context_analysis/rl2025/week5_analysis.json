[
    {
        "topic_description": "Chủ đề này kiểm tra hiểu biết về các khái niệm cơ bản liên quan đến Quy trình Quyết định Markov (MDP), bao gồm Định nghĩa Tính chất Markov, Ma trận Chuyển đổi Trạng thái và Phương trình Bellman. Học sinh sẽ cần phải hiểu và định nghĩa các thuật ngữ này để trả lời các câu hỏi trắc nghiệm.",
        "output_with_context": {
            "question": "Quá trình Quyết định Markov (MDP) được định nghĩa bởi bộ năm phần tử nào?",
            "answer": "(S, A, P, R, γ)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- Episodic MDP là một loại Quá trình Quyết định Markov có điểm kết thúc rõ ràng, được gọi là trạng thái kết thúc (terminal state). Các ví dụ bao gồm các trò chơi cờ hoặc robot đi mê cung, nơi nhiệm vụ kết thúc khi đạt được mục tiêu hoặc một điều kiện nhất định.\n- Continuing MDP là một loại Quá trình Quyết định Markov không có điểm kết thúc rõ ràng và chạy vô hạn. Các ví dụ bao gồm hệ thống kiểm soát nhiệt độ hoặc các tác vụ điều khiển liên tục, nơi tác nhân phải hoạt động liên tục mà không có một \"kết thúc\" cụ thể.\n- Finite MDP là một loại Quá trình Quyết định Markov trong đó tập hợp các trạng thái (S) và tập hợp các hành động (A) đều hữu hạn. Điều này làm cho chúng dễ tính toán và phân tích hơn, cho phép sử dụng các phương pháp giải chính xác như Value Iteration và Policy Iteration.\n\n**Mối quan hệ:**\n- Finite MDP có tính chất là tập trạng thái S là hữu hạn.\n- Finite MDP có tính chất là tập hành động A là hữu hạn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n\n### 1. Giới thiệu về MDP\n\nQuá trình Quyết định Markov (Markov Decision Process - MDP) là một framework toán học để mô hình hóa việc ra quyết định trong các tình huống mà kết quả có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường.\n\n### 2. Các thành phần cơ bản của MDP\n\nMột MDP được định nghĩa bởi bộ năm phần tử (S, A, P, R, γ):\n\n#### 2.1. Tập trạng thái (State Space - S)\n- **Định nghĩa**: Tập hợp tất cả các trạng thái có thể mà agent có thể gặp trong môi trường\n- **Ký hiệu**: S = {s₁, s₂, ..., sₙ}\n- **Ví dụ**: Trong game cờ vua, mỗi trạng thái là một cấu hình cụ thể của bàn cờ\n\n#### 2.2. Tập hành động (Action Space - A)\n- **Định nghĩa**: Tập hợp tất cả các hành động mà agent có thể thực hiện\n- **Ký hiệu**: A = {a₁, a₂, ..., aₘ}\n- **Phân loại**:\n  - Không gian hành động rời rạc: Số lượng hành động hữu hạn\n  - Không gian hành động liên tục: Hành động có thể nhận giá trị trong một khoảng liên tục\n\n#### 2.3. Hàm chuyển trạng thái (State Transition Function - P)\n- **Định nghĩa**: Xác suất chuyển từ trạng thái s sang trạng thái s' khi thực hiện hành động a\n- **Công thức**: P(s'|s,a) = P[Sₜ₊₁ = s' | Sₜ = s, Aₜ = a]\n- **Tính chất Markov**: Trạng thái tương lai chỉ phụ thuộc vào trạng thái hiện tại, không phụ thuộc vào lịch sử\n\n#### 2.4. Hàm phần thưởng (Reward Function - R)\n- **Định nghĩa**: Phần thưởng nhận được khi thực hiện hành động a từ trạng thái s\n- **Công thức**: R(s,a) hoặc R(s,a,s')\n- **Mục đích**: Định hướng agent học hành vi tối ưu\n\n#### 2.5. Hệ số chiết khấu (Discount Factor - γ)\n- **Định nghĩa**: Hệ số để cân bằng giữa phần thưởng tức thời và phần thưởng dài hạn\n- **Giá trị**: 0 ≤ γ ≤ 1\n- **Ý nghĩa**:\n  - γ = 0: Chỉ quan tâm đến phần thưởng tức thời\n  - γ = 1: Phần thưởng tương lai có giá trị bằng phần thưởng hiện tại\n  - 0 < γ < 1: Phần thưởng tương lai bị chiết khấu\n\n### 3. Tính chất Markov\n\n#### 3.1. Định nghĩa\n\n**Các khái niệm quan trọng:**\n- Markov Decision Process (MDP) là một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự, nơi một agent tương tác với môi trường để tối đa hóa tổng phần thưởng chiết khấu. Trong MDP, kết quả của các quyết định có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường (Reinforcement Learning) và được định nghĩa bởi các thành phần: tập trạng thái S, tập hành động A, hàm phần thưởng R, xác suất chuyển trạng thái P, và discount factor γ.\n- S là tập hợp tất cả các trạng thái có thể có trong môi trường của Markov Decision Process (MDP).\n\n**Mối quan hệ:**\n- Một Markov Decision Process yêu cầu hàm phần thưởng R để định hướng agent học hành vi tối ưu.\n- Một Markov Decision Process yêu cầu tập trạng thái S để định nghĩa các trạng thái mà agent có thể gặp.\n- Một Markov Decision Process yêu cầu tập hành động A để định nghĩa các hành động mà agent có thể thực hiện.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- S là tập hợp tất cả các trạng thái có thể có trong môi trường của Markov Decision Process (MDP).\n\n**Mối quan hệ:**\n- Một Markov Decision Process yêu cầu tập trạng thái S để định nghĩa các trạng thái mà agent có thể gặp.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nMột quá trình được gọi là có tính chất Markov nếu:\n```\nP[Sₜ₊₁ | Sₜ] = P[Sₜ₊₁ | S₁, S₂, ..., Sₜ]\n```\n\n#### 3.2. Ý nghĩa\n- Trạng thái hiện tại chứa đầy đủ thông tin để dự đoán tương lai\n- Không cần phải nhớ toàn bộ lịch sử\n- Đơn giản hóa việc tính toán và phân tích\n\n### 4. Chính sách (Policy)\n\n#### 4.1. Định nghĩa\nChính sách π là một hàm ánh xạ từ trạng thái đến hành động:\n- **Chính sách xác định (Deterministic)**: π(s) = a\n- **Chính sách ngẫu nhiên (Stochastic)**: π(a|s) = P[Aₜ = a | Sₜ = s]\n\n#### 4.2. Phân loại\n- **Chính sách tĩnh (Stationary)**: Không thay đổi theo thời gian\n- **Chính sách động (Non-stationary)**: Thay đổi theo thời gian\n\n### 5. Hàm giá trị (Value Functions)\n\n#### 5.1. Hàm giá trị trạng thái (State-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s và tuân theo chính sách π\n\n```\nVᵖ(s) = Eᵖ[Gₜ | Sₜ = s]\n      = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s]\n```\n\n#### 5.2. Hàm giá trị hành động (Action-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s, thực hiện hành động a, và sau đó tuân theo chính sách π\n\n```\nQᵖ(s,a) = Eᵖ[Gₜ | Sₜ = s, Aₜ = a]\n        = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s, Aₜ = a]\n```\n\n#### 5.3. Mối quan hệ giữa V và Q\n```\nVᵖ(s) = Σₐ π(a|s)Qᵖ(s,a)\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')\n```\n\n### 6. Phương trình Bellman\n\n#### 6.1. Phương trình Bellman cho Vᵖ\n```\nVᵖ(s) = Σₐ π(a|s)[R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')]\n```\n\n**Giải thích**:\n- Giá trị của một trạng thái bằng phần thưởng tức thời cộng với giá trị chiết khấu của trạng thái tiếp theo\n- Đây là một phương trình đệ quy\n\n#### 6.2. Phương trình Bellman cho Qᵖ\n```\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) Σₐ' π(a'|s')Qᵖ(s',a')\n```\n\n#### 6.3. Phương trình Bellman tối ưu\n```\n\n**Các khái niệm quan trọng:**\n- Sₜ là trạng thái tại thời điểm t trong một quá trình Markov Decision Process. Nó chứa đầy đủ thông tin để dự đoán tương lai theo tính chất Markov."
        },
        "output_without_context": {
            "question": "Tính chất Markov trong Quy trình Quyết định Markov (MDP) được định nghĩa là gì?",
            "answer": "Tính chất Markov là tính chất mà trạng thái hiện tại chứa đầy đủ thông tin để dự đoán trạng thái tương lai, không phụ thuộc vào các trạng thái trước đó."
        }
    },
    {
        "topic_description": "Chủ đề này tập trung vào Lập trình động (DP) như một phương pháp mạnh mẽ cho việc tối ưu hóa trong MDP. Học sinh sẽ được thử nghiệm kiến thức về các thuật toán DP như Đánh giá chính sách lặp và Lặp giá trị, cùng với các công thức toán học liên quan và ứng dụng thực tiễn.",
        "output_with_context": {
            "question": "Trong lập trình động, phương trình Bellman được sử dụng để tính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước. Công thức này có dạng nào?",
            "answer": "V^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]}",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Ví dụ: Bàn cờ vây 19×19 có ~10^170 trạng thái\n\n**Giải pháp**:\n- Function approximation (phần sau)\n- Sampling-based methods\n- Hierarchical RL\n\n### 10. Ví dụ thực tế: Car Rental Problem\n\n#### 10.1. Mô tả bài toán\n**Jack's Car Rental**:\n- 2 địa điểm cho thuê xe\n- Mỗi đêm có thể di chuyển tối đa 5 xe giữa 2 địa điểm\n- Chi phí di chuyển: $2/xe\n- Thu nhập thuê xe: $10/xe\n- Số xe được thuê/trả theo phân phối Poisson\n\n#### 10.2. Biểu diễn MDP\n```\nState: (n1, n2) - số xe tại mỗi địa điểm (0-20)\nAction: -5 đến +5 (số xe di chuyển từ địa điểm 1 đến 2)\nReward: 10 × (số xe thuê được) - 2 × |action|\nTransition: Theo phân phối Poisson\n```\n\n#### 10.3. Giải bằng Policy Iteration\n\n**Chính sách khởi tạo**: Không di chuyển xe\n**Sau iteration 1**: Di chuyển xe từ địa điểm thừa sang thiếu\n**Hội tụ**: ~4-5 iterations\n**Chính sách tối ưu**: Cân bằng số xe giữa 2 địa điểm\n\n#### 10.4. Kết quả\n```\nV*(10,10) ≈ $500\nChiến lược: Luôn giữ ~10 xe mỗi địa điểm\n```\n\n### 11. Ví dụ thực tế: Inventory Management\n\n#### 11.1. Bài toán quản lý kho\n\n**Setup**:\n- Tồn kho từ 0 đến MAX_INVENTORY\n- Mỗi kỳ: Quyết định đặt hàng bao nhiêu\n- Chi phí đặt hàng + chi phí lưu kho\n- Doanh thu từ bán hàng\n- Mất khách nếu hết hàng\n\n#### 11.2. MDP Formulation\n```\nState: Mức tồn kho hiện tại\nAction: Số lượng đặt hàng\nReward: Revenue - Ordering_cost - Holding_cost - Stockout_penalty\nTransition: Stochastic demand\n```\n\n#### 11.3. Giải pháp\n- Sử dụng Value Iteration\n- Tìm chính sách (s,S): Đặt hàng đến mức S khi tồn kho ≤ s\n- Optimize (s,S) parameters\n\n### 12. Ưu điểm và hạn chế của Dynamic Programming\n\n#### 12.1. Ưu điểm\n✅ **Đảm bảo tối ưu**: Hội tụ đến chính sách tối ưu π*\n✅ **Cơ sở lý thuyết vững chắc**: Dựa trên phương trình Bellman\n✅ **Hiệu quả với medium-sized problems**: Polynomial complexity\n✅ **Framework cho nhiều thuật toán khác**: Nền tảng cho TD, Q-learning\n\n#### 12.2. Hạn chế\n❌ **Yêu cầu biết hoàn toàn mô hình**: Cần biết P và R\n❌ **Curse of dimensionality**: Không mở rộng cho không gian lớn\n❌ **Cần duyệt tất cả trạng thái**: Không practical với continuous states\n❌ **Computation cost**: Tốn nhiều tính toán mỗi iteration\n\n### 13. Mở rộng và cải tiến\n\n\n**Các khái niệm quan trọng:**\n- Optimality là tính chất đảm bảo rằng một thuật toán hoặc chính sách đạt được kết quả tốt nhất có thể (tối đa hóa tổng Reward chiết khấu). Dynamic Programming (Policy Iteration, Value Iteration) có ưu điểm là đảm bảo hội tụ đến chính sách tối ưu π*.\n- Dynamic Programming (DP) là một tập hợp các kỹ thuật và một nền tảng lý thuyết quan trọng trong Reinforcement Learning (RL). DP giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng. Trong RL, DP cung cấp một framework lý thuyết để hiểu về các hàm giá trị tối ưu và chính sách tối ưu, đồng thời là cơ sở thuật toán cho các phương pháp model-free. DP được sử dụng để giải quyết các bài toán Markov Decision Process (MDP) khi mô hình môi trường (P và R) được biết hoàn toàn (model-based), sử dụng phương trình Bellman để tính toán hàm giá trị. Các thuật toán như Policy Iteration và Value Iteration thuộc nhóm này. DP đảm bảo tìm ra chính sách tối ưu nhưng bị hạn chế bởi \"curse of dimensionality\" và yêu cầu biết mô hình môi trường.\n- Dynamic Programming (Quy hoạch động) là một kỹ thuật giải thuật bằng cách chia bài toán lớn thành các bài toán con chồng chéo và lưu trữ kết quả của các bài toán con để tránh tính toán lại. Trong Floyd-Warshall, nó được sử dụng để tính toán đường đi ngắn nhất qua các tập hợp đỉnh trung gian tăng dần.\n\n**Mối quan hệ:**\n- Dynamic Programming cung cấp một framework lý thuyết để hiểu rõ về các chính sách tối ưu.\n- Dynamic Programming có ưu điểm là đảm bảo tối ưu, hội tụ đến chính sách tối ưu π*.\n- Dynamic Programming cung cấp một framework lý thuyết để hiểu rõ về các hàm giá trị tối ưu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Dynamic Programming (DP) là một tập hợp các kỹ thuật và một nền tảng lý thuyết quan trọng trong Reinforcement Learning (RL). DP giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng. Trong RL, DP cung cấp một framework lý thuyết để hiểu về các hàm giá trị tối ưu và chính sách tối ưu, đồng thời là cơ sở thuật toán cho các phương pháp model-free. DP được sử dụng để giải quyết các bài toán Markov Decision Process (MDP) khi mô hình môi trường (P và R) được biết hoàn toàn (model-based), sử dụng phương trình Bellman để tính toán hàm giá trị. Các thuật toán như Policy Iteration và Value Iteration thuộc nhóm này. DP đảm bảo tìm ra chính sách tối ưu nhưng bị hạn chế bởi \"curse of dimensionality\" và yêu cầu biết mô hình môi trường.\n- Dynamic Programming (Quy hoạch động) là một kỹ thuật giải thuật bằng cách chia bài toán lớn thành các bài toán con chồng chéo và lưu trữ kết quả của các bài toán con để tránh tính toán lại. Trong Floyd-Warshall, nó được sử dụng để tính toán đường đi ngắn nhất qua các tập hợp đỉnh trung gian tăng dần.\n\n**Mối quan hệ:**\n- Dynamic Programming cung cấp một framework lý thuyết để hiểu rõ về các chính sách tối ưu.\n- Dynamic Programming có ưu điểm là đảm bảo tối ưu, hội tụ đến chính sách tối ưu π*.\n- Dynamic Programming cung cấp một framework lý thuyết để hiểu rõ về các hàm giá trị tối ưu."
        },
        "output_without_context": {
            "question": "Trong lập trình động, thuật toán nào được sử dụng để tối ưu hóa chính sách trong MDP?",
            "answer": "Đánh giá chính sách lặp"
        }
    },
    {
        "topic_description": "Chủ đề này khai thác các khái niệm liên quan đến Học khác biệt thời gian, bao gồm sự khác biệt giữa MC và TD cũng như các ứng dụng thực tiễn của TD. Học sinh cần phân biệt TD và các phương pháp học khác, cung cấp câu trả lời cho các câu hỏi cụ thể từ nội dung đã học.",
        "output_with_context": {
            "question": "Trong thuật toán TD(0), công thức cập nhật hàm giá trị V(S_t) được biểu diễn như thế nào?",
            "answer": "V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n        Nếu S_t xuất hiện lần đầu tiên trong episode:\n            Thêm G vào Returns(S_t)\n            V(S_t) = average(Returns(S_t))\n```\n\n**Đặc điểm**:\n- Chỉ cập nhật cho lần xuất hiện đầu tiên của trạng thái\n- Không có bias\n- Đảm bảo hội tụ đến V^π(s) khi số episodes → ∞\n\n#### 2.4. Every-Visit Monte Carlo\n\n**Khác biệt**: Cập nhật cho mọi lần xuất hiện của trạng thái trong episode\n\n```\nVới mỗi bước t = T-1, T-2, ..., 0:\n    G = γG + R_{t+1}\n    Thêm G vào Returns(S_t)\n    V(S_t) = average(Returns(S_t))\n```\n\n**So sánh**:\n- Every-visit có variance thấp hơn\n- Cả hai đều hội tụ đến V^π(s)\n- Every-visit thường được sử dụng nhiều hơn\n\n#### 2.5. Incremental Mean Update\n\n**Vấn đề**: Lưu trữ tất cả returns không hiệu quả\n\n**Giải pháp**: Cập nhật incremental\n```\nCông thức tổng quát:\n    μ_k = μ_{k-1} + (1/k)(x_k - μ_{k-1})\n\nÁp dụng cho MC:\n    N(s) = N(s) + 1\n    V(s) = V(s) + (1/N(s))(G - V(s))\n    \nHoặc dùng learning rate α cố định:\n    V(s) = V(s) + α(G - V(s))\n```\n\n**Lợi ích**:\n- Tiết kiệm bộ nhớ\n- Cập nhật online\n- Quên dần các ước lượng cũ (với α cố định)\n\n#### 2.6. Ví dụ: Blackjack\n\n**Mô tả bài toán**:\n- **Trạng thái**: (tổng bài của người chơi, bài úp của dealer, có ace không)\n- **Hành động**: Hit (rút thêm) hoặc Stick (dừng)\n- **Phần thưởng**: +1 thắng, -1 thua, 0 hòa\n- **Chính sách**: Stick nếu tổng ≥ 20, ngược lại Hit\n\n**Ứng dụng MC**:\n1. Chơi nhiều games theo chính sách\n2. Ghi lại returns cho mỗi trạng thái\n3. Tính V^π bằng trung bình returns\n\n**Kết quả**:\n```\nV^π(20, ACE, usable_ace) ≈ 0.8  (rất tốt)\nV^π(12, 2, no_ace) ≈ -0.3       (tệ)\n```\n\n### 3. Temporal-Difference Learning (TD)\n\n#### 3.1. Giới thiệu\nTD Learning kết hợp ý tưởng của MC và DP:\n- Như MC: Học từ kinh nghiệm, không cần mô hình\n- Như DP: Bootstrap từ ước lượng hiện tại, không cần episode hoàn chỉnh\n\n#### 3.2. TD(0) - Temporal Difference cơ bản\n\n**TD Update Rule**:\n```\nV(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]\n```\n\n**Các thành phần**:\n- **TD Target**: R_{t+1} + γV(S_{t+1})\n- **TD Error**: δ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\n- **Update**: V(S_t) ← V(S_t) + α × δ_t\n\n#### 3.3. Thuật toán TD(0)\n\n```\n\n**Các khái niệm quan trọng:**\n- TD(0) (Temporal Difference learning với λ=0) là thuật toán Temporal-Difference Learning cơ bản nhất, một phương pháp học tăng cường model-free, online và bootstrap. Nó học từng bước một từ kinh nghiệm mà không cần đợi đến cuối episode, phù hợp với các tác vụ liên tục. TD(0) cập nhật hàm giá trị V(s) của trạng thái hiện tại (S_t) dựa trên phần thưởng tức thời (R_{t+1}) và ước lượng giá trị của trạng thái tiếp theo (V(S_{t+1})). Công thức cập nhật là V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]. Thuật toán này có bias cao nhưng variance thấp, thường hội tụ nhanh hơn Monte Carlo trong thực tế. SARSA là một ứng dụng của TD(0) cho Q-function.\n- Temporal-Difference Learning (TD) là một loại thuật toán học tăng cường kết hợp ý tưởng từ Monte Carlo và Dynamic Programming. Giống như Monte Carlo, TD học từ kinh nghiệm mà không cần mô hình môi trường. Giống như Dynamic Programming, TD bootstrap từ các ước lượng giá trị hiện tại của các trạng thái tiếp theo, cho phép cập nhật giá trị mà không cần chờ đến cuối episode.\n\n**Mối quan hệ:**\n- TD(0) có khả năng hoạt động và giải quyết các bài toán Continuing tasks, không yêu cầu điểm kết thúc episode.\n- TD(0) không phải là thuật toán episodic only.\n- Temporal-Difference Learning (TD) kết hợp ý tưởng bootstrap từ ước lượng hiện tại của Dynamic Programming, cho phép cập nhật mà không cần chờ đến cuối episode.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nKhởi tạo V(s) arbitrarily, ∀s ∈ S\n\nVới mỗi episode:\n    Khởi tạo S\n    \n    Lặp cho mỗi bước của episode:\n        A ← hành động từ S theo π\n        Thực hiện A, quan sát R, S'\n        \n        V(S) ← V(S) + α[R + γV(S') - V(S)]\n        \n        S ← S'\n    cho đến khi S là terminal\n```\n\n#### 3.4. So sánh MC vs TD(0)\n\n| Tiêu chí | Monte Carlo | TD(0) |\n|----------|-------------|-------|\n| Cập nhật | Cuối episode | Sau mỗi bước |\n| Bootstrap | Không | Có |\n| Target | G_t (actual return) | R + γV(S') |\n| Bias | Unbiased | Biased |\n| Variance | High variance | Low variance |\n| Hội tụ | Chậm | Nhanh hơn |\n| Môi trường | Cần episodic | Cả episodic và continuing |\n\n#### 3.5. Ưu điểm của TD\n\n✅ **Học online**: Cập nhật sau mỗi bước, không cần chờ episode kết thúc\n✅ **Continuing tasks**: Hoạt động với tasks không có điểm kết thúc\n✅ **Lower variance**: Bootstrap giảm variance so với MC\n✅ **Học nhanh hơn**: Thường hội tụ nhanh hơn MC trong thực tế\n✅ **Hiệu quả dữ liệu**: Sử dụng thông tin từ ước lượng hiện tại\n\n#### 3.6. Ví dụ minh họa: Random Walk\n\n**Setup**:\n```\nStates: [A] [B] [C] [D] [E]\nStart: C\nTerminal: Left of A hoặc Right of E\nReward: 0 mọi nơi, +1 khi đến Right of E\nAction: Left hoặc Right (random với p=0.5)\n```\n\n**True values**:\n```\nV^π(A) = 1/6, V^π(B) = 2/6, V^π(C) = 3/6,\nV^π(D) = 4/6, V^π(E) = 5/6\n```\n\n**So sánh MC vs TD**:\n- TD hội tụ nhanh hơn với cùng số episodes\n- TD ít sensitive với khởi tạo\n- MC có RMS error cao hơn trong giai đoạn đầu\n\n### 4. Batch Methods và Certainty Equivalence\n\n#### 4.1. Batch Learning\n**Ý tưởng**: Lặp lại huấn luyện trên cùng một batch experience cho đến hội tụ\n\n```\nCho trước batch experience: \n    {(S₁, A₁, R₁, S'₁), (S₂, A₂, R₂, S'₂), ...}\n\nLặp cho đến hội tụ:\n    Với mỗi experience (S, A, R, S'):\n        Cập nhật V(S) theo MC hoặc TD\n```\n\n#### 4.2. Certainty Equivalence\n\n**MC**: Tìm V^π minimize mean-squared error với observed returns\n```\nV^π = argmin_V Σ_episodes Σ_t (G_t - V(S_t))²\n```\n\n**TD**: Tìm V^π thỏa mãn phương trình Bellman cho MDP ước lượng\n```\nV^π(s) = E[R + γV^π(S') | s]\n```\n(ước lượng từ experience)\n\n**Kết quả**:\n- TD tận dụng cấu trúc Markov\n- MC chỉ minimize error, không exploit Markov property\n- TD thường hiệu quả hơn trong môi trường Markov\n\n#### 4.3. Ví dụ: AB Example\n\n**Experience**:\n```\nA, 0, B, 0\nB, 1 (terminal)\nB, 1 (terminal)\n\n**Các khái niệm quan trọng:**\n- Temporal Difference (TD(0)) là một thuật toán học tăng cường model-free, on-policy để ước lượng hàm giá trị V(s). Nó cập nhật hàm giá trị sau mỗi bước thời gian, sử dụng ước lượng của chính nó (bootstrap) để tính toán target. Công thức cập nhật là V(S) ← V(S) + α[R + γV(S') - V(S)]. TD(0) có ưu điểm là học online, hoạt động với continuing tasks, có variance thấp hơn và hội tụ nhanh hơn Monte Carlo.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nB, 1 (terminal)\nB, 1 (terminal)\nB, 1 (terminal)\nB, 0 (terminal)\n```\n\n**MC Estimate**:\n- V(A) = 0 (chỉ có 1 episode từ A, return = 0)\n- V(B) = 5/6 ≈ 0.83\n\n**TD Estimate**:\n- V(B) = 5/6 (từ data)\n- V(A) = 5/6 (vì 100% chuyển đến B)\n\n**TD tốt hơn**: Exploit structure, generalize better\n\n### 5. Unified View: TD(λ)\n\n#### 5.1. n-Step TD\n\n**Ý tưởng**: Thay vì bootstrap sau 1 bước, sử dụng n bước\n\n**n-Step Return**:\n```\nG_t^(n) = R_{t+1} + γR_{t+2} + ... + γ^{n-1}R_{t+n} + γ^n V(S_{t+n})\n```\n\n**Update**:\n```\nV(S_t) ← V(S_t) + α[G_t^(n) - V(S_t)]\n```\n\n**Đặc điểm**:\n- n=1: TD(0)\n- n=∞: Monte Carlo\n- n trung gian: Cân bằng bias và variance\n\n#### 5.2. TD(λ) - Eligibility Traces\n\n**Motivation**: Thay vì chọn một n, kết hợp tất cả n-step returns\n\n**λ-Return**:\n```\nG_t^λ = (1-λ) Σ_{n=1}^∞ λ^{n-1} G_t^(n)\n```\n\n**Eligibility Trace**:\n```\nE_0(s) = 0\nE_t(s) = γλE_{t-1}(s) + 1(S_t = s)\n```\n\n**TD(λ) Update**:\n```\nδ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\nV(s) ← V(s) + αδ_t E_t(s), ∀s\n```\n\n#### 5.3. Forward View vs Backward View\n\n**Forward View**:\n- Nhìn về tương lai\n- Tính G_t^λ từ future returns\n- Offline algorithm\n\n**Backward View**:\n- Nhìn về quá khứ\n- Sử dụng eligibility traces\n- Online algorithm\n- Computationally efficient\n\n**Quan hệ**: Hai view tương đương về mặt toán học\n\n#### 5.4. Chọn λ\n\n| λ | Đặc điểm | Khi nào dùng |\n|---|----------|--------------|\n| 0 | TD(0), low variance | Môi trường noisy |\n| 0.5-0.9 | Cân bằng | Thường dùng nhất |\n| 1 | MC, unbiased | Episodic, deterministic |\n\n### 6. Các biến thể TD nâng cao\n\n#### 6.1. Double Learning\n\n**Vấn đề**: Maximization bias trong TD\n```\nOverestimation: E[max(X₁, X₂)] ≥ max(E[X₁], E[X₂])\n```\n\n**Giải pháp**: Duy trì 2 value functions\n```\nV₁(S) và V₂(S)\n\nUpdate V₁:\n    V₁(S) ← V₁(S) + α[R + γV₂(S') - V₁(S)]\n    \nUpdate V₂:\n    V₂(S) ← V₂(S) + α[R + γV₁(S') - V₂(S)]\n```\n\n#### 6.2. Gradient TD\n\n**Motivation**: TD không theo gradient descent thực sự\n\n**True Gradient TD (TDC)**:\n```\nMinimize: MSBE = ||V - Π_T^π V||²\n\nUpdate:\n    w ← w + α(R + γV(S') - V(S))∇V(S)\n    - αγ(∇V(S'))⊤w ∇V(S)\n```\n\n**Lợi ích**: Hội tụ vững vàng hơn với function approximation\n\n### 7. Ứng dụng thực tế\n\n\n**Các khái niệm quan trọng:**\n- TD Estimate (Ước lượng Temporal Difference) là một kỹ thuật trong Model-Free Prediction để ước lượng hàm giá trị V(s) bằng cách cập nhật giá trị hiện tại dựa trên ước lượng của giá trị ở trạng thái tiếp theo (bootstrapping). TD tốt hơn MC vì nó khai thác cấu trúc của MDP và tổng quát hóa tốt hơn, đặc biệt trong các môi trường có nhiều trạng thái hoặc nhiệm vụ liên tục. TD có bias cao hơn MC nhưng variance thấp hơn.\n\n**Mối quan hệ:**\n- TD Estimate cải thiện hơn MC Estimate bằng cách khai thác cấu trúc của MDP và tổng quát hóa tốt hơn, đặc biệt trong các môi trường có nhiều trạng thái hoặc nhiệm vụ liên tục, mặc dù có thể có bias cao hơn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n- Cần học nhanh\n- Online learning\n\n**TD(λ)**:\n- Khi cần cân bằng bias-variance\n- Credit assignment phức tạp\n- Eligibility traces quan trọng\n\n### 10. Code Implementation\n\n#### 10.1. Monte Carlo First-Visit\n```python\ndef monte_carlo_prediction(env, policy, num_episodes, gamma=0.99):\n    V = defaultdict(float)\n    returns = defaultdict(list)\n    \n    for _ in range(num_episodes):\n        episode = generate_episode(env, policy)\n        G = 0\n        visited = set()\n        \n        # Duyệt ngược từ cuối episode\n        for t in range(len(episode)-1, -1, -1):\n            state, action, reward = episode[t]\n            G = gamma * G + reward\n            \n            if state not in visited:\n                returns[state].append(G)\n                V[state] = np.mean(returns[state])\n                visited.add(state)\n    \n    return V\n```\n\n#### 10.2. TD(0)\n```python\ndef td_prediction(env, policy, num_episodes, alpha=0.1, gamma=0.99):\n    V = defaultdict(float)\n    \n    for _ in range(num_episodes):\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # TD update\n            td_target = reward + gamma * V[next_state]\n            td_error = td_target - V[state]\n            V[state] += alpha * td_error\n            \n            state = next_state\n    \n    return V\n```\n\n#### 10.3. TD(λ) with Eligibility Traces\n```python\ndef td_lambda_prediction(env, policy, num_episodes, \n                        alpha=0.1, gamma=0.99, lambda_=0.9):\n    V = defaultdict(float)\n    \n    for _ in range(num_episodes):\n        E = defaultdict(float)  # Eligibility traces\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # TD error\n            delta = reward + gamma * V[next_state] - V[state]\n            \n            # Update eligibility trace\n            E[state] += 1\n            \n            # Update all states\n            for s in E:\n                V[s] += alpha * delta * E[s]\n                E[s] *= gamma * lambda_\n            \n            state = next_state\n    \n    return V\n```\n\n### 11. Bài tập thực hành\n\n#### 11.1. Bài tập cơ bản\n1. Implement First-Visit MC cho Blackjack\n2. So sánh MC vs TD(0) trên Random Walk\n3. Visualize learning curves với different α\n\n#### 11.2. Bài tập nâng cao\n1. Implement n-step TD với n = 1, 3, 5, 10\n2. Compare TD(λ) với λ = 0, 0.5, 0.9, 1.0\n3. Analyze bias-variance tradeoff empirically\n\n#### 11.3. Dự án\n1. Build Tic-Tac-Toe AI với TD learning\n2. Robot navigation trong gridworld phức tạp\n3. Stock price prediction với TD methods\n\n### 12. Kết luận\n\nModel-Free Prediction giải quyết vấn đề quan trọng: **Đánh giá chính sách mà không cần biết mô hình môi trường**.\n\n**Các phương pháp chính**:\n- **Monte Carlo**: Đơn giản, unbiased, nhưng high variance\n- **TD(0)**: Efficient, online, nhưng biased\n- **TD(λ)**: Cân bằng tốt nhất, flexible\n\n**Key insights**:\n1. Bootstrap (TD) vs Full returns (MC) là tradeoff cơ bản\n2. TD thường hiệu quả hơn trong môi trường Markov\n3. Eligibility traces (TD(λ)) cung cấp spectrum liên tục\n4. Lựa chọn phương pháp phụ thuộc vào đặc điểm bài toán\n\n\n**Các khái niệm quan trọng:**\n- TD(0) (Temporal Difference learning với λ=0) là thuật toán Temporal-Difference Learning cơ bản nhất, một phương pháp học tăng cường model-free, online và bootstrap. Nó học từng bước một từ kinh nghiệm mà không cần đợi đến cuối episode, phù hợp với các tác vụ liên tục. TD(0) cập nhật hàm giá trị V(s) của trạng thái hiện tại (S_t) dựa trên phần thưởng tức thời (R_{t+1}) và ước lượng giá trị của trạng thái tiếp theo (V(S_{t+1})). Công thức cập nhật là V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]. Thuật toán này có bias cao nhưng variance thấp, thường hội tụ nhanh hơn Monte Carlo trong thực tế. SARSA là một ứng dụng của TD(0) cho Q-function.\n\n**Mối quan hệ:**\n- TD(0) có khả năng hoạt động và giải quyết các bài toán Continuing tasks, không yêu cầu điểm kết thúc episode.\n- TD(0) không phải là thuật toán episodic only.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n#### 7.1. Game Playing\n\n**TD-Gammon** (1992):\n- Backgammon AI\n- Sử dụng TD(λ) với neural network\n- Self-play\n- Đạt world-champion level\n\n**AlphaGo Zero**:\n- Sử dụng TD-style updates\n- Self-play + MCTS\n- Không cần human knowledge\n\n#### 7.2. Robot Navigation\n\n**Task**: Robot tìm đường trong môi trường chưa biết\n- State: Vị trí robot\n- Action: Di chuyển\n- Reward: -1 mỗi bước, +100 khi đến đích\n\n**Ưu điểm TD**:\n- Học online trong quá trình điều hướng\n- Không cần đợi đến đích mới cập nhật\n- Adapt với môi trường thay đổi\n\n#### 7.3. Resource Management\n\n**Elevator Control**:\n- State: Vị trí thang máy, yêu cầu chờ\n- Action: Lên/xuống/đứng yên\n- Reward: -1 × tổng thời gian chờ\n\n**TD Learning**:\n- Học value function cho mỗi trạng thái\n- Online learning từ hoạt động hàng ngày\n- Cải thiện liên tục\n\n### 8. Phân tích lý thuyết\n\n#### 8.1. Tốc độ hội tụ\n\n**Monte Carlo**:\n```\nV_k(s) → V^π(s) với rate O(1/√k)\nk: số episodes\n```\n\n**TD(0)**:\n```\nV_k(s) → V^π(s) nhanh hơn trong thực tế\nKhông có bound lý thuyết chặt chẽ\n```\n\n**Thực nghiệm**: TD thường nhanh hơn MC 2-10 lần\n\n#### 8.2. Điều kiện hội tụ\n\n**Robbins-Monro conditions** cho learning rate α_t:\n```\nΣ_{t=1}^∞ α_t = ∞     (đảm bảo hội tụ)\nΣ_{t=1}^∞ α_t² < ∞    (đảm bảo variance hội tụ về 0)\n```\n\n**Ví dụ**:\n- α_t = 1/t: Thỏa mãn\n- α_t = 0.01: Không thỏa mãn điều kiện 1, nhưng practical\n\n#### 8.3. Bias-Variance Tradeoff\n\n**Monte Carlo**:\n- Bias = 0 (ước lượng không chệch)\n- Variance cao (phụ thuộc vào toàn bộ trajectory)\n\n**TD(0)**:\n- Bias > 0 (phụ thuộc vào V hiện tại)\n- Variance thấp (chỉ phụ thuộc 1 bước)\n\n**n-Step TD**: Cân bằng\n```\nBias giảm khi n tăng\nVariance tăng khi n tăng\n```\n\n### 9. So sánh tổng hợp\n\n#### 9.1. Bảng so sánh đầy đủ\n\n| Tiêu chí | MC | TD(0) | TD(λ) | DP |\n|----------|----|----|-------|-----|\n| Model-free | ✓ | ✓ | ✓ | ✗ |\n| Bootstrap | ✗ | ✓ | ✓ | ✓ |\n| Online | ✗ | ✓ | ✓ | ✓ |\n| Episodic only | ✓ | ✗ | ✗ | ✗ |\n| Bias | Low | High | Medium | Low |\n| Variance | High | Low | Medium | Low |\n| Convergence | Slow | Fast | Medium | Fastest |\n\n#### 9.2. Khi nào dùng phương pháp nào?\n\n**Monte Carlo**:\n- Môi trường không Markov\n- Cần ước lượng unbiased\n- Episodic tasks ngắn\n\n**TD(0)**:\n- Môi trường Markov\n- Continuing tasks\n\n**Các khái niệm quan trọng:**\n- TD(0) (Temporal Difference learning với λ=0) là thuật toán Temporal-Difference Learning cơ bản nhất, một phương pháp học tăng cường model-free, online và bootstrap. Nó học từng bước một từ kinh nghiệm mà không cần đợi đến cuối episode, phù hợp với các tác vụ liên tục. TD(0) cập nhật hàm giá trị V(s) của trạng thái hiện tại (S_t) dựa trên phần thưởng tức thời (R_{t+1}) và ước lượng giá trị của trạng thái tiếp theo (V(S_{t+1})). Công thức cập nhật là V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]. Thuật toán này có bias cao nhưng variance thấp, thường hội tụ nhanh hơn Monte Carlo trong thực tế. SARSA là một ứng dụng của TD(0) cho Q-function.\n\n**Mối quan hệ:**\n- TD(0) có khả năng hoạt động và giải quyết các bài toán Continuing tasks, không yêu cầu điểm kết thúc episode.\n- TD(0) không phải là thuật toán episodic only.\n- TD(0) phù hợp để giải quyết các tác vụ liên tục (continuing tasks)."
        },
        "output_without_context": {
            "question": "Học khác biệt thời gian (TD) có thể được áp dụng trong tình huống nào sau đây để cải thiện hiệu quả học tập?",
            "answer": "Sử dụng các phương pháp học tập thích ứng với thời gian và tốc độ học của từng cá nhân."
        }
    },
    {
        "topic_description": "Chủ đề này kiểm tra khả năng tối ưu hóa hàm giá trị của một MDP, tập trung vào Lặp lại chính sách và Q-learning. Học sinh sẽ cần sử dụng các công thức liên quan cho các khái niệm kiểm soát không mô hình.",
        "output_with_context": {
            "question": "Trong thuật toán Q-Learning, công thức cập nhật Q-value là gì?",
            "answer": "Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A) ]",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n- Học được đường đi tối ưu sau ~170 episodes\n- Tận dụng wind để di chuyển nhanh hơn\n- An toàn hơn Q-learning (tính đến exploration)\n\n### 5. Q-Learning - Off-Policy TD Control\n\n#### 5.1. Ý tưởng chính\nHọc về chính sách tham lam (greedy) trong khi hành động theo ε-greedy\n\n**Q-Learning Update**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\n                                            ↑ Greedy!\n```\n\n#### 5.2. Thuật toán Q-Learning\n\n```\nKhởi tạo Q(s,a) arbitrarily, ∀s,a\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        Chọn A từ S theo ε-greedy(Q)  ← Behavior policy\n        Thực hiện A, quan sát R, S'\n        \n        Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n                                   ↑ Target policy (greedy)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 5.3. So sánh SARSA vs Q-Learning\n\n**SARSA Update**:\n```\nQ(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n                          ↑ A' theo ε-greedy\n```\n\n**Q-Learning Update**:\n```\nQ(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n                          ↑ Greedy choice\n```\n\n**Khác biệt**:\n- SARSA: Conservative, tính đến exploration risk\n- Q-Learning: Optimistic, học optimal ignoring exploration\n\n#### 5.4. Cliff Walking Example\n\n**Setup**:\n```\n[S] [ ] [ ] ... [ ] [G]\n[ ] [ ] [ ] ... [ ] [ ]\n[C] [C] [C] ... [C] [ ]\n```\n- S: Start, G: Goal, C: Cliff (reward = -100)\n- Normal step: reward = -1\n\n**Kết quả**:\n- **Q-Learning**: Học đường ngắn nhất (sát cliff) nhưng thường rơi xuống khi explore\n- **SARSA**: Học đường an toàn (xa cliff) vì tính đến khả năng explore\n- **Performance**: SARSA tốt hơn trong training, Q-Learning tốt hơn nếu greedy\n\n#### 5.5. Tính chất Q-Learning\n\n**Hội tụ**:\n- Đảm bảo hội tụ đến Q* với:\n  - Mọi (s,a) được visit vô hạn lần\n  - Learning rate thỏa mãn Robbins-Monro\n- Không phụ thuộc vào behavior policy!\n\n**Ưu điểm**:\n✅ Học optimal policy\n✅ Có thể tái sử dụng old experience\n✅ Học từ demonstrations\n✅ Simple và popular\n\n**Nhược điểm**:\n❌ Có thể không ổn định\n❌ Overestimation bias\n❌ Higher variance\n\n### 6. Expected SARSA\n\n#### 6.1. Ý tưởng\nThay vì sample A', lấy expectation theo policy\n\n**Update Rule**:\n```\nQ(S,A) ← Q(S,A) + α[R + γ Σ_a π(a|S')Q(S',a) - Q(S,A)]\n                          ↑ Expected value\n```\n\n**Với ε-greedy**:\n```\nQ(S,A) ← Q(S,A) + α[R + γ E_π[Q(S',·)] - Q(S,A)]\n\nE_π[Q(S',·)] = Σ_a π(a|S')Q(S',a)\n\n**Các khái niệm quan trọng:**\n- Q-Learning là một thuật toán học tăng cường (RL) off-policy và model-free, thuộc nhóm Value-Based Methods, được sử dụng để học hàm Q-function tối ưu. Nó sử dụng phương pháp Temporal Difference (TD) để ước lượng giá trị của việc thực hiện một hành động trong một trạng thái cụ thể. Q-Learning cập nhật Q-value dựa trên phần thưởng tức thời và Q-value tối đa của trạng thái tiếp theo, bất kể hành động nào thực sự được chọn, cho phép nó học một chính sách tối ưu (target policy) trong khi vẫn sử dụng một chính sách khám phá (behavior policy) khác (thường là ε-greedy). Công thức cập nhật điển hình của Q-Learning là Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]. Thuật toán này hội tụ đến Q* (hàm Q-function tối ưu) khi mỗi cặp (s,a) được thăm vô hạn lần và learning rate α thỏa mãn điều kiện Robbins-Monro, không phụ thuộc vào behavior policy. Q-Learning suy ra chính sách bằng cách chọn hành động có Q-value cao nhất và được coi là \"tối ưu\" vì nó học chính sách tối ưu bất kể chính sách hành vi. Tuy nhiên, Q-Learning gặp khó khăn với không gian hành động liên tục, tạo ra các chính sách xác định và có xu hướng ước lượng quá mức giá trị do sử dụng phép toán max. Q-Learning có thể được tăng tốc đáng kể bằng các bước lập kế hoạch (planning steps) và là một trong những lựa chọn phổ biến nhất trong học tăng cường.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n\n### 1. Giới thiệu về MDP\n\nQuá trình Quyết định Markov (Markov Decision Process - MDP) là một framework toán học để mô hình hóa việc ra quyết định trong các tình huống mà kết quả có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường.\n\n### 2. Các thành phần cơ bản của MDP\n\nMột MDP được định nghĩa bởi bộ năm phần tử (S, A, P, R, γ):\n\n#### 2.1. Tập trạng thái (State Space - S)\n- **Định nghĩa**: Tập hợp tất cả các trạng thái có thể mà agent có thể gặp trong môi trường\n- **Ký hiệu**: S = {s₁, s₂, ..., sₙ}\n- **Ví dụ**: Trong game cờ vua, mỗi trạng thái là một cấu hình cụ thể của bàn cờ\n\n#### 2.2. Tập hành động (Action Space - A)\n- **Định nghĩa**: Tập hợp tất cả các hành động mà agent có thể thực hiện\n- **Ký hiệu**: A = {a₁, a₂, ..., aₘ}\n- **Phân loại**:\n  - Không gian hành động rời rạc: Số lượng hành động hữu hạn\n  - Không gian hành động liên tục: Hành động có thể nhận giá trị trong một khoảng liên tục\n\n#### 2.3. Hàm chuyển trạng thái (State Transition Function - P)\n- **Định nghĩa**: Xác suất chuyển từ trạng thái s sang trạng thái s' khi thực hiện hành động a\n- **Công thức**: P(s'|s,a) = P[Sₜ₊₁ = s' | Sₜ = s, Aₜ = a]\n- **Tính chất Markov**: Trạng thái tương lai chỉ phụ thuộc vào trạng thái hiện tại, không phụ thuộc vào lịch sử\n\n#### 2.4. Hàm phần thưởng (Reward Function - R)\n- **Định nghĩa**: Phần thưởng nhận được khi thực hiện hành động a từ trạng thái s\n- **Công thức**: R(s,a) hoặc R(s,a,s')\n- **Mục đích**: Định hướng agent học hành vi tối ưu\n\n#### 2.5. Hệ số chiết khấu (Discount Factor - γ)\n- **Định nghĩa**: Hệ số để cân bằng giữa phần thưởng tức thời và phần thưởng dài hạn\n- **Giá trị**: 0 ≤ γ ≤ 1\n- **Ý nghĩa**:\n  - γ = 0: Chỉ quan tâm đến phần thưởng tức thời\n  - γ = 1: Phần thưởng tương lai có giá trị bằng phần thưởng hiện tại\n  - 0 < γ < 1: Phần thưởng tương lai bị chiết khấu\n\n### 3. Tính chất Markov\n\n#### 3.1. Định nghĩa\n\n**Các khái niệm quan trọng:**\n- Markov Decision Process (MDP) là một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự, nơi một agent tương tác với môi trường để tối đa hóa tổng phần thưởng chiết khấu. Trong MDP, kết quả của các quyết định có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường (Reinforcement Learning) và được định nghĩa bởi các thành phần: tập trạng thái S, tập hành động A, hàm phần thưởng R, xác suất chuyển trạng thái P, và discount factor γ.\n\n**Mối quan hệ:**\n- Một Markov Decision Process yêu cầu hàm phần thưởng R để định hướng agent học hành vi tối ưu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Ví dụ: Bàn cờ vây 19×19 có ~10^170 trạng thái\n\n**Giải pháp**:\n- Function approximation (phần sau)\n- Sampling-based methods\n- Hierarchical RL\n\n### 10. Ví dụ thực tế: Car Rental Problem\n\n#### 10.1. Mô tả bài toán\n**Jack's Car Rental**:\n- 2 địa điểm cho thuê xe\n- Mỗi đêm có thể di chuyển tối đa 5 xe giữa 2 địa điểm\n- Chi phí di chuyển: $2/xe\n- Thu nhập thuê xe: $10/xe\n- Số xe được thuê/trả theo phân phối Poisson\n\n#### 10.2. Biểu diễn MDP\n```\nState: (n1, n2) - số xe tại mỗi địa điểm (0-20)\nAction: -5 đến +5 (số xe di chuyển từ địa điểm 1 đến 2)\nReward: 10 × (số xe thuê được) - 2 × |action|\nTransition: Theo phân phối Poisson\n```\n\n#### 10.3. Giải bằng Policy Iteration\n\n**Chính sách khởi tạo**: Không di chuyển xe\n**Sau iteration 1**: Di chuyển xe từ địa điểm thừa sang thiếu\n**Hội tụ**: ~4-5 iterations\n**Chính sách tối ưu**: Cân bằng số xe giữa 2 địa điểm\n\n#### 10.4. Kết quả\n```\nV*(10,10) ≈ $500\nChiến lược: Luôn giữ ~10 xe mỗi địa điểm\n```\n\n### 11. Ví dụ thực tế: Inventory Management\n\n#### 11.1. Bài toán quản lý kho\n\n**Setup**:\n- Tồn kho từ 0 đến MAX_INVENTORY\n- Mỗi kỳ: Quyết định đặt hàng bao nhiêu\n- Chi phí đặt hàng + chi phí lưu kho\n- Doanh thu từ bán hàng\n- Mất khách nếu hết hàng\n\n#### 11.2. MDP Formulation\n```\nState: Mức tồn kho hiện tại\nAction: Số lượng đặt hàng\nReward: Revenue - Ordering_cost - Holding_cost - Stockout_penalty\nTransition: Stochastic demand\n```\n\n#### 11.3. Giải pháp\n- Sử dụng Value Iteration\n- Tìm chính sách (s,S): Đặt hàng đến mức S khi tồn kho ≤ s\n- Optimize (s,S) parameters\n\n### 12. Ưu điểm và hạn chế của Dynamic Programming\n\n#### 12.1. Ưu điểm\n✅ **Đảm bảo tối ưu**: Hội tụ đến chính sách tối ưu π*\n✅ **Cơ sở lý thuyết vững chắc**: Dựa trên phương trình Bellman\n✅ **Hiệu quả với medium-sized problems**: Polynomial complexity\n✅ **Framework cho nhiều thuật toán khác**: Nền tảng cho TD, Q-learning\n\n#### 12.2. Hạn chế\n❌ **Yêu cầu biết hoàn toàn mô hình**: Cần biết P và R\n❌ **Curse of dimensionality**: Không mở rộng cho không gian lớn\n❌ **Cần duyệt tất cả trạng thái**: Không practical với continuous states\n❌ **Computation cost**: Tốn nhiều tính toán mỗi iteration\n\n### 13. Mở rộng và cải tiến\n\n\n**Các khái niệm quan trọng:**\n- Value Iteration và Policy Iteration là hai thuật toán quy hoạch động (Dynamic Programming) được sử dụng để tìm hàm giá trị tối ưu V* và chính sách tối ưu π* trong một Markov Decision Process (MDP) khi mô hình môi trường được biết.\n\n**Value Iteration (Lặp giá trị)**\nValue Iteration là một thuật toán tập trung vào việc tìm hàm giá trị tối ưu V*(s). Nó hoạt động bằng cách lặp đi lặp lại việc cập nhật hàm giá trị V(s) của mỗi trạng thái dựa trên phương trình Bellman Optimality Equation: `V[s] = max(action_values)` với `q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`. Quá trình này tiếp tục cho đến khi V(s) hội tụ về V*(s). Sau khi V*(s) hội tụ, chính sách tối ưu π* được trích xuất từ V*. Value Iteration đảm bảo hội tụ đến V* và π*, tìm được nghiệm tối ưu cho các MDP hữu hạn. Nó phù hợp khi không gian hành động lớn, cần giải nhanh và chỉ quan tâm đến chính sách cuối cùng, thực hiện một bước evaluation và một bước improvement trong mỗi lần lặp. Ví dụ ứng dụng là tìm chính sách (s,S) trong Quản lý tồn kho.\n\n**Policy Iteration (Lặp chính sách)**\nPolicy Iteration là một thuật toán tập trung vào việc tìm chính sách tối ưu π*. Nó bao gồm hai bước lặp đi lặp lại:\n1.  **Policy Evaluation (Đánh giá chính sách):** Tính toán hàm giá trị Vπ cho chính sách hiện tại π cho đến khi hội tụ, sử dụng công thức `V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`.\n2.  **Policy Improvement (Cải thiện chính sách):** Tạo ra một chính sách mới π' tham lam dựa trên Vπ đã tính, sử dụng công thức `policy[s] = np.argmax(action_values)` với `q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`.\nQuá trình này lặp lại cho đến khi chính sách không còn được cải thiện, tức là đã đạt được chính sách tối ưu và hàm giá trị tối ưu. Policy Iteration là một phần của framework Generalized Policy Iteration (GPI) và đảm bảo hội tụ đến chính sách tối ưu π*. So với Value Iteration, Policy Iteration thường có ít tổng số lần lặp hơn nhưng mỗi lần lặp lại tốn nhiều thời gian hơn do bước evaluation đầy đủ, và thường hội tụ nhanh hơn.\n\n**Mối quan hệ:**\n- Policy Iteration và Value Iteration là hai thuật toán quy hoạch động được so sánh về số bước, thời gian mỗi iteration, tổng thời gian và khả năng sử dụng chính sách trung gian.\n- Value Iteration được sử dụng để giải quyết bài toán Gambler's Problem bằng cách tìm ra chính sách đặt cược tối ưu.\n- Value Iteration được sử dụng để giải quyết bài toán Inventory Management và tìm ra chính sách (s,S) tối ưu.\n- Value Iteration sử dụng Bellman optimality update để cập nhật hàm giá trị V.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- Value Iteration và Policy Iteration là hai thuật toán quy hoạch động (Dynamic Programming) được sử dụng để tìm hàm giá trị tối ưu V* và chính sách tối ưu π* trong một Markov Decision Process (MDP) khi mô hình môi trường được biết.\n\n**Value Iteration (Lặp giá trị)**\nValue Iteration là một thuật toán tập trung vào việc tìm hàm giá trị tối ưu V*(s). Nó hoạt động bằng cách lặp đi lặp lại việc cập nhật hàm giá trị V(s) của mỗi trạng thái dựa trên phương trình Bellman Optimality Equation: `V[s] = max(action_values)` với `q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`. Quá trình này tiếp tục cho đến khi V(s) hội tụ về V*(s). Sau khi V*(s) hội tụ, chính sách tối ưu π* được trích xuất từ V*. Value Iteration đảm bảo hội tụ đến V* và π*, tìm được nghiệm tối ưu cho các MDP hữu hạn. Nó phù hợp khi không gian hành động lớn, cần giải nhanh và chỉ quan tâm đến chính sách cuối cùng, thực hiện một bước evaluation và một bước improvement trong mỗi lần lặp. Ví dụ ứng dụng là tìm chính sách (s,S) trong Quản lý tồn kho.\n\n**Policy Iteration (Lặp chính sách)**\nPolicy Iteration là một thuật toán tập trung vào việc tìm chính sách tối ưu π*. Nó bao gồm hai bước lặp đi lặp lại:\n1.  **Policy Evaluation (Đánh giá chính sách):** Tính toán hàm giá trị Vπ cho chính sách hiện tại π cho đến khi hội tụ, sử dụng công thức `V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`.\n2.  **Policy Improvement (Cải thiện chính sách):** Tạo ra một chính sách mới π' tham lam dựa trên Vπ đã tính, sử dụng công thức `policy[s] = np.argmax(action_values)` với `q_value = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)])`.\nQuá trình này lặp lại cho đến khi chính sách không còn được cải thiện, tức là đã đạt được chính sách tối ưu và hàm giá trị tối ưu. Policy Iteration là một phần của framework Generalized Policy Iteration (GPI) và đảm bảo hội tụ đến chính sách tối ưu π*. So với Value Iteration, Policy Iteration thường có ít tổng số lần lặp hơn nhưng mỗi lần lặp lại tốn nhiều thời gian hơn do bước evaluation đầy đủ, và thường hội tụ nhanh hơn.\n\n**Mối quan hệ:**\n- Policy Iteration và Value Iteration là hai thuật toán quy hoạch động được so sánh về số bước, thời gian mỗi iteration, tổng thời gian và khả năng sử dụng chính sách trung gian.\n- Value Iteration được sử dụng để giải quyết bài toán Gambler's Problem bằng cách tìm ra chính sách đặt cược tối ưu.\n- Value Iteration được sử dụng để giải quyết bài toán Inventory Management và tìm ra chính sách (s,S) tối ưu.\n- Value Iteration sử dụng Bellman optimality update để cập nhật hàm giá trị V."
        },
        "output_without_context": {
            "question": "Trong quá trình tối ưu hóa hàm giá trị của một MDP thông qua Q-learning, công thức nào được sử dụng để cập nhật giá trị Q cho một trạng thái và hành động cụ thể?",
            "answer": "Q(s, a) ← Q(s, a) + α[r + γ max Q(s', a') - Q(s, a)]"
        }
    },
    {
        "topic_description": "Chủ đề này sinh ra từ các khái niệm liên quan đến việc xấp xỉ hàm giá trị cho các MDP lớn. Học sinh sẽ được yêu cầu hiểu các phương pháp khác nhau của xấp xỉ hàm, bao gồm xấp xỉ tuyến tính và phi tuyến tính, cũng như cách chúng áp dụng trong học tăng cường.",
        "output_with_context": {
            "question": "Trong xấp xỉ hàm giá trị cho các MDP lớn, công thức nào được sử dụng để tối thiểu hóa độ sai lệch giữa hàm giá trị thực V^π và hàm xấp xỉ V̂(s; w)?",
            "answer": "Minimize J(w) = ||V^π - V̂_w||²_d",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n#### 13.1. Approximate Dynamic Programming\n- Sử dụng function approximation\n- Neural networks để biểu diễn V hoặc π\n- Trade-off giữa accuracy và scalability\n\n#### 13.2. Model-Free Methods\n- Không cần biết P và R\n- Học từ experience\n- Temporal-Difference Learning, Q-Learning (phần sau)\n\n#### 13.3. Deep Reinforcement Learning\n- Kết hợp DP với deep learning\n- DQN, Actor-Critic, PPO\n- Giải quyết được bài toán phức tạp\n\n### 14. Code Implementation - Ví dụ Python\n\n#### 14.1. Policy Iteration\n```python\ndef policy_iteration(env, gamma=0.9, theta=1e-6):\n    # Khởi tạo\n    V = np.zeros(env.num_states)\n    policy = np.zeros(env.num_states, dtype=int)\n    \n    while True:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.num_states):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([p * (r + gamma * V[s_]) \n                           for p, s_, r in env.transitions(s, a)])\n                delta = max(delta, abs(v - V[s]))\n            if delta < theta:\n                break\n        \n        # Policy Improvement\n        policy_stable = True\n        for s in range(env.num_states):\n            old_action = policy[s]\n            # Tìm hành động tốt nhất\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            policy[s] = np.argmax(action_values)\n            \n            if old_action != policy[s]:\n                policy_stable = False\n        \n        if policy_stable:\n            return V, policy\n```\n\n#### 14.2. Value Iteration\n```python\ndef value_iteration(env, gamma=0.9, theta=1e-6):\n    V = np.zeros(env.num_states)\n    \n    while True:\n        delta = 0\n        for s in range(env.num_states):\n            v = V[s]\n            # Bellman optimality update\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            V[s] = max(action_values)\n            delta = max(delta, abs(v - V[s]))\n        \n        if delta < theta:\n            break\n    \n    # Trích xuất chính sách\n    policy = np.zeros(env.num_states, dtype=int)\n    for s in range(env.num_states):\n        action_values = []\n        for a in range(env.num_actions):\n            q_value = sum([p * (r + gamma * V[s_]) \n                          for p, s_, r in env.transitions(s, a)])\n            action_values.append(q_value)\n        policy[s] = np.argmax(action_values)\n    \n    return V, policy\n```\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement policy evaluation cho Gridworld\n2. So sánh tốc độ hội tụ của in-place và two-array DP\n3. Visualize quá trình hội tụ của value iteration\n\n#### 15.2. Bài tập nâng cao\n1. Giải Gambler's Problem với các giá trị p khác nhau\n2. Implement prioritized sweeping\n3. Modified policy iteration với k=1, k=3, k=∞\n\n#### 15.3. Dự án\n1. Xây dựng AI cho game 2048 bằng DP\n2. Tối ưu hóa việc sạc pin cho robot\n3. Quản lý danh mục đầu tư bằng MDP\n\n### 16. Kết luận\n\n\n**Các khái niệm quan trọng:**\n- Accuracy (Độ chính xác) trong ngữ cảnh của function approximation đề cập đến mức độ gần đúng của hàm xấp xỉ so với hàm giá trị hoặc chính sách thực tế. Có một sự đánh đổi giữa độ chính xác và khả năng mở rộng khi sử dụng function approximation: các mô hình phức tạp hơn có thể chính xác hơn nhưng khó huấn luyện và mở rộng hơn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n     = Σₛ d(s)(V^π(s) - V̂(s; w))²\n```\n- d(s): distribution của states dưới policy π\n\n**Mục tiêu**: Minimize J(w) = ||V^π - V̂_w||²_d\n\n### 3. Stochastic Gradient Descent (SGD)\n\n#### 3.1. Gradient Descent\n\n**Update Rule**:\n```\nw_{t+1} = w_t - (α/2)∇_w J(w_t)\n        = w_t + α E[(V^π(s) - V̂(s; w))∇_w V̂(s; w)]\n```\n\n**Stochastic Gradient Descent**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]∇_w V̂(S_t; w_t)\n```\n\n#### 3.2. Linear SGD\n\n**Gradient của linear function**:\n```\n∇_w V̂(s; w) = ∇_w(w^T φ(s)) = φ(s)\n```\n\n**Update rule**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]φ(S_t)\n```\n\n**Đặc điểm**:\n- Converge đến local optimum (global cho linear)\n- Learning rate α quan trọng\n- Simple và efficient\n\n#### 3.3. Feature Scaling\n\n**Vấn đề**: Features có scale khác nhau → học không ổn định\n\n**Giải pháp**:\n```\nNormalization: φᵢ = (φᵢ - μᵢ)/σᵢ\nStandardization: φᵢ ∈ [0, 1]\n```\n\n### 4. Incremental Prediction Methods\n\n#### 4.1. Monte Carlo với Function Approximation\n\n**Update**:\n```\nw ← w + α[G_t - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ Target: actual return\n```\n\n**Đặc điểm**:\n- Unbiased target\n- High variance\n- Non-stationary target (G_t khác nhau mỗi episode)\n\n#### 4.2. TD(0) với Function Approximation\n\n**Update**:\n```\nw ← w + α[R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ TD target\n```\n\n**Semi-gradient**: Không lấy gradient qua V̂(S_{t+1}; w)\n\n**Thuật toán Semi-gradient TD(0)**:\n```\nKhởi tạo w arbitrarily\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        A ← π(S)\n        Thực hiện A, quan sát R, S'\n        \n        w ← w + α[R + γV̂(S'; w) - V̂(S; w)]∇_w V̂(S; w)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 4.3. TD(λ) với Function Approximation\n\n**Eligibility Traces**:\n```\nz_0 = 0\nz_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n```\n\n**Update**:\n```\nδ_t = R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)\nw ← w + αδ_t z_t\n```\n\n**Accumulating vs Replacing traces**:\n- Accumulating: z_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n- Replacing: Phức tạp hơn, phụ thuộc feature type\n\n### 5. Control với Function Approximation\n\n#### 5.1. Action-Value Function Approximation\n\n**Parameterization**:\n```\nQ̂(s, a; w) = w^T φ(s, a)\n```\n\n\n**Các khái niệm quan trọng:**\n- w là vector trọng số được sử dụng trong các phương pháp xấp xỉ hàm giá trị, ví dụ như trong Least Squares TD (LSTD) để biểu diễn hàm giá trị V̂(S; w) hoặc hàm Q̂(S, A; w). Mục tiêu của các thuật toán này là tìm ra w tối ưu để hàm xấp xỉ gần đúng nhất với hàm giá trị thực.\n- Control với Function Approximation là việc mở rộng các phương pháp học tăng cường để tìm kiếm chính sách tối ưu khi sử dụng xấp xỉ hàm giá trị, đặc biệt là xấp xỉ hàm giá trị hành động Q̂(s, a; w). Mục tiêu là học một chính sách tối ưu mà không cần lưu trữ bảng Q-value cho mọi cặp (s,a).\n- Monte Carlo với Function Approximation là một phương pháp học tăng cường sử dụng kỹ thuật Monte Carlo để ước lượng hàm giá trị, kết hợp với xấp xỉ hàm giá trị. Nó cập nhật các tham số w dựa trên return thực tế G_t quan sát được từ một episode hoàn chỉnh. Công thức cập nhật là w ← w + α[G_t - V̂(S_t; w)]∇_w V̂(S_t; w). Đặc điểm của nó là mục tiêu (G_t) không thiên vị nhưng có phương sai cao và không dừng (non-stationary).\n\n**Mối quan hệ:**\n- Monte Carlo với Function Approximation cập nhật các tham số w dựa trên return thực tế G_t.\n- Control với Function Approximation thường sử dụng Action-Value Function Approximation để học chính sách tối ưu.\n- TD(0) với Function Approximation cập nhật các tham số w dựa trên TD target.\n- TD(λ) với Function Approximation cập nhật các tham số w dựa trên TD error và eligibility traces.\n- Monte Carlo với Function Approximation sử dụng G_t làm mục tiêu để cập nhật hàm giá trị xấp xỉ.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\nReplay Buffer D = {(s₁, a₁, r₁, s'₁), ..., (sₙ, aₙ, rₙ, s'ₙ)}\n\nMỗi update:\n    Sample mini-batch từ D\n    Perform SGD update\n```\n\n**Lợi ích**:\n- Phá vỡ correlation giữa consecutive samples\n- Tái sử dụng data hiệu quả\n- Stabilize learning\n\n#### 7.2. Least Squares Methods\n\n**Least Squares TD (LSTD)**:\n```\nTìm w minimize:\n    E[(R + γV̂(S'; w) - V̂(S; w))²]\n\nClosed-form solution:\n    w = A^{-1}b\n    A = Σ φ(s)(φ(s) - γφ(s'))^T\n    b = Σ φ(s)r\n```\n\n**Đặc điểm**:\n- Data efficient\n- No learning rate\n- Computationally expensive: O(n³)\n\n#### 7.3. Fitted Q-Iteration\n\n**Ý tưởng**: Regression trên Bellman targets\n```\nLặp:\n    1. Tính targets: yᵢ = rᵢ + γ max_a Q̂(s'ᵢ, a; w)\n    2. Fit Q̂ để minimize: Σ(yᵢ - Q̂(sᵢ, aᵢ; w))²\n```\n\n**Batch Fitted Q-Iteration**:\n```\nGiven dataset D = {(s, a, r, s')}\n\nKhởi tạo Q̂₀ arbitrarily\n\nLặp k = 1, 2, ...:\n    Với mỗi (s, a, r, s') trong D:\n        y = r + γ max_{a'} Q̂_{k-1}(s', a')\n    \n    Q̂_k = argmin_Q Σ(y - Q(s, a))²\n```\n\n### 8. Deep Q-Networks (DQN)\n\n#### 8.1. Neural Networks làm Function Approximators\n\n**Architecture**:\n```\nInput: State s (hoặc raw observations như pixels)\nHidden Layers: Fully connected / Convolutional\nOutput: Q-values cho mỗi action\n```\n\n**Advantages**:\n- Automatic feature learning\n- Powerful representation capacity\n- End-to-end training\n\n#### 8.2. DQN Innovations\n\n**1. Experience Replay**:\n```\nReplay Buffer D với capacity N\nStore transitions: (s, a, r, s', done)\nSample random mini-batch để train\n```\n\n**2. Target Network**:\n```\nQ-network: Q(s, a; θ)\nTarget network: Q(s, a; θ⁻)\n\nUpdate Q-network mỗi step\nCopy θ → θ⁻ mỗi C steps\n```\n\n**TD Target**:\n```\ny = r + γ max_a Q(s', a; θ⁻)\n         ↑ Dùng target network\n```\n\n#### 8.3. DQN Algorithm\n\n```\nKhởi tạo replay buffer D\nKhởi tạo Q-network với random weights θ\nKhởi tạo target network θ⁻ = θ\n\nLặp với mỗi episode:\n    Khởi tạo state s\n    \n    Lặp với mỗi step:\n        Chọn action a:\n            - Với xác suất ε: random\n            - Ngược lại: a = argmax_a Q(s, a; θ)\n        \n        Thực hiện a, quan sát r, s'\n        Store transition (s, a, r, s', done) vào D\n        \n        Sample random mini-batch từ D\n        Với mỗi (s_j, a_j, r_j, s'_j, done_j):\n            y_j = r_j + γ(1 - done_j) max_a Q(s'_j, a; θ⁻)\n        \n        Perform SGD step trên (y_j - Q(s_j, a_j; θ))²\n        \n        Mỗi C steps: θ⁻ ← θ\n        \n        s ← s'\n```\n\n#### 8.4. DQN Improvements\n\n**Double DQN**:\n```\ny = r + γQ(s', argmax_a Q(s', a; θ), θ⁻)\n\n**Các khái niệm quan trọng:**\n- w là vector trọng số được sử dụng trong các phương pháp xấp xỉ hàm giá trị, ví dụ như trong Least Squares TD (LSTD) để biểu diễn hàm giá trị V̂(S; w) hoặc hàm Q̂(S, A; w). Mục tiêu của các thuật toán này là tìm ra w tối ưu để hàm xấp xỉ gần đúng nhất với hàm giá trị thực.\n\n**Mối quan hệ:**\n- TD(λ) với Function Approximation cập nhật các tham số w dựa trên TD error và eligibility traces.\n- Monte Carlo với Function Approximation cập nhật các tham số w dựa trên return thực tế G_t.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- Infinite MDP là một loại Quá trình Quyết định Markov trong đó tập hợp các trạng thái (S) hoặc tập hợp các hành động (A) (hoặc cả hai) là vô hạn. Đối với các MDP này, cần phải sử dụng các kỹ thuật xấp xỉ, chẳng hạn như xấp xỉ hàm giá trị, để giải quyết bài toán.\n\n**Mối quan hệ:**\n- Infinite MDP có tính chất là tập hành động A là vô hạn hoặc tập trạng thái S là vô hạn."
        },
        "output_without_context": {
            "question": "Trong xấp xỉ hàm giá trị cho các MDP lớn, phương pháp nào thường được sử dụng để cải thiện độ chính xác của dự đoán trong học tăng cường?",
            "answer": "Xấp xỉ phi tuyến tính"
        }
    },
    {
        "topic_description": "Chủ đề này yêu cầu học sinh so sánh ưu và nhược điểm của Học Monte-Carlo và Học khác biệt thời gian (TD). Các câu hỏi sẽ kiểm tra khả năng phân tích và đánh giá giữa hai phương pháp này trong bối cảnh học tăng cường.",
        "output_with_context": {
            "question": "So sánh ưu và nhược điểm của Học Monte-Carlo và Học khác biệt thời gian (TD) trong học tăng cường, phương pháp nào có độ thiên lệch thấp hơn và phương pháp nào có độ biến thiên cao hơn?",
            "answer": "Học Monte-Carlo có độ thiên lệch thấp hơn nhưng có độ biến thiên cao hơn.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n- Cần học nhanh\n- Online learning\n\n**TD(λ)**:\n- Khi cần cân bằng bias-variance\n- Credit assignment phức tạp\n- Eligibility traces quan trọng\n\n### 10. Code Implementation\n\n#### 10.1. Monte Carlo First-Visit\n```python\ndef monte_carlo_prediction(env, policy, num_episodes, gamma=0.99):\n    V = defaultdict(float)\n    returns = defaultdict(list)\n    \n    for _ in range(num_episodes):\n        episode = generate_episode(env, policy)\n        G = 0\n        visited = set()\n        \n        # Duyệt ngược từ cuối episode\n        for t in range(len(episode)-1, -1, -1):\n            state, action, reward = episode[t]\n            G = gamma * G + reward\n            \n            if state not in visited:\n                returns[state].append(G)\n                V[state] = np.mean(returns[state])\n                visited.add(state)\n    \n    return V\n```\n\n#### 10.2. TD(0)\n```python\ndef td_prediction(env, policy, num_episodes, alpha=0.1, gamma=0.99):\n    V = defaultdict(float)\n    \n    for _ in range(num_episodes):\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # TD update\n            td_target = reward + gamma * V[next_state]\n            td_error = td_target - V[state]\n            V[state] += alpha * td_error\n            \n            state = next_state\n    \n    return V\n```\n\n#### 10.3. TD(λ) with Eligibility Traces\n```python\ndef td_lambda_prediction(env, policy, num_episodes, \n                        alpha=0.1, gamma=0.99, lambda_=0.9):\n    V = defaultdict(float)\n    \n    for _ in range(num_episodes):\n        E = defaultdict(float)  # Eligibility traces\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # TD error\n            delta = reward + gamma * V[next_state] - V[state]\n            \n            # Update eligibility trace\n            E[state] += 1\n            \n            # Update all states\n            for s in E:\n                V[s] += alpha * delta * E[s]\n                E[s] *= gamma * lambda_\n            \n            state = next_state\n    \n    return V\n```\n\n### 11. Bài tập thực hành\n\n#### 11.1. Bài tập cơ bản\n1. Implement First-Visit MC cho Blackjack\n2. So sánh MC vs TD(0) trên Random Walk\n3. Visualize learning curves với different α\n\n#### 11.2. Bài tập nâng cao\n1. Implement n-step TD với n = 1, 3, 5, 10\n2. Compare TD(λ) với λ = 0, 0.5, 0.9, 1.0\n3. Analyze bias-variance tradeoff empirically\n\n#### 11.3. Dự án\n1. Build Tic-Tac-Toe AI với TD learning\n2. Robot navigation trong gridworld phức tạp\n3. Stock price prediction với TD methods\n\n### 12. Kết luận\n\nModel-Free Prediction giải quyết vấn đề quan trọng: **Đánh giá chính sách mà không cần biết mô hình môi trường**.\n\n**Các phương pháp chính**:\n- **Monte Carlo**: Đơn giản, unbiased, nhưng high variance\n- **TD(0)**: Efficient, online, nhưng biased\n- **TD(λ)**: Cân bằng tốt nhất, flexible\n\n**Key insights**:\n1. Bootstrap (TD) vs Full returns (MC) là tradeoff cơ bản\n2. TD thường hiệu quả hơn trong môi trường Markov\n3. Eligibility traces (TD(λ)) cung cấp spectrum liên tục\n4. Lựa chọn phương pháp phụ thuộc vào đặc điểm bài toán\n\n\n**Các khái niệm quan trọng:**\n- Bias-Variance Tradeoff là một khái niệm quan trọng trong học máy và các thuật toán ước lượng, mô tả sự đánh đổi giữa lỗi do giả định sai (bias) và lỗi do sự nhạy cảm với các biến động nhỏ trong tập huấn luyện (variance). Trong học tăng cường (RL), các phương pháp như Monte Carlo có bias thấp nhưng variance cao, trong khi TD(0) có bias cao hơn nhưng variance thấp hơn. Các phương pháp như n-Step TD và TD(λ) được thiết kế để cân bằng giữa hai yếu tố này.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n        Nếu S_t xuất hiện lần đầu tiên trong episode:\n            Thêm G vào Returns(S_t)\n            V(S_t) = average(Returns(S_t))\n```\n\n**Đặc điểm**:\n- Chỉ cập nhật cho lần xuất hiện đầu tiên của trạng thái\n- Không có bias\n- Đảm bảo hội tụ đến V^π(s) khi số episodes → ∞\n\n#### 2.4. Every-Visit Monte Carlo\n\n**Khác biệt**: Cập nhật cho mọi lần xuất hiện của trạng thái trong episode\n\n```\nVới mỗi bước t = T-1, T-2, ..., 0:\n    G = γG + R_{t+1}\n    Thêm G vào Returns(S_t)\n    V(S_t) = average(Returns(S_t))\n```\n\n**So sánh**:\n- Every-visit có variance thấp hơn\n- Cả hai đều hội tụ đến V^π(s)\n- Every-visit thường được sử dụng nhiều hơn\n\n#### 2.5. Incremental Mean Update\n\n**Vấn đề**: Lưu trữ tất cả returns không hiệu quả\n\n**Giải pháp**: Cập nhật incremental\n```\nCông thức tổng quát:\n    μ_k = μ_{k-1} + (1/k)(x_k - μ_{k-1})\n\nÁp dụng cho MC:\n    N(s) = N(s) + 1\n    V(s) = V(s) + (1/N(s))(G - V(s))\n    \nHoặc dùng learning rate α cố định:\n    V(s) = V(s) + α(G - V(s))\n```\n\n**Lợi ích**:\n- Tiết kiệm bộ nhớ\n- Cập nhật online\n- Quên dần các ước lượng cũ (với α cố định)\n\n#### 2.6. Ví dụ: Blackjack\n\n**Mô tả bài toán**:\n- **Trạng thái**: (tổng bài của người chơi, bài úp của dealer, có ace không)\n- **Hành động**: Hit (rút thêm) hoặc Stick (dừng)\n- **Phần thưởng**: +1 thắng, -1 thua, 0 hòa\n- **Chính sách**: Stick nếu tổng ≥ 20, ngược lại Hit\n\n**Ứng dụng MC**:\n1. Chơi nhiều games theo chính sách\n2. Ghi lại returns cho mỗi trạng thái\n3. Tính V^π bằng trung bình returns\n\n**Kết quả**:\n```\nV^π(20, ACE, usable_ace) ≈ 0.8  (rất tốt)\nV^π(12, 2, no_ace) ≈ -0.3       (tệ)\n```\n\n### 3. Temporal-Difference Learning (TD)\n\n#### 3.1. Giới thiệu\nTD Learning kết hợp ý tưởng của MC và DP:\n- Như MC: Học từ kinh nghiệm, không cần mô hình\n- Như DP: Bootstrap từ ước lượng hiện tại, không cần episode hoàn chỉnh\n\n#### 3.2. TD(0) - Temporal Difference cơ bản\n\n**TD Update Rule**:\n```\nV(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]\n```\n\n**Các thành phần**:\n- **TD Target**: R_{t+1} + γV(S_{t+1})\n- **TD Error**: δ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\n- **Update**: V(S_t) ← V(S_t) + α × δ_t\n\n#### 3.3. Thuật toán TD(0)\n\n```\n\n**Các khái niệm quan trọng:**\n- Temporal-Difference Learning (TD) là một loại thuật toán học tăng cường kết hợp ý tưởng từ Monte Carlo và Dynamic Programming. Giống như Monte Carlo, TD học từ kinh nghiệm mà không cần mô hình môi trường. Giống như Dynamic Programming, TD bootstrap từ các ước lượng giá trị hiện tại của các trạng thái tiếp theo, cho phép cập nhật giá trị mà không cần chờ đến cuối episode.\n\n**Mối quan hệ:**\n- Temporal-Difference Learning (TD) kết hợp ý tưởng bootstrap từ ước lượng hiện tại của Dynamic Programming, cho phép cập nhật mà không cần chờ đến cuối episode.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nKhởi tạo V(s) arbitrarily, ∀s ∈ S\n\nVới mỗi episode:\n    Khởi tạo S\n    \n    Lặp cho mỗi bước của episode:\n        A ← hành động từ S theo π\n        Thực hiện A, quan sát R, S'\n        \n        V(S) ← V(S) + α[R + γV(S') - V(S)]\n        \n        S ← S'\n    cho đến khi S là terminal\n```\n\n#### 3.4. So sánh MC vs TD(0)\n\n| Tiêu chí | Monte Carlo | TD(0) |\n|----------|-------------|-------|\n| Cập nhật | Cuối episode | Sau mỗi bước |\n| Bootstrap | Không | Có |\n| Target | G_t (actual return) | R + γV(S') |\n| Bias | Unbiased | Biased |\n| Variance | High variance | Low variance |\n| Hội tụ | Chậm | Nhanh hơn |\n| Môi trường | Cần episodic | Cả episodic và continuing |\n\n#### 3.5. Ưu điểm của TD\n\n✅ **Học online**: Cập nhật sau mỗi bước, không cần chờ episode kết thúc\n✅ **Continuing tasks**: Hoạt động với tasks không có điểm kết thúc\n✅ **Lower variance**: Bootstrap giảm variance so với MC\n✅ **Học nhanh hơn**: Thường hội tụ nhanh hơn MC trong thực tế\n✅ **Hiệu quả dữ liệu**: Sử dụng thông tin từ ước lượng hiện tại\n\n#### 3.6. Ví dụ minh họa: Random Walk\n\n**Setup**:\n```\nStates: [A] [B] [C] [D] [E]\nStart: C\nTerminal: Left of A hoặc Right of E\nReward: 0 mọi nơi, +1 khi đến Right of E\nAction: Left hoặc Right (random với p=0.5)\n```\n\n**True values**:\n```\nV^π(A) = 1/6, V^π(B) = 2/6, V^π(C) = 3/6,\nV^π(D) = 4/6, V^π(E) = 5/6\n```\n\n**So sánh MC vs TD**:\n- TD hội tụ nhanh hơn với cùng số episodes\n- TD ít sensitive với khởi tạo\n- MC có RMS error cao hơn trong giai đoạn đầu\n\n### 4. Batch Methods và Certainty Equivalence\n\n#### 4.1. Batch Learning\n**Ý tưởng**: Lặp lại huấn luyện trên cùng một batch experience cho đến hội tụ\n\n```\nCho trước batch experience: \n    {(S₁, A₁, R₁, S'₁), (S₂, A₂, R₂, S'₂), ...}\n\nLặp cho đến hội tụ:\n    Với mỗi experience (S, A, R, S'):\n        Cập nhật V(S) theo MC hoặc TD\n```\n\n#### 4.2. Certainty Equivalence\n\n**MC**: Tìm V^π minimize mean-squared error với observed returns\n```\nV^π = argmin_V Σ_episodes Σ_t (G_t - V(S_t))²\n```\n\n**TD**: Tìm V^π thỏa mãn phương trình Bellman cho MDP ước lượng\n```\nV^π(s) = E[R + γV^π(S') | s]\n```\n(ước lượng từ experience)\n\n**Kết quả**:\n- TD tận dụng cấu trúc Markov\n- MC chỉ minimize error, không exploit Markov property\n- TD thường hiệu quả hơn trong môi trường Markov\n\n#### 4.3. Ví dụ: AB Example\n\n**Experience**:\n```\nA, 0, B, 0\nB, 1 (terminal)\nB, 1 (terminal)\n\n**Các khái niệm quan trọng:**\n- Monte Carlo (MC) là một thuật toán học tăng cường model-free để ước lượng hàm giá trị. Nó cập nhật hàm giá trị chỉ sau khi một episode kết thúc, sử dụng tổng phần thưởng thực tế (actual return G_t) từ episode đó. MC là unbiased nhưng có variance cao. Nó yêu cầu các tasks phải là episodic.\n\n**Mối quan hệ:**\n- Monte Carlo yêu cầu các tasks phải là Episodic để có thể tính toán actual return G_t.\n- Trong Batch Learning, Monte Carlo tìm V^π bằng cách minimize Mean-squared error với observed returns.\n- Monte Carlo có tính chất High variance do phụ thuộc vào toàn bộ chuỗi phần thưởng ngẫu nhiên.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n#### 7.1. Game Playing\n\n**TD-Gammon** (1992):\n- Backgammon AI\n- Sử dụng TD(λ) với neural network\n- Self-play\n- Đạt world-champion level\n\n**AlphaGo Zero**:\n- Sử dụng TD-style updates\n- Self-play + MCTS\n- Không cần human knowledge\n\n#### 7.2. Robot Navigation\n\n**Task**: Robot tìm đường trong môi trường chưa biết\n- State: Vị trí robot\n- Action: Di chuyển\n- Reward: -1 mỗi bước, +100 khi đến đích\n\n**Ưu điểm TD**:\n- Học online trong quá trình điều hướng\n- Không cần đợi đến đích mới cập nhật\n- Adapt với môi trường thay đổi\n\n#### 7.3. Resource Management\n\n**Elevator Control**:\n- State: Vị trí thang máy, yêu cầu chờ\n- Action: Lên/xuống/đứng yên\n- Reward: -1 × tổng thời gian chờ\n\n**TD Learning**:\n- Học value function cho mỗi trạng thái\n- Online learning từ hoạt động hàng ngày\n- Cải thiện liên tục\n\n### 8. Phân tích lý thuyết\n\n#### 8.1. Tốc độ hội tụ\n\n**Monte Carlo**:\n```\nV_k(s) → V^π(s) với rate O(1/√k)\nk: số episodes\n```\n\n**TD(0)**:\n```\nV_k(s) → V^π(s) nhanh hơn trong thực tế\nKhông có bound lý thuyết chặt chẽ\n```\n\n**Thực nghiệm**: TD thường nhanh hơn MC 2-10 lần\n\n#### 8.2. Điều kiện hội tụ\n\n**Robbins-Monro conditions** cho learning rate α_t:\n```\nΣ_{t=1}^∞ α_t = ∞     (đảm bảo hội tụ)\nΣ_{t=1}^∞ α_t² < ∞    (đảm bảo variance hội tụ về 0)\n```\n\n**Ví dụ**:\n- α_t = 1/t: Thỏa mãn\n- α_t = 0.01: Không thỏa mãn điều kiện 1, nhưng practical\n\n#### 8.3. Bias-Variance Tradeoff\n\n**Monte Carlo**:\n- Bias = 0 (ước lượng không chệch)\n- Variance cao (phụ thuộc vào toàn bộ trajectory)\n\n**TD(0)**:\n- Bias > 0 (phụ thuộc vào V hiện tại)\n- Variance thấp (chỉ phụ thuộc 1 bước)\n\n**n-Step TD**: Cân bằng\n```\nBias giảm khi n tăng\nVariance tăng khi n tăng\n```\n\n### 9. So sánh tổng hợp\n\n#### 9.1. Bảng so sánh đầy đủ\n\n| Tiêu chí | MC | TD(0) | TD(λ) | DP |\n|----------|----|----|-------|-----|\n| Model-free | ✓ | ✓ | ✓ | ✗ |\n| Bootstrap | ✗ | ✓ | ✓ | ✓ |\n| Online | ✗ | ✓ | ✓ | ✓ |\n| Episodic only | ✓ | ✗ | ✗ | ✗ |\n| Bias | Low | High | Medium | Low |\n| Variance | High | Low | Medium | Low |\n| Convergence | Slow | Fast | Medium | Fastest |\n\n#### 9.2. Khi nào dùng phương pháp nào?\n\n**Monte Carlo**:\n- Môi trường không Markov\n- Cần ước lượng unbiased\n- Episodic tasks ngắn\n\n**TD(0)**:\n- Môi trường Markov\n- Continuing tasks\n\n**Các khái niệm quan trọng:**\n- Episodic only là một tính chất của các thuật toán học tăng cường chỉ có thể áp dụng cho các tác vụ có điểm kết thúc rõ ràng (episodes). Monte Carlo là thuật toán episodic only, trong khi TD(0), TD(λ), và DP có thể xử lý cả tác vụ episodic và continuing.\n\n**Mối quan hệ:**\n- Monte Carlo chỉ áp dụng cho các tác vụ episodic.\n- TD(0) không phải là thuật toán episodic only.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\nDynamic Programming là nền tảng quan trọng trong Reinforcement Learning. Mặc dù có hạn chế về yêu cầu biết mô hình và curse of dimensionality, DP cung cấp:\n\n- **Framework lý thuyết**: Hiểu rõ về optimal value functions và policies\n- **Nền tảng thuật toán**: Cơ sở cho các phương pháp model-free\n- **Công cụ thực tế**: Giải quyết được nhiều bài toán thực tế\n\nCác khái niệm từ DP (đặc biệt là GPI và Bellman equations) sẽ xuất hiện xuyên suốt trong các phương pháp học tăng cường tiếp theo, từ Temporal-Difference Learning đến Deep Q-Networks.\n\n---\n\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Learning\n\n#### 1.1. Định nghĩa\nModel-Free Learning là các phương pháp học tăng cường không yêu cầu biết trước mô hình môi trường (hàm chuyển trạng thái P và hàm phần thưởng R). Agent học trực tiếp từ kinh nghiệm tương tác với môi trường.\n\n#### 1.2. So sánh Model-Based vs Model-Free\n\n| Đặc điểm | Model-Based | Model-Free |\n|----------|-------------|------------|\n| Yêu cầu mô hình | Cần biết P và R | Không cần |\n| Học từ | Phương trình Bellman | Kinh nghiệm thực tế |\n| Ưu điểm | Hiệu quả dữ liệu | Linh hoạt, thực tế |\n| Nhược điểm | Khó có mô hình chính xác | Cần nhiều dữ liệu |\n| Ví dụ | DP, Model-Based RL | MC, TD, Q-Learning |\n\n#### 1.3. Bài toán Prediction\n**Mục tiêu**: Đánh giá một chính sách π cho trước\n- Input: Chính sách π\n- Output: Hàm giá trị V^π(s)\n- Không cần biết mô hình môi trường\n\n### 2. Monte Carlo Methods - Phương Pháp Monte Carlo\n\n#### 2.1. Ý tưởng cơ bản\nMonte Carlo (MC) ước lượng giá trị của trạng thái bằng cách lấy trung bình các return thực tế nhận được từ nhiều episodes.\n\n**Nguyên lý**:\n```\nV^π(s) = E_π[G_t | S_t = s]\n       ≈ average of returns từ trạng thái s\n```\n\n#### 2.2. Episode và Return\n\n**Episode**: Một chuỗi hoàn chỉnh từ trạng thái ban đầu đến kết thúc\n```\nS_0, A_0, R_1, S_1, A_1, R_2, ..., S_T\n```\n\n**Return**: Tổng phần thưởng chiết khấu\n```\nG_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... + γ^{T-t-1}R_T\n```\n\n#### 2.3. First-Visit Monte Carlo\n\n**Thuật toán**:\n```\nKhởi tạo:\n    V(s) = 0, ∀s\n    Returns(s) = danh sách rỗng, ∀s\n\nVới mỗi episode:\n    Tạo một episode tuân theo π: S_0, A_0, R_1, ..., S_T\n    G = 0\n    \n    Với mỗi bước t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        \n\n**Các khái niệm quan trọng:**\n- Monte Carlo (MC) là một nhóm các phương pháp model-free, không bootstrap trong Reinforcement Learning, học trực tiếp từ kinh nghiệm mà không yêu cầu biết mô hình môi trường. MC ước lượng giá trị của trạng thái hoặc cặp trạng thái-hành động, hoặc học chính sách, bằng cách lấy trung bình các return (tổng phần thưởng chiết khấu thực tế) nhận được từ nhiều episode hoàn chỉnh (từ trạng thái ban đầu đến trạng thái kết thúc). Việc học chỉ diễn ra sau khi một episode kết thúc. MC có bias thấp nhưng variance cao, chỉ phù hợp với các tác vụ episodic và có thể hội tụ đến local optimum hoặc global optimum với xấp xỉ hàm tuyến tính.\n\n**Mối quan hệ:**\n- Monte Carlo Methods ước lượng giá trị bằng cách lấy trung bình các return thực tế G_t.\n- Monte Carlo Methods yêu cầu các episode hoàn chỉnh để tính toán return.\n- Monte Carlo Methods ước lượng giá trị của trạng thái V^π(s)."
        },
        "output_without_context": {
            "question": "So với Học Monte-Carlo, ưu điểm chính của Học khác biệt thời gian (TD) trong học tăng cường là gì?",
            "answer": "Học TD có thể cập nhật giá trị trạng thái ngay lập tức mà không cần chờ đến kết quả cuối cùng."
        }
    },
    {
        "topic_description": "Chủ đề này kiểm tra khả năng ứng dụng xấp xỉ hàm giá trị hành động trong việc tối ưu hóa chính sách học. Học sinh sẽ gặp các câu hỏi trắc nghiệm yêu cầu giải thích mối liên hệ giữa các giá trị xấp xỉ và thực trong học tăng cường.",
        "output_with_context": {
            "question": "Xấp xỉ hàm giá trị hành động trong học tăng cường được định nghĩa bằng công thức nào sau đây?",
            "answer": "Q̂(s, a; w) = w^T φ(s, a)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n     = Σₛ d(s)(V^π(s) - V̂(s; w))²\n```\n- d(s): distribution của states dưới policy π\n\n**Mục tiêu**: Minimize J(w) = ||V^π - V̂_w||²_d\n\n### 3. Stochastic Gradient Descent (SGD)\n\n#### 3.1. Gradient Descent\n\n**Update Rule**:\n```\nw_{t+1} = w_t - (α/2)∇_w J(w_t)\n        = w_t + α E[(V^π(s) - V̂(s; w))∇_w V̂(s; w)]\n```\n\n**Stochastic Gradient Descent**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]∇_w V̂(S_t; w_t)\n```\n\n#### 3.2. Linear SGD\n\n**Gradient của linear function**:\n```\n∇_w V̂(s; w) = ∇_w(w^T φ(s)) = φ(s)\n```\n\n**Update rule**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]φ(S_t)\n```\n\n**Đặc điểm**:\n- Converge đến local optimum (global cho linear)\n- Learning rate α quan trọng\n- Simple và efficient\n\n#### 3.3. Feature Scaling\n\n**Vấn đề**: Features có scale khác nhau → học không ổn định\n\n**Giải pháp**:\n```\nNormalization: φᵢ = (φᵢ - μᵢ)/σᵢ\nStandardization: φᵢ ∈ [0, 1]\n```\n\n### 4. Incremental Prediction Methods\n\n#### 4.1. Monte Carlo với Function Approximation\n\n**Update**:\n```\nw ← w + α[G_t - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ Target: actual return\n```\n\n**Đặc điểm**:\n- Unbiased target\n- High variance\n- Non-stationary target (G_t khác nhau mỗi episode)\n\n#### 4.2. TD(0) với Function Approximation\n\n**Update**:\n```\nw ← w + α[R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ TD target\n```\n\n**Semi-gradient**: Không lấy gradient qua V̂(S_{t+1}; w)\n\n**Thuật toán Semi-gradient TD(0)**:\n```\nKhởi tạo w arbitrarily\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        A ← π(S)\n        Thực hiện A, quan sát R, S'\n        \n        w ← w + α[R + γV̂(S'; w) - V̂(S; w)]∇_w V̂(S; w)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 4.3. TD(λ) với Function Approximation\n\n**Eligibility Traces**:\n```\nz_0 = 0\nz_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n```\n\n**Update**:\n```\nδ_t = R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)\nw ← w + αδ_t z_t\n```\n\n**Accumulating vs Replacing traces**:\n- Accumulating: z_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n- Replacing: Phức tạp hơn, phụ thuộc feature type\n\n### 5. Control với Function Approximation\n\n#### 5.1. Action-Value Function Approximation\n\n**Parameterization**:\n```\nQ̂(s, a; w) = w^T φ(s, a)\n```\n\n\n**Các khái niệm quan trọng:**\n- Control với Function Approximation là việc mở rộng các phương pháp học tăng cường để tìm kiếm chính sách tối ưu khi sử dụng xấp xỉ hàm giá trị, đặc biệt là xấp xỉ hàm giá trị hành động Q̂(s, a; w). Mục tiêu là học một chính sách tối ưu mà không cần lưu trữ bảng Q-value cho mọi cặp (s,a).\n- Action-Value Function Approximation là kỹ thuật xấp xỉ hàm Q-function Q(s,a) bằng một hàm tham số Q̂(s, a; w), thay vì lưu trữ giá trị cho từng cặp trạng thái-hành động. Điều này đặc biệt hữu ích khi không gian trạng thái hoặc hành động lớn. Một dạng phổ biến là Q̂(s, a; w) = w^T φ(s, a), trong đó φ(s, a) là vector đặc trưng của cặp (s,a).\n\n**Mối quan hệ:**\n- Control với Function Approximation thường sử dụng Action-Value Function Approximation để học chính sách tối ưu.\n- Action-Value Function Approximation định nghĩa Q̂(s, a; w) là hàm giá trị hành động xấp xỉ.\n- Action-Value Function Approximation sử dụng vector đặc trưng φ(s, a) để tham số hóa Q̂(s, a; w).\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n2. Add Double DQN improvement\n3. Experiment với network architectures\n\n#### 13.3. Dự án\n1. Build Atari game player với DQN\n2. Implement Prioritized Experience Replay\n3. Compare tabular Q-learning vs FA Q-learning\n\n### 14. Kết luận\n\nValue Function Approximation là bước đột phá cho phép RL scale lên bài toán thực tế với large/continuous state spaces.\n\n**Key Takeaways**:\n\n1. **Function Approximation**: Generalization thay vì memorization\n2. **Linear Methods**: Simple, stable, nhưng limited expressiveness\n3. **Neural Networks**: Powerful nhưng cần careful engineering\n4. **DQN**: Breakthrough với experience replay + target network\n5. **Stability**: Deadly triad cần được xử lý cẩn thận\n\n**Convergence challenges**:\n- Semi-gradient methods không follow true gradient\n- Off-policy + FA + bootstrapping = risk of divergence\n- Practical techniques (replay, target network) help significantly\n\n**Tiếp theo**: **Policy Gradient Methods** - Học trực tiếp policy thay vì value function!\n\n---\n\n## Policy Gradient Methods - Phương Pháp Gradient Chính Sách\n\n### 1. Giới thiệu về Policy Gradient\n\n#### 1.1. Value-Based vs Policy-Based\n\n**Value-Based Methods** (Q-Learning, DQN):\n```\nHọc Q(s,a) → Derive policy: π(s) = argmax_a Q(s,a)\n```\n- Indirect: Học value rồi suy ra policy\n- Deterministic policies\n- Khó với continuous actions\n\n**Policy-Based Methods**:\n```\nHọc trực tiếp π(a|s; θ)\n```\n- Direct: Parameterize và optimize policy\n- Stochastic policies tự nhiên\n- Hiệu quả với continuous actions\n\n#### 1.2. Ưu điểm của Policy Gradient\n\n✅ **Continuous action spaces**: Không cần discretization\n✅ **Stochastic policies**: Tự nhiên cho exploration và game theory\n✅ **Better convergence**: Smooth optimization landscape\n✅ **Effective in high dimensions**: Especially với function approximation\n✅ **Learn policies directly**: Không qua intermediate value function\n\n#### 1.3. Nhược điểm\n\n❌ **High variance**: Gradient estimates có variance cao\n❌ **Sample inefficient**: Cần nhiều samples\n❌ **Local optima**: Có thể stuck tại local optima\n❌ **Slow convergence**: Thường chậm hơn value-based\n\n### 2. Policy Parameterization\n\n#### 2.1. Discrete Action Spaces\n\n**Softmax Policy** (Gibbs/Boltzmann):\n```\nπ(a|s; θ) = exp(h(s,a;θ)) / Σ_b exp(h(s,b;θ))\n```\n\n**Linear Preferences**:\n```\nh(s,a;θ) = θ^T φ(s,a)\n```\n\n**Neural Network**:\n```\nInput: State s\nHidden Layers: Neural network\nOutput: Logits h(s,a;θ)\nPolicy: π(a|s;θ) = softmax(h(s,·;θ))\n```\n\n#### 2.2. Continuous Action Spaces\n\n**Gaussian Policy**:\n```\nπ(a|s; θ) = N(μ(s;θ), σ²)\n\na ~ N(μ(s;θ), σ²)\n```\n\n**Parameterization**:\n```\nμ(s;θ) = θ^T φ(s)  [Linear]\nμ(s;θ) = NN(s;θ)   [Neural Network]\n\nσ có thể:\n- Fixed constant\n- State-dependent: σ(s;θ)\n- Action-dependent\n```\n\n**Beta Distribution** (bounded actions):\n```\na ∈ [0, 1]\nπ(a|s;θ) = Beta(α(s;θ), β(s;θ))\n```\n\n#### 2.3. Ví dụ minh họa\n\n**CartPole** (Discrete):\n```python\nclass PolicyNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, action_dim)\n    \n    def forward(self, state):\n\n**Các khái niệm quan trọng:**\n- Value-Based Methods là một loại thuật toán học tăng cường học một hàm giá trị (ví dụ: Q(s,a)) và sau đó suy ra một chính sách từ hàm giá trị đó, thường là π(s) = argmax_a Q(s,a). Các ví dụ bao gồm Q-Learning và DQN. Chúng thường tạo ra các chính sách xác định và có thể gặp khó khăn với không gian hành động liên tục.\n\n**Mối quan hệ:**\n- Value-Based Methods suy ra chính sách π(s) từ hàm Q(s,a).\n- Value-Based Methods học hàm Q-function Q(s,a).\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- Policy Improvement Theorem phát biểu rằng nếu Q-value của hành động được chọn bởi chính sách mới π' tại trạng thái s, dưới chính sách cũ π (tức là Q^π(s, π'(s))), lớn hơn hoặc bằng V-value của trạng thái s dưới chính sách cũ π (tức là V^π(s)), thì chính sách mới π' sẽ tốt hơn hoặc bằng chính sách cũ π. Điều này có nghĩa là hàm giá trị của chính sách mới π' sẽ lớn hơn hoặc bằng hàm giá trị của chính sách cũ π cho mọi trạng thái s.\n\n**Mối quan hệ:**\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n#### 13.1. Approximate Dynamic Programming\n- Sử dụng function approximation\n- Neural networks để biểu diễn V hoặc π\n- Trade-off giữa accuracy và scalability\n\n#### 13.2. Model-Free Methods\n- Không cần biết P và R\n- Học từ experience\n- Temporal-Difference Learning, Q-Learning (phần sau)\n\n#### 13.3. Deep Reinforcement Learning\n- Kết hợp DP với deep learning\n- DQN, Actor-Critic, PPO\n- Giải quyết được bài toán phức tạp\n\n### 14. Code Implementation - Ví dụ Python\n\n#### 14.1. Policy Iteration\n```python\ndef policy_iteration(env, gamma=0.9, theta=1e-6):\n    # Khởi tạo\n    V = np.zeros(env.num_states)\n    policy = np.zeros(env.num_states, dtype=int)\n    \n    while True:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.num_states):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([p * (r + gamma * V[s_]) \n                           for p, s_, r in env.transitions(s, a)])\n                delta = max(delta, abs(v - V[s]))\n            if delta < theta:\n                break\n        \n        # Policy Improvement\n        policy_stable = True\n        for s in range(env.num_states):\n            old_action = policy[s]\n            # Tìm hành động tốt nhất\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            policy[s] = np.argmax(action_values)\n            \n            if old_action != policy[s]:\n                policy_stable = False\n        \n        if policy_stable:\n            return V, policy\n```\n\n#### 14.2. Value Iteration\n```python\ndef value_iteration(env, gamma=0.9, theta=1e-6):\n    V = np.zeros(env.num_states)\n    \n    while True:\n        delta = 0\n        for s in range(env.num_states):\n            v = V[s]\n            # Bellman optimality update\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            V[s] = max(action_values)\n            delta = max(delta, abs(v - V[s]))\n        \n        if delta < theta:\n            break\n    \n    # Trích xuất chính sách\n    policy = np.zeros(env.num_states, dtype=int)\n    for s in range(env.num_states):\n        action_values = []\n        for a in range(env.num_actions):\n            q_value = sum([p * (r + gamma * V[s_]) \n                          for p, s_, r in env.transitions(s, a)])\n            action_values.append(q_value)\n        policy[s] = np.argmax(action_values)\n    \n    return V, policy\n```\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement policy evaluation cho Gridworld\n2. So sánh tốc độ hội tụ của in-place và two-array DP\n3. Visualize quá trình hội tụ của value iteration\n\n#### 15.2. Bài tập nâng cao\n1. Giải Gambler's Problem với các giá trị p khác nhau\n2. Implement prioritized sweeping\n3. Modified policy iteration với k=1, k=3, k=∞\n\n#### 15.3. Dự án\n1. Xây dựng AI cho game 2048 bằng DP\n2. Tối ưu hóa việc sạc pin cho robot\n3. Quản lý danh mục đầu tư bằng MDP\n\n### 16. Kết luận\n\n\n**Các khái niệm quan trọng:**\n- Accuracy (Độ chính xác) trong ngữ cảnh của function approximation đề cập đến mức độ gần đúng của hàm xấp xỉ so với hàm giá trị hoặc chính sách thực tế. Có một sự đánh đổi giữa độ chính xác và khả năng mở rộng khi sử dụng function approximation: các mô hình phức tạp hơn có thể chính xác hơn nhưng khó huấn luyện và mở rộng hơn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*."
        },
        "output_without_context": {
            "question": "Xấp xỉ hàm giá trị hành động trong học tăng cường được sử dụng để làm gì?",
            "answer": "Tối ưu hóa chính sách học."
        }
    },
    {
        "topic_description": "Chủ đề này kết nối các khái niệm từ MDP và kiểm soát không mô hình từ tuần 4 với các ứng dụng thực tiễn. Học sinh cần hiểu rõ mối quan hệ giữa các khái niệm đã học trong các tuần trước và tuần hiện tại.",
        "output_with_context": {
            "question": "Trong một Quá trình Quyết Định Markov (MDP), công thức nào được sử dụng để xác định chính sách tối ưu π*(s) từ hàm giá trị Q*(s,a)?",
            "answer": "π*(s) = argmaxₐ Q*(s,a)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- Episodic MDP là một loại Quá trình Quyết định Markov có điểm kết thúc rõ ràng, được gọi là trạng thái kết thúc (terminal state). Các ví dụ bao gồm các trò chơi cờ hoặc robot đi mê cung, nơi nhiệm vụ kết thúc khi đạt được mục tiêu hoặc một điều kiện nhất định.\n- Continuing MDP là một loại Quá trình Quyết định Markov không có điểm kết thúc rõ ràng và chạy vô hạn. Các ví dụ bao gồm hệ thống kiểm soát nhiệt độ hoặc các tác vụ điều khiển liên tục, nơi tác nhân phải hoạt động liên tục mà không có một \"kết thúc\" cụ thể.\n- State là một thành phần của MDP, biểu diễn tình trạng hiện tại của môi trường. Trong bài toán Car Rental, State là một cặp (n1, n2) biểu thị số xe tại địa điểm 1 và địa điểm 2. Trong Inventory Management, State là mức tồn kho hiện tại. Agent sử dụng State để đưa ra quyết định hành động.\n\n**Mối quan hệ:**\n- Markov Decision Processes có thành phần là State (Trạng thái), mô tả tình hình hiện tại của môi trường.\n- Finite MDP có tính chất là tập trạng thái S là hữu hạn.\n- Infinite MDP có tính chất là tập trạng thái S là vô hạn hoặc tập hành động A là vô hạn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Môi trường không hoàn toàn quan sát được là một thách thức trong Reinforcement Learning, nơi MDP chuẩn giả định biết hoàn toàn trạng thái, nhưng thực tế agent thường chỉ có thể quan sát một phần trạng thái (được mô hình hóa bởi POMDP - Partially Observable Markov Decision Process).\n\n**Mối quan hệ:**\n- Môi trường không hoàn toàn quan sát được là một giới hạn của mô hình Markov Decision Processes chuẩn, dẫn đến việc sử dụng POMDP.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\nDynamic Programming là nền tảng quan trọng trong Reinforcement Learning. Mặc dù có hạn chế về yêu cầu biết mô hình và curse of dimensionality, DP cung cấp:\n\n- **Framework lý thuyết**: Hiểu rõ về optimal value functions và policies\n- **Nền tảng thuật toán**: Cơ sở cho các phương pháp model-free\n- **Công cụ thực tế**: Giải quyết được nhiều bài toán thực tế\n\nCác khái niệm từ DP (đặc biệt là GPI và Bellman equations) sẽ xuất hiện xuyên suốt trong các phương pháp học tăng cường tiếp theo, từ Temporal-Difference Learning đến Deep Q-Networks.\n\n---\n\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Learning\n\n#### 1.1. Định nghĩa\nModel-Free Learning là các phương pháp học tăng cường không yêu cầu biết trước mô hình môi trường (hàm chuyển trạng thái P và hàm phần thưởng R). Agent học trực tiếp từ kinh nghiệm tương tác với môi trường.\n\n#### 1.2. So sánh Model-Based vs Model-Free\n\n| Đặc điểm | Model-Based | Model-Free |\n|----------|-------------|------------|\n| Yêu cầu mô hình | Cần biết P và R | Không cần |\n| Học từ | Phương trình Bellman | Kinh nghiệm thực tế |\n| Ưu điểm | Hiệu quả dữ liệu | Linh hoạt, thực tế |\n| Nhược điểm | Khó có mô hình chính xác | Cần nhiều dữ liệu |\n| Ví dụ | DP, Model-Based RL | MC, TD, Q-Learning |\n\n#### 1.3. Bài toán Prediction\n**Mục tiêu**: Đánh giá một chính sách π cho trước\n- Input: Chính sách π\n- Output: Hàm giá trị V^π(s)\n- Không cần biết mô hình môi trường\n\n### 2. Monte Carlo Methods - Phương Pháp Monte Carlo\n\n#### 2.1. Ý tưởng cơ bản\nMonte Carlo (MC) ước lượng giá trị của trạng thái bằng cách lấy trung bình các return thực tế nhận được từ nhiều episodes.\n\n**Nguyên lý**:\n```\nV^π(s) = E_π[G_t | S_t = s]\n       ≈ average of returns từ trạng thái s\n```\n\n#### 2.2. Episode và Return\n\n**Episode**: Một chuỗi hoàn chỉnh từ trạng thái ban đầu đến kết thúc\n```\nS_0, A_0, R_1, S_1, A_1, R_2, ..., S_T\n```\n\n**Return**: Tổng phần thưởng chiết khấu\n```\nG_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... + γ^{T-t-1}R_T\n```\n\n#### 2.3. First-Visit Monte Carlo\n\n**Thuật toán**:\n```\nKhởi tạo:\n    V(s) = 0, ∀s\n    Returns(s) = danh sách rỗng, ∀s\n\nVới mỗi episode:\n    Tạo một episode tuân theo π: S_0, A_0, R_1, ..., S_T\n    G = 0\n    \n    Với mỗi bước t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        \n\n**Các khái niệm quan trọng:**\n- Model-Based Learning là các phương pháp học tăng cường yêu cầu biết trước hoặc học một mô hình của môi trường (hàm chuyển trạng thái P và hàm phần thưởng R). Các phương pháp này thường hiệu quả dữ liệu hơn nhưng khó có mô hình chính xác trong các môi trường phức tạp.\n\n**Mối quan hệ:**\n- Model-Based Learning cần biết hàm phần thưởng R.\n- Model-Based Learning học từ các phương trình Bellman.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Ví dụ: Bàn cờ vây 19×19 có ~10^170 trạng thái\n\n**Giải pháp**:\n- Function approximation (phần sau)\n- Sampling-based methods\n- Hierarchical RL\n\n### 10. Ví dụ thực tế: Car Rental Problem\n\n#### 10.1. Mô tả bài toán\n**Jack's Car Rental**:\n- 2 địa điểm cho thuê xe\n- Mỗi đêm có thể di chuyển tối đa 5 xe giữa 2 địa điểm\n- Chi phí di chuyển: $2/xe\n- Thu nhập thuê xe: $10/xe\n- Số xe được thuê/trả theo phân phối Poisson\n\n#### 10.2. Biểu diễn MDP\n```\nState: (n1, n2) - số xe tại mỗi địa điểm (0-20)\nAction: -5 đến +5 (số xe di chuyển từ địa điểm 1 đến 2)\nReward: 10 × (số xe thuê được) - 2 × |action|\nTransition: Theo phân phối Poisson\n```\n\n#### 10.3. Giải bằng Policy Iteration\n\n**Chính sách khởi tạo**: Không di chuyển xe\n**Sau iteration 1**: Di chuyển xe từ địa điểm thừa sang thiếu\n**Hội tụ**: ~4-5 iterations\n**Chính sách tối ưu**: Cân bằng số xe giữa 2 địa điểm\n\n#### 10.4. Kết quả\n```\nV*(10,10) ≈ $500\nChiến lược: Luôn giữ ~10 xe mỗi địa điểm\n```\n\n### 11. Ví dụ thực tế: Inventory Management\n\n#### 11.1. Bài toán quản lý kho\n\n**Setup**:\n- Tồn kho từ 0 đến MAX_INVENTORY\n- Mỗi kỳ: Quyết định đặt hàng bao nhiêu\n- Chi phí đặt hàng + chi phí lưu kho\n- Doanh thu từ bán hàng\n- Mất khách nếu hết hàng\n\n#### 11.2. MDP Formulation\n```\nState: Mức tồn kho hiện tại\nAction: Số lượng đặt hàng\nReward: Revenue - Ordering_cost - Holding_cost - Stockout_penalty\nTransition: Stochastic demand\n```\n\n#### 11.3. Giải pháp\n- Sử dụng Value Iteration\n- Tìm chính sách (s,S): Đặt hàng đến mức S khi tồn kho ≤ s\n- Optimize (s,S) parameters\n\n### 12. Ưu điểm và hạn chế của Dynamic Programming\n\n#### 12.1. Ưu điểm\n✅ **Đảm bảo tối ưu**: Hội tụ đến chính sách tối ưu π*\n✅ **Cơ sở lý thuyết vững chắc**: Dựa trên phương trình Bellman\n✅ **Hiệu quả với medium-sized problems**: Polynomial complexity\n✅ **Framework cho nhiều thuật toán khác**: Nền tảng cho TD, Q-learning\n\n#### 12.2. Hạn chế\n❌ **Yêu cầu biết hoàn toàn mô hình**: Cần biết P và R\n❌ **Curse of dimensionality**: Không mở rộng cho không gian lớn\n❌ **Cần duyệt tất cả trạng thái**: Không practical với continuous states\n❌ **Computation cost**: Tốn nhiều tính toán mỗi iteration\n\n### 13. Mở rộng và cải tiến\n\n\n**Các khái niệm quan trọng:**\n- State là một thành phần của MDP, biểu diễn tình trạng hiện tại của môi trường. Trong bài toán Car Rental, State là một cặp (n1, n2) biểu thị số xe tại địa điểm 1 và địa điểm 2. Trong Inventory Management, State là mức tồn kho hiện tại. Agent sử dụng State để đưa ra quyết định hành động.\n\n**Mối quan hệ:**\n- Markov Decision Processes có thành phần là State (Trạng thái), mô tả tình hình hiện tại của môi trường.\n- Infinite MDP có tính chất là tập trạng thái S là vô hạn hoặc tập hành động A là vô hạn."
        },
        "output_without_context": {
            "question": "Làm thế nào bạn có thể áp dụng các khái niệm từ MDP để thiết kế một hệ thống kiểm soát không mô hình trong một tình huống thực tiễn?",
            "answer": "Sử dụng các chính sách tối ưu từ MDP để hướng dẫn quyết định trong môi trường không mô hình."
        }
    }
]