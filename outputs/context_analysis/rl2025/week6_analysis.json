[
    {
        "topic_description": "Chủ đề này giải thích các khái niệm cơ bản về học tăng cường dựa trên chính sách, bao gồm cách thức mà chính sách được tham số hóa và áp dụng trong các tình huống khác nhau. Học sinh sẽ kiểm tra sự hiểu biết về công thức và ứng dụng cụ thể, chẳng hạn như trong trò chơi Rock-Paper-Scissors.",
        "output_with_context": {
            "question": "Chính sách π* trong học tăng cường được định nghĩa như thế nào?",
            "answer": "Chính sách π* được gọi là tối ưu nếu Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n**Định nghĩa**: Một dãy chính sách {π_t} thỏa mãn GLIE nếu:\n1. Mọi cặp (s,a) được visit vô hạn lần\n2. Chính sách hội tụ đến greedy policy\n\n**Ví dụ GLIE**:\n```\nε_t = 1/t\n```\n\n**Định lý**: GLIE Monte Carlo Control hội tụ đến π*\n\n#### 2.6. Ví dụ: Blackjack với MC Control\n\n**Setup**:\n- State: (player_sum, dealer_card, usable_ace)\n- Action: Hit hoặc Stick\n- Reward: +1 (win), -1 (lose), 0 (draw)\n\n**Kết quả**:\n```\nChính sách học được:\n- Stick khi player_sum ≥ 20\n- Hit khi player_sum < 12\n- Phức tạp hơn ở vùng 12-19 (phụ thuộc dealer card)\n```\n\n### 3. On-Policy vs Off-Policy Learning\n\n#### 3.1. Định nghĩa\n\n**On-Policy**:\n- Học về chính sách π từ experience generated bởi π\n- Evaluate và improve cùng một chính sách\n- Ví dụ: SARSA, Monte Carlo Control\n\n**Off-Policy**:\n- Học về chính sách π (target) từ experience của μ (behavior)\n- π ≠ μ\n- Ví dụ: Q-Learning, Importance Sampling\n\n#### 3.2. So sánh\n\n| Đặc điểm | On-Policy | Off-Policy |\n|----------|-----------|------------|\n| Chính sách | Một chính sách | Hai chính sách |\n| Sample efficiency | Thấp hơn | Cao hơn |\n| Variance | Thấp | Cao |\n| Converge | Ổn định | Có thể diverge |\n| Use old data | Không | Có thể |\n| Learn optimal | Không (nếu ε-greedy) | Có |\n\n### 4. SARSA - On-Policy TD Control\n\n#### 4.1. Ý tưởng\nÁp dụng TD(0) cho Q(s,a) thay vì V(s)\n\n**TD Update cho Q**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n```\n\n**Tên gọi**: SARSA = (S, A, R, S', A')\n\n#### 4.2. Thuật toán SARSA\n\n```\nKhởi tạo Q(s,a) arbitrarily, ∀s ∈ S, a ∈ A\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    Chọn A từ S theo ε-greedy(Q)\n    \n    Lặp với mỗi bước:\n        Thực hiện A, quan sát R, S'\n        Chọn A' từ S' theo ε-greedy(Q)\n        \n        Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n        \n        S ← S'; A ← A'\n    cho đến S là terminal\n```\n\n#### 4.3. Tính chất của SARSA\n\n**Hội tụ**:\n- GLIE schedule + Robbins-Monro conditions → Q → Q*\n- Trong thực tế, dùng ε nhỏ cố định hoặc decay\n\n**On-Policy**:\n- Học về chính sách ε-greedy đang dùng\n- Safe: Tính đến exploration trong học\n\n#### 4.4. Ví dụ: Windy Gridworld\n\n**Mô tả**:\n- Grid 7×10 với \"wind\" ở một số cột\n- Wind đẩy agent lên 1-2 ô\n- Start: (3,0), Goal: (3,7)\n- Actions: 4 hướng\n\n**Kết quả SARSA**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- On-Policy Learning là một phương pháp học tăng cường trong đó agent học về chính sách π từ kinh nghiệm được tạo ra bởi chính sách π đó, nghĩa là chính sách được sử dụng để tạo dữ liệu (behavior policy) cũng chính là chính sách đang được cải thiện (target policy), với các ví dụ điển hình như SARSA và Monte Carlo Control. Ngược lại, Off-Policy Learning là một phương pháp học tăng cường mà agent học về một chính sách π (target policy) từ kinh nghiệm được tạo ra bởi một chính sách khác μ (behavior policy), trong đó π ≠ μ, cho phép agent học về chính sách tối ưu trong khi vẫn thực hiện các hành động khám phá theo một chính sách khác, với các ví dụ điển hình là Q-Learning và Importance Sampling.\n\n**Mối quan hệ:**\n- SARSA là một ví dụ của thuật toán On-Policy Learning.\n- On-Policy Learning đánh giá và cải thiện cùng một chính sách π.\n- On-Policy Learning học về chính sách π từ kinh nghiệm được tạo ra bởi chính sách π.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- Greedy Policy Improvement là một kỹ thuật cải thiện chính sách bằng cách chọn hành động tối ưu (tham lam) dựa trên hàm giá trị hiện tại V^π(s). Chính sách mới π'(s) được xác định bằng cách chọn hành động a mang lại Q-value cao nhất tại trạng thái s, tức là π'(s) = argmax_a Q^π(s,a). Kỹ thuật này đảm bảo rằng chính sách mới sẽ tốt hơn hoặc bằng chính sách cũ.\n\n**Mối quan hệ:**\n- Greedy Policy Improvement sử dụng Q^π(s,a) để xác định hành động tối ưu tại mỗi trạng thái.\n- Greedy Policy Improvement sử dụng V^π(s) (thông qua Q^π(s,a)) để chọn hành động tốt nhất.\n- Greedy Policy Improvement là một ứng dụng của Policy Improvement Theorem, đảm bảo rằng chính sách mới sẽ tốt hơn hoặc bằng chính sách cũ.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nMột quá trình được gọi là có tính chất Markov nếu:\n```\nP[Sₜ₊₁ | Sₜ] = P[Sₜ₊₁ | S₁, S₂, ..., Sₜ]\n```\n\n#### 3.2. Ý nghĩa\n- Trạng thái hiện tại chứa đầy đủ thông tin để dự đoán tương lai\n- Không cần phải nhớ toàn bộ lịch sử\n- Đơn giản hóa việc tính toán và phân tích\n\n### 4. Chính sách (Policy)\n\n#### 4.1. Định nghĩa\nChính sách π là một hàm ánh xạ từ trạng thái đến hành động:\n- **Chính sách xác định (Deterministic)**: π(s) = a\n- **Chính sách ngẫu nhiên (Stochastic)**: π(a|s) = P[Aₜ = a | Sₜ = s]\n\n#### 4.2. Phân loại\n- **Chính sách tĩnh (Stationary)**: Không thay đổi theo thời gian\n- **Chính sách động (Non-stationary)**: Thay đổi theo thời gian\n\n### 5. Hàm giá trị (Value Functions)\n\n#### 5.1. Hàm giá trị trạng thái (State-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s và tuân theo chính sách π\n\n```\nVᵖ(s) = Eᵖ[Gₜ | Sₜ = s]\n      = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s]\n```\n\n#### 5.2. Hàm giá trị hành động (Action-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s, thực hiện hành động a, và sau đó tuân theo chính sách π\n\n```\nQᵖ(s,a) = Eᵖ[Gₜ | Sₜ = s, Aₜ = a]\n        = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s, Aₜ = a]\n```\n\n#### 5.3. Mối quan hệ giữa V và Q\n```\nVᵖ(s) = Σₐ π(a|s)Qᵖ(s,a)\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')\n```\n\n### 6. Phương trình Bellman\n\n#### 6.1. Phương trình Bellman cho Vᵖ\n```\nVᵖ(s) = Σₐ π(a|s)[R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')]\n```\n\n**Giải thích**:\n- Giá trị của một trạng thái bằng phần thưởng tức thời cộng với giá trị chiết khấu của trạng thái tiếp theo\n- Đây là một phương trình đệ quy\n\n#### 6.2. Phương trình Bellman cho Qᵖ\n```\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) Σₐ' π(a'|s')Qᵖ(s',a')\n```\n\n#### 6.3. Phương trình Bellman tối ưu\n```\n\n**Các khái niệm quan trọng:**\n- Deterministic Policy (Chính sách xác định) là một loại chính sách trong Reinforcement Learning (RL) mà tại mỗi trạng thái 's', chính sách π(s) (hoặc μ(s;θ)) sẽ luôn chọn một hành động 'a' cụ thể và duy nhất. Công thức biểu diễn là π(s) = a, trong đó θ là các tham số của chính sách. Các chính sách này thường được suy ra từ các phương pháp dựa trên giá trị (Value-Based Methods), ví dụ như π(s) = argmax_a Q(s,a), và thường hiệu quả hơn về mặt mẫu (sample efficient) trong các không gian hành động liên tục.\n\n**Mối quan hệ:**\n- Policy có thể là Deterministic Policy, nơi một hành động cụ thể được chọn cho mỗi trạng thái."
        },
        "output_without_context": {
            "question": "Học tăng cường dựa trên chính sách là gì?",
            "answer": "Một phương pháp học máy trong đó các quyết định được tối ưu hóa thông qua việc tham số hóa chính sách."
        }
    },
    {
        "topic_description": "Ngày nay, chủ đề này trình bày những lợi thế và bất lợi của học tăng cường dựa trên chính sách. Học sinh cần so sánh và đánh giá sự ưu việt và tác động của chúng đến hiệu suất của các chính sách ngẫu nhiên.",
        "output_with_context": {
            "question": "Học tăng cường có ưu điểm gì trong việc cải thiện chính sách so với các phương pháp học khác?",
            "answer": "Học tăng cường luôn hội tụ đến chính sách tối ưu và thường hội tụ trong số lần lặp nhỏ.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- Policy Improvement Theorem phát biểu rằng nếu Q-value của hành động được chọn bởi chính sách mới π' tại trạng thái s, dưới chính sách cũ π (tức là Q^π(s, π'(s))), lớn hơn hoặc bằng V-value của trạng thái s dưới chính sách cũ π (tức là V^π(s)), thì chính sách mới π' sẽ tốt hơn hoặc bằng chính sách cũ π. Điều này có nghĩa là hàm giá trị của chính sách mới π' sẽ lớn hơn hoặc bằng hàm giá trị của chính sách cũ π cho mọi trạng thái s.\n- Convergence là tính chất của một thuật toán học tăng cường khi các ước lượng của hàm giá trị hoặc chính sách của nó ổn định và tiến đến một giá trị hoặc chính sách tối ưu (hoặc gần tối ưu) sau một số lượng lớn các bước cập nhật. Điều kiện hội tụ thường phụ thuộc vào các tham số như learning rate và việc thăm dò không gian trạng thái/hành động.\n\n**Mối quan hệ:**\n- Policy Iteration có tính chất luôn hội tụ đến chính sách tối ưu.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n**Định nghĩa**: Một dãy chính sách {π_t} thỏa mãn GLIE nếu:\n1. Mọi cặp (s,a) được visit vô hạn lần\n2. Chính sách hội tụ đến greedy policy\n\n**Ví dụ GLIE**:\n```\nε_t = 1/t\n```\n\n**Định lý**: GLIE Monte Carlo Control hội tụ đến π*\n\n#### 2.6. Ví dụ: Blackjack với MC Control\n\n**Setup**:\n- State: (player_sum, dealer_card, usable_ace)\n- Action: Hit hoặc Stick\n- Reward: +1 (win), -1 (lose), 0 (draw)\n\n**Kết quả**:\n```\nChính sách học được:\n- Stick khi player_sum ≥ 20\n- Hit khi player_sum < 12\n- Phức tạp hơn ở vùng 12-19 (phụ thuộc dealer card)\n```\n\n### 3. On-Policy vs Off-Policy Learning\n\n#### 3.1. Định nghĩa\n\n**On-Policy**:\n- Học về chính sách π từ experience generated bởi π\n- Evaluate và improve cùng một chính sách\n- Ví dụ: SARSA, Monte Carlo Control\n\n**Off-Policy**:\n- Học về chính sách π (target) từ experience của μ (behavior)\n- π ≠ μ\n- Ví dụ: Q-Learning, Importance Sampling\n\n#### 3.2. So sánh\n\n| Đặc điểm | On-Policy | Off-Policy |\n|----------|-----------|------------|\n| Chính sách | Một chính sách | Hai chính sách |\n| Sample efficiency | Thấp hơn | Cao hơn |\n| Variance | Thấp | Cao |\n| Converge | Ổn định | Có thể diverge |\n| Use old data | Không | Có thể |\n| Learn optimal | Không (nếu ε-greedy) | Có |\n\n### 4. SARSA - On-Policy TD Control\n\n#### 4.1. Ý tưởng\nÁp dụng TD(0) cho Q(s,a) thay vì V(s)\n\n**TD Update cho Q**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n```\n\n**Tên gọi**: SARSA = (S, A, R, S', A')\n\n#### 4.2. Thuật toán SARSA\n\n```\nKhởi tạo Q(s,a) arbitrarily, ∀s ∈ S, a ∈ A\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    Chọn A từ S theo ε-greedy(Q)\n    \n    Lặp với mỗi bước:\n        Thực hiện A, quan sát R, S'\n        Chọn A' từ S' theo ε-greedy(Q)\n        \n        Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n        \n        S ← S'; A ← A'\n    cho đến S là terminal\n```\n\n#### 4.3. Tính chất của SARSA\n\n**Hội tụ**:\n- GLIE schedule + Robbins-Monro conditions → Q → Q*\n- Trong thực tế, dùng ε nhỏ cố định hoặc decay\n\n**On-Policy**:\n- Học về chính sách ε-greedy đang dùng\n- Safe: Tính đến exploration trong học\n\n#### 4.4. Ví dụ: Windy Gridworld\n\n**Mô tả**:\n- Grid 7×10 với \"wind\" ở một số cột\n- Wind đẩy agent lên 1-2 ô\n- Start: (3,0), Goal: (3,7)\n- Actions: 4 hướng\n\n**Kết quả SARSA**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- On-Policy Learning là một phương pháp học tăng cường trong đó agent học về chính sách π từ kinh nghiệm được tạo ra bởi chính sách π đó, nghĩa là chính sách được sử dụng để tạo dữ liệu (behavior policy) cũng chính là chính sách đang được cải thiện (target policy), với các ví dụ điển hình như SARSA và Monte Carlo Control. Ngược lại, Off-Policy Learning là một phương pháp học tăng cường mà agent học về một chính sách π (target policy) từ kinh nghiệm được tạo ra bởi một chính sách khác μ (behavior policy), trong đó π ≠ μ, cho phép agent học về chính sách tối ưu trong khi vẫn thực hiện các hành động khám phá theo một chính sách khác, với các ví dụ điển hình là Q-Learning và Importance Sampling.\n\n**Mối quan hệ:**\n- On-Policy Learning đánh giá và cải thiện cùng một chính sách π.\n- On-Policy Learning học về chính sách π từ kinh nghiệm được tạo ra bởi chính sách π.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n2. Add Double DQN improvement\n3. Experiment với network architectures\n\n#### 13.3. Dự án\n1. Build Atari game player với DQN\n2. Implement Prioritized Experience Replay\n3. Compare tabular Q-learning vs FA Q-learning\n\n### 14. Kết luận\n\nValue Function Approximation là bước đột phá cho phép RL scale lên bài toán thực tế với large/continuous state spaces.\n\n**Key Takeaways**:\n\n1. **Function Approximation**: Generalization thay vì memorization\n2. **Linear Methods**: Simple, stable, nhưng limited expressiveness\n3. **Neural Networks**: Powerful nhưng cần careful engineering\n4. **DQN**: Breakthrough với experience replay + target network\n5. **Stability**: Deadly triad cần được xử lý cẩn thận\n\n**Convergence challenges**:\n- Semi-gradient methods không follow true gradient\n- Off-policy + FA + bootstrapping = risk of divergence\n- Practical techniques (replay, target network) help significantly\n\n**Tiếp theo**: **Policy Gradient Methods** - Học trực tiếp policy thay vì value function!\n\n---\n\n## Policy Gradient Methods - Phương Pháp Gradient Chính Sách\n\n### 1. Giới thiệu về Policy Gradient\n\n#### 1.1. Value-Based vs Policy-Based\n\n**Value-Based Methods** (Q-Learning, DQN):\n```\nHọc Q(s,a) → Derive policy: π(s) = argmax_a Q(s,a)\n```\n- Indirect: Học value rồi suy ra policy\n- Deterministic policies\n- Khó với continuous actions\n\n**Policy-Based Methods**:\n```\nHọc trực tiếp π(a|s; θ)\n```\n- Direct: Parameterize và optimize policy\n- Stochastic policies tự nhiên\n- Hiệu quả với continuous actions\n\n#### 1.2. Ưu điểm của Policy Gradient\n\n✅ **Continuous action spaces**: Không cần discretization\n✅ **Stochastic policies**: Tự nhiên cho exploration và game theory\n✅ **Better convergence**: Smooth optimization landscape\n✅ **Effective in high dimensions**: Especially với function approximation\n✅ **Learn policies directly**: Không qua intermediate value function\n\n#### 1.3. Nhược điểm\n\n❌ **High variance**: Gradient estimates có variance cao\n❌ **Sample inefficient**: Cần nhiều samples\n❌ **Local optima**: Có thể stuck tại local optima\n❌ **Slow convergence**: Thường chậm hơn value-based\n\n### 2. Policy Parameterization\n\n#### 2.1. Discrete Action Spaces\n\n**Softmax Policy** (Gibbs/Boltzmann):\n```\nπ(a|s; θ) = exp(h(s,a;θ)) / Σ_b exp(h(s,b;θ))\n```\n\n**Linear Preferences**:\n```\nh(s,a;θ) = θ^T φ(s,a)\n```\n\n**Neural Network**:\n```\nInput: State s\nHidden Layers: Neural network\nOutput: Logits h(s,a;θ)\nPolicy: π(a|s;θ) = softmax(h(s,·;θ))\n```\n\n#### 2.2. Continuous Action Spaces\n\n**Gaussian Policy**:\n```\nπ(a|s; θ) = N(μ(s;θ), σ²)\n\na ~ N(μ(s;θ), σ²)\n```\n\n**Parameterization**:\n```\nμ(s;θ) = θ^T φ(s)  [Linear]\nμ(s;θ) = NN(s;θ)   [Neural Network]\n\nσ có thể:\n- Fixed constant\n- State-dependent: σ(s;θ)\n- Action-dependent\n```\n\n**Beta Distribution** (bounded actions):\n```\na ∈ [0, 1]\nπ(a|s;θ) = Beta(α(s;θ), β(s;θ))\n```\n\n#### 2.3. Ví dụ minh họa\n\n**CartPole** (Discrete):\n```python\nclass PolicyNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, action_dim)\n    \n    def forward(self, state):\n\n**Các khái niệm quan trọng:**\n- Value-Based Methods là một loại thuật toán học tăng cường học một hàm giá trị (ví dụ: Q(s,a)) và sau đó suy ra một chính sách từ hàm giá trị đó, thường là π(s) = argmax_a Q(s,a). Các ví dụ bao gồm Q-Learning và DQN. Chúng thường tạo ra các chính sách xác định và có thể gặp khó khăn với không gian hành động liên tục.\n- Sample inefficient là một nhược điểm của Policy Gradient Methods, có nghĩa là chúng cần một lượng lớn các mẫu tương tác với môi trường để học hiệu quả. Điều này làm cho chúng tốn kém về mặt tính toán và thời gian trong các môi trường thực tế.\n- High variance là một nhược điểm của Policy Gradient Methods, trong đó các ước lượng gradient có độ biến thiên cao, dẫn đến quá trình học không ổn định và chậm. Điều này đòi hỏi nhiều mẫu hơn để có được ước lượng gradient chính xác.\n\n**Mối quan hệ:**\n- Value-Based Methods suy ra chính sách π(s) từ hàm Q(s,a).\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*."
        },
        "output_without_context": {
            "question": "Học tăng cường có những ưu điểm nào trong việc cải thiện hiệu suất của các chính sách ngẫu nhiên?",
            "answer": "Cải thiện khả năng ra quyết định và tối ưu hóa kết quả."
        }
    },
    {
        "topic_description": "Chủ đề này nghiên cứu đạo hàm của chính sách và cách tối ưu hóa nó bằng các thuật toán đạo hàm chính sách. Học sinh sẽ được kiểm tra hiểu biết về công thức và cách thức áp dụng trong các tình huống thực tiễn.",
        "output_with_context": {
            "question": "Trong phương pháp Gradient Chính sách, công thức nào được sử dụng để cập nhật tham số chính sách θ trong thuật toán REINFORCE?",
            "answer": "θ ← θ + α γ^t G_t ∇_θ log π(A_t|S_t;θ)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Policy Gradient Methods - Phương Pháp Gradient Chính Sách\n        x = F.relu(self.fc1(state))\n        logits = self.fc2(x)\n        return F.softmax(logits, dim=-1)\n```\n\n**Continuous Control**:\n```python\nclass GaussianPolicy(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.mean = nn.Linear(128, action_dim)\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n    \n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        mean = self.mean(x)\n        std = torch.exp(self.log_std)\n        return mean, std\n```\n\n### 3. Policy Gradient Theorem\n\n#### 3.1. Objective Function\n\n**Mục tiêu**: Maximize expected return\n```\nJ(θ) = E_τ~π_θ[G_τ] = E[Σ_t γ^t R_t]\n```\n\nHoặc với episodic tasks:\n```\nJ(θ) = V^π_θ(s_0) = E_π_θ[G_0 | S_0 = s_0]\n```\n\n#### 3.2. Policy Gradient Theorem\n\n**Định lý**:\n```\n∇_θ J(θ) = E_π_θ[∇_θ log π(A|S;θ) Q^π(S,A)]\n         = E_π_θ[∇_θ log π(A|S;θ) G_t]\n```\n\n**Giải thích**:\n- ∇_θ log π(A|S;θ): Score function (hướng tăng probability của action)\n- Q^π(S,A): Weighting (actions tốt được tăng, xấu giảm)\n\n#### 3.3. REINFORCE Algorithm\n\n**Monte Carlo Policy Gradient**:\n```\nKhởi tạo θ\nThiết lập learning rate α\n\nLặp:\n    Tạo episode theo π(·|·;θ): S_0, A_0, R_1, ..., S_T\n    \n    Với mỗi bước t:\n        G_t = Σ_{k=t}^T γ^{k-t} R_k\n        θ ← θ + α γ^t G_t ∇_θ log π(A_t|S_t;θ)\n```\n\n**Intuition**:\n- Nếu G_t > 0: Tăng probability của actions đã chọn\n- Nếu G_t < 0: Giảm probability\n- Magnitude tỷ lệ với |G_t|\n\n#### 3.4. REINFORCE với Baseline\n\n**Vấn đề**: High variance trong gradient estimates\n\n**Giải pháp**: Subtract baseline b(s)\n```\n∇_θ J(θ) = E[∇_θ log π(A|S;θ) (G_t - b(S_t))]\n```\n\n**Baseline phổ biến**: V^π(s)\n```\nAdvantage: A^π(s,a) = Q^π(s,a) - V^π(s)\n∇_θ J(θ) = E[∇_θ log π(A|S;θ) A^π(S,A)]\n```\n\n**Thuật toán**:\n```\nKhởi tạo θ, w (cho value function baseline)\n\nLặp:\n    Tạo episode: S_0, A_0, R_1, ..., S_T\n    \n    Với mỗi bước t:\n        G_t = Σ_{k=t}^T γ^{k-t} R_k\n        δ_t = G_t - V̂(S_t; w)  # Advantage estimate\n        \n        # Update policy\n        θ ← θ + α_θ γ^t δ_t ∇_θ log π(A_t|S_t;θ)\n        \n        # Update value function\n        w ← w + α_w δ_t ∇_w V̂(S_t; w)\n```\n\n### 4. Actor-Critic Methods\n\n#### 4.1. Ý tưởng\n\n**Actor**: Policy π(a|s;θ)\n**Critic**: Value function V(s;w) hoặc Q(s,a;w)\n\n**Actor-Critic Framework**:\n```\nActor: Chọn actions theo policy\n\n**Các khái niệm quan trọng:**\n- Policy Gradient Methods là một nhóm các thuật toán trong Reinforcement Learning tập trung vào việc tối ưu hóa trực tiếp chính sách (policy) của agent bằng cách tính toán gradient của hàm mục tiêu (thường là tổng reward chiết khấu hoặc tổng reward kỳ vọng/expected return) đối với các tham số của chính sách. Thay vì học một hàm giá trị và suy ra chính sách, các phương pháp này điều chỉnh chính sách để tăng xác suất của các hành động tốt và giảm xác suất của các hành động xấu. Các phương pháp này thường hiệu quả và được sử dụng cho các không gian hành động liên tục (continuous action spaces) hoặc khi chính sách là một hàm phức tạp (ví dụ: mạng nơ-ron) hoặc khi hàm giá trị khó ước lượng. Một số phương pháp Policy Gradient sử dụng kiến trúc Actor-Critic, trong đó \"Actor\" học chính sách và \"Critic\" đánh giá các hành động của Actor.\n\n**Mối quan hệ:**\n- Policy Gradient Methods hiệu quả trong việc giải quyết các bài toán Continuous Control.\n- Policy Gradient Methods sử dụng cách tiếp cận Direct Policy Learning.\n- Policy Gradient Methods tối ưu hóa hàm mục tiêu J(θ) bằng cách tính toán gradient của nó đối với các tham số chính sách θ và thực hiện các bước cập nhật theo hướng gradient.\n- Policy Gradient Methods sử dụng kiến trúc Actor-Critic để kết hợp học giá trị và chính sách.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- Optimal Policy (Chính sách tối ưu) là một chính sách π* mà khi được tuân theo, sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng từ mọi trạng thái. Nó đảm bảo rằng hàm giá trị của chính sách tối ưu Vᵖ*(s) lớn hơn hoặc bằng hàm giá trị của bất kỳ chính sách nào khác Vᵖ(s) cho mọi trạng thái s.\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Policy Iteration luôn hội tụ đến chính sách tối ưu π*.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Ví dụ: Bàn cờ vây 19×19 có ~10^170 trạng thái\n\n**Giải pháp**:\n- Function approximation (phần sau)\n- Sampling-based methods\n- Hierarchical RL\n\n### 10. Ví dụ thực tế: Car Rental Problem\n\n#### 10.1. Mô tả bài toán\n**Jack's Car Rental**:\n- 2 địa điểm cho thuê xe\n- Mỗi đêm có thể di chuyển tối đa 5 xe giữa 2 địa điểm\n- Chi phí di chuyển: $2/xe\n- Thu nhập thuê xe: $10/xe\n- Số xe được thuê/trả theo phân phối Poisson\n\n#### 10.2. Biểu diễn MDP\n```\nState: (n1, n2) - số xe tại mỗi địa điểm (0-20)\nAction: -5 đến +5 (số xe di chuyển từ địa điểm 1 đến 2)\nReward: 10 × (số xe thuê được) - 2 × |action|\nTransition: Theo phân phối Poisson\n```\n\n#### 10.3. Giải bằng Policy Iteration\n\n**Chính sách khởi tạo**: Không di chuyển xe\n**Sau iteration 1**: Di chuyển xe từ địa điểm thừa sang thiếu\n**Hội tụ**: ~4-5 iterations\n**Chính sách tối ưu**: Cân bằng số xe giữa 2 địa điểm\n\n#### 10.4. Kết quả\n```\nV*(10,10) ≈ $500\nChiến lược: Luôn giữ ~10 xe mỗi địa điểm\n```\n\n### 11. Ví dụ thực tế: Inventory Management\n\n#### 11.1. Bài toán quản lý kho\n\n**Setup**:\n- Tồn kho từ 0 đến MAX_INVENTORY\n- Mỗi kỳ: Quyết định đặt hàng bao nhiêu\n- Chi phí đặt hàng + chi phí lưu kho\n- Doanh thu từ bán hàng\n- Mất khách nếu hết hàng\n\n#### 11.2. MDP Formulation\n```\nState: Mức tồn kho hiện tại\nAction: Số lượng đặt hàng\nReward: Revenue - Ordering_cost - Holding_cost - Stockout_penalty\nTransition: Stochastic demand\n```\n\n#### 11.3. Giải pháp\n- Sử dụng Value Iteration\n- Tìm chính sách (s,S): Đặt hàng đến mức S khi tồn kho ≤ s\n- Optimize (s,S) parameters\n\n### 12. Ưu điểm và hạn chế của Dynamic Programming\n\n#### 12.1. Ưu điểm\n✅ **Đảm bảo tối ưu**: Hội tụ đến chính sách tối ưu π*\n✅ **Cơ sở lý thuyết vững chắc**: Dựa trên phương trình Bellman\n✅ **Hiệu quả với medium-sized problems**: Polynomial complexity\n✅ **Framework cho nhiều thuật toán khác**: Nền tảng cho TD, Q-learning\n\n#### 12.2. Hạn chế\n❌ **Yêu cầu biết hoàn toàn mô hình**: Cần biết P và R\n❌ **Curse of dimensionality**: Không mở rộng cho không gian lớn\n❌ **Cần duyệt tất cả trạng thái**: Không practical với continuous states\n❌ **Computation cost**: Tốn nhiều tính toán mỗi iteration\n\n### 13. Mở rộng và cải tiến\n\n\n**Các khái niệm quan trọng:**\n- Optimality là tính chất đảm bảo rằng một thuật toán hoặc chính sách đạt được kết quả tốt nhất có thể (tối đa hóa tổng Reward chiết khấu). Dynamic Programming (Policy Iteration, Value Iteration) có ưu điểm là đảm bảo hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Policy Iteration luôn hội tụ đến chính sách tối ưu π*.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*."
        },
        "output_without_context": {
            "question": "Khi tối ưu hóa một chính sách trong học máy, công thức nào thường được sử dụng để tính toán đạo hàm của chính sách?",
            "answer": "Đạo hàm chính sách"
        }
    },
    {
        "topic_description": "Chủ đề này kết nối các kiến thức từ tuần 5 và 6 về xấp xỉ hàm giá trị và tính đạo hàm. Học sinh sẽ khám phá cách xấp xỉ hàm giá trị tương thích liên quan đến đạo hàm chính sách và ảnh hưởng của nó đến sự chính xác và hiệu suất.",
        "output_with_context": {
            "question": "Trong xấp xỉ hàm giá trị, phương trình nào được sử dụng để cập nhật trọng số w nhằm tối thiểu hóa độ sai lệch giữa hàm giá trị thực và hàm giá trị xấp xỉ?",
            "answer": "w_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]∇_w V̂(S_t; w_t)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n#### 13.1. Approximate Dynamic Programming\n- Sử dụng function approximation\n- Neural networks để biểu diễn V hoặc π\n- Trade-off giữa accuracy và scalability\n\n#### 13.2. Model-Free Methods\n- Không cần biết P và R\n- Học từ experience\n- Temporal-Difference Learning, Q-Learning (phần sau)\n\n#### 13.3. Deep Reinforcement Learning\n- Kết hợp DP với deep learning\n- DQN, Actor-Critic, PPO\n- Giải quyết được bài toán phức tạp\n\n### 14. Code Implementation - Ví dụ Python\n\n#### 14.1. Policy Iteration\n```python\ndef policy_iteration(env, gamma=0.9, theta=1e-6):\n    # Khởi tạo\n    V = np.zeros(env.num_states)\n    policy = np.zeros(env.num_states, dtype=int)\n    \n    while True:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.num_states):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([p * (r + gamma * V[s_]) \n                           for p, s_, r in env.transitions(s, a)])\n                delta = max(delta, abs(v - V[s]))\n            if delta < theta:\n                break\n        \n        # Policy Improvement\n        policy_stable = True\n        for s in range(env.num_states):\n            old_action = policy[s]\n            # Tìm hành động tốt nhất\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            policy[s] = np.argmax(action_values)\n            \n            if old_action != policy[s]:\n                policy_stable = False\n        \n        if policy_stable:\n            return V, policy\n```\n\n#### 14.2. Value Iteration\n```python\ndef value_iteration(env, gamma=0.9, theta=1e-6):\n    V = np.zeros(env.num_states)\n    \n    while True:\n        delta = 0\n        for s in range(env.num_states):\n            v = V[s]\n            # Bellman optimality update\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            V[s] = max(action_values)\n            delta = max(delta, abs(v - V[s]))\n        \n        if delta < theta:\n            break\n    \n    # Trích xuất chính sách\n    policy = np.zeros(env.num_states, dtype=int)\n    for s in range(env.num_states):\n        action_values = []\n        for a in range(env.num_actions):\n            q_value = sum([p * (r + gamma * V[s_]) \n                          for p, s_, r in env.transitions(s, a)])\n            action_values.append(q_value)\n        policy[s] = np.argmax(action_values)\n    \n    return V, policy\n```\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement policy evaluation cho Gridworld\n2. So sánh tốc độ hội tụ của in-place và two-array DP\n3. Visualize quá trình hội tụ của value iteration\n\n#### 15.2. Bài tập nâng cao\n1. Giải Gambler's Problem với các giá trị p khác nhau\n2. Implement prioritized sweeping\n3. Modified policy iteration với k=1, k=3, k=∞\n\n#### 15.3. Dự án\n1. Xây dựng AI cho game 2048 bằng DP\n2. Tối ưu hóa việc sạc pin cho robot\n3. Quản lý danh mục đầu tư bằng MDP\n\n### 16. Kết luận\n\n\n**Các khái niệm quan trọng:**\n- Accuracy (Độ chính xác) trong ngữ cảnh của function approximation đề cập đến mức độ gần đúng của hàm xấp xỉ so với hàm giá trị hoặc chính sách thực tế. Có một sự đánh đổi giữa độ chính xác và khả năng mở rộng khi sử dụng function approximation: các mô hình phức tạp hơn có thể chính xác hơn nhưng khó huấn luyện và mở rộng hơn.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\nBackpropagation là thuật toán cốt lõi để huấn luyện mạng nơ-ron sâu, cho phép tính gradient một cách hiệu quả thông qua quy tắc chuỗi (chain rule).\n\n**Ý tưởng cơ bản:**\n- Tính toán gradient của loss function theo tất cả các tham số (weights và biases)\n- Lan truyền gradient từ output về input qua các lớp\n- Sử dụng quy tắc chuỗi để phân rã gradient phức tạp thành các phần đơn giản\n\n**Quy tắc chuỗi (Chain Rule):**\n$$\frac{\\partial L}{\\partial w^{[l]}} = \frac{\\partial L}{\\partial a^{[l]}} \\cdot \frac{\\partial a^{[l]}}{\\partial z^{[l]}} \\cdot \frac{\\partial z^{[l]}}{\\partial w^{[l]}}$$\n\nTrong đó:\n- $\frac{\\partial L}{\\partial a^{[l]}}$: Gradient của loss theo activation\n- $\frac{\\partial a^{[l]}}{\\partial z^{[l]}}$: Đạo hàm của hàm kích hoạt\n- $\frac{\\partial z^{[l]}}{\\partial w^{[l]}}$: Gradient của pre-activation theo weights\n\n**Các bước chi tiết:**\n\n**1. Forward Pass (Lan truyền xuôi):**\n- Tính toán output của mỗi lớp từ input đến output\n- Lưu trữ tất cả các giá trị $z^{[l]}$ và $a^{[l]}$ (cần cho backward pass)\n\n**2. Tính Loss:**\n- So sánh prediction với ground truth\n- Tính giá trị loss: $L = Loss(y, \\hat{y})$\n\n**3. Backward Pass (Lan truyền ngược):**\n- Bắt đầu từ lớp output, tính gradient của loss theo output\n- Với mỗi lớp từ L về 1:\n  - Tính $\frac{\\partial L}{\\partial z^{[l]}} = \frac{\\partial L}{\\partial a^{[l]}} \\odot \\sigma'(z^{[l]})$ (element-wise product)\n  - Tính $\frac{\\partial L}{\\partial W^{[l]}} = \frac{\\partial L}{\\partial z^{[l]}} \\cdot (a^{[l-1]})^T$\n  - Tính $\frac{\\partial L}{\\partial b^{[l]}} = \frac{\\partial L}{\\partial z^{[l]}}$\n  - Lan truyền về lớp trước: $\frac{\\partial L}{\\partial a^{[l-1]}} = (W^{[l]})^T \\cdot \frac{\\partial L}{\\partial z^{[l]}}$\n\n**4. Cập nhật Weights:**\n- Sử dụng gradient descent hoặc các optimizer khác\n- $W^{[l]} := W^{[l]} - \\alpha \frac{\\partial L}{\\partial W^{[l]}}$\n- $b^{[l]} := b^{[l]} - \\alpha \frac{\\partial L}{\\partial b^{[l]}}$\n\n**Ví dụ minh họa:**\nMạng 2 lớp: Input → Hidden → Output\n- Forward: $a^{[1]} = \\sigma(W^{[1]}x + b^{[1]})$, $\\hat{y} = \\sigma(W^{[2]}a^{[1]} + b^{[2]})$\n- Loss: $L = (y - \\hat{y})^2$\n- Backward:\n  - $\frac{\\partial L}{\\partial \\hat{y}} = -2(y - \\hat{y})$\n  - $\frac{\\partial L}{\\partial W^{[2]}} = \frac{\\partial L}{\\partial \\hat{y}} \\cdot \\sigma'(z^{[2]}) \\cdot a^{[1]}$\n  - Lan truyền về hidden layer tương tự\n\n**Computational Graph:**\n\n**Các khái niệm quan trọng:**\n- Đây là công thức tính gradient của hàm loss $L = (y - \\hat{y})^2$ theo giá trị dự đoán $\\hat{y}$. Nó cho biết mức độ thay đổi của loss khi giá trị dự đoán thay đổi, và được sử dụng làm điểm khởi đầu cho Backward Pass trong ví dụ minh họa.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nMột quá trình được gọi là có tính chất Markov nếu:\n```\nP[Sₜ₊₁ | Sₜ] = P[Sₜ₊₁ | S₁, S₂, ..., Sₜ]\n```\n\n#### 3.2. Ý nghĩa\n- Trạng thái hiện tại chứa đầy đủ thông tin để dự đoán tương lai\n- Không cần phải nhớ toàn bộ lịch sử\n- Đơn giản hóa việc tính toán và phân tích\n\n### 4. Chính sách (Policy)\n\n#### 4.1. Định nghĩa\nChính sách π là một hàm ánh xạ từ trạng thái đến hành động:\n- **Chính sách xác định (Deterministic)**: π(s) = a\n- **Chính sách ngẫu nhiên (Stochastic)**: π(a|s) = P[Aₜ = a | Sₜ = s]\n\n#### 4.2. Phân loại\n- **Chính sách tĩnh (Stationary)**: Không thay đổi theo thời gian\n- **Chính sách động (Non-stationary)**: Thay đổi theo thời gian\n\n### 5. Hàm giá trị (Value Functions)\n\n#### 5.1. Hàm giá trị trạng thái (State-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s và tuân theo chính sách π\n\n```\nVᵖ(s) = Eᵖ[Gₜ | Sₜ = s]\n      = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s]\n```\n\n#### 5.2. Hàm giá trị hành động (Action-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s, thực hiện hành động a, và sau đó tuân theo chính sách π\n\n```\nQᵖ(s,a) = Eᵖ[Gₜ | Sₜ = s, Aₜ = a]\n        = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s, Aₜ = a]\n```\n\n#### 5.3. Mối quan hệ giữa V và Q\n```\nVᵖ(s) = Σₐ π(a|s)Qᵖ(s,a)\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')\n```\n\n### 6. Phương trình Bellman\n\n#### 6.1. Phương trình Bellman cho Vᵖ\n```\nVᵖ(s) = Σₐ π(a|s)[R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')]\n```\n\n**Giải thích**:\n- Giá trị của một trạng thái bằng phần thưởng tức thời cộng với giá trị chiết khấu của trạng thái tiếp theo\n- Đây là một phương trình đệ quy\n\n#### 6.2. Phương trình Bellman cho Qᵖ\n```\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) Σₐ' π(a'|s')Qᵖ(s',a')\n```\n\n#### 6.3. Phương trình Bellman tối ưu\n```\n\n**Các khái niệm quan trọng:**\n- Value Functions (Hàm giá trị) là các hàm ước lượng mức độ \"tốt\" của một trạng thái hoặc một cặp trạng thái-hành động, bằng cách tính tổng phần thưởng chiết khấu kỳ vọng trong tương lai. Có hai loại chính là State-Value Function V(s) và Action-Value Function Q(s,a).\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n     = Σₛ d(s)(V^π(s) - V̂(s; w))²\n```\n- d(s): distribution của states dưới policy π\n\n**Mục tiêu**: Minimize J(w) = ||V^π - V̂_w||²_d\n\n### 3. Stochastic Gradient Descent (SGD)\n\n#### 3.1. Gradient Descent\n\n**Update Rule**:\n```\nw_{t+1} = w_t - (α/2)∇_w J(w_t)\n        = w_t + α E[(V^π(s) - V̂(s; w))∇_w V̂(s; w)]\n```\n\n**Stochastic Gradient Descent**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]∇_w V̂(S_t; w_t)\n```\n\n#### 3.2. Linear SGD\n\n**Gradient của linear function**:\n```\n∇_w V̂(s; w) = ∇_w(w^T φ(s)) = φ(s)\n```\n\n**Update rule**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]φ(S_t)\n```\n\n**Đặc điểm**:\n- Converge đến local optimum (global cho linear)\n- Learning rate α quan trọng\n- Simple và efficient\n\n#### 3.3. Feature Scaling\n\n**Vấn đề**: Features có scale khác nhau → học không ổn định\n\n**Giải pháp**:\n```\nNormalization: φᵢ = (φᵢ - μᵢ)/σᵢ\nStandardization: φᵢ ∈ [0, 1]\n```\n\n### 4. Incremental Prediction Methods\n\n#### 4.1. Monte Carlo với Function Approximation\n\n**Update**:\n```\nw ← w + α[G_t - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ Target: actual return\n```\n\n**Đặc điểm**:\n- Unbiased target\n- High variance\n- Non-stationary target (G_t khác nhau mỗi episode)\n\n#### 4.2. TD(0) với Function Approximation\n\n**Update**:\n```\nw ← w + α[R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ TD target\n```\n\n**Semi-gradient**: Không lấy gradient qua V̂(S_{t+1}; w)\n\n**Thuật toán Semi-gradient TD(0)**:\n```\nKhởi tạo w arbitrarily\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        A ← π(S)\n        Thực hiện A, quan sát R, S'\n        \n        w ← w + α[R + γV̂(S'; w) - V̂(S; w)]∇_w V̂(S; w)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 4.3. TD(λ) với Function Approximation\n\n**Eligibility Traces**:\n```\nz_0 = 0\nz_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n```\n\n**Update**:\n```\nδ_t = R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)\nw ← w + αδ_t z_t\n```\n\n**Accumulating vs Replacing traces**:\n- Accumulating: z_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n- Replacing: Phức tạp hơn, phụ thuộc feature type\n\n### 5. Control với Function Approximation\n\n#### 5.1. Action-Value Function Approximation\n\n**Parameterization**:\n```\nQ̂(s, a; w) = w^T φ(s, a)\n```\n\n\n**Các khái niệm quan trọng:**\n- w là vector trọng số được sử dụng trong các phương pháp xấp xỉ hàm giá trị, ví dụ như trong Least Squares TD (LSTD) để biểu diễn hàm giá trị V̂(S; w) hoặc hàm Q̂(S, A; w). Mục tiêu của các thuật toán này là tìm ra w tối ưu để hàm xấp xỉ gần đúng nhất với hàm giá trị thực.\n- TD(0) với Function Approximation là một thuật toán học tăng cường sử dụng phương pháp Temporal Difference (TD) để cập nhật hàm giá trị xấp xỉ, sử dụng TD target (R_{t+1} + γV̂(S_{t+1}; w)) để cập nhật tham số w theo công thức w ← w + α[R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)]∇_w V̂(S_t; w). Đây là một thuật toán semi-gradient vì gradient không được lấy qua V̂(S_{t+1}; w). TD(λ) với Function Approximation là một thuật toán học tăng cường mở rộng TD(0) bằng cách kết hợp Temporal Difference với Eligibility Traces (dấu vết đủ điều kiện) và xấp xỉ hàm giá trị. Nó sử dụng tham số λ để cân bằng giữa các cập nhật TD một bước (TD(0)) và Monte Carlo, với công thức cập nhật là w ← w + αδ_t z_t, trong đó δ_t là TD error và z_t là eligibility trace được tính bằng z_t = γλz_{t-1} + ∇_w V̂(S_t; w).\n- Control với Function Approximation là việc mở rộng các phương pháp học tăng cường để tìm kiếm chính sách tối ưu khi sử dụng xấp xỉ hàm giá trị, đặc biệt là xấp xỉ hàm giá trị hành động Q̂(s, a; w). Mục tiêu là học một chính sách tối ưu mà không cần lưu trữ bảng Q-value cho mọi cặp (s,a).\n\n**Mối quan hệ:**\n- TD(λ) với Function Approximation sử dụng z_t (eligibility trace) trong công thức cập nhật w ← w + αδ_t z_t.\n- Control với Function Approximation thường sử dụng Action-Value Function Approximation để học chính sách tối ưu.\n- TD(0) với Function Approximation cập nhật các tham số w dựa trên TD target.\n- TD(λ) với Function Approximation cập nhật các tham số w dựa trên TD error và eligibility traces.\n- TD(λ) với Function Approximation sử dụng δ_t (TD error) trong công thức cập nhật w ← w + αδ_t z_t.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\nReplay Buffer D = {(s₁, a₁, r₁, s'₁), ..., (sₙ, aₙ, rₙ, s'ₙ)}\n\nMỗi update:\n    Sample mini-batch từ D\n    Perform SGD update\n```\n\n**Lợi ích**:\n- Phá vỡ correlation giữa consecutive samples\n- Tái sử dụng data hiệu quả\n- Stabilize learning\n\n#### 7.2. Least Squares Methods\n\n**Least Squares TD (LSTD)**:\n```\nTìm w minimize:\n    E[(R + γV̂(S'; w) - V̂(S; w))²]\n\nClosed-form solution:\n    w = A^{-1}b\n    A = Σ φ(s)(φ(s) - γφ(s'))^T\n    b = Σ φ(s)r\n```\n\n**Đặc điểm**:\n- Data efficient\n- No learning rate\n- Computationally expensive: O(n³)\n\n#### 7.3. Fitted Q-Iteration\n\n**Ý tưởng**: Regression trên Bellman targets\n```\nLặp:\n    1. Tính targets: yᵢ = rᵢ + γ max_a Q̂(s'ᵢ, a; w)\n    2. Fit Q̂ để minimize: Σ(yᵢ - Q̂(sᵢ, aᵢ; w))²\n```\n\n**Batch Fitted Q-Iteration**:\n```\nGiven dataset D = {(s, a, r, s')}\n\nKhởi tạo Q̂₀ arbitrarily\n\nLặp k = 1, 2, ...:\n    Với mỗi (s, a, r, s') trong D:\n        y = r + γ max_{a'} Q̂_{k-1}(s', a')\n    \n    Q̂_k = argmin_Q Σ(y - Q(s, a))²\n```\n\n### 8. Deep Q-Networks (DQN)\n\n#### 8.1. Neural Networks làm Function Approximators\n\n**Architecture**:\n```\nInput: State s (hoặc raw observations như pixels)\nHidden Layers: Fully connected / Convolutional\nOutput: Q-values cho mỗi action\n```\n\n**Advantages**:\n- Automatic feature learning\n- Powerful representation capacity\n- End-to-end training\n\n#### 8.2. DQN Innovations\n\n**1. Experience Replay**:\n```\nReplay Buffer D với capacity N\nStore transitions: (s, a, r, s', done)\nSample random mini-batch để train\n```\n\n**2. Target Network**:\n```\nQ-network: Q(s, a; θ)\nTarget network: Q(s, a; θ⁻)\n\nUpdate Q-network mỗi step\nCopy θ → θ⁻ mỗi C steps\n```\n\n**TD Target**:\n```\ny = r + γ max_a Q(s', a; θ⁻)\n         ↑ Dùng target network\n```\n\n#### 8.3. DQN Algorithm\n\n```\nKhởi tạo replay buffer D\nKhởi tạo Q-network với random weights θ\nKhởi tạo target network θ⁻ = θ\n\nLặp với mỗi episode:\n    Khởi tạo state s\n    \n    Lặp với mỗi step:\n        Chọn action a:\n            - Với xác suất ε: random\n            - Ngược lại: a = argmax_a Q(s, a; θ)\n        \n        Thực hiện a, quan sát r, s'\n        Store transition (s, a, r, s', done) vào D\n        \n        Sample random mini-batch từ D\n        Với mỗi (s_j, a_j, r_j, s'_j, done_j):\n            y_j = r_j + γ(1 - done_j) max_a Q(s'_j, a; θ⁻)\n        \n        Perform SGD step trên (y_j - Q(s_j, a_j; θ))²\n        \n        Mỗi C steps: θ⁻ ← θ\n        \n        s ← s'\n```\n\n#### 8.4. DQN Improvements\n\n**Double DQN**:\n```\ny = r + γQ(s', argmax_a Q(s', a; θ), θ⁻)\n\n**Các khái niệm quan trọng:**\n- w là vector trọng số được sử dụng trong các phương pháp xấp xỉ hàm giá trị, ví dụ như trong Least Squares TD (LSTD) để biểu diễn hàm giá trị V̂(S; w) hoặc hàm Q̂(S, A; w). Mục tiêu của các thuật toán này là tìm ra w tối ưu để hàm xấp xỉ gần đúng nhất với hàm giá trị thực.\n\n**Mối quan hệ:**\n- TD(0) với Function Approximation cập nhật các tham số w dựa trên TD target."
        },
        "output_without_context": {
            "question": "Khi xấp xỉ hàm giá trị tương thích, yếu tố nào ảnh hưởng đến độ chính xác và hiệu suất của đạo hàm chính sách?",
            "answer": "Cách chọn phương pháp xấp xỉ và cấu trúc của hàm giá trị."
        }
    },
    {
        "topic_description": "Chủ đề này yêu cầu học sinh tìm hiểu và áp dụng nguyên lý Quy trình Quyền lợi tối ưu Bellman từ tuần 1 và 2. Học sinh sẽ kiểm tra khả năng vận dụng phương trình vào các MDP thực tế để tìm ra chính sách tốt nhất.",
        "output_with_context": {
            "question": "Trong Quy trình Quyền lợi tối ưu Bellman, chính sách tối ưu π* được xác định bằng cách nào?",
            "answer": "π*(s) = argmaxₐ Q*(s,a)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\nDynamic Programming là nền tảng quan trọng trong Reinforcement Learning. Mặc dù có hạn chế về yêu cầu biết mô hình và curse of dimensionality, DP cung cấp:\n\n- **Framework lý thuyết**: Hiểu rõ về optimal value functions và policies\n- **Nền tảng thuật toán**: Cơ sở cho các phương pháp model-free\n- **Công cụ thực tế**: Giải quyết được nhiều bài toán thực tế\n\nCác khái niệm từ DP (đặc biệt là GPI và Bellman equations) sẽ xuất hiện xuyên suốt trong các phương pháp học tăng cường tiếp theo, từ Temporal-Difference Learning đến Deep Q-Networks.\n\n---\n\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Learning\n\n#### 1.1. Định nghĩa\nModel-Free Learning là các phương pháp học tăng cường không yêu cầu biết trước mô hình môi trường (hàm chuyển trạng thái P và hàm phần thưởng R). Agent học trực tiếp từ kinh nghiệm tương tác với môi trường.\n\n#### 1.2. So sánh Model-Based vs Model-Free\n\n| Đặc điểm | Model-Based | Model-Free |\n|----------|-------------|------------|\n| Yêu cầu mô hình | Cần biết P và R | Không cần |\n| Học từ | Phương trình Bellman | Kinh nghiệm thực tế |\n| Ưu điểm | Hiệu quả dữ liệu | Linh hoạt, thực tế |\n| Nhược điểm | Khó có mô hình chính xác | Cần nhiều dữ liệu |\n| Ví dụ | DP, Model-Based RL | MC, TD, Q-Learning |\n\n#### 1.3. Bài toán Prediction\n**Mục tiêu**: Đánh giá một chính sách π cho trước\n- Input: Chính sách π\n- Output: Hàm giá trị V^π(s)\n- Không cần biết mô hình môi trường\n\n### 2. Monte Carlo Methods - Phương Pháp Monte Carlo\n\n#### 2.1. Ý tưởng cơ bản\nMonte Carlo (MC) ước lượng giá trị của trạng thái bằng cách lấy trung bình các return thực tế nhận được từ nhiều episodes.\n\n**Nguyên lý**:\n```\nV^π(s) = E_π[G_t | S_t = s]\n       ≈ average of returns từ trạng thái s\n```\n\n#### 2.2. Episode và Return\n\n**Episode**: Một chuỗi hoàn chỉnh từ trạng thái ban đầu đến kết thúc\n```\nS_0, A_0, R_1, S_1, A_1, R_2, ..., S_T\n```\n\n**Return**: Tổng phần thưởng chiết khấu\n```\nG_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... + γ^{T-t-1}R_T\n```\n\n#### 2.3. First-Visit Monte Carlo\n\n**Thuật toán**:\n```\nKhởi tạo:\n    V(s) = 0, ∀s\n    Returns(s) = danh sách rỗng, ∀s\n\nVới mỗi episode:\n    Tạo một episode tuân theo π: S_0, A_0, R_1, ..., S_T\n    G = 0\n    \n    Với mỗi bước t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        \n\n**Các khái niệm quan trọng:**\n- Bellman equations là một tập hợp các phương trình đệ quy mô tả mối quan hệ giữa giá trị của một trạng thái và giá trị của các trạng thái kế tiếp. Chúng là nền tảng lý thuyết cho việc tính toán hàm giá trị trong Reinforcement Learning và Dynamic Programming. Các phương trình Bellman bao gồm Bellman Expectation Equation và Bellman Optimality Equation.\n\n**Mối quan hệ:**\n- Model-Based Learning học từ các phương trình Bellman.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- Optimal Policy (Chính sách tối ưu) là một chính sách π* mà khi được tuân theo, sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng từ mọi trạng thái. Nó đảm bảo rằng hàm giá trị của chính sách tối ưu Vᵖ*(s) lớn hơn hoặc bằng hàm giá trị của bất kỳ chính sách nào khác Vᵖ(s) cho mọi trạng thái s.\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Linear Programming giải quyết bài toán tìm chính sách tối ưu π* bằng cách chuyển đổi nó thành một bài toán quy hoạch tuyến tính.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Ví dụ: Bàn cờ vây 19×19 có ~10^170 trạng thái\n\n**Giải pháp**:\n- Function approximation (phần sau)\n- Sampling-based methods\n- Hierarchical RL\n\n### 10. Ví dụ thực tế: Car Rental Problem\n\n#### 10.1. Mô tả bài toán\n**Jack's Car Rental**:\n- 2 địa điểm cho thuê xe\n- Mỗi đêm có thể di chuyển tối đa 5 xe giữa 2 địa điểm\n- Chi phí di chuyển: $2/xe\n- Thu nhập thuê xe: $10/xe\n- Số xe được thuê/trả theo phân phối Poisson\n\n#### 10.2. Biểu diễn MDP\n```\nState: (n1, n2) - số xe tại mỗi địa điểm (0-20)\nAction: -5 đến +5 (số xe di chuyển từ địa điểm 1 đến 2)\nReward: 10 × (số xe thuê được) - 2 × |action|\nTransition: Theo phân phối Poisson\n```\n\n#### 10.3. Giải bằng Policy Iteration\n\n**Chính sách khởi tạo**: Không di chuyển xe\n**Sau iteration 1**: Di chuyển xe từ địa điểm thừa sang thiếu\n**Hội tụ**: ~4-5 iterations\n**Chính sách tối ưu**: Cân bằng số xe giữa 2 địa điểm\n\n#### 10.4. Kết quả\n```\nV*(10,10) ≈ $500\nChiến lược: Luôn giữ ~10 xe mỗi địa điểm\n```\n\n### 11. Ví dụ thực tế: Inventory Management\n\n#### 11.1. Bài toán quản lý kho\n\n**Setup**:\n- Tồn kho từ 0 đến MAX_INVENTORY\n- Mỗi kỳ: Quyết định đặt hàng bao nhiêu\n- Chi phí đặt hàng + chi phí lưu kho\n- Doanh thu từ bán hàng\n- Mất khách nếu hết hàng\n\n#### 11.2. MDP Formulation\n```\nState: Mức tồn kho hiện tại\nAction: Số lượng đặt hàng\nReward: Revenue - Ordering_cost - Holding_cost - Stockout_penalty\nTransition: Stochastic demand\n```\n\n#### 11.3. Giải pháp\n- Sử dụng Value Iteration\n- Tìm chính sách (s,S): Đặt hàng đến mức S khi tồn kho ≤ s\n- Optimize (s,S) parameters\n\n### 12. Ưu điểm và hạn chế của Dynamic Programming\n\n#### 12.1. Ưu điểm\n✅ **Đảm bảo tối ưu**: Hội tụ đến chính sách tối ưu π*\n✅ **Cơ sở lý thuyết vững chắc**: Dựa trên phương trình Bellman\n✅ **Hiệu quả với medium-sized problems**: Polynomial complexity\n✅ **Framework cho nhiều thuật toán khác**: Nền tảng cho TD, Q-learning\n\n#### 12.2. Hạn chế\n❌ **Yêu cầu biết hoàn toàn mô hình**: Cần biết P và R\n❌ **Curse of dimensionality**: Không mở rộng cho không gian lớn\n❌ **Cần duyệt tất cả trạng thái**: Không practical với continuous states\n❌ **Computation cost**: Tốn nhiều tính toán mỗi iteration\n\n### 13. Mở rộng và cải tiến\n\n\n**Các khái niệm quan trọng:**\n- Optimality là tính chất đảm bảo rằng một thuật toán hoặc chính sách đạt được kết quả tốt nhất có thể (tối đa hóa tổng Reward chiết khấu). Dynamic Programming (Policy Iteration, Value Iteration) có ưu điểm là đảm bảo hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Linear Programming giải quyết bài toán tìm chính sách tối ưu π* bằng cách chuyển đổi nó thành một bài toán quy hoạch tuyến tính.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nPhần tiếp theo sẽ mở rộng sang **Model-Free Control**: Không chỉ đánh giá mà còn tìm chính sách tối ưu mà không cần mô hình!\n\n---\n\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Control\n\n#### 1.1. Định nghĩa bài toán\n**Control Problem**: Tìm chính sách tối ưu π* mà không biết trước mô hình môi trường (P, R)\n\n**So sánh với Prediction**:\n- Prediction: Đánh giá V^π cho π cho trước\n- Control: Tối ưu hóa π để maximize V^π\n\n#### 1.2. Thách thức\n- Không biết mô hình → không thể dùng Dynamic Programming\n- Phải học từ interaction với môi trường\n- Cần cân bằng exploration và exploitation\n\n#### 1.3. Ý tưởng chính\nSử dụng **Generalized Policy Iteration (GPI)** framework:\n```\nPolicy Evaluation (Model-Free) → Policy Improvement → Repeat\n```\n\n### 2. Monte Carlo Control\n\n#### 2.1. Từ V(s) sang Q(s,a)\n\n**Vấn đề với V(s)**:\n```\nPolicy Improvement cần:\nπ'(s) = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n                 ↑ Cần biết mô hình!\n```\n\n**Giải pháp**: Sử dụng Q(s,a)\n```\nπ'(s) = argmax_a Q(s,a)  ← Không cần mô hình!\n```\n\n#### 2.2. Monte Carlo Policy Iteration\n\n**Thuật toán**:\n```\n1. Khởi tạo:\n   Q(s,a) = 0, ∀s,a\n   π = chính sách khởi tạo\n\n2. Lặp:\n   a) Policy Evaluation (MC):\n      - Tạo nhiều episodes theo π\n      - Cập nhật Q^π(s,a) bằng MC\n   \n   b) Policy Improvement:\n      π(s) = argmax_a Q(s,a), ∀s\n```\n\n#### 2.3. Vấn đề Exploration\n\n**Greedy Policy**:\n```\nπ(s) = argmax_a Q(s,a)\n```\n→ Chỉ exploit, không explore → Có thể bỏ lỡ chính sách tốt hơn\n\n**Giải pháp 1: ε-Greedy Policy**\n```\nπ(a|s) = {\n    1 - ε + ε/|A|,  nếu a = argmax Q(s,a)\n    ε/|A|,          ngược lại\n}\n```\n\n**Đặc điểm**:\n- Xác suất 1-ε: Chọn hành động tốt nhất (exploit)\n- Xác suất ε: Chọn ngẫu nhiên (explore)\n- ε decay theo thời gian: Explore nhiều lúc đầu, exploit nhiều sau\n\n#### 2.4. ε-Greedy Monte Carlo Control\n\n**Thuật toán**:\n```\nKhởi tạo:\n    Q(s,a) arbitrarily, ∀s,a\n    π = ε-greedy policy dựa trên Q\n    Returns(s,a) = empty list, ∀s,a\n\nLặp forever:\n    1. Tạo episode theo π:\n       S_0, A_0, R_1, ..., S_T\n    \n    2. Với mỗi cặp (s,a) xuất hiện trong episode:\n       G = return sau lần xuất hiện đầu tiên\n       Thêm G vào Returns(s,a)\n       Q(s,a) = average(Returns(s,a))\n    \n    3. Với mỗi s trong episode:\n       π(s) = ε-greedy(Q(s,·))\n```\n\n#### 2.5. Greedy in the Limit of Infinite Exploration (GLIE)\n\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Linear Programming giải quyết bài toán tìm chính sách tối ưu π* bằng cách chuyển đổi nó thành một bài toán quy hoạch tuyến tính.\n- Policy Iteration luôn hội tụ đến chính sách tối ưu π*.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*."
        },
        "output_without_context": {
            "question": "Trong Quy trình Quyền lợi tối ưu Bellman, khi nào bạn nên cập nhật giá trị của một trạng thái trong một MDP?",
            "answer": "Khi bạn tìm thấy một chính sách tốt hơn cho trạng thái đó."
        }
    },
    {
        "topic_description": "Chủ đề này khám phá cách đánh giá chất lượng của một chính sách thông qua các hàm mục tiêu và giá trị khởi điểm trong các MDP. Học sinh sẽ được kiểm tra khả năng tính toán và hiểu mối quan hệ giữa giá trị và chính sách.",
        "output_with_context": {
            "question": "Trong quá trình đánh giá chính sách, phương trình Bellman cho hàm giá trị trạng thái V^π(s) được biểu diễn như thế nào?",
            "answer": "V^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n- Policy Evaluation là quá trình ước lượng hàm giá trị (V^π hoặc Q^π) cho một chính sách π đã cho. Trong Model-Free Control, Policy Evaluation phải được thực hiện mà không cần biết mô hình môi trường, thường thông qua các phương pháp như Monte Carlo hoặc Temporal Difference.\n\n**Mối quan hệ:**\n- Policy Evaluation đánh giá hàm giá trị V cho chính sách hiện tại.\n- Policy Evaluation ước lượng hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n- Policy Evaluation đánh giá hàm giá trị V(s) cho một chính sách cụ thể π.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- Policy Evaluation (Đánh giá chính sách), còn được gọi là Iterative Policy Evaluation, là một kỹ thuật trong Dynamic Programming và là một bước quan trọng trong các thuật toán như Policy Iteration và Generalized Policy Iteration (GPI). Mục tiêu chính của nó là tính toán hàm giá trị trạng thái V^π(s) hoặc hàm giá trị hành động Q(s,a) cho một chính sách π đã cho. Quá trình này được thực hiện bằng cách lặp đi lặp lại việc cập nhật hàm giá trị dựa trên Bellman Expectation Equation (hoặc phương trình Bellman cho Policy Evaluation) cho đến khi hàm giá trị hội tụ. Sự hội tụ được xác định khi sự thay đổi (Δ) của V(s) giữa các lần lặp nhỏ hơn một ngưỡng nhất định (θ). Công thức cập nhật V(s) thường là V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r in env.transitions(s, a)]). Thuật toán thường khởi tạo V(s) bằng 0 hoặc một giá trị ngẫu nhiên cho tất cả các trạng thái.\n- Policy Evaluation là quá trình ước lượng hàm giá trị (V^π hoặc Q^π) cho một chính sách π đã cho. Trong Model-Free Control, Policy Evaluation phải được thực hiện mà không cần biết mô hình môi trường, thường thông qua các phương pháp như Monte Carlo hoặc Temporal Difference.\n\n**Mối quan hệ:**\n- Policy Evaluation đánh giá hàm giá trị V cho chính sách hiện tại.\n- Policy Evaluation đánh giá hàm giá trị V(s) cho một chính sách cụ thể π."
        },
        "output_without_context": {
            "question": "Trong một MDP, hàm mục tiêu được sử dụng để đánh giá chất lượng của chính sách nào?",
            "answer": "Giá trị của chính sách."
        }
    },
    {
        "topic_description": "Chủ đề này trình bày về thuật toán REINFORCE và ứng dụng của nó trong việc cập nhật tham số thông qua đạo hàm chính sách Monte-Carlo. Học sinh sẽ đánh giá khả năng áp dụng trong các bài toán thực tế.",
        "output_with_context": {
            "question": "Trong thuật toán REINFORCE, công thức nào được sử dụng để cập nhật tham số của chính sách dựa trên tổng reward chiết khấu từ một episode hoàn chỉnh?",
            "answer": "θ ← θ + α γ^t G_t ∇_θ log π(A_t|S_t;θ)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Policy Gradient Methods - Phương Pháp Gradient Chính Sách\n**Algorithm components**:\n1. Stochastic actor: π(a|s;θ)\n2. Soft Q-functions: Q₁, Q₂\n3. Target Q-functions\n4. Automatic entropy tuning\n\n### 8. So sánh các thuật toán\n\n#### 8.1. Bảng so sánh\n\n| Algorithm | Type | Action Space | Stability | Sample Efficiency | Performance |\n|-----------|------|--------------|-----------|-------------------|-------------|\n| REINFORCE | On-policy | Both | Low | Low | Baseline |\n| A2C | On-policy | Both | Medium | Medium | Good |\n| A3C | On-policy | Both | Medium | Medium | Good |\n| TRPO | On-policy | Both | High | Low | Very Good |\n| PPO | On-policy | Both | High | Medium | Very Good |\n| DDPG | Off-policy | Continuous | Medium | High | Good |\n| TD3 | Off-policy | Continuous | High | High | Very Good |\n| SAC | Off-policy | Continuous | High | High | Excellent |\n\n#### 8.2. Khi nào dùng gì?\n\n**REINFORCE**: \n- Simple tasks\n- Educational purposes\n- Baseline comparison\n\n**A2C/A3C**:\n- Need fast training với parallel workers\n- Good general-purpose algorithm\n\n**PPO**:\n- Current go-to cho nhiều tasks\n- Stable và reliable\n- Robotics, games\n\n**DDPG/TD3**:\n- Continuous control\n- Robotics\n- Physical simulation\n\n**SAC**:\n- Best performance cho continuous control\n- Robust và stable\n- State-of-the-art\n\n### 9. Ứng dụng thực tế\n\n#### 9.1. Robotics\n\n**Manipulation Tasks**:\n- Grasping objects\n- Assembly\n- Method: PPO, SAC\n\n**Locomotion**:\n- Walking, running\n- Complex terrain navigation\n- Method: TD3, SAC\n\n#### 9.2. Game Playing\n\n**Atari Games**:\n- A3C đạt human-level\n- PPO improvements\n\n**Continuous Control Games**:\n- Racing games\n- Flight simulators\n- Method: SAC, TD3\n\n#### 9.3. Autonomous Systems\n\n**Drone Control**:\n- Navigation\n- Obstacle avoidance\n- Method: PPO với safety constraints\n\n**Self-Driving**:\n- Lane keeping\n- Parking\n- Method: SAC với hierarchical RL\n\n### 10. Code Implementation\n\n#### 10.1. REINFORCE\n```python\nclass REINFORCE:\n    def __init__(self, policy_net, lr=1e-3, gamma=0.99):\n        self.policy = policy_net\n        self.optimizer = optim.Adam(policy.parameters(), lr=lr)\n        self.gamma = gamma\n    \n    def select_action(self, state):\n        state = torch.FloatTensor(state)\n        probs = self.policy(state)\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n    \n    def train(self, episode_data):\n        \"\"\"episode_data: [(log_prob, reward), ...]\"\"\"\n        policy_loss = []\n        returns = []\n        \n        # Calculate returns\n        R = 0\n        for _, reward in reversed(episode_data):\n            R = reward + self.gamma * R\n            returns.insert(0, R)\n        \n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n        \n        # Calculate policy loss\n        for (log_prob, _), R in zip(episode_data, returns):\n            policy_loss.append(-log_prob * R)\n        \n        # Update\n        self.optimizer.zero_grad()\n        policy_loss = torch.cat(policy_loss).sum()\n        policy_loss.backward()\n        self.optimizer.step()\n```\n\n#### 10.2. Actor-Critic\n```python\nclass ActorCritic:\n    def __init__(self, actor_net, critic_net, \n                 lr_actor=1e-3, lr_critic=1e-3, gamma=0.99):\n        self.actor = actor_net\n        self.critic = critic_net\n        self.actor_optimizer = optim.Adam(actor.parameters(), lr=lr_actor)\n        self.critic_optimizer = optim.Adam(critic.parameters(), lr=lr_critic)\n        self.gamma = gamma\n    \n    def select_action(self, state):\n        state = torch.FloatTensor(state)\n        probs = self.actor(state)\n        m = Categorical(probs)\n        action = m.sample()\n\n**Các khái niệm quan trọng:**\n- REINFORCE là một thuật toán Policy Gradient on-policy, model-free cơ bản, sử dụng Monte Carlo để ước lượng gradient của hàm mục tiêu. Nó cập nhật chính sách dựa trên tổng reward chiết khấu (return) từ một episode hoàn chỉnh, sử dụng nó làm hệ số cho gradient log-probability của hành động. Phương trình cập nhật chính sách dựa trên gradient của hàm mục tiêu J(θ) = E[∑_t γ^t R_t]. REINFORCE thường có phương sai cao, độ ổn định thấp và hiệu quả mẫu thấp, do đó thường được dùng cho các tác vụ đơn giản hoặc làm baseline.\n\n**Mối quan hệ:**\n- REINFORCE sử dụng optimizer để cập nhật các tham số của policy_net dựa trên policy_loss.\n- REINFORCE tối ưu hóa policy_loss để cập nhật chính sách, nhằm tối đa hóa kỳ vọng tổng reward.\n- REINFORCE sử dụng policy_net để chọn hành động và tính toán log-probability của hành động đó.\n- REINFORCE sử dụng returns (tổng reward chiết khấu) để làm hệ số cho gradient chính sách trong policy_loss.\n- REINFORCE sử dụng phân phối Categorical để lấy mẫu hành động từ phân phối xác suất của chính sách.\n- REINFORCE sử dụng lr (learning rate) để điều chỉnh tốc độ cập nhật các tham số của policy_net.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n#### 13.1. Approximate Dynamic Programming\n- Sử dụng function approximation\n- Neural networks để biểu diễn V hoặc π\n- Trade-off giữa accuracy và scalability\n\n#### 13.2. Model-Free Methods\n- Không cần biết P và R\n- Học từ experience\n- Temporal-Difference Learning, Q-Learning (phần sau)\n\n#### 13.3. Deep Reinforcement Learning\n- Kết hợp DP với deep learning\n- DQN, Actor-Critic, PPO\n- Giải quyết được bài toán phức tạp\n\n### 14. Code Implementation - Ví dụ Python\n\n#### 14.1. Policy Iteration\n```python\ndef policy_iteration(env, gamma=0.9, theta=1e-6):\n    # Khởi tạo\n    V = np.zeros(env.num_states)\n    policy = np.zeros(env.num_states, dtype=int)\n    \n    while True:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.num_states):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([p * (r + gamma * V[s_]) \n                           for p, s_, r in env.transitions(s, a)])\n                delta = max(delta, abs(v - V[s]))\n            if delta < theta:\n                break\n        \n        # Policy Improvement\n        policy_stable = True\n        for s in range(env.num_states):\n            old_action = policy[s]\n            # Tìm hành động tốt nhất\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            policy[s] = np.argmax(action_values)\n            \n            if old_action != policy[s]:\n                policy_stable = False\n        \n        if policy_stable:\n            return V, policy\n```\n\n#### 14.2. Value Iteration\n```python\ndef value_iteration(env, gamma=0.9, theta=1e-6):\n    V = np.zeros(env.num_states)\n    \n    while True:\n        delta = 0\n        for s in range(env.num_states):\n            v = V[s]\n            # Bellman optimality update\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            V[s] = max(action_values)\n            delta = max(delta, abs(v - V[s]))\n        \n        if delta < theta:\n            break\n    \n    # Trích xuất chính sách\n    policy = np.zeros(env.num_states, dtype=int)\n    for s in range(env.num_states):\n        action_values = []\n        for a in range(env.num_actions):\n            q_value = sum([p * (r + gamma * V[s_]) \n                          for p, s_, r in env.transitions(s, a)])\n            action_values.append(q_value)\n        policy[s] = np.argmax(action_values)\n    \n    return V, policy\n```\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement policy evaluation cho Gridworld\n2. So sánh tốc độ hội tụ của in-place và two-array DP\n3. Visualize quá trình hội tụ của value iteration\n\n#### 15.2. Bài tập nâng cao\n1. Giải Gambler's Problem với các giá trị p khác nhau\n2. Implement prioritized sweeping\n3. Modified policy iteration với k=1, k=3, k=∞\n\n#### 15.3. Dự án\n1. Xây dựng AI cho game 2048 bằng DP\n2. Tối ưu hóa việc sạc pin cho robot\n3. Quản lý danh mục đầu tư bằng MDP\n\n### 16. Kết luận\n\n\n**Các khái niệm quan trọng:**\n- Reinforcement Learning (RL), hay Học tăng cường, là một lĩnh vực của Machine Learning (học máy) trong đó một agent (tác nhân) học cách đưa ra các quyết định tối ưu trong một môi trường nhằm tối đa hóa tổng phần thưởng tích lũy. Agent học thông qua tương tác thử và sai với môi trường, nhận phản hồi dưới dạng phần thưởng hoặc hình phạt cho các hành động của mình mà không cần được lập trình rõ ràng. Các phương pháp trong RL thường liên quan đến việc giải quyết các bài toán Markov Decision Process (MDP).\n- Deep Reinforcement Learning (Học tăng cường sâu) là một lĩnh vực kết hợp Reinforcement Learning với Deep Learning. Nó sử dụng mạng neural sâu làm function approximator để biểu diễn các hàm giá trị hoặc chính sách, cho phép giải quyết các bài toán RL phức tạp với không gian trạng thái/hành động lớn, chẳng hạn như chơi game hoặc điều khiển robot. Các thuật toán nổi bật bao gồm DQN, Actor-Critic và PPO.\n\n**Mối quan hệ:**\n- Deep Reinforcement Learning bao gồm thuật toán Actor-Critic.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Policy Gradient Methods - Phương Pháp Gradient Chính Sách\n        x = F.relu(self.fc1(state))\n        logits = self.fc2(x)\n        return F.softmax(logits, dim=-1)\n```\n\n**Continuous Control**:\n```python\nclass GaussianPolicy(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.mean = nn.Linear(128, action_dim)\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n    \n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        mean = self.mean(x)\n        std = torch.exp(self.log_std)\n        return mean, std\n```\n\n### 3. Policy Gradient Theorem\n\n#### 3.1. Objective Function\n\n**Mục tiêu**: Maximize expected return\n```\nJ(θ) = E_τ~π_θ[G_τ] = E[Σ_t γ^t R_t]\n```\n\nHoặc với episodic tasks:\n```\nJ(θ) = V^π_θ(s_0) = E_π_θ[G_0 | S_0 = s_0]\n```\n\n#### 3.2. Policy Gradient Theorem\n\n**Định lý**:\n```\n∇_θ J(θ) = E_π_θ[∇_θ log π(A|S;θ) Q^π(S,A)]\n         = E_π_θ[∇_θ log π(A|S;θ) G_t]\n```\n\n**Giải thích**:\n- ∇_θ log π(A|S;θ): Score function (hướng tăng probability của action)\n- Q^π(S,A): Weighting (actions tốt được tăng, xấu giảm)\n\n#### 3.3. REINFORCE Algorithm\n\n**Monte Carlo Policy Gradient**:\n```\nKhởi tạo θ\nThiết lập learning rate α\n\nLặp:\n    Tạo episode theo π(·|·;θ): S_0, A_0, R_1, ..., S_T\n    \n    Với mỗi bước t:\n        G_t = Σ_{k=t}^T γ^{k-t} R_k\n        θ ← θ + α γ^t G_t ∇_θ log π(A_t|S_t;θ)\n```\n\n**Intuition**:\n- Nếu G_t > 0: Tăng probability của actions đã chọn\n- Nếu G_t < 0: Giảm probability\n- Magnitude tỷ lệ với |G_t|\n\n#### 3.4. REINFORCE với Baseline\n\n**Vấn đề**: High variance trong gradient estimates\n\n**Giải pháp**: Subtract baseline b(s)\n```\n∇_θ J(θ) = E[∇_θ log π(A|S;θ) (G_t - b(S_t))]\n```\n\n**Baseline phổ biến**: V^π(s)\n```\nAdvantage: A^π(s,a) = Q^π(s,a) - V^π(s)\n∇_θ J(θ) = E[∇_θ log π(A|S;θ) A^π(S,A)]\n```\n\n**Thuật toán**:\n```\nKhởi tạo θ, w (cho value function baseline)\n\nLặp:\n    Tạo episode: S_0, A_0, R_1, ..., S_T\n    \n    Với mỗi bước t:\n        G_t = Σ_{k=t}^T γ^{k-t} R_k\n        δ_t = G_t - V̂(S_t; w)  # Advantage estimate\n        \n        # Update policy\n        θ ← θ + α_θ γ^t δ_t ∇_θ log π(A_t|S_t;θ)\n        \n        # Update value function\n        w ← w + α_w δ_t ∇_w V̂(S_t; w)\n```\n\n### 4. Actor-Critic Methods\n\n#### 4.1. Ý tưởng\n\n**Actor**: Policy π(a|s;θ)\n**Critic**: Value function V(s;w) hoặc Q(s,a;w)\n\n**Actor-Critic Framework**:\n```\nActor: Chọn actions theo policy\n\n**Các khái niệm quan trọng:**\n- Policy Gradient Methods là một nhóm các thuật toán trong Reinforcement Learning tập trung vào việc tối ưu hóa trực tiếp chính sách (policy) của agent bằng cách tính toán gradient của hàm mục tiêu (thường là tổng reward chiết khấu hoặc tổng reward kỳ vọng/expected return) đối với các tham số của chính sách. Thay vì học một hàm giá trị và suy ra chính sách, các phương pháp này điều chỉnh chính sách để tăng xác suất của các hành động tốt và giảm xác suất của các hành động xấu. Các phương pháp này thường hiệu quả và được sử dụng cho các không gian hành động liên tục (continuous action spaces) hoặc khi chính sách là một hàm phức tạp (ví dụ: mạng nơ-ron) hoặc khi hàm giá trị khó ước lượng. Một số phương pháp Policy Gradient sử dụng kiến trúc Actor-Critic, trong đó \"Actor\" học chính sách và \"Critic\" đánh giá các hành động của Actor.\n\n**Mối quan hệ:**\n- Policy Gradient Methods sử dụng kiến trúc Actor-Critic để kết hợp học giá trị và chính sách.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Policy Gradient Methods - Phương Pháp Gradient Chính Sách\nCritic: Đánh giá actions\nActor học từ feedback của Critic\n```\n\n#### 4.2. Advantage Actor-Critic (A2C)\n\n**TD Error làm advantage**:\n```\nδ_t = R_{t+1} + γV(S_{t+1};w) - V(S_t;w)\n```\n\n**Updates**:\n```\n# Actor update\nθ ← θ + α_θ δ_t ∇_θ log π(A_t|S_t;θ)\n\n# Critic update\nw ← w + α_w δ_t ∇_w V(S_t;w)\n```\n\n**Algorithm**:\n```\nKhởi tạo θ, w\nThiết lập α_θ, α_w\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        A ~ π(·|S;θ)\n        Thực hiện A, quan sát R, S'\n        \n        δ = R + γV(S';w) - V(S;w)\n        \n        w ← w + α_w δ ∇_w V(S;w)\n        θ ← θ + α_θ δ ∇_θ log π(A|S;θ)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 4.3. Asynchronous Advantage Actor-Critic (A3C)\n\n**Ý tưởng**: Parallel actors với shared parameters\n\n**Architecture**:\n```\nGlobal Network (θ, w)\n    ↓ Copy\nMultiple Workers (θ', w')\n    ↓ Collect experience\n    ↓ Compute gradients\n    ↑ Update global network\n```\n\n**Benefits**:\n- Faster learning (parallel experience collection)\n- Decorrelated experience (different workers explore differently)\n- Stable learning\n\n#### 4.4. Generalized Advantage Estimation (GAE)\n\n**n-Step TD Error**:\n```\nδ_t^(n) = R_{t+1} + γR_{t+2} + ... + γ^{n-1}R_{t+n} + γ^n V(S_{t+n}) - V(S_t)\n```\n\n**GAE**:\n```\nA_t^GAE(λ) = Σ_{l=0}^∞ (γλ)^l δ_{t+l}\n           = (1-λ) Σ_{n=1}^∞ λ^{n-1} δ_t^(n)\n```\n\n**Đặc điểm**:\n- λ = 0: TD(0), low variance, high bias\n- λ = 1: Monte Carlo, high variance, low bias\n- λ ∈ (0,1): Trade-off\n\n### 5. Trust Region Methods\n\n#### 5.1. Vấn đề với Vanilla Policy Gradient\n\n**Large updates**: Có thể làm policy collapse\n**Solution**: Constrain update size\n\n#### 5.2. Trust Region Policy Optimization (TRPO)\n\n**Objective**:\n```\nmaximize E[π_θ_new(a|s) / π_θ_old(a|s) · A^π_old(s,a)]\nsubject to: KL(π_θ_old || π_θ_new) ≤ δ\n```\n\n**KL Divergence constraint**: Đảm bảo new policy không quá khác old policy\n\n**Implementation**: Sử dụng conjugate gradient và line search\n\n**Đặc điểm**:\n✅ Monotonic improvement guarantee\n✅ Stable learning\n❌ Computationally expensive\n❌ Difficult to implement\n\n#### 5.3. Proximal Policy Optimization (PPO)\n\n**Ý tưởng**: Approximate TRPO constraint bằng clipping\n\n**Clipped Surrogate Objective**:\n```\nL^CLIP(θ) = E[min(r_t(θ)A_t, clip(r_t(θ), 1-ε, 1+ε)A_t)]\n\nr_t(θ) = π_θ(a|s) / π_θ_old(a|s)  # Importance ratio\n```\n\n**Giải thích**:\n- Clip r_t ∈ [1-ε, 1+ε] (thường ε=0.2)\n- Prevent too large policy updates\n- Simpler và faster than TRPO\n\n**PPO Algorithm**:\n```\nKhởi tạo θ, w\n\nLặp:\n    # Collect trajectories\n    Với mỗi worker:\n\n**Các khái niệm quan trọng:**\n- Policy Gradient Methods là một nhóm các thuật toán trong Reinforcement Learning tập trung vào việc tối ưu hóa trực tiếp chính sách (policy) của agent bằng cách tính toán gradient của hàm mục tiêu (thường là tổng reward chiết khấu hoặc tổng reward kỳ vọng/expected return) đối với các tham số của chính sách. Thay vì học một hàm giá trị và suy ra chính sách, các phương pháp này điều chỉnh chính sách để tăng xác suất của các hành động tốt và giảm xác suất của các hành động xấu. Các phương pháp này thường hiệu quả và được sử dụng cho các không gian hành động liên tục (continuous action spaces) hoặc khi chính sách là một hàm phức tạp (ví dụ: mạng nơ-ron) hoặc khi hàm giá trị khó ước lượng. Một số phương pháp Policy Gradient sử dụng kiến trúc Actor-Critic, trong đó \"Actor\" học chính sách và \"Critic\" đánh giá các hành động của Actor.\n\n**Mối quan hệ:**\n- Policy Gradient Methods sử dụng kiến trúc Actor-Critic để kết hợp học giá trị và chính sách."
        },
        "output_without_context": {
            "question": "Trong thuật toán REINFORCE, việc cập nhật tham số chính sách dựa trên đạo hàm Monte-Carlo được thực hiện thông qua công thức nào sau đây?",
            "answer": "Đạo hàm chính sách Monte-Carlo."
        }
    },
    {
        "topic_description": "Chủ đề này kết nối và làm rõ khái niệm về đạo hàm chính sách tự nhiên và lý thuyết phía sau nó. Học sinh sẽ kiểm tra khả năng phân tích mối quan hệ giữa quá trình học và kết quả đạt được từ các quyết định trong môi trường phức tạp.",
        "output_with_context": {
            "question": "Trong phương pháp cải thiện chính sách, điều kiện nào đảm bảo rằng chính sách mới π' sẽ tốt hơn hoặc bằng chính sách cũ π?",
            "answer": "Q^π(s, π'(s)) ≥ V^π(s)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\nDynamic Programming là nền tảng quan trọng trong Reinforcement Learning. Mặc dù có hạn chế về yêu cầu biết mô hình và curse of dimensionality, DP cung cấp:\n\n- **Framework lý thuyết**: Hiểu rõ về optimal value functions và policies\n- **Nền tảng thuật toán**: Cơ sở cho các phương pháp model-free\n- **Công cụ thực tế**: Giải quyết được nhiều bài toán thực tế\n\nCác khái niệm từ DP (đặc biệt là GPI và Bellman equations) sẽ xuất hiện xuyên suốt trong các phương pháp học tăng cường tiếp theo, từ Temporal-Difference Learning đến Deep Q-Networks.\n\n---\n\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Learning\n\n#### 1.1. Định nghĩa\nModel-Free Learning là các phương pháp học tăng cường không yêu cầu biết trước mô hình môi trường (hàm chuyển trạng thái P và hàm phần thưởng R). Agent học trực tiếp từ kinh nghiệm tương tác với môi trường.\n\n#### 1.2. So sánh Model-Based vs Model-Free\n\n| Đặc điểm | Model-Based | Model-Free |\n|----------|-------------|------------|\n| Yêu cầu mô hình | Cần biết P và R | Không cần |\n| Học từ | Phương trình Bellman | Kinh nghiệm thực tế |\n| Ưu điểm | Hiệu quả dữ liệu | Linh hoạt, thực tế |\n| Nhược điểm | Khó có mô hình chính xác | Cần nhiều dữ liệu |\n| Ví dụ | DP, Model-Based RL | MC, TD, Q-Learning |\n\n#### 1.3. Bài toán Prediction\n**Mục tiêu**: Đánh giá một chính sách π cho trước\n- Input: Chính sách π\n- Output: Hàm giá trị V^π(s)\n- Không cần biết mô hình môi trường\n\n### 2. Monte Carlo Methods - Phương Pháp Monte Carlo\n\n#### 2.1. Ý tưởng cơ bản\nMonte Carlo (MC) ước lượng giá trị của trạng thái bằng cách lấy trung bình các return thực tế nhận được từ nhiều episodes.\n\n**Nguyên lý**:\n```\nV^π(s) = E_π[G_t | S_t = s]\n       ≈ average of returns từ trạng thái s\n```\n\n#### 2.2. Episode và Return\n\n**Episode**: Một chuỗi hoàn chỉnh từ trạng thái ban đầu đến kết thúc\n```\nS_0, A_0, R_1, S_1, A_1, R_2, ..., S_T\n```\n\n**Return**: Tổng phần thưởng chiết khấu\n```\nG_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... + γ^{T-t-1}R_T\n```\n\n#### 2.3. First-Visit Monte Carlo\n\n**Thuật toán**:\n```\nKhởi tạo:\n    V(s) = 0, ∀s\n    Returns(s) = danh sách rỗng, ∀s\n\nVới mỗi episode:\n    Tạo một episode tuân theo π: S_0, A_0, R_1, ..., S_T\n    G = 0\n    \n    Với mỗi bước t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        \n\n**Các khái niệm quan trọng:**\n- Policies (chính sách) là một ánh xạ từ trạng thái đến hành động, hoặc phân phối xác suất trên các hành động cho mỗi trạng thái. Chính sách định nghĩa hành vi của agent trong môi trường. Mục tiêu của RL thường là tìm ra chính sách tối ưu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n2. Add Double DQN improvement\n3. Experiment với network architectures\n\n#### 13.3. Dự án\n1. Build Atari game player với DQN\n2. Implement Prioritized Experience Replay\n3. Compare tabular Q-learning vs FA Q-learning\n\n### 14. Kết luận\n\nValue Function Approximation là bước đột phá cho phép RL scale lên bài toán thực tế với large/continuous state spaces.\n\n**Key Takeaways**:\n\n1. **Function Approximation**: Generalization thay vì memorization\n2. **Linear Methods**: Simple, stable, nhưng limited expressiveness\n3. **Neural Networks**: Powerful nhưng cần careful engineering\n4. **DQN**: Breakthrough với experience replay + target network\n5. **Stability**: Deadly triad cần được xử lý cẩn thận\n\n**Convergence challenges**:\n- Semi-gradient methods không follow true gradient\n- Off-policy + FA + bootstrapping = risk of divergence\n- Practical techniques (replay, target network) help significantly\n\n**Tiếp theo**: **Policy Gradient Methods** - Học trực tiếp policy thay vì value function!\n\n---\n\n## Policy Gradient Methods - Phương Pháp Gradient Chính Sách\n\n### 1. Giới thiệu về Policy Gradient\n\n#### 1.1. Value-Based vs Policy-Based\n\n**Value-Based Methods** (Q-Learning, DQN):\n```\nHọc Q(s,a) → Derive policy: π(s) = argmax_a Q(s,a)\n```\n- Indirect: Học value rồi suy ra policy\n- Deterministic policies\n- Khó với continuous actions\n\n**Policy-Based Methods**:\n```\nHọc trực tiếp π(a|s; θ)\n```\n- Direct: Parameterize và optimize policy\n- Stochastic policies tự nhiên\n- Hiệu quả với continuous actions\n\n#### 1.2. Ưu điểm của Policy Gradient\n\n✅ **Continuous action spaces**: Không cần discretization\n✅ **Stochastic policies**: Tự nhiên cho exploration và game theory\n✅ **Better convergence**: Smooth optimization landscape\n✅ **Effective in high dimensions**: Especially với function approximation\n✅ **Learn policies directly**: Không qua intermediate value function\n\n#### 1.3. Nhược điểm\n\n❌ **High variance**: Gradient estimates có variance cao\n❌ **Sample inefficient**: Cần nhiều samples\n❌ **Local optima**: Có thể stuck tại local optima\n❌ **Slow convergence**: Thường chậm hơn value-based\n\n### 2. Policy Parameterization\n\n#### 2.1. Discrete Action Spaces\n\n**Softmax Policy** (Gibbs/Boltzmann):\n```\nπ(a|s; θ) = exp(h(s,a;θ)) / Σ_b exp(h(s,b;θ))\n```\n\n**Linear Preferences**:\n```\nh(s,a;θ) = θ^T φ(s,a)\n```\n\n**Neural Network**:\n```\nInput: State s\nHidden Layers: Neural network\nOutput: Logits h(s,a;θ)\nPolicy: π(a|s;θ) = softmax(h(s,·;θ))\n```\n\n#### 2.2. Continuous Action Spaces\n\n**Gaussian Policy**:\n```\nπ(a|s; θ) = N(μ(s;θ), σ²)\n\na ~ N(μ(s;θ), σ²)\n```\n\n**Parameterization**:\n```\nμ(s;θ) = θ^T φ(s)  [Linear]\nμ(s;θ) = NN(s;θ)   [Neural Network]\n\nσ có thể:\n- Fixed constant\n- State-dependent: σ(s;θ)\n- Action-dependent\n```\n\n**Beta Distribution** (bounded actions):\n```\na ∈ [0, 1]\nπ(a|s;θ) = Beta(α(s;θ), β(s;θ))\n```\n\n#### 2.3. Ví dụ minh họa\n\n**CartPole** (Discrete):\n```python\nclass PolicyNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, action_dim)\n    \n    def forward(self, state):\n\n**Các khái niệm quan trọng:**\n- Deterministic Policy (Chính sách xác định) là một loại chính sách trong Reinforcement Learning (RL) mà tại mỗi trạng thái 's', chính sách π(s) (hoặc μ(s;θ)) sẽ luôn chọn một hành động 'a' cụ thể và duy nhất. Công thức biểu diễn là π(s) = a, trong đó θ là các tham số của chính sách. Các chính sách này thường được suy ra từ các phương pháp dựa trên giá trị (Value-Based Methods), ví dụ như π(s) = argmax_a Q(s,a), và thường hiệu quả hơn về mặt mẫu (sample efficient) trong các không gian hành động liên tục.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n        V(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n        Δ = max(Δ, |v - V(s)|)\ncho đến khi Δ < θ (ngưỡng hội tụ)\n```\n\n#### 2.4. Ví dụ: Gridworld 4x4\n\n**Thiết lập**:\n- Lưới 4x4 với 16 ô\n- Ô góc trên trái và dưới phải là trạng thái kết thúc\n- Chính sách: Di chuyển ngẫu nhiên đều (0.25 cho mỗi hướng)\n- Phần thưởng: -1 cho mỗi bước\n- γ = 1\n\n**Quá trình hội tụ**:\n```\nIteration 0: V(s) = 0 (∀s)\nIteration 1: V(s) = -1 (∀s không phải terminal)\nIteration 2: V(s) = -1.75 (các ô giữa)\n...\nHội tụ sau ~100 iterations\n```\n\n#### 2.5. Độ phức tạp\n- **Thời gian mỗi iteration**: O(|S|² × |A|)\n- **Số lượng iterations**: Phụ thuộc vào γ và θ\n\n### 3. Policy Improvement - Cải Thiện Chính Sách\n\n#### 3.1. Policy Improvement Theorem\n\nNếu với mọi trạng thái s:\n```\nQ^π(s, π'(s)) ≥ V^π(s)\n```\nThì chính sách π' tốt hơn hoặc bằng π:\n```\nV^{π'}(s) ≥ V^π(s), ∀s\n```\n\n#### 3.2. Greedy Policy Improvement\n\n**Công thức**:\n```\nπ'(s) = argmax_a Q^π(s,a)\n      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n**Ý nghĩa**: Chọn hành động tốt nhất theo hàm giá trị hiện tại\n\n#### 3.3. Ví dụ: Cải thiện chính sách trong Gridworld\n\n**Trước khi cải thiện** (chính sách ngẫu nhiên):\n```\nV^π = [-14, -20, -22, -20,\n       -20, -22, -20, -14,\n       -22, -20, -14,   0]\n```\n\n**Sau khi cải thiện** (chính sách tham lam):\n- Chọn hướng di chuyển có V(s') cao nhất\n- Luôn đi về phía trạng thái kết thúc\n\n### 4. Policy Iteration - Lặp Chính Sách\n\n#### 4.1. Thuật toán Policy Iteration\n\n```\n1. Khởi tạo:\n   V(s) = 0, ∀s ∈ S\n   π(s) = random, ∀s ∈ S\n\n2. Lặp:\n   a) Policy Evaluation:\n      Tính V^π bằng iterative policy evaluation\n   \n   b) Policy Improvement:\n      π' = greedy(V^π)\n      \n   c) Kiểm tra hội tụ:\n      Nếu π' = π, dừng và trả về V^π và π\n      Ngược lại, π = π' và quay lại bước 2a\n```\n\n#### 4.2. Tính chất của Policy Iteration\n\n**Ưu điểm**:\n- Luôn hội tụ đến chính sách tối ưu π*\n- Thường hội tụ trong số lần lặp nhỏ (thường < 10)\n- Đảm bảo cải thiện hoặc giữ nguyên chất lượng chính sách\n\n**Nhược điểm**:\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n- Policy Improvement Theorem phát biểu rằng nếu Q-value của hành động được chọn bởi chính sách mới π' tại trạng thái s, dưới chính sách cũ π (tức là Q^π(s, π'(s))), lớn hơn hoặc bằng V-value của trạng thái s dưới chính sách cũ π (tức là V^π(s)), thì chính sách mới π' sẽ tốt hơn hoặc bằng chính sách cũ π. Điều này có nghĩa là hàm giá trị của chính sách mới π' sẽ lớn hơn hoặc bằng hàm giá trị của chính sách cũ π cho mọi trạng thái s.\n\n**Mối quan hệ:**\n- Policy Iteration luôn hội tụ đến chính sách tối ưu π*.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nMột quá trình được gọi là có tính chất Markov nếu:\n```\nP[Sₜ₊₁ | Sₜ] = P[Sₜ₊₁ | S₁, S₂, ..., Sₜ]\n```\n\n#### 3.2. Ý nghĩa\n- Trạng thái hiện tại chứa đầy đủ thông tin để dự đoán tương lai\n- Không cần phải nhớ toàn bộ lịch sử\n- Đơn giản hóa việc tính toán và phân tích\n\n### 4. Chính sách (Policy)\n\n#### 4.1. Định nghĩa\nChính sách π là một hàm ánh xạ từ trạng thái đến hành động:\n- **Chính sách xác định (Deterministic)**: π(s) = a\n- **Chính sách ngẫu nhiên (Stochastic)**: π(a|s) = P[Aₜ = a | Sₜ = s]\n\n#### 4.2. Phân loại\n- **Chính sách tĩnh (Stationary)**: Không thay đổi theo thời gian\n- **Chính sách động (Non-stationary)**: Thay đổi theo thời gian\n\n### 5. Hàm giá trị (Value Functions)\n\n#### 5.1. Hàm giá trị trạng thái (State-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s và tuân theo chính sách π\n\n```\nVᵖ(s) = Eᵖ[Gₜ | Sₜ = s]\n      = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s]\n```\n\n#### 5.2. Hàm giá trị hành động (Action-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s, thực hiện hành động a, và sau đó tuân theo chính sách π\n\n```\nQᵖ(s,a) = Eᵖ[Gₜ | Sₜ = s, Aₜ = a]\n        = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s, Aₜ = a]\n```\n\n#### 5.3. Mối quan hệ giữa V và Q\n```\nVᵖ(s) = Σₐ π(a|s)Qᵖ(s,a)\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')\n```\n\n### 6. Phương trình Bellman\n\n#### 6.1. Phương trình Bellman cho Vᵖ\n```\nVᵖ(s) = Σₐ π(a|s)[R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')]\n```\n\n**Giải thích**:\n- Giá trị của một trạng thái bằng phần thưởng tức thời cộng với giá trị chiết khấu của trạng thái tiếp theo\n- Đây là một phương trình đệ quy\n\n#### 6.2. Phương trình Bellman cho Qᵖ\n```\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) Σₐ' π(a'|s')Qᵖ(s',a')\n```\n\n#### 6.3. Phương trình Bellman tối ưu\n```\n\n**Các khái niệm quan trọng:**\n- Non-stationary Policy (Chính sách động) là một chính sách thay đổi theo thời gian, nghĩa là hành vi của agent có thể điều chỉnh theo các giai đoạn khác nhau của quá trình học.\n- Policy (Chính sách) π là một hàm ánh xạ từ trạng thái đến hành động, định nghĩa hành vi của agent. Chính sách có thể là xác định (deterministic) hoặc ngẫu nhiên (stochastic).\n\n**Mối quan hệ:**\n- Policy có thể là Deterministic Policy, nơi một hành động cụ thể được chọn cho mỗi trạng thái.\n- Policy có thể là Non-stationary Policy, thay đổi theo thời gian.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- π* là ký hiệu cho chính sách tối ưu (optimal policy), là một ánh xạ từ trạng thái sang hành động (hoặc phân phối xác suất trên các hành động) mà khi tuân theo sẽ tối đa hóa tổng phần thưởng chiết khấu kỳ vọng trong tương lai từ mọi trạng thái. Chính sách này đảm bảo rằng Vᵖ*(s) ≥ Vᵖ(s) cho mọi trạng thái s và mọi chính sách π khác. Mục tiêu của hầu hết các thuật toán học tăng cường, bao gồm cả Policy Iteration, là tìm ra chính sách tối ưu này, và nó có thể được tìm thấy từ Q* thông qua π*(s) = argmaxₐ Q*(s,a).\n\n**Mối quan hệ:**\n- Policy Iteration luôn hội tụ đến chính sách tối ưu π*.\n- Policy Iteration xen kẽ giữa đánh giá và cải thiện chính sách, cuối cùng hội tụ đến chính sách tối ưu π*."
        },
        "output_without_context": {
            "question": "Đạo hàm chính sách tự nhiên có thể được đánh giá như thế nào trong việc cải thiện quyết định trong môi trường phức tạp?",
            "answer": "Bằng cách phân tích mối quan hệ giữa quá trình học và kết quả đạt được từ các quyết định."
        }
    }
]