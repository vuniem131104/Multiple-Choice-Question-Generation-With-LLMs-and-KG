[
    {
        "topic_description": "Chủ đề này khám phá sự cân bằng giữa việc khám phá các lựa chọn mới để thu thập thông tin và khai thác các lựa chọn tốt nhất đã biết. Học sinh sẽ được kiểm tra về các định nghĩa cơ bản, ví dụ thực tế, và cách thức quản lý sự bối rối này trong thực tiễn.",
        "output_with_context": {
            "question": "Khám phá và khai thác trong học tăng cường đề cập đến việc cân bằng giữa việc chọn hành động tốt nhất đã biết (exploitation) và thử nghiệm các hành động mới (exploration). Định nghĩa nào sau đây đúng với exploitation?",
            "answer": "Chọn các hành động mà tác nhân tin là tốt nhất để tối đa hóa phần thưởng tức thời.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n\n### 1. Giới thiệu về Exploration-Exploitation Dilemma\n\n#### 1.1. Định nghĩa\n\n**Exploitation**: Chọn actions mà agent tin là tốt nhất (maximize immediate reward)\n**Exploration**: Thử actions mới để discover potentially better options\n\n**Dilemma**: Làm sao cân bằng?\n```\nPure Exploitation: Có thể stuck tại suboptimal policy\nPure Exploration: Không bao giờ maximize rewards\n```\n\n#### 1.2. Tại sao quan trọng?\n\n- **Learning**: Cần explore để learn về environment\n- **Optimization**: Cần exploit để maximize returns\n- **Trade-off**: Exploration cost short-term reward for long-term gain\n\n#### 1.3. Multi-Armed Bandit Problem\n\n**Setup**: K slot machines (arms), mỗi arm có unknown reward distribution\n\n**Goal**: Maximize cumulative reward over time\n\n**Regret**: \n```\nRegret = Σ_t (V* - V(a_t))\nV* = Value của best arm\n```\n\n### 2. Exploration Strategies\n\n#### 2.1. ε-Greedy\n\n**Strategy**:\n```\nVới xác suất ε: Chọn random action\nVới xác suất 1-ε: Chọn best known action\n```\n\n**Variants**:\n```\n# Fixed\nε = 0.1\n\n# Decay\nε_t = ε_0 / t\nε_t = ε_min + (ε_max - ε_min) * exp(-decay * t)\n\n# Adaptive\nTăng ε khi performance kém\nGiảm ε khi performance tốt\n```\n\n**Pros**: Simple, effective\n**Cons**: Random exploration không efficient\n\n#### 2.2. Softmax / Boltzmann Exploration\n\n**Strategy**: Sample proportional to estimated values\n```\nπ(a|s) = exp(Q(s,a)/τ) / Σ_b exp(Q(s,b)/τ)\n```\n\n**Temperature τ**:\n- τ → 0: Greedy (exploitation)\n- τ → ∞: Uniform random (exploration)\n- τ decay over time\n\n**Pros**: Probabilistic, smooth\n**Cons**: Need tune τ\n\n#### 2.3. Optimistic Initialization\n\n**Idea**: Initialize Q-values optimistically (higher than true values)\n```\nQ(s,a) = Q_max  (instead of 0)\n```\n\n**Effect**: Agent naturally explores unvisited (s,a) pairs\n\n**Pros**: Simple, no hyperparameters\n**Cons**: Temporary effect only\n\n### 3. Upper Confidence Bound (UCB)\n\n#### 3.1. UCB Algorithm\n\n**Principle**: \"Optimism in face of uncertainty\"\n\n**UCB Formula**:\n```\nUCB(a) = Q̄(a) + c√(ln t / N(a))\n         ↑ Exploitation  ↑ Exploration\n\nQ̄(a): Average reward của arm a\nN(a): Number of times arm a played\nt: Total time steps\nc: Exploration parameter\n```\n\n**Action Selection**:\n```\na_t = argmax_a UCB(a)\n```\n\n**Properties**:\n- Logarithmic regret: O(log t)\n- Optimal trong bandit setting\n\n#### 3.2. UCB trong RL\n\n**UCB applied to Q-Learning**:\n```\nQ_UCB(s,a) = Q(s,a) + c√(ln N(s) / N(s,a))\n\nSelect: a = argmax_a Q_UCB(s,a)\n```\n\n**Challenges**: Need track visit counts\n\n### 4. Thompson Sampling\n\n#### 4.1. Bayesian Approach\n\n**Idea**: Maintain distribution over Q-values\n\n**Algorithm**:\n```\nInitialize: Prior distribution P(θ)\n\nLặp:\n    Sample θ_t ~ P(θ|history)\n    Select a_t = argmax_a Q(s,a;θ_t)\n    Observe reward r_t\n    Update P(θ|history, r_t) bằng Bayes rule\n```\n\n#### 4.2. Beta-Bernoulli Thompson Sampling\n\n**For binary rewards**:\n```\nPrior: Beta(α_a, β_a) cho mỗi arm a\n\nSample: θ_a ~ Beta(α_a, β_a)\nChoose: a = argmax_a θ_a\n\nUpdate winner:\n    Success: α_a ← α_a + 1\n\n**Các khái niệm quan trọng:**\n- Exploration-Exploitation Dilemma là bài toán cân bằng giữa việc chọn các hành động đã biết là tốt nhất (exploitation) và thử các hành động mới để tìm kiếm các lựa chọn tốt hơn (exploration). Việc cân bằng này rất quan trọng để tác nhân có thể học hỏi hiệu quả và tối đa hóa tổng phần thưởng dài hạn.\n- Exploration là hành động thử các hành động mới để khám phá các lựa chọn tiềm năng tốt hơn trong môi trường. Đây là một phần của sự cân bằng giữa khám phá và khai thác, cần thiết để tác nhân học hỏi về môi trường và tránh mắc kẹt ở các chính sách dưới tối ưu.\n- Exploitation là hành động chọn các hành động mà tác nhân tin là tốt nhất để tối đa hóa phần thưởng tức thời. Đây là một phần của sự cân bằng giữa khám phá và khai thác trong Học tăng cường, tập trung vào việc tận dụng kiến thức hiện có.\n\n**Mối quan hệ:**\n- Upper Confidence Bound (UCB) giải quyết Exploration-Exploitation Dilemma bằng cách cân bằng giữa khai thác và khám phá dựa trên sự không chắc chắn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n    Failure: β_a ← β_a + 1\n```\n\n**Properties**:\n- Optimal regret bounds\n- Often better than UCB trong practice\n\n### 5. Count-Based Exploration\n\n#### 5.1. Exploration Bonuses\n\n**Idea**: Add bonus reward for visiting rare states\n\n**Intrinsic Motivation**:\n```\nr_total = r_extrinsic + β · r_intrinsic\n\nr_intrinsic = 1/√N(s)  hoặc  1/√N(s,a)\n```\n\n**Effect**: Encourage visiting under-explored regions\n\n#### 5.2. Pseudo-Count Methods\n\n**For large state spaces**: Cannot count exactly\n\n**Density Models**:\n```\nρ(s): Estimate density/frequency of state s\nPseudo-count: N(s) ∝ ρ(s) / (1 - ρ(s))\nBonus: r_intrinsic = β/√N(s)\n```\n\n**Examples**:\n- Context Tree Switching (CTS)\n- Neural density models\n\n### 6. Curiosity-Driven Exploration\n\n#### 6.1. Intrinsic Curiosity Module (ICM)\n\n**Components**:\n```\n1. Forward Model: Predict next state feature\n   ŝ_{t+1} = f(s_t, a_t)\n\n2. Inverse Model: Predict action from states\n   â_t = g(s_t, s_{t+1})\n\n3. Intrinsic Reward: Prediction error\n   r_intrinsic = ||ŝ_{t+1} - s_{t+1}||²\n```\n\n**Intuition**: States that are hard to predict are \"interesting\"\n\n#### 6.2. Random Network Distillation (RND)\n\n**Setup**:\n```\nFixed random network: f_target(s)\nLearned predictor: f_pred(s; θ)\n```\n\n**Intrinsic Reward**:\n```\nr_intrinsic = ||f_target(s) - f_pred(s; θ)||²\n```\n\n**Properties**:\n- Novel states have high prediction error\n- Visited states have low error\n- Non-stationary targets avoided\n\n#### 6.3. Never Give Up (NGU)\n\n**Combines**:\n- Episodic novelty (memory-based)\n- Life-long novelty (RND-based)\n\n**Two-timescale curiosity**:\n```\nr_episodic: Within episode novelty\nr_lifelong: Across episodes novelty\n```\n\n### 7. Information-Theoretic Exploration\n\n#### 7.1. Information Gain\n\n**Maximum Information Gain**:\n```\nMaximize: I(Θ; O | a) = H(Θ) - H(Θ|O,a)\nΘ: Parameters/state of world\nO: Observations\n```\n\n**Intuition**: Choose actions that reveal most information\n\n#### 7.2. Entropy Maximization\n\n**Maximum Entropy RL** (covered in SAC):\n```\nπ* = argmax_π E[Σ_t (R_t + α H(π(·|S_t)))]\n```\n\n**Benefits**:\n- Natural exploration\n- Robust policies\n- Multiple solutions\n\n#### 7.3. Empowerment\n\n**Definition**: Mutual information between actions và future states\n```\nEmpowerment = I(A_t; S_{t+k} | S_t)\n```\n\n**Intuition**: Maximize control over future\n\n### 8. Goal-Driven Exploration\n\n#### 8.1. Hindsight Experience Replay (HER)\n\n**Problem**: Sparse rewards → most episodes fail → little learning\n\n**Idea**: Learn from failures by relabeling goals\n```\nOriginal: Goal = g, achieved = g', reward = 0 (failure)\nHER: Goal = g', achieved = g', reward = 0 (success!)\n```\n\n**Algorithm**:\n```\nStore transition (s, a, r, s', g) vào replay buffer\n\nAdditionally store:\n    (s, a, r', s', g') where g' = achieved_goal(s')\n    r' = reward(s', a, g')\n```\n\n**Effect**: Every trajectory teaches something\n\n#### 8.2. Curriculum Learning\n\n**Progressive Difficulty**:\n```\nEasy tasks → Medium tasks → Hard tasks\n```\n\n**Automatic Curriculum**:\n- Track success rates\n- Adjust task distribution\n- Focus on \"frontier\" of capability\n\n### 9. Multi-Agent Exploration\n\n#### 9.1. Population-Based Training\n\n**Idea**: Train population of agents với different hyperparameters\n\n**Process**:\n```\nPopulation: {Agent_1, ..., Agent_N}\n\nPeriodically:\n\n**Các khái niệm quan trọng:**\n- Exploration and Exploitation là một thách thức và bài toán cân bằng cơ bản, trung tâm trong Học tăng cường (RL). Nó đòi hỏi agent phải quyết định giữa việc khám phá các hành động và trạng thái mới để tìm kiếm phần thưởng tiềm năng cao hơn (exploration) và khai thác các hành động đã biết hoặc kiến thức hiện có để tối đa hóa phần thưởng hiện tại hoặc tức thời (exploitation). Đây là một tình thế tiến thoái lưỡng nan không thể tránh khỏi và phải được quản lý hiệu quả để đạt được hiệu suất tối ưu trong RL.\n- Information-Theoretic Exploration là một nhóm các kỹ thuật khám phá trong RL dựa trên lý thuyết thông tin, nhằm mục đích tối đa hóa lượng thông tin thu được về môi trường hoặc tối đa hóa entropy của chính sách để khuyến khích sự đa dạng trong hành vi.\n\n**Mối quan hệ:**\n- Việc quản lý Exploration and Exploitation đòi hỏi sự hiểu biết về State space của môi trường để thiết kế chiến lược khám phá hiệu quả.\n- Information-Theoretic Exploration sử dụng Entropy Maximization để khuyến khích khám phá tự nhiên và tạo chính sách mạnh mẽ.\n- Information-Theoretic Exploration sử dụng Information Gain để chọn hành động tiết lộ nhiều thông tin nhất.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n            nn.Linear(256, action_dim)\n        )\n        \n        # Forward model\n        self.forward = nn.Sequential(\n            nn.Linear(state_dim + action_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, state_dim)\n        )\n    \n    def intrinsic_reward(self, state, action, next_state):\n        # Predict next state\n        pred_next = self.forward(torch.cat([state, action], dim=-1))\n        \n        # Prediction error = intrinsic reward\n        error = F.mse_loss(pred_next, next_state, reduction='none')\n        return error.mean(dim=-1)\n```\n\n### 13. Ứng dụng thực tế\n\n#### 13.1. Robotics\n\n**Exploration challenges**:\n- Safety constraints\n- Real-world sample cost\n- Reset difficulty\n\n**Solutions**:\n- Curiosity in simulation → transfer\n- Safe exploration policies\n- Curriculum learning\n\n#### 13.2. Game Playing\n\n**Montezuma's Revenge**:\n- Extremely sparse rewards\n- Deep exploration needed\n- RND, NGU achieve breakthroughs\n\n**StarCraft II**:\n- Huge action space\n- League training (multi-agent)\n- Diversity for robustness\n\n#### 13.3. Recommendation Systems\n\n**Exploration needed**:\n- Discover user preferences\n- Cold start problem\n- Avoid filter bubbles\n\n**Methods**:\n- Thompson Sampling\n- UCB\n- ε-greedy với contextual information\n\n### 14. Advanced Topics\n\n#### 14.1. Risk-Sensitive Exploration\n\n**Balance**: Exploration vs safety\n\n**CVaR (Conditional Value at Risk)**:\n```\nOptimize worst-case returns\nConstraint: Probability(return < threshold) < α\n```\n\n#### 14.2. Multi-Objective Exploration\n\n**Multiple goals**:\n- Reward maximization\n- State coverage\n- Safety\n- Entropy\n\n**Pareto optimality**: Trade-offs between objectives\n\n#### 14.3. Transfer Learning và Meta-Learning\n\n**Meta-RL**:\n- Learn exploration strategy across tasks\n- Fast adaptation to new tasks\n- MAML, RL²\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement ε-greedy, UCB, Thompson Sampling cho bandits\n2. Compare strategies on gridworld\n3. Visualize exploration trajectories\n\n#### 15.2. Bài tập nâng cao\n1. Implement count-based exploration bonus\n2. Add ICM to PPO agent\n3. HER cho sparse reward task\n\n#### 15.3. Dự án\n1. Curiosity-driven agent cho Montezuma's Revenge\n2. Multi-armed bandit cho ad recommendation\n3. Safe exploration cho robot simulation\n\n### 16. Kết luận\n\nExploration-Exploitation là central challenge trong RL, requiring careful balance và creative solutions.\n\n**Key Takeaways**:\n\n1. **Fundamental Dilemma**: Cannot be avoided, must be managed\n2. **Simple Methods Work**: ε-greedy, UCB often sufficient\n3. **Curiosity Helps**: Intrinsic motivation powerful cho hard exploration\n4. **Context Matters**: Different strategies cho different problems\n5. **Active Research Area**: New methods constantly emerging\n\n**Practical Recommendations**:\n\n**Start Simple**:\n- ε-greedy với decay\n- Tune epsilon schedule\n\n**If stuck**:\n- Add intrinsic rewards\n- Try count-based bonuses\n- Consider curriculum\n\n**For sparse rewards**: \n- HER essential\n- Curiosity-driven exploration\n- Shaped rewards (carefully!)\n\n**Monitor**:\n- Coverage metrics\n- Exploration vs exploitation ratio\n- Learning curves\n\n**Evolution of Exploration**:\n```\nRandom → ε-greedy → UCB/Thompson Sampling\n         ↓\n    Curiosity → Information Theory\n         ↓\n    Multi-Agent → Meta-Learning\n```\n\n**Final Thoughts**:\n\nExploration là nghệ thuật và khoa học. Không có one-size-fits-all solution. Understand your problem:\n- Reward structure (dense/sparse)\n- State space (size, structure)\n- Safety requirements\n- Sample efficiency needs\n\nExperiment với different methods và monitor carefully. Good exploration strategy có thể là difference giữa success và failure trong RL!\n\n**Các khái niệm quan trọng:**\n- Exploration vs exploitation ratio là một chỉ số đo lường sự cân bằng giữa việc khám phá các hành động mới và khai thác các hành động đã biết của agent. Việc theo dõi tỷ lệ này theo thời gian giúp đánh giá xem chiến lược khám phá có đang hoạt động như mong đợi hay không, ví dụ, liệu agent có đang chuyển từ khám phá sang khai thác một cách hợp lý hay không.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nPhần tiếp theo sẽ mở rộng sang **Model-Free Control**: Không chỉ đánh giá mà còn tìm chính sách tối ưu mà không cần mô hình!\n\n---\n\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Control\n\n#### 1.1. Định nghĩa bài toán\n**Control Problem**: Tìm chính sách tối ưu π* mà không biết trước mô hình môi trường (P, R)\n\n**So sánh với Prediction**:\n- Prediction: Đánh giá V^π cho π cho trước\n- Control: Tối ưu hóa π để maximize V^π\n\n#### 1.2. Thách thức\n- Không biết mô hình → không thể dùng Dynamic Programming\n- Phải học từ interaction với môi trường\n- Cần cân bằng exploration và exploitation\n\n#### 1.3. Ý tưởng chính\nSử dụng **Generalized Policy Iteration (GPI)** framework:\n```\nPolicy Evaluation (Model-Free) → Policy Improvement → Repeat\n```\n\n### 2. Monte Carlo Control\n\n#### 2.1. Từ V(s) sang Q(s,a)\n\n**Vấn đề với V(s)**:\n```\nPolicy Improvement cần:\nπ'(s) = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n                 ↑ Cần biết mô hình!\n```\n\n**Giải pháp**: Sử dụng Q(s,a)\n```\nπ'(s) = argmax_a Q(s,a)  ← Không cần mô hình!\n```\n\n#### 2.2. Monte Carlo Policy Iteration\n\n**Thuật toán**:\n```\n1. Khởi tạo:\n   Q(s,a) = 0, ∀s,a\n   π = chính sách khởi tạo\n\n2. Lặp:\n   a) Policy Evaluation (MC):\n      - Tạo nhiều episodes theo π\n      - Cập nhật Q^π(s,a) bằng MC\n   \n   b) Policy Improvement:\n      π(s) = argmax_a Q(s,a), ∀s\n```\n\n#### 2.3. Vấn đề Exploration\n\n**Greedy Policy**:\n```\nπ(s) = argmax_a Q(s,a)\n```\n→ Chỉ exploit, không explore → Có thể bỏ lỡ chính sách tốt hơn\n\n**Giải pháp 1: ε-Greedy Policy**\n```\nπ(a|s) = {\n    1 - ε + ε/|A|,  nếu a = argmax Q(s,a)\n    ε/|A|,          ngược lại\n}\n```\n\n**Đặc điểm**:\n- Xác suất 1-ε: Chọn hành động tốt nhất (exploit)\n- Xác suất ε: Chọn ngẫu nhiên (explore)\n- ε decay theo thời gian: Explore nhiều lúc đầu, exploit nhiều sau\n\n#### 2.4. ε-Greedy Monte Carlo Control\n\n**Thuật toán**:\n```\nKhởi tạo:\n    Q(s,a) arbitrarily, ∀s,a\n    π = ε-greedy policy dựa trên Q\n    Returns(s,a) = empty list, ∀s,a\n\nLặp forever:\n    1. Tạo episode theo π:\n       S_0, A_0, R_1, ..., S_T\n    \n    2. Với mỗi cặp (s,a) xuất hiện trong episode:\n       G = return sau lần xuất hiện đầu tiên\n       Thêm G vào Returns(s,a)\n       Q(s,a) = average(Returns(s,a))\n    \n    3. Với mỗi s trong episode:\n       π(s) = ε-greedy(Q(s,·))\n```\n\n#### 2.5. Greedy in the Limit of Infinite Exploration (GLIE)\n\n\n**Các khái niệm quan trọng:**\n- \"Exploration vs Exploitation (hay Exploration-Exploitation Tradeoff) là một bài toán cơ bản trong Reinforcement Learning (Học tăng cường), liên quan đến việc cân bằng giữa việc khám phá các hành động mới để tìm ra những phần thưởng tiềm năng tốt hơn (exploration) và việc khai thác các hành động đã biết là tốt nhất dựa trên kiến thức hiện tại để tối đa hóa phần thưởng (exploitation). Kỹ thuật ε-greedy là một giải pháp đơn giản và hiệu quả cho bài toán này, trong đó tham số ε (exploration rate) được sử dụng để kiểm soát sự cân bằng này.\"\n- Exploration vs Exploitation là một thách thức cơ bản trong Reinforcement Learning, nơi agent phải cân bằng giữa việc khám phá các hành động và trạng thái mới (exploration) để tìm kiếm phần thưởng tiềm năng cao hơn, và việc khai thác kiến thức hiện có (exploitation) để chọn hành động tốt nhất dựa trên thông tin đã học.\n\n**Mối quan hệ:**\n- Tham số ε cân bằng giữa exploration và exploitation trong quá trình chọn action.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n    Evaluate all agents\n    Replace worst với copies of best\n    Mutate hyperparameters\n```\n\n**Benefits**: Automatic hyperparameter tuning + diversity\n\n#### 9.2. Quality Diversity (QD)\n\n**Goal**: Find diverse set of good solutions\n\n**MAP-Elites**:\n```\nGrid of cells, mỗi cell = behavior niche\nStore best solution in mỗi cell\nExplore để fill all cells\n```\n\n**Applications**: Robotics, game playing\n\n### 10. Exploration trong Deep RL\n\n#### 10.1. Noisy Networks\n\n**Idea**: Add noise to network weights\n\n**NoisyNet**:\n```\ny = (μ^w + σ^w ⊙ ε^w) x + (μ^b + σ^b ⊙ ε^b)\n\nμ, σ: Learned parameters\nε: Random noise\n```\n\n**Benefits**:\n- Automatic exploration\n- State-dependent noise\n- No epsilon scheduling\n\n#### 10.2. Parameter Space Noise\n\n**Add noise to policy parameters**:\n```\nθ_noisy = θ + N(0, σ²I)\n```\n\n**Adaptive noise**:\n```\nAdjust σ based on action space distance\n```\n\n#### 10.3. Bootstrapped DQN\n\n**Multiple heads** cho uncertainty:\n```\nQ_1(s,a), Q_2(s,a), ..., Q_K(s,a)\n\nSample head k uniformly\nUse Q_k for exploration\n```\n\n**Effect**: Deep exploration (multi-step exploration)\n\n### 11. Practical Guidelines\n\n#### 11.1. Choosing Exploration Strategy\n\n**Simple tasks**:\n- ε-greedy với decay\n- Optimistic initialization\n\n**Complex tasks**:\n- UCB / Thompson Sampling\n- Curiosity-driven\n\n**Sparse rewards**:\n- HER\n- Intrinsic motivation\n- Count-based\n\n**Continuous control**:\n- Gaussian noise\n- Parameter space noise\n- NoisyNet\n\n#### 11.2. Debugging Exploration\n\n**Metrics to track**:\n```\n- State/action coverage\n- Entropy của policy\n- Intrinsic rewards\n- Episode diversity\n```\n\n**Signs of poor exploration**:\n- Low state coverage\n- Policy becomes deterministic too early\n- Training plateaus early\n\n#### 11.3. Hyperparameter Tuning\n\n**ε-greedy**:\n- Start: 1.0\n- End: 0.01-0.1\n- Decay: Linear hoặc exponential\n\n**Curiosity coefficient β**:\n- Start small: 0.01-0.1\n- Tune based on reward scale\n\n**UCB constant c**:\n- Typical: 1.0-2.0\n- Higher = more exploration\n\n### 12. Code Examples\n\n#### 12.1. ε-Greedy với Decay\n```python\nclass EpsilonGreedy:\n    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, \n                 epsilon_decay=0.995):\n        self.epsilon = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.epsilon_decay = epsilon_decay\n    \n    def select_action(self, q_values):\n        if np.random.random() < self.epsilon:\n            return np.random.randint(len(q_values))\n        else:\n            return np.argmax(q_values)\n    \n    def decay(self):\n        self.epsilon = max(self.epsilon_end, \n                          self.epsilon * self.epsilon_decay)\n```\n\n#### 12.2. UCB\n```python\nclass UCB:\n    def __init__(self, num_actions, c=2.0):\n        self.Q = np.zeros(num_actions)\n        self.N = np.zeros(num_actions)\n        self.c = c\n        self.t = 0\n    \n    def select_action(self):\n        self.t += 1\n        \n        # Avoid division by zero\n        ucb_values = np.where(\n            self.N > 0,\n            self.Q + self.c * np.sqrt(np.log(self.t) / self.N),\n            np.inf\n        )\n        \n        return np.argmax(ucb_values)\n    \n    def update(self, action, reward):\n        self.N[action] += 1\n        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n```\n\n#### 12.3. Curiosity-Driven (Simplified)\n```python\nclass CuriosityModule(nn.Module):\n    def __init__(self, state_dim, action_dim, feature_dim=128):\n        super().__init__()\n        # Inverse model\n        self.inverse = nn.Sequential(\n            nn.Linear(state_dim * 2, 256),\n            nn.ReLU(),\n\n**Các khái niệm quan trọng:**\n- Exploration and Exploitation là một thách thức và bài toán cân bằng cơ bản, trung tâm trong Học tăng cường (RL). Nó đòi hỏi agent phải quyết định giữa việc khám phá các hành động và trạng thái mới để tìm kiếm phần thưởng tiềm năng cao hơn (exploration) và khai thác các hành động đã biết hoặc kiến thức hiện có để tối đa hóa phần thưởng hiện tại hoặc tức thời (exploitation). Đây là một tình thế tiến thoái lưỡng nan không thể tránh khỏi và phải được quản lý hiệu quả để đạt được hiệu suất tối ưu trong RL.\n\n**Mối quan hệ:**\n- Việc quản lý Exploration and Exploitation đòi hỏi sự hiểu biết về State space của môi trường để thiết kế chiến lược khám phá hiệu quả."
        },
        "output_without_context": {
            "question": "Khám phá và khai thác là hai khái niệm quan trọng trong quản lý quyết định. Khám phá chủ yếu liên quan đến việc:",
            "answer": "Tìm kiếm các lựa chọn mới để thu thập thông tin."
        }
    },
    {
        "topic_description": "Chủ đề này tập trung vào các nguyên tắc nhìn nhận của Khám Phá và Khai Thác, như khởi tạo lạc quan và sự không chắc chắn. Sinh viên sẽ phải hiểu và ứng dụng các nguyên tắc này trong các bài toán ra quyết định.",
        "output_with_context": {
            "question": "Trong bối cảnh của Exploration-Exploitation Dilemma, chiến lược nào được sử dụng để cân bằng giữa việc chọn hành động tốt nhất đã biết và thử nghiệm các hành động mới?",
            "answer": "Chiến lược ε-greedy.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n\n### 1. Giới thiệu về Exploration-Exploitation Dilemma\n\n#### 1.1. Định nghĩa\n\n**Exploitation**: Chọn actions mà agent tin là tốt nhất (maximize immediate reward)\n**Exploration**: Thử actions mới để discover potentially better options\n\n**Dilemma**: Làm sao cân bằng?\n```\nPure Exploitation: Có thể stuck tại suboptimal policy\nPure Exploration: Không bao giờ maximize rewards\n```\n\n#### 1.2. Tại sao quan trọng?\n\n- **Learning**: Cần explore để learn về environment\n- **Optimization**: Cần exploit để maximize returns\n- **Trade-off**: Exploration cost short-term reward for long-term gain\n\n#### 1.3. Multi-Armed Bandit Problem\n\n**Setup**: K slot machines (arms), mỗi arm có unknown reward distribution\n\n**Goal**: Maximize cumulative reward over time\n\n**Regret**: \n```\nRegret = Σ_t (V* - V(a_t))\nV* = Value của best arm\n```\n\n### 2. Exploration Strategies\n\n#### 2.1. ε-Greedy\n\n**Strategy**:\n```\nVới xác suất ε: Chọn random action\nVới xác suất 1-ε: Chọn best known action\n```\n\n**Variants**:\n```\n# Fixed\nε = 0.1\n\n# Decay\nε_t = ε_0 / t\nε_t = ε_min + (ε_max - ε_min) * exp(-decay * t)\n\n# Adaptive\nTăng ε khi performance kém\nGiảm ε khi performance tốt\n```\n\n**Pros**: Simple, effective\n**Cons**: Random exploration không efficient\n\n#### 2.2. Softmax / Boltzmann Exploration\n\n**Strategy**: Sample proportional to estimated values\n```\nπ(a|s) = exp(Q(s,a)/τ) / Σ_b exp(Q(s,b)/τ)\n```\n\n**Temperature τ**:\n- τ → 0: Greedy (exploitation)\n- τ → ∞: Uniform random (exploration)\n- τ decay over time\n\n**Pros**: Probabilistic, smooth\n**Cons**: Need tune τ\n\n#### 2.3. Optimistic Initialization\n\n**Idea**: Initialize Q-values optimistically (higher than true values)\n```\nQ(s,a) = Q_max  (instead of 0)\n```\n\n**Effect**: Agent naturally explores unvisited (s,a) pairs\n\n**Pros**: Simple, no hyperparameters\n**Cons**: Temporary effect only\n\n### 3. Upper Confidence Bound (UCB)\n\n#### 3.1. UCB Algorithm\n\n**Principle**: \"Optimism in face of uncertainty\"\n\n**UCB Formula**:\n```\nUCB(a) = Q̄(a) + c√(ln t / N(a))\n         ↑ Exploitation  ↑ Exploration\n\nQ̄(a): Average reward của arm a\nN(a): Number of times arm a played\nt: Total time steps\nc: Exploration parameter\n```\n\n**Action Selection**:\n```\na_t = argmax_a UCB(a)\n```\n\n**Properties**:\n- Logarithmic regret: O(log t)\n- Optimal trong bandit setting\n\n#### 3.2. UCB trong RL\n\n**UCB applied to Q-Learning**:\n```\nQ_UCB(s,a) = Q(s,a) + c√(ln N(s) / N(s,a))\n\nSelect: a = argmax_a Q_UCB(s,a)\n```\n\n**Challenges**: Need track visit counts\n\n### 4. Thompson Sampling\n\n#### 4.1. Bayesian Approach\n\n**Idea**: Maintain distribution over Q-values\n\n**Algorithm**:\n```\nInitialize: Prior distribution P(θ)\n\nLặp:\n    Sample θ_t ~ P(θ|history)\n    Select a_t = argmax_a Q(s,a;θ_t)\n    Observe reward r_t\n    Update P(θ|history, r_t) bằng Bayes rule\n```\n\n#### 4.2. Beta-Bernoulli Thompson Sampling\n\n**For binary rewards**:\n```\nPrior: Beta(α_a, β_a) cho mỗi arm a\n\nSample: θ_a ~ Beta(α_a, β_a)\nChoose: a = argmax_a θ_a\n\nUpdate winner:\n    Success: α_a ← α_a + 1\n\n**Các khái niệm quan trọng:**\n- Exploration-Exploitation Dilemma là bài toán cân bằng giữa việc chọn các hành động đã biết là tốt nhất (exploitation) và thử các hành động mới để tìm kiếm các lựa chọn tốt hơn (exploration). Việc cân bằng này rất quan trọng để tác nhân có thể học hỏi hiệu quả và tối đa hóa tổng phần thưởng dài hạn.\n- Exploration là hành động thử các hành động mới để khám phá các lựa chọn tiềm năng tốt hơn trong môi trường. Đây là một phần của sự cân bằng giữa khám phá và khai thác, cần thiết để tác nhân học hỏi về môi trường và tránh mắc kẹt ở các chính sách dưới tối ưu.\n\n**Mối quan hệ:**\n- Upper Confidence Bound (UCB) giải quyết Exploration-Exploitation Dilemma bằng cách cân bằng giữa khai thác và khám phá dựa trên sự không chắc chắn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n    Evaluate all agents\n    Replace worst với copies of best\n    Mutate hyperparameters\n```\n\n**Benefits**: Automatic hyperparameter tuning + diversity\n\n#### 9.2. Quality Diversity (QD)\n\n**Goal**: Find diverse set of good solutions\n\n**MAP-Elites**:\n```\nGrid of cells, mỗi cell = behavior niche\nStore best solution in mỗi cell\nExplore để fill all cells\n```\n\n**Applications**: Robotics, game playing\n\n### 10. Exploration trong Deep RL\n\n#### 10.1. Noisy Networks\n\n**Idea**: Add noise to network weights\n\n**NoisyNet**:\n```\ny = (μ^w + σ^w ⊙ ε^w) x + (μ^b + σ^b ⊙ ε^b)\n\nμ, σ: Learned parameters\nε: Random noise\n```\n\n**Benefits**:\n- Automatic exploration\n- State-dependent noise\n- No epsilon scheduling\n\n#### 10.2. Parameter Space Noise\n\n**Add noise to policy parameters**:\n```\nθ_noisy = θ + N(0, σ²I)\n```\n\n**Adaptive noise**:\n```\nAdjust σ based on action space distance\n```\n\n#### 10.3. Bootstrapped DQN\n\n**Multiple heads** cho uncertainty:\n```\nQ_1(s,a), Q_2(s,a), ..., Q_K(s,a)\n\nSample head k uniformly\nUse Q_k for exploration\n```\n\n**Effect**: Deep exploration (multi-step exploration)\n\n### 11. Practical Guidelines\n\n#### 11.1. Choosing Exploration Strategy\n\n**Simple tasks**:\n- ε-greedy với decay\n- Optimistic initialization\n\n**Complex tasks**:\n- UCB / Thompson Sampling\n- Curiosity-driven\n\n**Sparse rewards**:\n- HER\n- Intrinsic motivation\n- Count-based\n\n**Continuous control**:\n- Gaussian noise\n- Parameter space noise\n- NoisyNet\n\n#### 11.2. Debugging Exploration\n\n**Metrics to track**:\n```\n- State/action coverage\n- Entropy của policy\n- Intrinsic rewards\n- Episode diversity\n```\n\n**Signs of poor exploration**:\n- Low state coverage\n- Policy becomes deterministic too early\n- Training plateaus early\n\n#### 11.3. Hyperparameter Tuning\n\n**ε-greedy**:\n- Start: 1.0\n- End: 0.01-0.1\n- Decay: Linear hoặc exponential\n\n**Curiosity coefficient β**:\n- Start small: 0.01-0.1\n- Tune based on reward scale\n\n**UCB constant c**:\n- Typical: 1.0-2.0\n- Higher = more exploration\n\n### 12. Code Examples\n\n#### 12.1. ε-Greedy với Decay\n```python\nclass EpsilonGreedy:\n    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, \n                 epsilon_decay=0.995):\n        self.epsilon = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.epsilon_decay = epsilon_decay\n    \n    def select_action(self, q_values):\n        if np.random.random() < self.epsilon:\n            return np.random.randint(len(q_values))\n        else:\n            return np.argmax(q_values)\n    \n    def decay(self):\n        self.epsilon = max(self.epsilon_end, \n                          self.epsilon * self.epsilon_decay)\n```\n\n#### 12.2. UCB\n```python\nclass UCB:\n    def __init__(self, num_actions, c=2.0):\n        self.Q = np.zeros(num_actions)\n        self.N = np.zeros(num_actions)\n        self.c = c\n        self.t = 0\n    \n    def select_action(self):\n        self.t += 1\n        \n        # Avoid division by zero\n        ucb_values = np.where(\n            self.N > 0,\n            self.Q + self.c * np.sqrt(np.log(self.t) / self.N),\n            np.inf\n        )\n        \n        return np.argmax(ucb_values)\n    \n    def update(self, action, reward):\n        self.N[action] += 1\n        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n```\n\n#### 12.3. Curiosity-Driven (Simplified)\n```python\nclass CuriosityModule(nn.Module):\n    def __init__(self, state_dim, action_dim, feature_dim=128):\n        super().__init__()\n        # Inverse model\n        self.inverse = nn.Sequential(\n            nn.Linear(state_dim * 2, 256),\n            nn.ReLU(),\n\n**Các khái niệm quan trọng:**\n- Exploration and Exploitation là một thách thức và bài toán cân bằng cơ bản, trung tâm trong Học tăng cường (RL). Nó đòi hỏi agent phải quyết định giữa việc khám phá các hành động và trạng thái mới để tìm kiếm phần thưởng tiềm năng cao hơn (exploration) và khai thác các hành động đã biết hoặc kiến thức hiện có để tối đa hóa phần thưởng hiện tại hoặc tức thời (exploitation). Đây là một tình thế tiến thoái lưỡng nan không thể tránh khỏi và phải được quản lý hiệu quả để đạt được hiệu suất tối ưu trong RL.\n\n**Mối quan hệ:**\n- Việc quản lý Exploration and Exploitation đòi hỏi sự hiểu biết về State space của môi trường để thiết kế chiến lược khám phá hiệu quả.\n- Việc quản lý Exploration and Exploitation cần xem xét Safety requirements để đảm bảo agent hoạt động an toàn trong quá trình khám phá.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n            nn.Linear(256, action_dim)\n        )\n        \n        # Forward model\n        self.forward = nn.Sequential(\n            nn.Linear(state_dim + action_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, state_dim)\n        )\n    \n    def intrinsic_reward(self, state, action, next_state):\n        # Predict next state\n        pred_next = self.forward(torch.cat([state, action], dim=-1))\n        \n        # Prediction error = intrinsic reward\n        error = F.mse_loss(pred_next, next_state, reduction='none')\n        return error.mean(dim=-1)\n```\n\n### 13. Ứng dụng thực tế\n\n#### 13.1. Robotics\n\n**Exploration challenges**:\n- Safety constraints\n- Real-world sample cost\n- Reset difficulty\n\n**Solutions**:\n- Curiosity in simulation → transfer\n- Safe exploration policies\n- Curriculum learning\n\n#### 13.2. Game Playing\n\n**Montezuma's Revenge**:\n- Extremely sparse rewards\n- Deep exploration needed\n- RND, NGU achieve breakthroughs\n\n**StarCraft II**:\n- Huge action space\n- League training (multi-agent)\n- Diversity for robustness\n\n#### 13.3. Recommendation Systems\n\n**Exploration needed**:\n- Discover user preferences\n- Cold start problem\n- Avoid filter bubbles\n\n**Methods**:\n- Thompson Sampling\n- UCB\n- ε-greedy với contextual information\n\n### 14. Advanced Topics\n\n#### 14.1. Risk-Sensitive Exploration\n\n**Balance**: Exploration vs safety\n\n**CVaR (Conditional Value at Risk)**:\n```\nOptimize worst-case returns\nConstraint: Probability(return < threshold) < α\n```\n\n#### 14.2. Multi-Objective Exploration\n\n**Multiple goals**:\n- Reward maximization\n- State coverage\n- Safety\n- Entropy\n\n**Pareto optimality**: Trade-offs between objectives\n\n#### 14.3. Transfer Learning và Meta-Learning\n\n**Meta-RL**:\n- Learn exploration strategy across tasks\n- Fast adaptation to new tasks\n- MAML, RL²\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement ε-greedy, UCB, Thompson Sampling cho bandits\n2. Compare strategies on gridworld\n3. Visualize exploration trajectories\n\n#### 15.2. Bài tập nâng cao\n1. Implement count-based exploration bonus\n2. Add ICM to PPO agent\n3. HER cho sparse reward task\n\n#### 15.3. Dự án\n1. Curiosity-driven agent cho Montezuma's Revenge\n2. Multi-armed bandit cho ad recommendation\n3. Safe exploration cho robot simulation\n\n### 16. Kết luận\n\nExploration-Exploitation là central challenge trong RL, requiring careful balance và creative solutions.\n\n**Key Takeaways**:\n\n1. **Fundamental Dilemma**: Cannot be avoided, must be managed\n2. **Simple Methods Work**: ε-greedy, UCB often sufficient\n3. **Curiosity Helps**: Intrinsic motivation powerful cho hard exploration\n4. **Context Matters**: Different strategies cho different problems\n5. **Active Research Area**: New methods constantly emerging\n\n**Practical Recommendations**:\n\n**Start Simple**:\n- ε-greedy với decay\n- Tune epsilon schedule\n\n**If stuck**:\n- Add intrinsic rewards\n- Try count-based bonuses\n- Consider curriculum\n\n**For sparse rewards**: \n- HER essential\n- Curiosity-driven exploration\n- Shaped rewards (carefully!)\n\n**Monitor**:\n- Coverage metrics\n- Exploration vs exploitation ratio\n- Learning curves\n\n**Evolution of Exploration**:\n```\nRandom → ε-greedy → UCB/Thompson Sampling\n         ↓\n    Curiosity → Information Theory\n         ↓\n    Multi-Agent → Meta-Learning\n```\n\n**Final Thoughts**:\n\nExploration là nghệ thuật và khoa học. Không có one-size-fits-all solution. Understand your problem:\n- Reward structure (dense/sparse)\n- State space (size, structure)\n- Safety requirements\n- Sample efficiency needs\n\nExperiment với different methods và monitor carefully. Good exploration strategy có thể là difference giữa success và failure trong RL!\n\n**Các khái niệm quan trọng:**\n- Exploration and Exploitation là một thách thức và bài toán cân bằng cơ bản, trung tâm trong Học tăng cường (RL). Nó đòi hỏi agent phải quyết định giữa việc khám phá các hành động và trạng thái mới để tìm kiếm phần thưởng tiềm năng cao hơn (exploration) và khai thác các hành động đã biết hoặc kiến thức hiện có để tối đa hóa phần thưởng hiện tại hoặc tức thời (exploitation). Đây là một tình thế tiến thoái lưỡng nan không thể tránh khỏi và phải được quản lý hiệu quả để đạt được hiệu suất tối ưu trong RL.\n\n**Mối quan hệ:**\n- Việc quản lý Exploration and Exploitation đòi hỏi sự hiểu biết về State space của môi trường để thiết kế chiến lược khám phá hiệu quả.\n- Việc quản lý Exploration and Exploitation cần xem xét Safety requirements để đảm bảo agent hoạt động an toàn trong quá trình khám phá.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n    Failure: β_a ← β_a + 1\n```\n\n**Properties**:\n- Optimal regret bounds\n- Often better than UCB trong practice\n\n### 5. Count-Based Exploration\n\n#### 5.1. Exploration Bonuses\n\n**Idea**: Add bonus reward for visiting rare states\n\n**Intrinsic Motivation**:\n```\nr_total = r_extrinsic + β · r_intrinsic\n\nr_intrinsic = 1/√N(s)  hoặc  1/√N(s,a)\n```\n\n**Effect**: Encourage visiting under-explored regions\n\n#### 5.2. Pseudo-Count Methods\n\n**For large state spaces**: Cannot count exactly\n\n**Density Models**:\n```\nρ(s): Estimate density/frequency of state s\nPseudo-count: N(s) ∝ ρ(s) / (1 - ρ(s))\nBonus: r_intrinsic = β/√N(s)\n```\n\n**Examples**:\n- Context Tree Switching (CTS)\n- Neural density models\n\n### 6. Curiosity-Driven Exploration\n\n#### 6.1. Intrinsic Curiosity Module (ICM)\n\n**Components**:\n```\n1. Forward Model: Predict next state feature\n   ŝ_{t+1} = f(s_t, a_t)\n\n2. Inverse Model: Predict action from states\n   â_t = g(s_t, s_{t+1})\n\n3. Intrinsic Reward: Prediction error\n   r_intrinsic = ||ŝ_{t+1} - s_{t+1}||²\n```\n\n**Intuition**: States that are hard to predict are \"interesting\"\n\n#### 6.2. Random Network Distillation (RND)\n\n**Setup**:\n```\nFixed random network: f_target(s)\nLearned predictor: f_pred(s; θ)\n```\n\n**Intrinsic Reward**:\n```\nr_intrinsic = ||f_target(s) - f_pred(s; θ)||²\n```\n\n**Properties**:\n- Novel states have high prediction error\n- Visited states have low error\n- Non-stationary targets avoided\n\n#### 6.3. Never Give Up (NGU)\n\n**Combines**:\n- Episodic novelty (memory-based)\n- Life-long novelty (RND-based)\n\n**Two-timescale curiosity**:\n```\nr_episodic: Within episode novelty\nr_lifelong: Across episodes novelty\n```\n\n### 7. Information-Theoretic Exploration\n\n#### 7.1. Information Gain\n\n**Maximum Information Gain**:\n```\nMaximize: I(Θ; O | a) = H(Θ) - H(Θ|O,a)\nΘ: Parameters/state of world\nO: Observations\n```\n\n**Intuition**: Choose actions that reveal most information\n\n#### 7.2. Entropy Maximization\n\n**Maximum Entropy RL** (covered in SAC):\n```\nπ* = argmax_π E[Σ_t (R_t + α H(π(·|S_t)))]\n```\n\n**Benefits**:\n- Natural exploration\n- Robust policies\n- Multiple solutions\n\n#### 7.3. Empowerment\n\n**Definition**: Mutual information between actions và future states\n```\nEmpowerment = I(A_t; S_{t+k} | S_t)\n```\n\n**Intuition**: Maximize control over future\n\n### 8. Goal-Driven Exploration\n\n#### 8.1. Hindsight Experience Replay (HER)\n\n**Problem**: Sparse rewards → most episodes fail → little learning\n\n**Idea**: Learn from failures by relabeling goals\n```\nOriginal: Goal = g, achieved = g', reward = 0 (failure)\nHER: Goal = g', achieved = g', reward = 0 (success!)\n```\n\n**Algorithm**:\n```\nStore transition (s, a, r, s', g) vào replay buffer\n\nAdditionally store:\n    (s, a, r', s', g') where g' = achieved_goal(s')\n    r' = reward(s', a, g')\n```\n\n**Effect**: Every trajectory teaches something\n\n#### 8.2. Curriculum Learning\n\n**Progressive Difficulty**:\n```\nEasy tasks → Medium tasks → Hard tasks\n```\n\n**Automatic Curriculum**:\n- Track success rates\n- Adjust task distribution\n- Focus on \"frontier\" of capability\n\n### 9. Multi-Agent Exploration\n\n#### 9.1. Population-Based Training\n\n**Idea**: Train population of agents với different hyperparameters\n\n**Process**:\n```\nPopulation: {Agent_1, ..., Agent_N}\n\nPeriodically:\n\n**Các khái niệm quan trọng:**\n- Exploration and Exploitation là một thách thức và bài toán cân bằng cơ bản, trung tâm trong Học tăng cường (RL). Nó đòi hỏi agent phải quyết định giữa việc khám phá các hành động và trạng thái mới để tìm kiếm phần thưởng tiềm năng cao hơn (exploration) và khai thác các hành động đã biết hoặc kiến thức hiện có để tối đa hóa phần thưởng hiện tại hoặc tức thời (exploitation). Đây là một tình thế tiến thoái lưỡng nan không thể tránh khỏi và phải được quản lý hiệu quả để đạt được hiệu suất tối ưu trong RL.\n\n**Mối quan hệ:**\n- Việc quản lý Exploration and Exploitation đòi hỏi sự hiểu biết về State space của môi trường để thiết kế chiến lược khám phá hiệu quả.\n- Việc quản lý Exploration and Exploitation cần xem xét Safety requirements để đảm bảo agent hoạt động an toàn trong quá trình khám phá.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nPhần tiếp theo sẽ mở rộng sang **Model-Free Control**: Không chỉ đánh giá mà còn tìm chính sách tối ưu mà không cần mô hình!\n\n---\n\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Control\n\n#### 1.1. Định nghĩa bài toán\n**Control Problem**: Tìm chính sách tối ưu π* mà không biết trước mô hình môi trường (P, R)\n\n**So sánh với Prediction**:\n- Prediction: Đánh giá V^π cho π cho trước\n- Control: Tối ưu hóa π để maximize V^π\n\n#### 1.2. Thách thức\n- Không biết mô hình → không thể dùng Dynamic Programming\n- Phải học từ interaction với môi trường\n- Cần cân bằng exploration và exploitation\n\n#### 1.3. Ý tưởng chính\nSử dụng **Generalized Policy Iteration (GPI)** framework:\n```\nPolicy Evaluation (Model-Free) → Policy Improvement → Repeat\n```\n\n### 2. Monte Carlo Control\n\n#### 2.1. Từ V(s) sang Q(s,a)\n\n**Vấn đề với V(s)**:\n```\nPolicy Improvement cần:\nπ'(s) = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n                 ↑ Cần biết mô hình!\n```\n\n**Giải pháp**: Sử dụng Q(s,a)\n```\nπ'(s) = argmax_a Q(s,a)  ← Không cần mô hình!\n```\n\n#### 2.2. Monte Carlo Policy Iteration\n\n**Thuật toán**:\n```\n1. Khởi tạo:\n   Q(s,a) = 0, ∀s,a\n   π = chính sách khởi tạo\n\n2. Lặp:\n   a) Policy Evaluation (MC):\n      - Tạo nhiều episodes theo π\n      - Cập nhật Q^π(s,a) bằng MC\n   \n   b) Policy Improvement:\n      π(s) = argmax_a Q(s,a), ∀s\n```\n\n#### 2.3. Vấn đề Exploration\n\n**Greedy Policy**:\n```\nπ(s) = argmax_a Q(s,a)\n```\n→ Chỉ exploit, không explore → Có thể bỏ lỡ chính sách tốt hơn\n\n**Giải pháp 1: ε-Greedy Policy**\n```\nπ(a|s) = {\n    1 - ε + ε/|A|,  nếu a = argmax Q(s,a)\n    ε/|A|,          ngược lại\n}\n```\n\n**Đặc điểm**:\n- Xác suất 1-ε: Chọn hành động tốt nhất (exploit)\n- Xác suất ε: Chọn ngẫu nhiên (explore)\n- ε decay theo thời gian: Explore nhiều lúc đầu, exploit nhiều sau\n\n#### 2.4. ε-Greedy Monte Carlo Control\n\n**Thuật toán**:\n```\nKhởi tạo:\n    Q(s,a) arbitrarily, ∀s,a\n    π = ε-greedy policy dựa trên Q\n    Returns(s,a) = empty list, ∀s,a\n\nLặp forever:\n    1. Tạo episode theo π:\n       S_0, A_0, R_1, ..., S_T\n    \n    2. Với mỗi cặp (s,a) xuất hiện trong episode:\n       G = return sau lần xuất hiện đầu tiên\n       Thêm G vào Returns(s,a)\n       Q(s,a) = average(Returns(s,a))\n    \n    3. Với mỗi s trong episode:\n       π(s) = ε-greedy(Q(s,·))\n```\n\n#### 2.5. Greedy in the Limit of Infinite Exploration (GLIE)\n\n\n**Các khái niệm quan trọng:**\n- \"Exploration vs Exploitation (hay Exploration-Exploitation Tradeoff) là một bài toán cơ bản trong Reinforcement Learning (Học tăng cường), liên quan đến việc cân bằng giữa việc khám phá các hành động mới để tìm ra những phần thưởng tiềm năng tốt hơn (exploration) và việc khai thác các hành động đã biết là tốt nhất dựa trên kiến thức hiện tại để tối đa hóa phần thưởng (exploitation). Kỹ thuật ε-greedy là một giải pháp đơn giản và hiệu quả cho bài toán này, trong đó tham số ε (exploration rate) được sử dụng để kiểm soát sự cân bằng này.\"\n- Exploration vs Exploitation là một thách thức cơ bản trong Reinforcement Learning, nơi agent phải cân bằng giữa việc khám phá các hành động và trạng thái mới (exploration) để tìm kiếm phần thưởng tiềm năng cao hơn, và việc khai thác kiến thức hiện có (exploitation) để chọn hành động tốt nhất dựa trên thông tin đã học.\n\n**Mối quan hệ:**\n- Tham số ε cân bằng giữa exploration và exploitation trong quá trình chọn action."
        },
        "output_without_context": {
            "question": "Trong nguyên tắc khám phá và khai thác, khởi tạo lạc quan có vai trò gì trong quá trình ra quyết định?",
            "answer": "Khởi tạo lạc quan giúp tạo ra các giả thuyết tích cực để khám phá các khả năng mới."
        }
    },
    {
        "topic_description": "Chủ đề sẽ phân tích thuật toán ε-greedy, cách thức hoạt động và hiệu quả của nó trong việc tối ưu hóa khám phá và khai thác. Học sinh sẽ cần làm rõ khái niệm thống kê và cách nó ảnh hưởng đến quyết định của thuật toán.",
        "output_with_context": {
            "question": "Trong thuật toán ε-greedy, xác suất nào được sử dụng để chọn hành động ngẫu nhiên từ không gian hành động?",
            "answer": "ε",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n\n### 1. Giới thiệu về Exploration-Exploitation Dilemma\n\n#### 1.1. Định nghĩa\n\n**Exploitation**: Chọn actions mà agent tin là tốt nhất (maximize immediate reward)\n**Exploration**: Thử actions mới để discover potentially better options\n\n**Dilemma**: Làm sao cân bằng?\n```\nPure Exploitation: Có thể stuck tại suboptimal policy\nPure Exploration: Không bao giờ maximize rewards\n```\n\n#### 1.2. Tại sao quan trọng?\n\n- **Learning**: Cần explore để learn về environment\n- **Optimization**: Cần exploit để maximize returns\n- **Trade-off**: Exploration cost short-term reward for long-term gain\n\n#### 1.3. Multi-Armed Bandit Problem\n\n**Setup**: K slot machines (arms), mỗi arm có unknown reward distribution\n\n**Goal**: Maximize cumulative reward over time\n\n**Regret**: \n```\nRegret = Σ_t (V* - V(a_t))\nV* = Value của best arm\n```\n\n### 2. Exploration Strategies\n\n#### 2.1. ε-Greedy\n\n**Strategy**:\n```\nVới xác suất ε: Chọn random action\nVới xác suất 1-ε: Chọn best known action\n```\n\n**Variants**:\n```\n# Fixed\nε = 0.1\n\n# Decay\nε_t = ε_0 / t\nε_t = ε_min + (ε_max - ε_min) * exp(-decay * t)\n\n# Adaptive\nTăng ε khi performance kém\nGiảm ε khi performance tốt\n```\n\n**Pros**: Simple, effective\n**Cons**: Random exploration không efficient\n\n#### 2.2. Softmax / Boltzmann Exploration\n\n**Strategy**: Sample proportional to estimated values\n```\nπ(a|s) = exp(Q(s,a)/τ) / Σ_b exp(Q(s,b)/τ)\n```\n\n**Temperature τ**:\n- τ → 0: Greedy (exploitation)\n- τ → ∞: Uniform random (exploration)\n- τ decay over time\n\n**Pros**: Probabilistic, smooth\n**Cons**: Need tune τ\n\n#### 2.3. Optimistic Initialization\n\n**Idea**: Initialize Q-values optimistically (higher than true values)\n```\nQ(s,a) = Q_max  (instead of 0)\n```\n\n**Effect**: Agent naturally explores unvisited (s,a) pairs\n\n**Pros**: Simple, no hyperparameters\n**Cons**: Temporary effect only\n\n### 3. Upper Confidence Bound (UCB)\n\n#### 3.1. UCB Algorithm\n\n**Principle**: \"Optimism in face of uncertainty\"\n\n**UCB Formula**:\n```\nUCB(a) = Q̄(a) + c√(ln t / N(a))\n         ↑ Exploitation  ↑ Exploration\n\nQ̄(a): Average reward của arm a\nN(a): Number of times arm a played\nt: Total time steps\nc: Exploration parameter\n```\n\n**Action Selection**:\n```\na_t = argmax_a UCB(a)\n```\n\n**Properties**:\n- Logarithmic regret: O(log t)\n- Optimal trong bandit setting\n\n#### 3.2. UCB trong RL\n\n**UCB applied to Q-Learning**:\n```\nQ_UCB(s,a) = Q(s,a) + c√(ln N(s) / N(s,a))\n\nSelect: a = argmax_a Q_UCB(s,a)\n```\n\n**Challenges**: Need track visit counts\n\n### 4. Thompson Sampling\n\n#### 4.1. Bayesian Approach\n\n**Idea**: Maintain distribution over Q-values\n\n**Algorithm**:\n```\nInitialize: Prior distribution P(θ)\n\nLặp:\n    Sample θ_t ~ P(θ|history)\n    Select a_t = argmax_a Q(s,a;θ_t)\n    Observe reward r_t\n    Update P(θ|history, r_t) bằng Bayes rule\n```\n\n#### 4.2. Beta-Bernoulli Thompson Sampling\n\n**For binary rewards**:\n```\nPrior: Beta(α_a, β_a) cho mỗi arm a\n\nSample: θ_a ~ Beta(α_a, β_a)\nChoose: a = argmax_a θ_a\n\nUpdate winner:\n    Success: α_a ← α_a + 1\n\n**Các khái niệm quan trọng:**\n- ε-greedy là một kỹ thuật lựa chọn hành động (action selection) đơn giản và phổ biến trong Học tăng cường (Reinforcement Learning), được sử dụng để cân bằng giữa khám phá (exploration) và khai thác (exploitation). Với xác suất ε, agent chọn một hành động ngẫu nhiên từ không gian hành động (khám phá); và với xác suất 1-ε, agent chọn hành động được cho là tốt nhất dựa trên ước tính hiện tại, thường là hành động có giá trị Q cao nhất (khai thác). Giá trị ε thường được giảm dần theo thời gian hoặc số bước huấn luyện (ε-decay) để chuyển từ khám phá sang khai thác khi agent học được nhiều hơn về môi trường. Đây là một chính sách hành vi (behavior policy) phổ biến trong các thuật toán như Q-Learning và SARSA, và cũng có thể được sử dụng với contextual information trong Recommendation Systems.\n\n**Mối quan hệ:**\n- ε-Greedy sử dụng tham số ε để kiểm soát tỷ lệ khám phá.\n- ε-Greedy là một chiến lược giải quyết Exploration-Exploitation Dilemma bằng cách cân bằng giữa chọn hành động ngẫu nhiên và hành động tốt nhất đã biết."
        },
        "output_without_context": {
            "question": "Trong thuật toán ε-greedy, giá trị của ε đại diện cho điều gì trong quá trình ra quyết định?",
            "answer": "Tỷ lệ khám phá các lựa chọn ngẫu nhiên."
        }
    },
    {
        "topic_description": "Khái niệm hối tiếc sẽ được giải thích qua cách tính toán giá trị hành động tối ưu và hiểu được sự khác biệt giữa tổng hối tiếc và giá trị hành động hiện có. Học sinh cần phân tích các công thức liên quan để đánh giá sự hiệu quả.",
        "output_with_context": {
            "question": "Trong bối cảnh học tăng cường, công thức nào sau đây mô tả mối quan hệ giữa hàm giá trị trạng thái V(s) và hàm giá trị hành động Q(s,a)?",
            "answer": "Vᵖ(s) = Σₐ π(a|s)Qᵖ(s,a)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nMột quá trình được gọi là có tính chất Markov nếu:\n```\nP[Sₜ₊₁ | Sₜ] = P[Sₜ₊₁ | S₁, S₂, ..., Sₜ]\n```\n\n#### 3.2. Ý nghĩa\n- Trạng thái hiện tại chứa đầy đủ thông tin để dự đoán tương lai\n- Không cần phải nhớ toàn bộ lịch sử\n- Đơn giản hóa việc tính toán và phân tích\n\n### 4. Chính sách (Policy)\n\n#### 4.1. Định nghĩa\nChính sách π là một hàm ánh xạ từ trạng thái đến hành động:\n- **Chính sách xác định (Deterministic)**: π(s) = a\n- **Chính sách ngẫu nhiên (Stochastic)**: π(a|s) = P[Aₜ = a | Sₜ = s]\n\n#### 4.2. Phân loại\n- **Chính sách tĩnh (Stationary)**: Không thay đổi theo thời gian\n- **Chính sách động (Non-stationary)**: Thay đổi theo thời gian\n\n### 5. Hàm giá trị (Value Functions)\n\n#### 5.1. Hàm giá trị trạng thái (State-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s và tuân theo chính sách π\n\n```\nVᵖ(s) = Eᵖ[Gₜ | Sₜ = s]\n      = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s]\n```\n\n#### 5.2. Hàm giá trị hành động (Action-Value Function)\n**Định nghĩa**: Tổng phần thưởng chiết khấu kỳ vọng khi bắt đầu từ trạng thái s, thực hiện hành động a, và sau đó tuân theo chính sách π\n\n```\nQᵖ(s,a) = Eᵖ[Gₜ | Sₜ = s, Aₜ = a]\n        = Eᵖ[Σ(k=0 to ∞) γᵏRₜ₊ₖ₊₁ | Sₜ = s, Aₜ = a]\n```\n\n#### 5.3. Mối quan hệ giữa V và Q\n```\nVᵖ(s) = Σₐ π(a|s)Qᵖ(s,a)\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')\n```\n\n### 6. Phương trình Bellman\n\n#### 6.1. Phương trình Bellman cho Vᵖ\n```\nVᵖ(s) = Σₐ π(a|s)[R(s,a) + γ Σₛ' P(s'|s,a)Vᵖ(s')]\n```\n\n**Giải thích**:\n- Giá trị của một trạng thái bằng phần thưởng tức thời cộng với giá trị chiết khấu của trạng thái tiếp theo\n- Đây là một phương trình đệ quy\n\n#### 6.2. Phương trình Bellman cho Qᵖ\n```\nQᵖ(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) Σₐ' π(a'|s')Qᵖ(s',a')\n```\n\n#### 6.3. Phương trình Bellman tối ưu\n```\n\n**Các khái niệm quan trọng:**\n- Value Functions (Hàm giá trị) là các hàm ước lượng mức độ \"tốt\" của một trạng thái hoặc một cặp trạng thái-hành động, bằng cách tính tổng phần thưởng chiết khấu kỳ vọng trong tương lai. Có hai loại chính là State-Value Function V(s) và Action-Value Function Q(s,a).\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\nDynamic Programming là nền tảng quan trọng trong Reinforcement Learning. Mặc dù có hạn chế về yêu cầu biết mô hình và curse of dimensionality, DP cung cấp:\n\n- **Framework lý thuyết**: Hiểu rõ về optimal value functions và policies\n- **Nền tảng thuật toán**: Cơ sở cho các phương pháp model-free\n- **Công cụ thực tế**: Giải quyết được nhiều bài toán thực tế\n\nCác khái niệm từ DP (đặc biệt là GPI và Bellman equations) sẽ xuất hiện xuyên suốt trong các phương pháp học tăng cường tiếp theo, từ Temporal-Difference Learning đến Deep Q-Networks.\n\n---\n\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Learning\n\n#### 1.1. Định nghĩa\nModel-Free Learning là các phương pháp học tăng cường không yêu cầu biết trước mô hình môi trường (hàm chuyển trạng thái P và hàm phần thưởng R). Agent học trực tiếp từ kinh nghiệm tương tác với môi trường.\n\n#### 1.2. So sánh Model-Based vs Model-Free\n\n| Đặc điểm | Model-Based | Model-Free |\n|----------|-------------|------------|\n| Yêu cầu mô hình | Cần biết P và R | Không cần |\n| Học từ | Phương trình Bellman | Kinh nghiệm thực tế |\n| Ưu điểm | Hiệu quả dữ liệu | Linh hoạt, thực tế |\n| Nhược điểm | Khó có mô hình chính xác | Cần nhiều dữ liệu |\n| Ví dụ | DP, Model-Based RL | MC, TD, Q-Learning |\n\n#### 1.3. Bài toán Prediction\n**Mục tiêu**: Đánh giá một chính sách π cho trước\n- Input: Chính sách π\n- Output: Hàm giá trị V^π(s)\n- Không cần biết mô hình môi trường\n\n### 2. Monte Carlo Methods - Phương Pháp Monte Carlo\n\n#### 2.1. Ý tưởng cơ bản\nMonte Carlo (MC) ước lượng giá trị của trạng thái bằng cách lấy trung bình các return thực tế nhận được từ nhiều episodes.\n\n**Nguyên lý**:\n```\nV^π(s) = E_π[G_t | S_t = s]\n       ≈ average of returns từ trạng thái s\n```\n\n#### 2.2. Episode và Return\n\n**Episode**: Một chuỗi hoàn chỉnh từ trạng thái ban đầu đến kết thúc\n```\nS_0, A_0, R_1, S_1, A_1, R_2, ..., S_T\n```\n\n**Return**: Tổng phần thưởng chiết khấu\n```\nG_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... + γ^{T-t-1}R_T\n```\n\n#### 2.3. First-Visit Monte Carlo\n\n**Thuật toán**:\n```\nKhởi tạo:\n    V(s) = 0, ∀s\n    Returns(s) = danh sách rỗng, ∀s\n\nVới mỗi episode:\n    Tạo một episode tuân theo π: S_0, A_0, R_1, ..., S_T\n    G = 0\n    \n    Với mỗi bước t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        \n\n**Các khái niệm quan trọng:**\n- Optimal value functions là các hàm giá trị biểu diễn giá trị tối đa có thể đạt được từ một trạng thái hoặc cặp trạng thái-hành động, tuân theo một chính sách tối ưu. Chúng là mục tiêu chính trong nhiều bài toán Reinforcement Learning.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Policy Gradient Methods - Phương Pháp Gradient Chính Sách\n        x = F.relu(self.fc1(state))\n        logits = self.fc2(x)\n        return F.softmax(logits, dim=-1)\n```\n\n**Continuous Control**:\n```python\nclass GaussianPolicy(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.mean = nn.Linear(128, action_dim)\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n    \n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        mean = self.mean(x)\n        std = torch.exp(self.log_std)\n        return mean, std\n```\n\n### 3. Policy Gradient Theorem\n\n#### 3.1. Objective Function\n\n**Mục tiêu**: Maximize expected return\n```\nJ(θ) = E_τ~π_θ[G_τ] = E[Σ_t γ^t R_t]\n```\n\nHoặc với episodic tasks:\n```\nJ(θ) = V^π_θ(s_0) = E_π_θ[G_0 | S_0 = s_0]\n```\n\n#### 3.2. Policy Gradient Theorem\n\n**Định lý**:\n```\n∇_θ J(θ) = E_π_θ[∇_θ log π(A|S;θ) Q^π(S,A)]\n         = E_π_θ[∇_θ log π(A|S;θ) G_t]\n```\n\n**Giải thích**:\n- ∇_θ log π(A|S;θ): Score function (hướng tăng probability của action)\n- Q^π(S,A): Weighting (actions tốt được tăng, xấu giảm)\n\n#### 3.3. REINFORCE Algorithm\n\n**Monte Carlo Policy Gradient**:\n```\nKhởi tạo θ\nThiết lập learning rate α\n\nLặp:\n    Tạo episode theo π(·|·;θ): S_0, A_0, R_1, ..., S_T\n    \n    Với mỗi bước t:\n        G_t = Σ_{k=t}^T γ^{k-t} R_k\n        θ ← θ + α γ^t G_t ∇_θ log π(A_t|S_t;θ)\n```\n\n**Intuition**:\n- Nếu G_t > 0: Tăng probability của actions đã chọn\n- Nếu G_t < 0: Giảm probability\n- Magnitude tỷ lệ với |G_t|\n\n#### 3.4. REINFORCE với Baseline\n\n**Vấn đề**: High variance trong gradient estimates\n\n**Giải pháp**: Subtract baseline b(s)\n```\n∇_θ J(θ) = E[∇_θ log π(A|S;θ) (G_t - b(S_t))]\n```\n\n**Baseline phổ biến**: V^π(s)\n```\nAdvantage: A^π(s,a) = Q^π(s,a) - V^π(s)\n∇_θ J(θ) = E[∇_θ log π(A|S;θ) A^π(S,A)]\n```\n\n**Thuật toán**:\n```\nKhởi tạo θ, w (cho value function baseline)\n\nLặp:\n    Tạo episode: S_0, A_0, R_1, ..., S_T\n    \n    Với mỗi bước t:\n        G_t = Σ_{k=t}^T γ^{k-t} R_k\n        δ_t = G_t - V̂(S_t; w)  # Advantage estimate\n        \n        # Update policy\n        θ ← θ + α_θ γ^t δ_t ∇_θ log π(A_t|S_t;θ)\n        \n        # Update value function\n        w ← w + α_w δ_t ∇_w V̂(S_t; w)\n```\n\n### 4. Actor-Critic Methods\n\n#### 4.1. Ý tưởng\n\n**Actor**: Policy π(a|s;θ)\n**Critic**: Value function V(s;w) hoặc Q(s,a;w)\n\n**Actor-Critic Framework**:\n```\nActor: Chọn actions theo policy\n\n**Các khái niệm quan trọng:**\n- Critic là một thành phần thiết yếu trong kiến trúc Actor-Critic và các thuật toán Actor-Critic Methods, có nhiệm vụ ước lượng hàm giá trị (Value Function) của chính sách hiện tại. Critic có thể ước lượng hàm giá trị của trạng thái (V(s) hoặc V(s;w)) hoặc cặp trạng thái-hành động (Q(s,a) hoặc Q(s,a;w)). Trong các thuật toán như DDPG, Critic thường được biểu diễn bởi mạng Q(s,a;w), ước lượng giá trị của cặp (trạng thái, hành động). Nhiệm vụ chính của Critic là đánh giá các hành động được thực hiện bởi Actor bằng cách ước lượng giá trị của trạng thái hoặc cặp trạng thái-hành động, từ đó cung cấp tín hiệu đánh giá hoặc tín hiệu lỗi (ví dụ: TD error hoặc Advantage) cho Actor. Tín hiệu này giúp Actor điều chỉnh và cập nhật chính sách của mình. Trong đoạn mã, Critic được cập nhật bằng cách tối thiểu hóa bình phương của TD error (td_error.pow(2)).\n\n**Mối quan hệ:**\n- Critic tối ưu hóa critic_loss (bình phương của td_error) để học hàm giá trị.\n- Critic tính toán td_error bằng cách so sánh td_target với giá trị ước lượng hiện tại của trạng thái.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Policy Gradient Methods - Phương Pháp Gradient Chính Sách\nCritic: Đánh giá actions\nActor học từ feedback của Critic\n```\n\n#### 4.2. Advantage Actor-Critic (A2C)\n\n**TD Error làm advantage**:\n```\nδ_t = R_{t+1} + γV(S_{t+1};w) - V(S_t;w)\n```\n\n**Updates**:\n```\n# Actor update\nθ ← θ + α_θ δ_t ∇_θ log π(A_t|S_t;θ)\n\n# Critic update\nw ← w + α_w δ_t ∇_w V(S_t;w)\n```\n\n**Algorithm**:\n```\nKhởi tạo θ, w\nThiết lập α_θ, α_w\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        A ~ π(·|S;θ)\n        Thực hiện A, quan sát R, S'\n        \n        δ = R + γV(S';w) - V(S;w)\n        \n        w ← w + α_w δ ∇_w V(S;w)\n        θ ← θ + α_θ δ ∇_θ log π(A|S;θ)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 4.3. Asynchronous Advantage Actor-Critic (A3C)\n\n**Ý tưởng**: Parallel actors với shared parameters\n\n**Architecture**:\n```\nGlobal Network (θ, w)\n    ↓ Copy\nMultiple Workers (θ', w')\n    ↓ Collect experience\n    ↓ Compute gradients\n    ↑ Update global network\n```\n\n**Benefits**:\n- Faster learning (parallel experience collection)\n- Decorrelated experience (different workers explore differently)\n- Stable learning\n\n#### 4.4. Generalized Advantage Estimation (GAE)\n\n**n-Step TD Error**:\n```\nδ_t^(n) = R_{t+1} + γR_{t+2} + ... + γ^{n-1}R_{t+n} + γ^n V(S_{t+n}) - V(S_t)\n```\n\n**GAE**:\n```\nA_t^GAE(λ) = Σ_{l=0}^∞ (γλ)^l δ_{t+l}\n           = (1-λ) Σ_{n=1}^∞ λ^{n-1} δ_t^(n)\n```\n\n**Đặc điểm**:\n- λ = 0: TD(0), low variance, high bias\n- λ = 1: Monte Carlo, high variance, low bias\n- λ ∈ (0,1): Trade-off\n\n### 5. Trust Region Methods\n\n#### 5.1. Vấn đề với Vanilla Policy Gradient\n\n**Large updates**: Có thể làm policy collapse\n**Solution**: Constrain update size\n\n#### 5.2. Trust Region Policy Optimization (TRPO)\n\n**Objective**:\n```\nmaximize E[π_θ_new(a|s) / π_θ_old(a|s) · A^π_old(s,a)]\nsubject to: KL(π_θ_old || π_θ_new) ≤ δ\n```\n\n**KL Divergence constraint**: Đảm bảo new policy không quá khác old policy\n\n**Implementation**: Sử dụng conjugate gradient và line search\n\n**Đặc điểm**:\n✅ Monotonic improvement guarantee\n✅ Stable learning\n❌ Computationally expensive\n❌ Difficult to implement\n\n#### 5.3. Proximal Policy Optimization (PPO)\n\n**Ý tưởng**: Approximate TRPO constraint bằng clipping\n\n**Clipped Surrogate Objective**:\n```\nL^CLIP(θ) = E[min(r_t(θ)A_t, clip(r_t(θ), 1-ε, 1+ε)A_t)]\n\nr_t(θ) = π_θ(a|s) / π_θ_old(a|s)  # Importance ratio\n```\n\n**Giải thích**:\n- Clip r_t ∈ [1-ε, 1+ε] (thường ε=0.2)\n- Prevent too large policy updates\n- Simpler và faster than TRPO\n\n**PPO Algorithm**:\n```\nKhởi tạo θ, w\n\nLặp:\n    # Collect trajectories\n    Với mỗi worker:\n\n**Các khái niệm quan trọng:**\n- Critic là một thành phần thiết yếu trong kiến trúc Actor-Critic và các thuật toán Actor-Critic Methods, có nhiệm vụ ước lượng hàm giá trị (Value Function) của chính sách hiện tại. Critic có thể ước lượng hàm giá trị của trạng thái (V(s) hoặc V(s;w)) hoặc cặp trạng thái-hành động (Q(s,a) hoặc Q(s,a;w)). Trong các thuật toán như DDPG, Critic thường được biểu diễn bởi mạng Q(s,a;w), ước lượng giá trị của cặp (trạng thái, hành động). Nhiệm vụ chính của Critic là đánh giá các hành động được thực hiện bởi Actor bằng cách ước lượng giá trị của trạng thái hoặc cặp trạng thái-hành động, từ đó cung cấp tín hiệu đánh giá hoặc tín hiệu lỗi (ví dụ: TD error hoặc Advantage) cho Actor. Tín hiệu này giúp Actor điều chỉnh và cập nhật chính sách của mình. Trong đoạn mã, Critic được cập nhật bằng cách tối thiểu hóa bình phương của TD error (td_error.pow(2)).\n\n**Mối quan hệ:**\n- Critic tối ưu hóa critic_loss (bình phương của td_error) để học hàm giá trị.\n- Critic tính toán td_error bằng cách so sánh td_target với giá trị ước lượng hiện tại của trạng thái.\n\n**Nội dung từ tài liệu:**\n# Cấu trúc dữ liệu và giải thuật\n## Graph Algorithms – Shortest Path\n    return prices[dst] if prices[dst] != float('inf') else -1\n\n# Test\nflights = [[0,1,100], [1,2,100], [0,2,500]]\nprint(f\"Cheapest: {cheapest_flights_k_stops(3, flights, 0, 2, 1)}\")\n```\n\n---\n\n### 9. Tối ưu hóa và Biến thể\n\n#### 9.1. A* Algorithm\n\nCải tiến Dijkstra với heuristic function:\n\n```python\ndef a_star(graph, start, goal, heuristic):\n    \"\"\"\n    A* algorithm - Dijkstra + heuristic\n    heuristic(node, goal) = ước lượng khoảng cách đến goal\n    \n    f(n) = g(n) + h(n)\n    g(n) = cost từ start đến n\n    h(n) = heuristic từ n đến goal\n    \"\"\"\n    import heapq\n    \n    open_set = [(0 + heuristic(start, goal), 0, start)]\n    came_from = {}\n    g_score = {start: 0}\n    \n    while open_set:\n        _, current_g, current = heapq.heappop(open_set)\n        \n        if current == goal:\n            # Reconstruct path\n            path = []\n            while current in came_from:\n                path.append(current)\n                current = came_from[current]\n            path.append(start)\n            return path[::-1], g_score[goal]\n        \n        for neighbor, weight in graph[current].items():\n            tentative_g = current_g + weight\n            \n            if neighbor not in g_score or tentative_g < g_score[neighbor]:\n                came_from[neighbor] = current\n                g_score[neighbor] = tentative_g\n                f_score = tentative_g + heuristic(neighbor, goal)\n                heapq.heappush(open_set, (f_score, tentative_g, neighbor))\n    \n    return None, float('inf')\n```\n\n#### 9.2. Bidirectional Dijkstra\n\nChạy Dijkstra từ cả 2 phía (start và end):\n\n```python\ndef bidirectional_dijkstra(graph, start, end):\n    \"\"\"\n    Dijkstra từ cả 2 phía\n    Nhanh hơn khi chỉ cần tìm 1 đường đi\n    \"\"\"\n    # Implement tương tự Dijkstra nhưng:\n    # - Chạy 2 searches song song\n    # - Dừng khi 2 searches gặp nhau\n    # - Kết hợp 2 paths\n    pass\n```\n\n\n**Các khái niệm quan trọng:**\n- f(n) = g(n) + h(n) là công thức đánh giá chi phí tổng thể của một đường đi qua đỉnh n trong thuật toán A*. Trong đó, g(n) là chi phí thực tế từ đỉnh bắt đầu đến n, và h(n) là ước lượng chi phí từ n đến đỉnh đích (heuristic).\n\n**Mối quan hệ:**\n- A* Algorithm sử dụng công thức f(n) = g(n) + h(n) để đánh giá ưu tiên của các đỉnh trong hàng đợi ưu tiên."
        },
        "output_without_context": {
            "question": "Trong bối cảnh hối tiếc trong Bandit, tổng hối tiếc được tính toán như thế nào so với giá trị hành động hiện có?",
            "answer": "Tổng hối tiếc là sự khác biệt giữa giá trị tối ưu và giá trị hành động hiện tại."
        }
    },
    {
        "topic_description": "Chủ đề này liên kết MAB với các quy trình quyết định Markov (MDPs) dưới góc nhìn lập kế hoạch. Học sinh sẽ cần nắm vững nguyên lý cơ bản của MAB và áp dụng nó trong bối cảnh MDPs từ tuần 1 và 4.",
        "output_with_context": {
            "question": "Trong một Quá trình Quyết Định Markov (MDP), điều kiện nào là cần thiết để xác định một chính sách tối ưu π*?",
            "answer": "Vᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\nV*(s) = maxₐ[R(s,a) + γ Σₛ' P(s'|s,a)V*(s')]\nQ*(s,a) = R(s,a) + γ Σₛ' P(s'|s,a) maxₐ' Q*(s',a')\n```\n\n### 7. Chính sách tối ưu\n\n#### 7.1. Định nghĩa\nChính sách π* được gọi là tối ưu nếu:\n```\nVᵖ*(s) ≥ Vᵖ(s), ∀s ∈ S, ∀π\n```\n\n#### 7.2. Tính chất\n- Luôn tồn tại ít nhất một chính sách tối ưu\n- Tất cả các chính sách tối ưu đều có cùng hàm giá trị V*\n- Chính sách tối ưu có thể được tìm từ Q*:\n```\nπ*(s) = argmaxₐ Q*(s,a)\n```\n\n### 8. Các loại MDP đặc biệt\n\n#### 8.1. Episodic MDP\n- Có điểm kết thúc rõ ràng (terminal state)\n- Ví dụ: Game cờ, robot đi mê cung\n\n#### 8.2. Continuing MDP\n- Không có điểm kết thúc\n- Chạy vô hạn\n- Ví dụ: Hệ thống kiểm soát nhiệt độ\n\n#### 8.3. Finite MDP\n- Tập trạng thái và hành động hữu hạn\n- Dễ tính toán và phân tích\n\n#### 8.4. Infinite MDP\n- Tập trạng thái hoặc hành động vô hạn\n- Cần các kỹ thuật xấp xỉ\n\n### 9. Ví dụ minh họa: Robot đi mê cung\n\n#### 9.1. Mô tả bài toán\n- **Trạng thái**: Vị trí của robot trên lưới\n- **Hành động**: Lên, xuống, trái, phải\n- **Phần thưởng**: \n  - +10 khi đến đích\n  - -1 cho mỗi bước di chuyển\n  - -10 khi va vào tường\n- **Mục tiêu**: Tìm đường đi ngắn nhất đến đích\n\n#### 9.2. Biểu diễn MDP\n```\nS = {(x,y) | 0 ≤ x < width, 0 ≤ y < height, không phải tường}\nA = {UP, DOWN, LEFT, RIGHT}\nP(s'|s,a): Xác định bởi quy tắc di chuyển\nR(s,a,s'): Như mô tả ở trên\nγ = 0.9\n```\n\n### 10. Các thuật toán giải MDP\n\n#### 10.1. Value Iteration\n- Lặp lại cập nhật hàm giá trị cho đến khi hội tụ\n- Đảm bảo tìm được nghiệm tối ưu\n\n#### 10.2. Policy Iteration\n- Xen kẽ giữa đánh giá chính sách và cải thiện chính sách\n- Thường hội tụ nhanh hơn Value Iteration\n\n#### 10.3. Linear Programming\n- Biểu diễn bài toán dưới dạng quy hoạch tuyến tính\n- Giải bằng các solver LP chuẩn\n\n### 11. Ứng dụng thực tế\n\n#### 11.1. Robot tự động\n- Điều hướng và tránh vật cản\n- Lập kế hoạch đường đi\n\n#### 11.2. Quản lý tài nguyên\n\n**Các khái niệm quan trọng:**\n- Finite MDP là một loại Quá trình Quyết định Markov trong đó tập hợp các trạng thái (S) và tập hợp các hành động (A) đều hữu hạn. Điều này làm cho chúng dễ tính toán và phân tích hơn, cho phép sử dụng các phương pháp giải chính xác như Value Iteration và Policy Iteration.\n- Infinite MDP là một loại Quá trình Quyết định Markov trong đó tập hợp các trạng thái (S) hoặc tập hợp các hành động (A) (hoặc cả hai) là vô hạn. Đối với các MDP này, cần phải sử dụng các kỹ thuật xấp xỉ, chẳng hạn như xấp xỉ hàm giá trị, để giải quyết bài toán.\n- Continuing MDP là một loại Quá trình Quyết định Markov không có điểm kết thúc rõ ràng và chạy vô hạn. Các ví dụ bao gồm hệ thống kiểm soát nhiệt độ hoặc các tác vụ điều khiển liên tục, nơi tác nhân phải hoạt động liên tục mà không có một \"kết thúc\" cụ thể.\n\n**Mối quan hệ:**\n- Infinite MDP có tính chất là tập hành động A là vô hạn hoặc tập trạng thái S là vô hạn.\n- Finite MDP có tính chất là tập hành động A là hữu hạn.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Integrating Learning and Planning - Tích Hợp Học và Lập Kế Hoạch\n- Q-Learning: ~25 episodes\n- Planning steps significantly speed up learning\n\n### 4. Simulation-Based Search\n\n#### 4.1. Monte Carlo Tree Search (MCTS)\n\n**Ý tưởng**: Build search tree bằng simulation\n\n**Four Steps**:\n```\n1. Selection: Chọn nodes theo UCT policy\n2. Expansion: Thêm child node\n3. Simulation: Rollout policy từ new node\n4. Backpropagation: Update values dọc path\n```\n\n**UCT (Upper Confidence Bound for Trees)**:\n```\nUCT(s,a) = Q(s,a) + c√(ln N(s) / N(s,a))\n           ↑ Exploitation  ↑ Exploration\n```\n\n**Algorithm**:\n```\nfunction MCTS(state, num_simulations):\n    Khởi tạo root node với state\n    \n    Lặp num_simulations lần:\n        node = root\n        \n        # Selection\n        while node is fully expanded and not terminal:\n            node = select_child(node, UCT)\n        \n        # Expansion\n        if node is not terminal:\n            node = expand(node)\n        \n        # Simulation (rollout)\n        reward = simulate(node.state)\n        \n        # Backpropagation\n        backpropagate(node, reward)\n    \n    return best_action(root)\n```\n\n#### 4.2. AlphaGo và AlphaZero\n\n**AlphaGo**:\n- MCTS + Deep Neural Networks\n- Policy network: Suggest moves\n- Value network: Evaluate positions\n- Human expert data + self-play\n\n**AlphaZero**:\n- Pure self-play, no human data\n- Single network: Both policy và value\n- MCTS for search\n- Defeats AlphaGo 100-0\n\n**Key Innovation**: Deep RL + MCTS\n\n### 5. Model Predictive Control (MPC)\n\n#### 5.1. MPC trong RL\n\n**Idea**: Plan optimal actions using model\n\n**Process**:\n```\n1. Use model to predict future states\n2. Optimize action sequence\n3. Execute first action\n4. Replan at next step (receding horizon)\n```\n\n**Optimization**:\n```\nargmax_{a_0,...,a_H} Σ_t R(s_t, a_t)\nsubject to: s_{t+1} = f(s_t, a_t)\n```\n\n#### 5.2. Cross-Entropy Method (CEM)\n\n**Sampling-based optimization**:\n```\nKhởi tạo distribution μ, σ\n\nLặp:\n    Sample N action sequences từ N(μ, σ)\n    Evaluate mỗi sequence bằng rollouts\n    Select top K sequences\n    Update μ, σ từ elite sequences\n```\n\n**Applications**: Robot control, continuous control\n\n### 6. World Models\n\n#### 6.1. Learning World Models\n\n**Representation Learning**:\n```\nVAE (Variational Autoencoder):\n    Encode: z = Encoder(observation)\n    Decode: observation' = Decoder(z)\n\nRecurrent Model:\n    z_{t+1} = RNN(z_t, a_t)\n```\n\n**World Models Paper**:\n- V: Vision model (VAE)\n- M: Memory (RNN)\n- C: Controller (linear/simple)\n\n**Train entirely in dream**: Agent learns in latent space!\n\n#### 6.2. Imagination-Augmented Agents\n\n**Architecture**:\n```\nStandard path: observation → policy\n\nImagination path:\n    observation → model → future predictions\n              ↓              ↓\n              ↳──── aggregate ───→ policy\n```\n\n**Benefits**: Better sample efficiency và performance\n\n### 7. Model Errors và Solutions\n\n#### 7.1. Model Bias Problem\n\n**Issue**: Model errors compound over long horizons\n```\nError at t: ε_t\nError at T: O(T·ε_t) hoặc worse\n```\n\n#### 7.2. Solutions\n\n**1. Short Horizon Planning**:\n- Plan chỉ vài steps ahead\n- Replan frequently\n\n**2. Model Ensemble**:\n```\nTrain N models: {M_1, ..., M_N}\nEstimate uncertainty\nUse pessimistic/conservative planning\n```\n\n**3. Model-Based Value Expansion (MVE)**:\n```\nMix model rollouts với value function:\nV(s) = R(s,a) + γ^k V(s_{t+k})\n       ↑ k-step model rollout\n```\n\n**4. Learn in Real Environment**:\n- Use model cho exploration/planning\n- Learn value/policy from real data\n\n\n**Các khái niệm quan trọng:**\n- Model Predictive Control (MPC) là một kỹ thuật lập kế hoạch trong RL sử dụng một mô hình của môi trường để dự đoán các trạng thái tương lai và tối ưu hóa một chuỗi hành động. MPC hoạt động bằng cách lập kế hoạch một chuỗi hành động tối ưu, thực hiện hành động đầu tiên, và sau đó lập kế hoạch lại ở bước tiếp theo (receding horizon).\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n\n### 1. Giới thiệu về MDP\n\nQuá trình Quyết định Markov (Markov Decision Process - MDP) là một framework toán học để mô hình hóa việc ra quyết định trong các tình huống mà kết quả có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường.\n\n### 2. Các thành phần cơ bản của MDP\n\nMột MDP được định nghĩa bởi bộ năm phần tử (S, A, P, R, γ):\n\n#### 2.1. Tập trạng thái (State Space - S)\n- **Định nghĩa**: Tập hợp tất cả các trạng thái có thể mà agent có thể gặp trong môi trường\n- **Ký hiệu**: S = {s₁, s₂, ..., sₙ}\n- **Ví dụ**: Trong game cờ vua, mỗi trạng thái là một cấu hình cụ thể của bàn cờ\n\n#### 2.2. Tập hành động (Action Space - A)\n- **Định nghĩa**: Tập hợp tất cả các hành động mà agent có thể thực hiện\n- **Ký hiệu**: A = {a₁, a₂, ..., aₘ}\n- **Phân loại**:\n  - Không gian hành động rời rạc: Số lượng hành động hữu hạn\n  - Không gian hành động liên tục: Hành động có thể nhận giá trị trong một khoảng liên tục\n\n#### 2.3. Hàm chuyển trạng thái (State Transition Function - P)\n- **Định nghĩa**: Xác suất chuyển từ trạng thái s sang trạng thái s' khi thực hiện hành động a\n- **Công thức**: P(s'|s,a) = P[Sₜ₊₁ = s' | Sₜ = s, Aₜ = a]\n- **Tính chất Markov**: Trạng thái tương lai chỉ phụ thuộc vào trạng thái hiện tại, không phụ thuộc vào lịch sử\n\n#### 2.4. Hàm phần thưởng (Reward Function - R)\n- **Định nghĩa**: Phần thưởng nhận được khi thực hiện hành động a từ trạng thái s\n- **Công thức**: R(s,a) hoặc R(s,a,s')\n- **Mục đích**: Định hướng agent học hành vi tối ưu\n\n#### 2.5. Hệ số chiết khấu (Discount Factor - γ)\n- **Định nghĩa**: Hệ số để cân bằng giữa phần thưởng tức thời và phần thưởng dài hạn\n- **Giá trị**: 0 ≤ γ ≤ 1\n- **Ý nghĩa**:\n  - γ = 0: Chỉ quan tâm đến phần thưởng tức thời\n  - γ = 1: Phần thưởng tương lai có giá trị bằng phần thưởng hiện tại\n  - 0 < γ < 1: Phần thưởng tương lai bị chiết khấu\n\n### 3. Tính chất Markov\n\n#### 3.1. Định nghĩa\n\n**Các khái niệm quan trọng:**\n- Markov Decision Process (MDP) là một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự, nơi một agent tương tác với môi trường để tối đa hóa tổng phần thưởng chiết khấu. Trong MDP, kết quả của các quyết định có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường (Reinforcement Learning) và được định nghĩa bởi các thành phần: tập trạng thái S, tập hành động A, hàm phần thưởng R, xác suất chuyển trạng thái P, và discount factor γ.\n- A là tập hợp tất cả các hành động có thể có mà agent có thể thực hiện trong môi trường của Markov Decision Process (MDP).\n\n**Mối quan hệ:**\n- Một Markov Decision Process yêu cầu tập hành động A để định nghĩa các hành động mà agent có thể thực hiện.\n- Một Markov Decision Process yêu cầu tập trạng thái S để định nghĩa các trạng thái mà agent có thể gặp.\n- Một Markov Decision Process yêu cầu hàm phần thưởng R để định hướng agent học hành vi tối ưu.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- A là tập hợp tất cả các hành động có thể có mà agent có thể thực hiện trong môi trường của Markov Decision Process (MDP).\n\n**Mối quan hệ:**\n- Một Markov Decision Process yêu cầu tập hành động A để định nghĩa các hành động mà agent có thể thực hiện.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nKhởi tạo V(s) arbitrarily, ∀s ∈ S\n\nVới mỗi episode:\n    Khởi tạo S\n    \n    Lặp cho mỗi bước của episode:\n        A ← hành động từ S theo π\n        Thực hiện A, quan sát R, S'\n        \n        V(S) ← V(S) + α[R + γV(S') - V(S)]\n        \n        S ← S'\n    cho đến khi S là terminal\n```\n\n#### 3.4. So sánh MC vs TD(0)\n\n| Tiêu chí | Monte Carlo | TD(0) |\n|----------|-------------|-------|\n| Cập nhật | Cuối episode | Sau mỗi bước |\n| Bootstrap | Không | Có |\n| Target | G_t (actual return) | R + γV(S') |\n| Bias | Unbiased | Biased |\n| Variance | High variance | Low variance |\n| Hội tụ | Chậm | Nhanh hơn |\n| Môi trường | Cần episodic | Cả episodic và continuing |\n\n#### 3.5. Ưu điểm của TD\n\n✅ **Học online**: Cập nhật sau mỗi bước, không cần chờ episode kết thúc\n✅ **Continuing tasks**: Hoạt động với tasks không có điểm kết thúc\n✅ **Lower variance**: Bootstrap giảm variance so với MC\n✅ **Học nhanh hơn**: Thường hội tụ nhanh hơn MC trong thực tế\n✅ **Hiệu quả dữ liệu**: Sử dụng thông tin từ ước lượng hiện tại\n\n#### 3.6. Ví dụ minh họa: Random Walk\n\n**Setup**:\n```\nStates: [A] [B] [C] [D] [E]\nStart: C\nTerminal: Left of A hoặc Right of E\nReward: 0 mọi nơi, +1 khi đến Right of E\nAction: Left hoặc Right (random với p=0.5)\n```\n\n**True values**:\n```\nV^π(A) = 1/6, V^π(B) = 2/6, V^π(C) = 3/6,\nV^π(D) = 4/6, V^π(E) = 5/6\n```\n\n**So sánh MC vs TD**:\n- TD hội tụ nhanh hơn với cùng số episodes\n- TD ít sensitive với khởi tạo\n- MC có RMS error cao hơn trong giai đoạn đầu\n\n### 4. Batch Methods và Certainty Equivalence\n\n#### 4.1. Batch Learning\n**Ý tưởng**: Lặp lại huấn luyện trên cùng một batch experience cho đến hội tụ\n\n```\nCho trước batch experience: \n    {(S₁, A₁, R₁, S'₁), (S₂, A₂, R₂, S'₂), ...}\n\nLặp cho đến hội tụ:\n    Với mỗi experience (S, A, R, S'):\n        Cập nhật V(S) theo MC hoặc TD\n```\n\n#### 4.2. Certainty Equivalence\n\n**MC**: Tìm V^π minimize mean-squared error với observed returns\n```\nV^π = argmin_V Σ_episodes Σ_t (G_t - V(S_t))²\n```\n\n**TD**: Tìm V^π thỏa mãn phương trình Bellman cho MDP ước lượng\n```\nV^π(s) = E[R + γV^π(S') | s]\n```\n(ước lượng từ experience)\n\n**Kết quả**:\n- TD tận dụng cấu trúc Markov\n- MC chỉ minimize error, không exploit Markov property\n- TD thường hiệu quả hơn trong môi trường Markov\n\n#### 4.3. Ví dụ: AB Example\n\n**Experience**:\n```\nA, 0, B, 0\nB, 1 (terminal)\nB, 1 (terminal)\n\n**Các khái niệm quan trọng:**\n- Markov property (tính chất Markov) là một tính chất của môi trường trong Học tăng cường (Reinforcement Learning), trong đó trạng thái hiện tại chứa tất cả thông tin cần thiết để dự đoán trạng thái và phần thưởng tiếp theo, độc lập với lịch sử các trạng thái và hành động trước đó. Điều này có nghĩa là trạng thái tiếp theo chỉ phụ thuộc vào trạng thái và hành động hiện tại. Các thuật toán Temporal Difference (TD), bao gồm TD(0), tận dụng cấu trúc Markov này và thường hiệu quả hơn trong môi trường Markov, trong khi Monte Carlo (MC) thì không.\n\n**Mối quan hệ:**\n- Certainty Equivalence trong TD tận dụng cấu trúc Markov property của môi trường để đạt hiệu quả cao hơn."
        },
        "output_without_context": {
            "question": "Trong bối cảnh của Multi-Armed Bandit (MAB) và các quy trình quyết định Markov (MDPs), yếu tố nào là quan trọng nhất để đánh giá hiệu quả của một chính sách quyết định?",
            "answer": "Tỷ lệ thành công của các lựa chọn được thực hiện."
        }
    },
    {
        "topic_description": "Chủ đề này tìm hiểu kỹ thuật xấp xỉ hàm giá trị trong Học Tăng Cường, bao gồm các ứng dụng thực tiễn. Sinh viên sẽ phải phân tích và đánh giá các phương pháp khác nhau sẽ được lựa chọn.",
        "output_with_context": {
            "question": "Trong xấp xỉ hàm giá trị, công thức nào được sử dụng để ước lượng hàm giá trị V(s) bằng cách sử dụng vector đặc trưng φ(s) và tham số w?",
            "answer": "V̂(s; w) = w^T φ(s) = Σᵢ wᵢφᵢ(s)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n\n### 1. Giới thiệu về Function Approximation\n\n#### 1.1. Vấn đề với Tabular Methods\n\n**Giới hạn của phương pháp bảng**:\n- Lưu trữ V(s) hoặc Q(s,a) cho mọi trạng thái\n- Không khả thi với không gian trạng thái lớn:\n  - Backgammon: ~10²⁰ states\n  - Go: ~10¹⁷⁰ states\n  - Continuous states: Vô hạn\n- Không có khả năng generalization\n\n**Ví dụ**:\n```\nTabular: \n- State 1: Q(s₁, a) = 5.2\n- State 2: Q(s₂, a) = 4.8\n- State 3: Q(s₃, a) = ???  ← Chưa thấy bao giờ\n\nFunction Approximation:\n- Q(s, a) ≈ Q̂(s, a; w)\n- Có thể ước lượng cho states chưa thấy\n```\n\n#### 1.2. Function Approximation Paradigm\n\n**Ý tưởng**: Biểu diễn value function bằng parameterized function\n```\nV(s) ≈ V̂(s; w)\nQ(s,a) ≈ Q̂(s,a; w)\nπ(a|s) ≈ π̂(a|s; θ)\n```\n\n**Lợi ích**:\n✅ Xử lý được large/continuous state spaces\n✅ Generalization: Học từ state này áp dụng cho state tương tự\n✅ Compact representation: Ít parameters hơn states\n✅ Efficient computation và storage\n\n#### 1.3. Các loại Function Approximators\n\n**Linear Methods**:\n```\nV̂(s; w) = w^T φ(s) = Σᵢ wᵢφᵢ(s)\n```\n\n**Non-linear Methods**:\n- Polynomial features\n- Radial Basis Functions (RBF)\n- Neural Networks\n- Decision Trees\n\n**Deep Learning**:\n- Deep Neural Networks (DNN)\n- Convolutional Neural Networks (CNN)\n- Recurrent Neural Networks (RNN)\n\n### 2. Linear Value Function Approximation\n\n#### 2.1. Feature Vectors\n\n**Định nghĩa**: Biểu diễn state s bằng feature vector φ(s)\n```\ns → φ(s) = [φ₁(s), φ₂(s), ..., φₙ(s)]^T\n```\n\n**Value Function Approximation**:\n```\nV̂(s; w) = w^T φ(s) = Σᵢ₌₁ⁿ wᵢφᵢ(s)\n```\n\n**Ví dụ: Gridworld**\n```\nFeatures cho position (x, y):\nφ₁(s) = x\nφ₂(s) = y\nφ₃(s) = x²\nφ₄(s) = y²\nφ₅(s) = xy\nφ₆(s) = 1  (bias)\n```\n\n#### 2.2. Feature Design\n\n**Table Lookup (one-hot encoding)**:\n```\nφ(s) = [0, 0, ..., 1, ..., 0]^T\n           ↑ vị trí tương ứng với s\n```\n→ Equivalent với tabular methods\n\n**Tile Coding**:\n- Chia state space thành nhiều overlapping tilings\n- Mỗi tile là một binary feature\n- Hiệu quả với continuous states\n\n**Radial Basis Functions**:\n```\nφᵢ(s) = exp(-||s - cᵢ||² / (2σ²))\n```\n- cᵢ: center của basis function thứ i\n- σ: width parameter\n\n**Polynomial Features**:\n```\nφ(s) = [1, s₁, s₂, s₁², s₁s₂, s₂², ...]\n```\n\n#### 2.3. Objective Function\n\n**Mean Squared Value Error (MSVE)**:\n```\nJ(w) = E_π[(V^π(s) - V̂(s; w))²]\n\n**Các khái niệm quan trọng:**\n- Value Function Approximation (VFA) là một kỹ thuật trong Học tăng cường (Reinforcement Learning) được sử dụng để ước lượng hàm giá trị (V(s)) hoặc hàm Q-function (Q(s,a)). Kỹ thuật này sử dụng các hàm xấp xỉ có tham số (ví dụ: mạng nơ-ron, hàm tuyến tính) thay vì bảng tra cứu để khái quát hóa từ các trạng thái đã gặp sang các trạng thái chưa gặp. VFA là cần thiết khi không gian trạng thái hoặc hành động quá lớn hoặc liên tục, không thể lưu trữ hoặc tính toán chính xác các giá trị cho mọi trạng thái hoặc cặp trạng thái-hành động, cho phép các thuật toán RL mở rộng quy mô sang các bài toán thực tế.\n\n**Mối quan hệ:**\n- Value Function Approximation tối ưu hóa hàm mục tiêu J(w) bằng cách tìm các tham số w để giảm thiểu sai số giữa hàm giá trị thực và hàm giá trị xấp xỉ.\n- Value Function Approximation là một kỹ thuật để xấp xỉ Q-function khi không thể tính toán chính xác.\n- Value Function Approximation sử dụng kỹ thuật Function Approximation để ước lượng hàm giá trị.\n- Value Function Approximation xấp xỉ hàm giá trị hành động Q(s,a) bằng một hàm tham số Q̂(s,a; w).\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n**Features**:\n- **Separate features**: φ(s, a) cho mỗi cặp (s,a)\n- **State-action features**: Kết hợp φ_s và φ_a\n\n#### 5.2. Semi-gradient SARSA\n\n**Update Rule**:\n```\nw ← w + α[R + γQ̂(S', A'; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n```\n\n**Thuật toán**:\n```\nKhởi tạo w arbitrarily\nThiết lập ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    Chọn A từ S theo ε-greedy(Q̂(S, ·; w))\n    \n    Lặp với mỗi bước:\n        Thực hiện A, quan sát R, S'\n        Chọn A' từ S' theo ε-greedy(Q̂(S', ·; w))\n        \n        w ← w + α[R + γQ̂(S', A'; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n        \n        S ← S'; A ← A'\n    cho đến S là terminal\n```\n\n#### 5.3. Semi-gradient Q-Learning\n\n**Update Rule**:\n```\nw ← w + α[R + γ max_a Q̂(S', a; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n```\n\n**Thuật toán**:\n```\nKhởi tạo w arbitrarily\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        Chọn A từ S theo ε-greedy(Q̂(S, ·; w))\n        Thực hiện A, quan sát R, S'\n        \n        w ← w + α[R + γ max_a Q̂(S', a; w) - Q̂(S, A; w)]∇_w Q̂(S, A; w)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 5.4. Ví dụ: Mountain Car\n\n**Problem Setup**:\n- State: (position, velocity)\n- Action: {left, right, no-op}\n- Reward: -1 per step\n- Goal: Reach top of hill\n\n**Features**: Tile coding với multiple tilings\n```\nφ(s, a) = tile_coding(position, velocity, action)\n```\n\n**Results với Semi-gradient SARSA**:\n- Converge sau ~500 episodes\n- Learned policy: Build momentum bằng oscillation\n\n### 6. Convergence và Stability\n\n#### 6.1. Convergence Guarantees\n\n**Monte Carlo**:\n- ✅ Converge đến local optimum\n- ✅ Với linear FA: converge đến global optimum\n\n**TD với Linear FA**:\n- ✅ On-policy: Converge đến near-optimal\n- ❌ Off-policy: Có thể diverge (deadly triad)\n\n**Semi-gradient Methods**:\n- Không follow true gradient\n- Có thể không converge\n- Nhưng thường work well trong thực tế\n\n#### 6.2. Deadly Triad\n\n**Ba yếu tố gây divergence**:\n1. **Function Approximation**: Không phải tabular\n2. **Bootstrapping**: TD-style updates\n3. **Off-policy**: Behavior ≠ target policy\n\n**Khi có cả 3**: Divergence có thể xảy ra\n\n**Giải pháp**:\n- Loại bỏ một trong ba yếu tố\n- Sử dụng gradient TD methods\n- Importance sampling\n- Experience replay với target network (DQN)\n\n#### 6.3. Baird's Counterexample\n\n**Setup**: Simple MDP với linear FA và off-policy TD\n\n**Kết quả**: Parameters diverge đến infinity!\n\n**Ý nghĩa**: Cần cẩn thận với off-policy + FA + bootstrapping\n\n### 7. Batch Methods\n\n#### 7.1. Experience Replay\n\n**Ý tưởng**: Lưu trữ và replay experiences\n```\n\n**Các khái niệm quan trọng:**\n- Value Function Approximation (VFA) là một kỹ thuật trong Học tăng cường (Reinforcement Learning) được sử dụng để ước lượng hàm giá trị (V(s)) hoặc hàm Q-function (Q(s,a)). Kỹ thuật này sử dụng các hàm xấp xỉ có tham số (ví dụ: mạng nơ-ron, hàm tuyến tính) thay vì bảng tra cứu để khái quát hóa từ các trạng thái đã gặp sang các trạng thái chưa gặp. VFA là cần thiết khi không gian trạng thái hoặc hành động quá lớn hoặc liên tục, không thể lưu trữ hoặc tính toán chính xác các giá trị cho mọi trạng thái hoặc cặp trạng thái-hành động, cho phép các thuật toán RL mở rộng quy mô sang các bài toán thực tế.\n\n**Mối quan hệ:**\n- Value Function Approximation tối ưu hóa hàm mục tiêu J(w) bằng cách tìm các tham số w để giảm thiểu sai số giữa hàm giá trị thực và hàm giá trị xấp xỉ.\n- Value Function Approximation là một kỹ thuật để xấp xỉ Q-function khi không thể tính toán chính xác.\n- Value Function Approximation sử dụng kỹ thuật Function Approximation để ước lượng hàm giá trị.\n- Value Function Approximation xấp xỉ hàm giá trị hành động Q(s,a) bằng một hàm tham số Q̂(s,a; w).\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n     = Σₛ d(s)(V^π(s) - V̂(s; w))²\n```\n- d(s): distribution của states dưới policy π\n\n**Mục tiêu**: Minimize J(w) = ||V^π - V̂_w||²_d\n\n### 3. Stochastic Gradient Descent (SGD)\n\n#### 3.1. Gradient Descent\n\n**Update Rule**:\n```\nw_{t+1} = w_t - (α/2)∇_w J(w_t)\n        = w_t + α E[(V^π(s) - V̂(s; w))∇_w V̂(s; w)]\n```\n\n**Stochastic Gradient Descent**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]∇_w V̂(S_t; w_t)\n```\n\n#### 3.2. Linear SGD\n\n**Gradient của linear function**:\n```\n∇_w V̂(s; w) = ∇_w(w^T φ(s)) = φ(s)\n```\n\n**Update rule**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]φ(S_t)\n```\n\n**Đặc điểm**:\n- Converge đến local optimum (global cho linear)\n- Learning rate α quan trọng\n- Simple và efficient\n\n#### 3.3. Feature Scaling\n\n**Vấn đề**: Features có scale khác nhau → học không ổn định\n\n**Giải pháp**:\n```\nNormalization: φᵢ = (φᵢ - μᵢ)/σᵢ\nStandardization: φᵢ ∈ [0, 1]\n```\n\n### 4. Incremental Prediction Methods\n\n#### 4.1. Monte Carlo với Function Approximation\n\n**Update**:\n```\nw ← w + α[G_t - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ Target: actual return\n```\n\n**Đặc điểm**:\n- Unbiased target\n- High variance\n- Non-stationary target (G_t khác nhau mỗi episode)\n\n#### 4.2. TD(0) với Function Approximation\n\n**Update**:\n```\nw ← w + α[R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ TD target\n```\n\n**Semi-gradient**: Không lấy gradient qua V̂(S_{t+1}; w)\n\n**Thuật toán Semi-gradient TD(0)**:\n```\nKhởi tạo w arbitrarily\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        A ← π(S)\n        Thực hiện A, quan sát R, S'\n        \n        w ← w + α[R + γV̂(S'; w) - V̂(S; w)]∇_w V̂(S; w)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 4.3. TD(λ) với Function Approximation\n\n**Eligibility Traces**:\n```\nz_0 = 0\nz_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n```\n\n**Update**:\n```\nδ_t = R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)\nw ← w + αδ_t z_t\n```\n\n**Accumulating vs Replacing traces**:\n- Accumulating: z_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n- Replacing: Phức tạp hơn, phụ thuộc feature type\n\n### 5. Control với Function Approximation\n\n#### 5.1. Action-Value Function Approximation\n\n**Parameterization**:\n```\nQ̂(s, a; w) = w^T φ(s, a)\n```\n\n\n**Các khái niệm quan trọng:**\n- Monte Carlo với Function Approximation là một phương pháp học tăng cường sử dụng kỹ thuật Monte Carlo để ước lượng hàm giá trị, kết hợp với xấp xỉ hàm giá trị. Nó cập nhật các tham số w dựa trên return thực tế G_t quan sát được từ một episode hoàn chỉnh. Công thức cập nhật là w ← w + α[G_t - V̂(S_t; w)]∇_w V̂(S_t; w). Đặc điểm của nó là mục tiêu (G_t) không thiên vị nhưng có phương sai cao và không dừng (non-stationary).\n\n**Mối quan hệ:**\n- Monte Carlo với Function Approximation sử dụng G_t làm mục tiêu để cập nhật hàm giá trị xấp xỉ.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n#### 13.1. Approximate Dynamic Programming\n- Sử dụng function approximation\n- Neural networks để biểu diễn V hoặc π\n- Trade-off giữa accuracy và scalability\n\n#### 13.2. Model-Free Methods\n- Không cần biết P và R\n- Học từ experience\n- Temporal-Difference Learning, Q-Learning (phần sau)\n\n#### 13.3. Deep Reinforcement Learning\n- Kết hợp DP với deep learning\n- DQN, Actor-Critic, PPO\n- Giải quyết được bài toán phức tạp\n\n### 14. Code Implementation - Ví dụ Python\n\n#### 14.1. Policy Iteration\n```python\ndef policy_iteration(env, gamma=0.9, theta=1e-6):\n    # Khởi tạo\n    V = np.zeros(env.num_states)\n    policy = np.zeros(env.num_states, dtype=int)\n    \n    while True:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.num_states):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([p * (r + gamma * V[s_]) \n                           for p, s_, r in env.transitions(s, a)])\n                delta = max(delta, abs(v - V[s]))\n            if delta < theta:\n                break\n        \n        # Policy Improvement\n        policy_stable = True\n        for s in range(env.num_states):\n            old_action = policy[s]\n            # Tìm hành động tốt nhất\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            policy[s] = np.argmax(action_values)\n            \n            if old_action != policy[s]:\n                policy_stable = False\n        \n        if policy_stable:\n            return V, policy\n```\n\n#### 14.2. Value Iteration\n```python\ndef value_iteration(env, gamma=0.9, theta=1e-6):\n    V = np.zeros(env.num_states)\n    \n    while True:\n        delta = 0\n        for s in range(env.num_states):\n            v = V[s]\n            # Bellman optimality update\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            V[s] = max(action_values)\n            delta = max(delta, abs(v - V[s]))\n        \n        if delta < theta:\n            break\n    \n    # Trích xuất chính sách\n    policy = np.zeros(env.num_states, dtype=int)\n    for s in range(env.num_states):\n        action_values = []\n        for a in range(env.num_actions):\n            q_value = sum([p * (r + gamma * V[s_]) \n                          for p, s_, r in env.transitions(s, a)])\n            action_values.append(q_value)\n        policy[s] = np.argmax(action_values)\n    \n    return V, policy\n```\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement policy evaluation cho Gridworld\n2. So sánh tốc độ hội tụ của in-place và two-array DP\n3. Visualize quá trình hội tụ của value iteration\n\n#### 15.2. Bài tập nâng cao\n1. Giải Gambler's Problem với các giá trị p khác nhau\n2. Implement prioritized sweeping\n3. Modified policy iteration với k=1, k=3, k=∞\n\n#### 15.3. Dự án\n1. Xây dựng AI cho game 2048 bằng DP\n2. Tối ưu hóa việc sạc pin cho robot\n3. Quản lý danh mục đầu tư bằng MDP\n\n### 16. Kết luận\n\n\n**Các khái niệm quan trọng:**\n- Accuracy (Độ chính xác) trong ngữ cảnh của function approximation đề cập đến mức độ gần đúng của hàm xấp xỉ so với hàm giá trị hoặc chính sách thực tế. Có một sự đánh đổi giữa độ chính xác và khả năng mở rộng khi sử dụng function approximation: các mô hình phức tạp hơn có thể chính xác hơn nhưng khó huấn luyện và mở rộng hơn."
        },
        "output_without_context": {
            "question": "Trong Học Tăng Cường, phương pháp nào thường được sử dụng để xấp xỉ hàm giá trị nhằm tối ưu hóa quyết định của tác nhân?",
            "answer": "Hàm Q (Q-function)"
        }
    },
    {
        "topic_description": "Chủ đề này kết nối các nguyên tắc Bayesian với Học Tăng Cường và Bandit Contextual. Học sinh cần phải hiểu các khái niệm và ứng dụng thực tiễn của Bayesian Bandits trong việc tối ưu hóa quyết định.",
        "output_with_context": {
            "question": "Trong bối cảnh của Bayesian Bandits, công thức nào được sử dụng để tính toán Upper Confidence Bound (UCB) cho một hành động a?",
            "answer": "UCB(a) = Q̄(a) + c√(ln t / N(a))",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n\n### 1. Giới thiệu về Exploration-Exploitation Dilemma\n\n#### 1.1. Định nghĩa\n\n**Exploitation**: Chọn actions mà agent tin là tốt nhất (maximize immediate reward)\n**Exploration**: Thử actions mới để discover potentially better options\n\n**Dilemma**: Làm sao cân bằng?\n```\nPure Exploitation: Có thể stuck tại suboptimal policy\nPure Exploration: Không bao giờ maximize rewards\n```\n\n#### 1.2. Tại sao quan trọng?\n\n- **Learning**: Cần explore để learn về environment\n- **Optimization**: Cần exploit để maximize returns\n- **Trade-off**: Exploration cost short-term reward for long-term gain\n\n#### 1.3. Multi-Armed Bandit Problem\n\n**Setup**: K slot machines (arms), mỗi arm có unknown reward distribution\n\n**Goal**: Maximize cumulative reward over time\n\n**Regret**: \n```\nRegret = Σ_t (V* - V(a_t))\nV* = Value của best arm\n```\n\n### 2. Exploration Strategies\n\n#### 2.1. ε-Greedy\n\n**Strategy**:\n```\nVới xác suất ε: Chọn random action\nVới xác suất 1-ε: Chọn best known action\n```\n\n**Variants**:\n```\n# Fixed\nε = 0.1\n\n# Decay\nε_t = ε_0 / t\nε_t = ε_min + (ε_max - ε_min) * exp(-decay * t)\n\n# Adaptive\nTăng ε khi performance kém\nGiảm ε khi performance tốt\n```\n\n**Pros**: Simple, effective\n**Cons**: Random exploration không efficient\n\n#### 2.2. Softmax / Boltzmann Exploration\n\n**Strategy**: Sample proportional to estimated values\n```\nπ(a|s) = exp(Q(s,a)/τ) / Σ_b exp(Q(s,b)/τ)\n```\n\n**Temperature τ**:\n- τ → 0: Greedy (exploitation)\n- τ → ∞: Uniform random (exploration)\n- τ decay over time\n\n**Pros**: Probabilistic, smooth\n**Cons**: Need tune τ\n\n#### 2.3. Optimistic Initialization\n\n**Idea**: Initialize Q-values optimistically (higher than true values)\n```\nQ(s,a) = Q_max  (instead of 0)\n```\n\n**Effect**: Agent naturally explores unvisited (s,a) pairs\n\n**Pros**: Simple, no hyperparameters\n**Cons**: Temporary effect only\n\n### 3. Upper Confidence Bound (UCB)\n\n#### 3.1. UCB Algorithm\n\n**Principle**: \"Optimism in face of uncertainty\"\n\n**UCB Formula**:\n```\nUCB(a) = Q̄(a) + c√(ln t / N(a))\n         ↑ Exploitation  ↑ Exploration\n\nQ̄(a): Average reward của arm a\nN(a): Number of times arm a played\nt: Total time steps\nc: Exploration parameter\n```\n\n**Action Selection**:\n```\na_t = argmax_a UCB(a)\n```\n\n**Properties**:\n- Logarithmic regret: O(log t)\n- Optimal trong bandit setting\n\n#### 3.2. UCB trong RL\n\n**UCB applied to Q-Learning**:\n```\nQ_UCB(s,a) = Q(s,a) + c√(ln N(s) / N(s,a))\n\nSelect: a = argmax_a Q_UCB(s,a)\n```\n\n**Challenges**: Need track visit counts\n\n### 4. Thompson Sampling\n\n#### 4.1. Bayesian Approach\n\n**Idea**: Maintain distribution over Q-values\n\n**Algorithm**:\n```\nInitialize: Prior distribution P(θ)\n\nLặp:\n    Sample θ_t ~ P(θ|history)\n    Select a_t = argmax_a Q(s,a;θ_t)\n    Observe reward r_t\n    Update P(θ|history, r_t) bằng Bayes rule\n```\n\n#### 4.2. Beta-Bernoulli Thompson Sampling\n\n**For binary rewards**:\n```\nPrior: Beta(α_a, β_a) cho mỗi arm a\n\nSample: θ_a ~ Beta(α_a, β_a)\nChoose: a = argmax_a θ_a\n\nUpdate winner:\n    Success: α_a ← α_a + 1\n\n**Các khái niệm quan trọng:**\n- Multi-Armed Bandit Problem là một bài toán cổ điển trong Học tăng cường, trong đó tác nhân phải chọn giữa K máy đánh bạc (arms), mỗi máy có phân phối phần thưởng không xác định, với mục tiêu tối đa hóa tổng phần thưởng tích lũy theo thời gian. Bài toán này thường được sử dụng để nghiên cứu các chiến lược khám phá-khai thác.\n- Thompson Sampling là một kỹ thuật khám phá (exploration) dựa trên phương pháp xác suất Bayesian, được sử dụng trong Học tăng cường, đặc biệt hiệu quả trong các bài toán Multi-armed Bandit và Recommendation Systems. Thay vì chọn hành động tốt nhất dựa trên ước tính điểm, kỹ thuật này duy trì một phân phối xác suất hậu nghiệm trên các giá trị hành động (hoặc Q-value/tham số mô hình) và lấy mẫu một hành động từ phân phối này. Việc lựa chọn hành động dựa trên xác suất mà hành động đó là tối ưu, phản ánh sự không chắc chắn về giá trị thực của chúng. Điều này cho phép Thompson Sampling tự nhiên cân bằng giữa khám phá và khai thác.\n\n**Mối quan hệ:**\n- Thompson Sampling sử dụng Prior distribution P(θ) để khởi tạo niềm tin về các tham số.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n- Dễ overfit nếu không tune cẩn thận\n- Chậm hơn Random Forest (sequential)\n- Khó tune (nhiều hyperparameters)\n\n**3. AdaBoost (Adaptive Boosting):**\n\n**Nguyên lý:**\n- Tăng trọng số cho mẫu bị misclassified\n- Mỗi cây tập trung vào mẫu khó\n- Trọng số cho mô hình dựa trên accuracy\n\n**Ưu điểm:**\n- Đơn giản, hiệu quả\n- Ít tham số hơn Gradient Boosting\n- Tốt cho binary classification\n\n**Nhược điểm:**\n- Nhạy cảm với noise và outliers\n- Có thể overfit\n\n### Điều Chỉnh Hyperparameters\n\n**Tham số quan trọng:**\n\n**1. max_depth (Độ sâu tối đa):**\n- Giá trị nhỏ: Underfitting, mô hình đơn giản\n- Giá trị lớn: Overfitting, mô hình phức tạp\n- Thường: 3-10 cho cây đơn, 5-20 cho ensemble\n\n**2. min_samples_split:**\n- Số mẫu tối thiểu để split nút\n- Tăng lên: Giảm overfitting, cây đơn giản hơn\n- Thường: 2-100\n\n**3. min_samples_leaf:**\n- Số mẫu tối thiểu trong mỗi lá\n- Làm mượt decision boundary\n- Thường: 1-50\n\n**4. max_features:**\n- Số đặc trưng xem xét cho mỗi split\n- 'auto'/'sqrt': √n (cho classification)\n- 'log2': log₂(n)\n- None: Tất cả đặc trưng\n\n**5. criterion:**\n- 'gini': Gini impurity (mặc định, nhanh)\n- 'entropy': Information gain (chậm hơn)\n- 'squared_error': Cho regression\n\n**6. splitter:**\n- 'best': Chọn phân chia tốt nhất (mặc định)\n- 'random': Chọn ngẫu nhiên (nhanh hơn, thêm randomness)\n\n**Strategies cho Tuning:**\n- **Grid Search:** Thử tất cả combinations\n- **Random Search:** Sample ngẫu nhiên, hiệu quả hơn\n- **Bayesian Optimization:** Thông minh, ít iterations\n- **Cross-validation:** Luôn dùng CV để đánh giá\n\n**Tips:**\n- Bắt đầu với default parameters\n- Tune max_depth trước\n- Sau đó min_samples_split và min_samples_leaf\n- Cuối cùng các tham số khác\n- Monitor training vs validation performance\n\n### Ứng Dụng Thực Tế\n\n**1. Chẩn Đoán Y Tế:**\n- Chuỗi quyết định dựa trên triệu chứng\n- Dự đoán bệnh từ test results\n- Giải thích dễ dàng cho bác sĩ\n\n**2. Đánh Giá Rủi Ro Tín Dụng:**\n- Quyết định cho vay\n- Dự đoán default risk\n- Tuân thủ quy định (interpretability)\n\n**3. Dự Đoán Customer Churn:**\n- Xác định khách hàng có khả năng rời đi\n- Hành động marketing có mục tiêu\n- Hiểu lý do churn\n\n**4. Phát Hiện Gian Lận:**\n- Phát hiện transactions đáng ngờ\n- Real-time scoring\n- Giải thích cho investigation team\n\n**5. Feature Selection:**\n- Xác định đặc trưng quan trọng\n- Giảm dimensionality\n- Chuẩn bị cho mô hình khác\n\n**6. Hệ Thống Gợi Ý:**\n- Quyết định sản phẩm recommend\n- Personalization rules\n\n**Các khái niệm quan trọng:**\n- Bayesian Optimization là một thuật toán/kỹ thuật tối ưu hóa siêu tham số (hyperparameter) thông minh và thích ứng, sử dụng một mô hình xác suất (surrogate model) để hướng dẫn quá trình tìm kiếm. Nó xây dựng mô hình xác suất của hàm mục tiêu (ví dụ: hiệu suất mô hình trên tập validation) và học hỏi từ các đánh giá trước đó để cân bằng giữa khám phá (exploration) và khai thác (exploitation). Kỹ thuật này sử dụng các hàm thu nhận (acquisition function) như Expected Improvement (EI), Probability of Improvement (PI), hoặc Upper Confidence Bound (UCB) để quyết định điểm tiếp theo cần thử. Bayesian Optimization yêu cầu ít đánh giá/lần lặp hơn đáng kể so với Grid Search và Randomized Search, làm cho nó ít tốn kém về mặt tính toán và đặc biệt hữu ích khi việc đánh giá một tập siêu tham số tốn kém. Nó có khả năng xử lý cả tham số liên tục và phân loại. Các công cụ phổ biến triển khai Bayesian Optimization bao gồm Hyperopt, Optuna và Scikit-Optimize (skopt).\n\n**Mối quan hệ:**\n- Bayesian Optimization sử dụng công cụ Hyperopt để triển khai quá trình tối ưu hóa siêu tham số một cách hiệu quả.\n- Bayesian Optimization cải thiện hiệu quả tìm kiếm siêu tham số so với Grid Search bằng cách sử dụng mô hình xác suất và ít lần lặp hơn.\n- Bayesian Optimization sử dụng công cụ BayesSearchCV để thực hiện tìm kiếm siêu tham số hiệu quả hơn so với các phương pháp truyền thống.\n- Bayesian Optimization là một chiến lược áp dụng để điều chỉnh Hyperparameters một cách thông minh hơn.\n- Bayesian Optimization sử dụng công cụ Optuna để triển khai quá trình tối ưu hóa siêu tham số một cách linh hoạt và hiện đại.\n- Bayesian Optimization sử dụng công cụ Scikit-Optimize (skopt) để triển khai quá trình tối ưu hóa siêu tham số.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nB, 1 (terminal)\nB, 1 (terminal)\nB, 1 (terminal)\nB, 0 (terminal)\n```\n\n**MC Estimate**:\n- V(A) = 0 (chỉ có 1 episode từ A, return = 0)\n- V(B) = 5/6 ≈ 0.83\n\n**TD Estimate**:\n- V(B) = 5/6 (từ data)\n- V(A) = 5/6 (vì 100% chuyển đến B)\n\n**TD tốt hơn**: Exploit structure, generalize better\n\n### 5. Unified View: TD(λ)\n\n#### 5.1. n-Step TD\n\n**Ý tưởng**: Thay vì bootstrap sau 1 bước, sử dụng n bước\n\n**n-Step Return**:\n```\nG_t^(n) = R_{t+1} + γR_{t+2} + ... + γ^{n-1}R_{t+n} + γ^n V(S_{t+n})\n```\n\n**Update**:\n```\nV(S_t) ← V(S_t) + α[G_t^(n) - V(S_t)]\n```\n\n**Đặc điểm**:\n- n=1: TD(0)\n- n=∞: Monte Carlo\n- n trung gian: Cân bằng bias và variance\n\n#### 5.2. TD(λ) - Eligibility Traces\n\n**Motivation**: Thay vì chọn một n, kết hợp tất cả n-step returns\n\n**λ-Return**:\n```\nG_t^λ = (1-λ) Σ_{n=1}^∞ λ^{n-1} G_t^(n)\n```\n\n**Eligibility Trace**:\n```\nE_0(s) = 0\nE_t(s) = γλE_{t-1}(s) + 1(S_t = s)\n```\n\n**TD(λ) Update**:\n```\nδ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\nV(s) ← V(s) + αδ_t E_t(s), ∀s\n```\n\n#### 5.3. Forward View vs Backward View\n\n**Forward View**:\n- Nhìn về tương lai\n- Tính G_t^λ từ future returns\n- Offline algorithm\n\n**Backward View**:\n- Nhìn về quá khứ\n- Sử dụng eligibility traces\n- Online algorithm\n- Computationally efficient\n\n**Quan hệ**: Hai view tương đương về mặt toán học\n\n#### 5.4. Chọn λ\n\n| λ | Đặc điểm | Khi nào dùng |\n|---|----------|--------------|\n| 0 | TD(0), low variance | Môi trường noisy |\n| 0.5-0.9 | Cân bằng | Thường dùng nhất |\n| 1 | MC, unbiased | Episodic, deterministic |\n\n### 6. Các biến thể TD nâng cao\n\n#### 6.1. Double Learning\n\n**Vấn đề**: Maximization bias trong TD\n```\nOverestimation: E[max(X₁, X₂)] ≥ max(E[X₁], E[X₂])\n```\n\n**Giải pháp**: Duy trì 2 value functions\n```\nV₁(S) và V₂(S)\n\nUpdate V₁:\n    V₁(S) ← V₁(S) + α[R + γV₂(S') - V₁(S)]\n    \nUpdate V₂:\n    V₂(S) ← V₂(S) + α[R + γV₁(S') - V₂(S)]\n```\n\n#### 6.2. Gradient TD\n\n**Motivation**: TD không theo gradient descent thực sự\n\n**True Gradient TD (TDC)**:\n```\nMinimize: MSBE = ||V - Π_T^π V||²\n\nUpdate:\n    w ← w + α(R + γV(S') - V(S))∇V(S)\n    - αγ(∇V(S'))⊤w ∇V(S)\n```\n\n**Lợi ích**: Hội tụ vững vàng hơn với function approximation\n\n### 7. Ứng dụng thực tế\n\n\n**Các khái niệm quan trọng:**\n- Maximization bias là một vấn đề trong các thuật toán học tăng cường (RL), đặc biệt là những thuật toán sử dụng phép toán max để chọn hành động tốt nhất hoặc ước lượng giá trị tối ưu (ví dụ: Q-Learning). Vấn đề này dẫn đến việc ước lượng quá mức (overestimation) giá trị của các hành động hoặc các giá trị Q-function. Nguyên nhân là do bất đẳng thức E[max(X₁, X₂)] ≥ max(E[X₁], E[X₂]), nghĩa là giá trị kỳ vọng của giá trị tối đa luôn lớn hơn hoặc bằng giá trị tối đa của các giá trị kỳ vọng. Double Learning là một giải pháp được đề xuất để khắc phục vấn đề này.\n\n**Mối quan hệ:**\n- Double Learning giải quyết vấn đề maximization bias bằng cách duy trì hai hàm giá trị độc lập để tách biệt quá trình chọn và ước lượng hành động.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n#### 7.1. Game Playing\n\n**TD-Gammon** (1992):\n- Backgammon AI\n- Sử dụng TD(λ) với neural network\n- Self-play\n- Đạt world-champion level\n\n**AlphaGo Zero**:\n- Sử dụng TD-style updates\n- Self-play + MCTS\n- Không cần human knowledge\n\n#### 7.2. Robot Navigation\n\n**Task**: Robot tìm đường trong môi trường chưa biết\n- State: Vị trí robot\n- Action: Di chuyển\n- Reward: -1 mỗi bước, +100 khi đến đích\n\n**Ưu điểm TD**:\n- Học online trong quá trình điều hướng\n- Không cần đợi đến đích mới cập nhật\n- Adapt với môi trường thay đổi\n\n#### 7.3. Resource Management\n\n**Elevator Control**:\n- State: Vị trí thang máy, yêu cầu chờ\n- Action: Lên/xuống/đứng yên\n- Reward: -1 × tổng thời gian chờ\n\n**TD Learning**:\n- Học value function cho mỗi trạng thái\n- Online learning từ hoạt động hàng ngày\n- Cải thiện liên tục\n\n### 8. Phân tích lý thuyết\n\n#### 8.1. Tốc độ hội tụ\n\n**Monte Carlo**:\n```\nV_k(s) → V^π(s) với rate O(1/√k)\nk: số episodes\n```\n\n**TD(0)**:\n```\nV_k(s) → V^π(s) nhanh hơn trong thực tế\nKhông có bound lý thuyết chặt chẽ\n```\n\n**Thực nghiệm**: TD thường nhanh hơn MC 2-10 lần\n\n#### 8.2. Điều kiện hội tụ\n\n**Robbins-Monro conditions** cho learning rate α_t:\n```\nΣ_{t=1}^∞ α_t = ∞     (đảm bảo hội tụ)\nΣ_{t=1}^∞ α_t² < ∞    (đảm bảo variance hội tụ về 0)\n```\n\n**Ví dụ**:\n- α_t = 1/t: Thỏa mãn\n- α_t = 0.01: Không thỏa mãn điều kiện 1, nhưng practical\n\n#### 8.3. Bias-Variance Tradeoff\n\n**Monte Carlo**:\n- Bias = 0 (ước lượng không chệch)\n- Variance cao (phụ thuộc vào toàn bộ trajectory)\n\n**TD(0)**:\n- Bias > 0 (phụ thuộc vào V hiện tại)\n- Variance thấp (chỉ phụ thuộc 1 bước)\n\n**n-Step TD**: Cân bằng\n```\nBias giảm khi n tăng\nVariance tăng khi n tăng\n```\n\n### 9. So sánh tổng hợp\n\n#### 9.1. Bảng so sánh đầy đủ\n\n| Tiêu chí | MC | TD(0) | TD(λ) | DP |\n|----------|----|----|-------|-----|\n| Model-free | ✓ | ✓ | ✓ | ✗ |\n| Bootstrap | ✗ | ✓ | ✓ | ✓ |\n| Online | ✗ | ✓ | ✓ | ✓ |\n| Episodic only | ✓ | ✗ | ✗ | ✗ |\n| Bias | Low | High | Medium | Low |\n| Variance | High | Low | Medium | Low |\n| Convergence | Slow | Fast | Medium | Fastest |\n\n#### 9.2. Khi nào dùng phương pháp nào?\n\n**Monte Carlo**:\n- Môi trường không Markov\n- Cần ước lượng unbiased\n- Episodic tasks ngắn\n\n**TD(0)**:\n- Môi trường Markov\n- Continuing tasks\n\n**Các khái niệm quan trọng:**\n- Bias-Variance Tradeoff là một khái niệm quan trọng trong học máy và các thuật toán ước lượng, mô tả sự đánh đổi giữa lỗi do giả định sai (bias) và lỗi do sự nhạy cảm với các biến động nhỏ trong tập huấn luyện (variance). Trong học tăng cường (RL), các phương pháp như Monte Carlo có bias thấp nhưng variance cao, trong khi TD(0) có bias cao hơn nhưng variance thấp hơn. Các phương pháp như n-Step TD và TD(λ) được thiết kế để cân bằng giữa hai yếu tố này.\n\n**Mối quan hệ:**\n- n-Step TD cân bằng giữa bias và variance."
        },
        "output_without_context": {
            "question": "Trong bối cảnh Bayesian Bandits, khi nào thì việc sử dụng kiến thức trước là cần thiết để tối ưu hóa quyết định trong các tình huống không chắc chắn?",
            "answer": "Khi có thông tin trước về phân phối của các biến liên quan đến quyết định."
        }
    },
    {
        "topic_description": "Khám phá giá trị của thông tin trong quá trình ra quyết định là trọng tâm của chủ đề này. Học sinh cần phân tích tác động của khám phá và khai thác trong các MDP và Bandits, kết nối các khái niệm từ tuần 1 đến tuần 8.",
        "output_with_context": {
            "question": "Trong bối cảnh của quá trình ra quyết định Markov (MDP), giá trị thông tin được tối đa hóa bằng cách nào để đảm bảo tác nhân có thể kiểm soát tương lai tốt hơn?",
            "answer": "Bằng cách tối đa hóa thông tin thu được từ các hành động thông qua công thức Empowerment, I(A_t; S_{t+k} | S_t).",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n    Failure: β_a ← β_a + 1\n```\n\n**Properties**:\n- Optimal regret bounds\n- Often better than UCB trong practice\n\n### 5. Count-Based Exploration\n\n#### 5.1. Exploration Bonuses\n\n**Idea**: Add bonus reward for visiting rare states\n\n**Intrinsic Motivation**:\n```\nr_total = r_extrinsic + β · r_intrinsic\n\nr_intrinsic = 1/√N(s)  hoặc  1/√N(s,a)\n```\n\n**Effect**: Encourage visiting under-explored regions\n\n#### 5.2. Pseudo-Count Methods\n\n**For large state spaces**: Cannot count exactly\n\n**Density Models**:\n```\nρ(s): Estimate density/frequency of state s\nPseudo-count: N(s) ∝ ρ(s) / (1 - ρ(s))\nBonus: r_intrinsic = β/√N(s)\n```\n\n**Examples**:\n- Context Tree Switching (CTS)\n- Neural density models\n\n### 6. Curiosity-Driven Exploration\n\n#### 6.1. Intrinsic Curiosity Module (ICM)\n\n**Components**:\n```\n1. Forward Model: Predict next state feature\n   ŝ_{t+1} = f(s_t, a_t)\n\n2. Inverse Model: Predict action from states\n   â_t = g(s_t, s_{t+1})\n\n3. Intrinsic Reward: Prediction error\n   r_intrinsic = ||ŝ_{t+1} - s_{t+1}||²\n```\n\n**Intuition**: States that are hard to predict are \"interesting\"\n\n#### 6.2. Random Network Distillation (RND)\n\n**Setup**:\n```\nFixed random network: f_target(s)\nLearned predictor: f_pred(s; θ)\n```\n\n**Intrinsic Reward**:\n```\nr_intrinsic = ||f_target(s) - f_pred(s; θ)||²\n```\n\n**Properties**:\n- Novel states have high prediction error\n- Visited states have low error\n- Non-stationary targets avoided\n\n#### 6.3. Never Give Up (NGU)\n\n**Combines**:\n- Episodic novelty (memory-based)\n- Life-long novelty (RND-based)\n\n**Two-timescale curiosity**:\n```\nr_episodic: Within episode novelty\nr_lifelong: Across episodes novelty\n```\n\n### 7. Information-Theoretic Exploration\n\n#### 7.1. Information Gain\n\n**Maximum Information Gain**:\n```\nMaximize: I(Θ; O | a) = H(Θ) - H(Θ|O,a)\nΘ: Parameters/state of world\nO: Observations\n```\n\n**Intuition**: Choose actions that reveal most information\n\n#### 7.2. Entropy Maximization\n\n**Maximum Entropy RL** (covered in SAC):\n```\nπ* = argmax_π E[Σ_t (R_t + α H(π(·|S_t)))]\n```\n\n**Benefits**:\n- Natural exploration\n- Robust policies\n- Multiple solutions\n\n#### 7.3. Empowerment\n\n**Definition**: Mutual information between actions và future states\n```\nEmpowerment = I(A_t; S_{t+k} | S_t)\n```\n\n**Intuition**: Maximize control over future\n\n### 8. Goal-Driven Exploration\n\n#### 8.1. Hindsight Experience Replay (HER)\n\n**Problem**: Sparse rewards → most episodes fail → little learning\n\n**Idea**: Learn from failures by relabeling goals\n```\nOriginal: Goal = g, achieved = g', reward = 0 (failure)\nHER: Goal = g', achieved = g', reward = 0 (success!)\n```\n\n**Algorithm**:\n```\nStore transition (s, a, r, s', g) vào replay buffer\n\nAdditionally store:\n    (s, a, r', s', g') where g' = achieved_goal(s')\n    r' = reward(s', a, g')\n```\n\n**Effect**: Every trajectory teaches something\n\n#### 8.2. Curriculum Learning\n\n**Progressive Difficulty**:\n```\nEasy tasks → Medium tasks → Hard tasks\n```\n\n**Automatic Curriculum**:\n- Track success rates\n- Adjust task distribution\n- Focus on \"frontier\" of capability\n\n### 9. Multi-Agent Exploration\n\n#### 9.1. Population-Based Training\n\n**Idea**: Train population of agents với different hyperparameters\n\n**Process**:\n```\nPopulation: {Agent_1, ..., Agent_N}\n\nPeriodically:\n\n**Các khái niệm quan trọng:**\n- Information-Theoretic Exploration là một nhóm các kỹ thuật khám phá trong RL dựa trên lý thuyết thông tin, nhằm mục đích tối đa hóa lượng thông tin thu được về môi trường hoặc tối đa hóa entropy của chính sách để khuyến khích sự đa dạng trong hành vi.\n\n**Mối quan hệ:**\n- Information-Theoretic Exploration có thể sử dụng Empowerment để tối đa hóa khả năng kiểm soát của agent đối với tương lai.\n- Information-Theoretic Exploration sử dụng Entropy Maximization để khuyến khích khám phá tự nhiên và tạo chính sách mạnh mẽ.\n- Information-Theoretic Exploration sử dụng Information Gain để chọn hành động tiết lộ nhiều thông tin nhất.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n\n### 1. Giới thiệu về Exploration-Exploitation Dilemma\n\n#### 1.1. Định nghĩa\n\n**Exploitation**: Chọn actions mà agent tin là tốt nhất (maximize immediate reward)\n**Exploration**: Thử actions mới để discover potentially better options\n\n**Dilemma**: Làm sao cân bằng?\n```\nPure Exploitation: Có thể stuck tại suboptimal policy\nPure Exploration: Không bao giờ maximize rewards\n```\n\n#### 1.2. Tại sao quan trọng?\n\n- **Learning**: Cần explore để learn về environment\n- **Optimization**: Cần exploit để maximize returns\n- **Trade-off**: Exploration cost short-term reward for long-term gain\n\n#### 1.3. Multi-Armed Bandit Problem\n\n**Setup**: K slot machines (arms), mỗi arm có unknown reward distribution\n\n**Goal**: Maximize cumulative reward over time\n\n**Regret**: \n```\nRegret = Σ_t (V* - V(a_t))\nV* = Value của best arm\n```\n\n### 2. Exploration Strategies\n\n#### 2.1. ε-Greedy\n\n**Strategy**:\n```\nVới xác suất ε: Chọn random action\nVới xác suất 1-ε: Chọn best known action\n```\n\n**Variants**:\n```\n# Fixed\nε = 0.1\n\n# Decay\nε_t = ε_0 / t\nε_t = ε_min + (ε_max - ε_min) * exp(-decay * t)\n\n# Adaptive\nTăng ε khi performance kém\nGiảm ε khi performance tốt\n```\n\n**Pros**: Simple, effective\n**Cons**: Random exploration không efficient\n\n#### 2.2. Softmax / Boltzmann Exploration\n\n**Strategy**: Sample proportional to estimated values\n```\nπ(a|s) = exp(Q(s,a)/τ) / Σ_b exp(Q(s,b)/τ)\n```\n\n**Temperature τ**:\n- τ → 0: Greedy (exploitation)\n- τ → ∞: Uniform random (exploration)\n- τ decay over time\n\n**Pros**: Probabilistic, smooth\n**Cons**: Need tune τ\n\n#### 2.3. Optimistic Initialization\n\n**Idea**: Initialize Q-values optimistically (higher than true values)\n```\nQ(s,a) = Q_max  (instead of 0)\n```\n\n**Effect**: Agent naturally explores unvisited (s,a) pairs\n\n**Pros**: Simple, no hyperparameters\n**Cons**: Temporary effect only\n\n### 3. Upper Confidence Bound (UCB)\n\n#### 3.1. UCB Algorithm\n\n**Principle**: \"Optimism in face of uncertainty\"\n\n**UCB Formula**:\n```\nUCB(a) = Q̄(a) + c√(ln t / N(a))\n         ↑ Exploitation  ↑ Exploration\n\nQ̄(a): Average reward của arm a\nN(a): Number of times arm a played\nt: Total time steps\nc: Exploration parameter\n```\n\n**Action Selection**:\n```\na_t = argmax_a UCB(a)\n```\n\n**Properties**:\n- Logarithmic regret: O(log t)\n- Optimal trong bandit setting\n\n#### 3.2. UCB trong RL\n\n**UCB applied to Q-Learning**:\n```\nQ_UCB(s,a) = Q(s,a) + c√(ln N(s) / N(s,a))\n\nSelect: a = argmax_a Q_UCB(s,a)\n```\n\n**Challenges**: Need track visit counts\n\n### 4. Thompson Sampling\n\n#### 4.1. Bayesian Approach\n\n**Idea**: Maintain distribution over Q-values\n\n**Algorithm**:\n```\nInitialize: Prior distribution P(θ)\n\nLặp:\n    Sample θ_t ~ P(θ|history)\n    Select a_t = argmax_a Q(s,a;θ_t)\n    Observe reward r_t\n    Update P(θ|history, r_t) bằng Bayes rule\n```\n\n#### 4.2. Beta-Bernoulli Thompson Sampling\n\n**For binary rewards**:\n```\nPrior: Beta(α_a, β_a) cho mỗi arm a\n\nSample: θ_a ~ Beta(α_a, β_a)\nChoose: a = argmax_a θ_a\n\nUpdate winner:\n    Success: α_a ← α_a + 1\n\n**Các khái niệm quan trọng:**\n- Exploration-Exploitation Dilemma là bài toán cân bằng giữa việc chọn các hành động đã biết là tốt nhất (exploitation) và thử các hành động mới để tìm kiếm các lựa chọn tốt hơn (exploration). Việc cân bằng này rất quan trọng để tác nhân có thể học hỏi hiệu quả và tối đa hóa tổng phần thưởng dài hạn.\n- Exploration là hành động thử các hành động mới để khám phá các lựa chọn tiềm năng tốt hơn trong môi trường. Đây là một phần của sự cân bằng giữa khám phá và khai thác, cần thiết để tác nhân học hỏi về môi trường và tránh mắc kẹt ở các chính sách dưới tối ưu.\n- Exploitation là hành động chọn các hành động mà tác nhân tin là tốt nhất để tối đa hóa phần thưởng tức thời. Đây là một phần của sự cân bằng giữa khám phá và khai thác trong Học tăng cường, tập trung vào việc tận dụng kiến thức hiện có.\n\n**Mối quan hệ:**\n- Upper Confidence Bound (UCB) giải quyết Exploration-Exploitation Dilemma bằng cách cân bằng giữa khai thác và khám phá dựa trên sự không chắc chắn.\n- ε-Greedy là một chiến lược giải quyết Exploration-Exploitation Dilemma bằng cách cân bằng giữa chọn hành động ngẫu nhiên và hành động tốt nhất đã biết.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n\n### 1. Giới thiệu về MDP\n\nQuá trình Quyết định Markov (Markov Decision Process - MDP) là một framework toán học để mô hình hóa việc ra quyết định trong các tình huống mà kết quả có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường.\n\n### 2. Các thành phần cơ bản của MDP\n\nMột MDP được định nghĩa bởi bộ năm phần tử (S, A, P, R, γ):\n\n#### 2.1. Tập trạng thái (State Space - S)\n- **Định nghĩa**: Tập hợp tất cả các trạng thái có thể mà agent có thể gặp trong môi trường\n- **Ký hiệu**: S = {s₁, s₂, ..., sₙ}\n- **Ví dụ**: Trong game cờ vua, mỗi trạng thái là một cấu hình cụ thể của bàn cờ\n\n#### 2.2. Tập hành động (Action Space - A)\n- **Định nghĩa**: Tập hợp tất cả các hành động mà agent có thể thực hiện\n- **Ký hiệu**: A = {a₁, a₂, ..., aₘ}\n- **Phân loại**:\n  - Không gian hành động rời rạc: Số lượng hành động hữu hạn\n  - Không gian hành động liên tục: Hành động có thể nhận giá trị trong một khoảng liên tục\n\n#### 2.3. Hàm chuyển trạng thái (State Transition Function - P)\n- **Định nghĩa**: Xác suất chuyển từ trạng thái s sang trạng thái s' khi thực hiện hành động a\n- **Công thức**: P(s'|s,a) = P[Sₜ₊₁ = s' | Sₜ = s, Aₜ = a]\n- **Tính chất Markov**: Trạng thái tương lai chỉ phụ thuộc vào trạng thái hiện tại, không phụ thuộc vào lịch sử\n\n#### 2.4. Hàm phần thưởng (Reward Function - R)\n- **Định nghĩa**: Phần thưởng nhận được khi thực hiện hành động a từ trạng thái s\n- **Công thức**: R(s,a) hoặc R(s,a,s')\n- **Mục đích**: Định hướng agent học hành vi tối ưu\n\n#### 2.5. Hệ số chiết khấu (Discount Factor - γ)\n- **Định nghĩa**: Hệ số để cân bằng giữa phần thưởng tức thời và phần thưởng dài hạn\n- **Giá trị**: 0 ≤ γ ≤ 1\n- **Ý nghĩa**:\n  - γ = 0: Chỉ quan tâm đến phần thưởng tức thời\n  - γ = 1: Phần thưởng tương lai có giá trị bằng phần thưởng hiện tại\n  - 0 < γ < 1: Phần thưởng tương lai bị chiết khấu\n\n### 3. Tính chất Markov\n\n#### 3.1. Định nghĩa\n\n**Các khái niệm quan trọng:**\n- Markov Decision Process (MDP) là một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự, nơi một agent tương tác với môi trường để tối đa hóa tổng phần thưởng chiết khấu. Trong MDP, kết quả của các quyết định có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường (Reinforcement Learning) và được định nghĩa bởi các thành phần: tập trạng thái S, tập hành động A, hàm phần thưởng R, xác suất chuyển trạng thái P, và discount factor γ.\n\n**Mối quan hệ:**\n- Một Markov Decision Process yêu cầu tập hành động A để định nghĩa các hành động mà agent có thể thực hiện.\n- Một Markov Decision Process yêu cầu tập trạng thái S để định nghĩa các trạng thái mà agent có thể gặp.\n- Một Markov Decision Process yêu cầu hàm phần thưởng R để định hướng agent học hành vi tối ưu."
        },
        "output_without_context": {
            "question": "Trong quy trình ra quyết định, giá trị của thông tin được đánh giá như thế nào trong các MDP và Bandits?",
            "answer": "Giá trị của thông tin được đánh giá qua khả năng cải thiện quyết định và tối ưu hóa kết quả."
        }
    }
]