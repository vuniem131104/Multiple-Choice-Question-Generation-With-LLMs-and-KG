[
    {
        "topic_description": "Chủ đề này tập trung vào khái niệm Học tăng cường không mô hình và cách nó được sử dụng để ước tính hàm giá trị hoặc tối ưu hóa hàm giá trị trong một MDP mà không cần biết trước về các chuyển đổi hoặc phần thưởng. Học sinh sẽ được kiểm tra về định nghĩa và ứng dụng của phương pháp này trong các tình huống thực tế.",
        "output_with_context": {
            "question": "Học tăng cường không mô hình là gì trong bối cảnh của việc ước tính hàm giá trị mà không cần biết trước về các chuyển đổi hoặc phần thưởng?",
            "answer": "Học tăng cường không mô hình là phương pháp ước tính hàm giá trị hoặc tối ưu hóa hàm giá trị trong một MDP mà không cần biết trước mô hình môi trường.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nPhần tiếp theo sẽ mở rộng sang **Model-Free Control**: Không chỉ đánh giá mà còn tìm chính sách tối ưu mà không cần mô hình!\n\n---\n\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Control\n\n#### 1.1. Định nghĩa bài toán\n**Control Problem**: Tìm chính sách tối ưu π* mà không biết trước mô hình môi trường (P, R)\n\n**So sánh với Prediction**:\n- Prediction: Đánh giá V^π cho π cho trước\n- Control: Tối ưu hóa π để maximize V^π\n\n#### 1.2. Thách thức\n- Không biết mô hình → không thể dùng Dynamic Programming\n- Phải học từ interaction với môi trường\n- Cần cân bằng exploration và exploitation\n\n#### 1.3. Ý tưởng chính\nSử dụng **Generalized Policy Iteration (GPI)** framework:\n```\nPolicy Evaluation (Model-Free) → Policy Improvement → Repeat\n```\n\n### 2. Monte Carlo Control\n\n#### 2.1. Từ V(s) sang Q(s,a)\n\n**Vấn đề với V(s)**:\n```\nPolicy Improvement cần:\nπ'(s) = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n                 ↑ Cần biết mô hình!\n```\n\n**Giải pháp**: Sử dụng Q(s,a)\n```\nπ'(s) = argmax_a Q(s,a)  ← Không cần mô hình!\n```\n\n#### 2.2. Monte Carlo Policy Iteration\n\n**Thuật toán**:\n```\n1. Khởi tạo:\n   Q(s,a) = 0, ∀s,a\n   π = chính sách khởi tạo\n\n2. Lặp:\n   a) Policy Evaluation (MC):\n      - Tạo nhiều episodes theo π\n      - Cập nhật Q^π(s,a) bằng MC\n   \n   b) Policy Improvement:\n      π(s) = argmax_a Q(s,a), ∀s\n```\n\n#### 2.3. Vấn đề Exploration\n\n**Greedy Policy**:\n```\nπ(s) = argmax_a Q(s,a)\n```\n→ Chỉ exploit, không explore → Có thể bỏ lỡ chính sách tốt hơn\n\n**Giải pháp 1: ε-Greedy Policy**\n```\nπ(a|s) = {\n    1 - ε + ε/|A|,  nếu a = argmax Q(s,a)\n    ε/|A|,          ngược lại\n}\n```\n\n**Đặc điểm**:\n- Xác suất 1-ε: Chọn hành động tốt nhất (exploit)\n- Xác suất ε: Chọn ngẫu nhiên (explore)\n- ε decay theo thời gian: Explore nhiều lúc đầu, exploit nhiều sau\n\n#### 2.4. ε-Greedy Monte Carlo Control\n\n**Thuật toán**:\n```\nKhởi tạo:\n    Q(s,a) arbitrarily, ∀s,a\n    π = ε-greedy policy dựa trên Q\n    Returns(s,a) = empty list, ∀s,a\n\nLặp forever:\n    1. Tạo episode theo π:\n       S_0, A_0, R_1, ..., S_T\n    \n    2. Với mỗi cặp (s,a) xuất hiện trong episode:\n       G = return sau lần xuất hiện đầu tiên\n       Thêm G vào Returns(s,a)\n       Q(s,a) = average(Returns(s,a))\n    \n    3. Với mỗi s trong episode:\n       π(s) = ε-greedy(Q(s,·))\n```\n\n#### 2.5. Greedy in the Limit of Infinite Exploration (GLIE)\n\n\n**Các khái niệm quan trọng:**\n- Model-Free Methods (Các phương pháp không mô hình) là một loại thuật toán trong Reinforcement Learning (Học tăng cường) mà agent học trực tiếp từ kinh nghiệm tương tác với môi trường, không yêu cầu biết trước mô hình động học của môi trường (tức là không cần biết xác suất chuyển trạng thái P và hàm phần thưởng R). Các phương pháp này linh hoạt và thực tế, đặc biệt hữu ích khi mô hình môi trường phức tạp hoặc không thể biết trước, nhưng thường cần nhiều dữ liệu hơn so với Model-Based Learning.\n\nModel-Free Methods được chia thành hai nhánh chính:\n\n1.  **Model-Free Prediction**: Phương pháp này nhằm đánh giá hoặc ước lượng hàm giá trị (Value Function) của một chính sách mà không cần biết mô hình động học của môi trường. Nó học trực tiếp từ kinh nghiệm tương tác và thường được sử dụng khi cần học nhanh và học online. Các ví dụ bao gồm Monte Carlo (MC) và Temporal Difference (TD) Learning.\n\n2.  **Model-Free Control**: Phương pháp này cho phép agent tìm kiếm chính sách tối ưu (π*) mà không cần biết mô hình động lực học của môi trường. Khác với Model-Free Prediction, nó không chỉ đánh giá hàm giá trị mà còn tối ưu hóa chính sách để đạt được giá trị cao nhất. Agent học cách điều khiển trực tiếp từ kinh nghiệm tương tác. Các thuật toán như Q-Learning và SARSA (State-Action-Reward-State-Action) là ví dụ của Model-Free Control. Thách thức chính của Model-Free Control là phải học từ tương tác với môi trường và cân bằng giữa exploration (khám phá) và exploitation (khai thác).\n\n**Mối quan hệ:**\n- Model-Free Control cần cân bằng giữa exploration và exploitation để tìm chính sách tối ưu.\n- Model-Free Control giải quyết Control Problem bằng cách tìm chính sách tối ưu mà không cần biết mô hình môi trường.\n- Model-Free Control giải quyết bài toán Exploration-Exploitation Tradeoff thông qua các kỹ thuật như ε-greedy.\n- Model-Free Control sử dụng Q-function làm chìa khóa để cải thiện chính sách mà không cần biết mô hình môi trường.\n- Model-Free Control sử dụng khuôn khổ Generalized Policy Iteration (GPI) để lặp lại quá trình đánh giá và cải thiện chính sách.\n- Monte Carlo Control là một thuật toán thuộc nhóm Model-Free Control vì nó không yêu cầu mô hình môi trường.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n     = Σₛ d(s)(V^π(s) - V̂(s; w))²\n```\n- d(s): distribution của states dưới policy π\n\n**Mục tiêu**: Minimize J(w) = ||V^π - V̂_w||²_d\n\n### 3. Stochastic Gradient Descent (SGD)\n\n#### 3.1. Gradient Descent\n\n**Update Rule**:\n```\nw_{t+1} = w_t - (α/2)∇_w J(w_t)\n        = w_t + α E[(V^π(s) - V̂(s; w))∇_w V̂(s; w)]\n```\n\n**Stochastic Gradient Descent**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]∇_w V̂(S_t; w_t)\n```\n\n#### 3.2. Linear SGD\n\n**Gradient của linear function**:\n```\n∇_w V̂(s; w) = ∇_w(w^T φ(s)) = φ(s)\n```\n\n**Update rule**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]φ(S_t)\n```\n\n**Đặc điểm**:\n- Converge đến local optimum (global cho linear)\n- Learning rate α quan trọng\n- Simple và efficient\n\n#### 3.3. Feature Scaling\n\n**Vấn đề**: Features có scale khác nhau → học không ổn định\n\n**Giải pháp**:\n```\nNormalization: φᵢ = (φᵢ - μᵢ)/σᵢ\nStandardization: φᵢ ∈ [0, 1]\n```\n\n### 4. Incremental Prediction Methods\n\n#### 4.1. Monte Carlo với Function Approximation\n\n**Update**:\n```\nw ← w + α[G_t - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ Target: actual return\n```\n\n**Đặc điểm**:\n- Unbiased target\n- High variance\n- Non-stationary target (G_t khác nhau mỗi episode)\n\n#### 4.2. TD(0) với Function Approximation\n\n**Update**:\n```\nw ← w + α[R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ TD target\n```\n\n**Semi-gradient**: Không lấy gradient qua V̂(S_{t+1}; w)\n\n**Thuật toán Semi-gradient TD(0)**:\n```\nKhởi tạo w arbitrarily\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        A ← π(S)\n        Thực hiện A, quan sát R, S'\n        \n        w ← w + α[R + γV̂(S'; w) - V̂(S; w)]∇_w V̂(S; w)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 4.3. TD(λ) với Function Approximation\n\n**Eligibility Traces**:\n```\nz_0 = 0\nz_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n```\n\n**Update**:\n```\nδ_t = R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)\nw ← w + αδ_t z_t\n```\n\n**Accumulating vs Replacing traces**:\n- Accumulating: z_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n- Replacing: Phức tạp hơn, phụ thuộc feature type\n\n### 5. Control với Function Approximation\n\n#### 5.1. Action-Value Function Approximation\n\n**Parameterization**:\n```\nQ̂(s, a; w) = w^T φ(s, a)\n```\n\n\n**Các khái niệm quan trọng:**\n- Control với Function Approximation là việc mở rộng các phương pháp học tăng cường để tìm kiếm chính sách tối ưu khi sử dụng xấp xỉ hàm giá trị, đặc biệt là xấp xỉ hàm giá trị hành động Q̂(s, a; w). Mục tiêu là học một chính sách tối ưu mà không cần lưu trữ bảng Q-value cho mọi cặp (s,a).\n- Monte Carlo với Function Approximation là một phương pháp học tăng cường sử dụng kỹ thuật Monte Carlo để ước lượng hàm giá trị, kết hợp với xấp xỉ hàm giá trị. Nó cập nhật các tham số w dựa trên return thực tế G_t quan sát được từ một episode hoàn chỉnh. Công thức cập nhật là w ← w + α[G_t - V̂(S_t; w)]∇_w V̂(S_t; w). Đặc điểm của nó là mục tiêu (G_t) không thiên vị nhưng có phương sai cao và không dừng (non-stationary).\n\n**Mối quan hệ:**\n- Control với Function Approximation thường sử dụng Action-Value Function Approximation để học chính sách tối ưu.\n- Monte Carlo với Function Approximation sử dụng G_t làm mục tiêu để cập nhật hàm giá trị xấp xỉ.\n- Monte Carlo với Function Approximation cập nhật các tham số w dựa trên return thực tế G_t."
        },
        "output_without_context": {
            "question": "Học tăng cường không mô hình là gì?",
            "answer": "Phương pháp ước tính hàm giá trị hoặc tối ưu hóa hàm giá trị trong một MDP mà không cần biết trước về các chuyển đổi hoặc phần thưởng."
        }
    },
    {
        "topic_description": "Chủ đề này khám phá các phương pháp Monte-Carlo trong học tăng cường, sử dụng các ví dụ thực tế như Blackjack để minh họa cách ước tính hàm giá trị của các trạng thái khác nhau dựa trên kết quả. Học sinh sẽ cần hiểu và áp dụng các công thức liên quan đến phương pháp này.",
        "output_with_context": {
            "question": "Trong phương pháp Monte-Carlo, công thức nào được sử dụng để cập nhật giá trị V(S_t) cho trạng thái S_t sau khi nó xuất hiện lần đầu tiên trong episode?",
            "answer": "V(S_t) = average(Returns(S_t))",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n        Nếu S_t xuất hiện lần đầu tiên trong episode:\n            Thêm G vào Returns(S_t)\n            V(S_t) = average(Returns(S_t))\n```\n\n**Đặc điểm**:\n- Chỉ cập nhật cho lần xuất hiện đầu tiên của trạng thái\n- Không có bias\n- Đảm bảo hội tụ đến V^π(s) khi số episodes → ∞\n\n#### 2.4. Every-Visit Monte Carlo\n\n**Khác biệt**: Cập nhật cho mọi lần xuất hiện của trạng thái trong episode\n\n```\nVới mỗi bước t = T-1, T-2, ..., 0:\n    G = γG + R_{t+1}\n    Thêm G vào Returns(S_t)\n    V(S_t) = average(Returns(S_t))\n```\n\n**So sánh**:\n- Every-visit có variance thấp hơn\n- Cả hai đều hội tụ đến V^π(s)\n- Every-visit thường được sử dụng nhiều hơn\n\n#### 2.5. Incremental Mean Update\n\n**Vấn đề**: Lưu trữ tất cả returns không hiệu quả\n\n**Giải pháp**: Cập nhật incremental\n```\nCông thức tổng quát:\n    μ_k = μ_{k-1} + (1/k)(x_k - μ_{k-1})\n\nÁp dụng cho MC:\n    N(s) = N(s) + 1\n    V(s) = V(s) + (1/N(s))(G - V(s))\n    \nHoặc dùng learning rate α cố định:\n    V(s) = V(s) + α(G - V(s))\n```\n\n**Lợi ích**:\n- Tiết kiệm bộ nhớ\n- Cập nhật online\n- Quên dần các ước lượng cũ (với α cố định)\n\n#### 2.6. Ví dụ: Blackjack\n\n**Mô tả bài toán**:\n- **Trạng thái**: (tổng bài của người chơi, bài úp của dealer, có ace không)\n- **Hành động**: Hit (rút thêm) hoặc Stick (dừng)\n- **Phần thưởng**: +1 thắng, -1 thua, 0 hòa\n- **Chính sách**: Stick nếu tổng ≥ 20, ngược lại Hit\n\n**Ứng dụng MC**:\n1. Chơi nhiều games theo chính sách\n2. Ghi lại returns cho mỗi trạng thái\n3. Tính V^π bằng trung bình returns\n\n**Kết quả**:\n```\nV^π(20, ACE, usable_ace) ≈ 0.8  (rất tốt)\nV^π(12, 2, no_ace) ≈ -0.3       (tệ)\n```\n\n### 3. Temporal-Difference Learning (TD)\n\n#### 3.1. Giới thiệu\nTD Learning kết hợp ý tưởng của MC và DP:\n- Như MC: Học từ kinh nghiệm, không cần mô hình\n- Như DP: Bootstrap từ ước lượng hiện tại, không cần episode hoàn chỉnh\n\n#### 3.2. TD(0) - Temporal Difference cơ bản\n\n**TD Update Rule**:\n```\nV(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]\n```\n\n**Các thành phần**:\n- **TD Target**: R_{t+1} + γV(S_{t+1})\n- **TD Error**: δ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\n- **Update**: V(S_t) ← V(S_t) + α × δ_t\n\n#### 3.3. Thuật toán TD(0)\n\n```\n\n**Các khái niệm quan trọng:**\n- Blackjack là một bài toán ví dụ phổ biến trong Reinforcement Learning, được sử dụng để minh họa các thuật toán như Monte Carlo Control. Trong bài toán này, agent (người chơi) đưa ra các quyết định \"Hit\" (rút thêm bài) hoặc \"Stick\" (dừng). Các trạng thái được định nghĩa bởi tổng điểm hiện tại của người chơi, lá bài ngửa của nhà cái (dealer), và việc có \"usable ace\" (ace có thể sử dụng được) hay không. Mục tiêu của agent là đạt tổng điểm gần 21 nhất mà không vượt quá. Phần thưởng được định nghĩa là +1 khi thắng, -1 khi thua và 0 khi hòa.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n**Định nghĩa**: Một dãy chính sách {π_t} thỏa mãn GLIE nếu:\n1. Mọi cặp (s,a) được visit vô hạn lần\n2. Chính sách hội tụ đến greedy policy\n\n**Ví dụ GLIE**:\n```\nε_t = 1/t\n```\n\n**Định lý**: GLIE Monte Carlo Control hội tụ đến π*\n\n#### 2.6. Ví dụ: Blackjack với MC Control\n\n**Setup**:\n- State: (player_sum, dealer_card, usable_ace)\n- Action: Hit hoặc Stick\n- Reward: +1 (win), -1 (lose), 0 (draw)\n\n**Kết quả**:\n```\nChính sách học được:\n- Stick khi player_sum ≥ 20\n- Hit khi player_sum < 12\n- Phức tạp hơn ở vùng 12-19 (phụ thuộc dealer card)\n```\n\n### 3. On-Policy vs Off-Policy Learning\n\n#### 3.1. Định nghĩa\n\n**On-Policy**:\n- Học về chính sách π từ experience generated bởi π\n- Evaluate và improve cùng một chính sách\n- Ví dụ: SARSA, Monte Carlo Control\n\n**Off-Policy**:\n- Học về chính sách π (target) từ experience của μ (behavior)\n- π ≠ μ\n- Ví dụ: Q-Learning, Importance Sampling\n\n#### 3.2. So sánh\n\n| Đặc điểm | On-Policy | Off-Policy |\n|----------|-----------|------------|\n| Chính sách | Một chính sách | Hai chính sách |\n| Sample efficiency | Thấp hơn | Cao hơn |\n| Variance | Thấp | Cao |\n| Converge | Ổn định | Có thể diverge |\n| Use old data | Không | Có thể |\n| Learn optimal | Không (nếu ε-greedy) | Có |\n\n### 4. SARSA - On-Policy TD Control\n\n#### 4.1. Ý tưởng\nÁp dụng TD(0) cho Q(s,a) thay vì V(s)\n\n**TD Update cho Q**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n```\n\n**Tên gọi**: SARSA = (S, A, R, S', A')\n\n#### 4.2. Thuật toán SARSA\n\n```\nKhởi tạo Q(s,a) arbitrarily, ∀s ∈ S, a ∈ A\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    Chọn A từ S theo ε-greedy(Q)\n    \n    Lặp với mỗi bước:\n        Thực hiện A, quan sát R, S'\n        Chọn A' từ S' theo ε-greedy(Q)\n        \n        Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n        \n        S ← S'; A ← A'\n    cho đến S là terminal\n```\n\n#### 4.3. Tính chất của SARSA\n\n**Hội tụ**:\n- GLIE schedule + Robbins-Monro conditions → Q → Q*\n- Trong thực tế, dùng ε nhỏ cố định hoặc decay\n\n**On-Policy**:\n- Học về chính sách ε-greedy đang dùng\n- Safe: Tính đến exploration trong học\n\n#### 4.4. Ví dụ: Windy Gridworld\n\n**Mô tả**:\n- Grid 7×10 với \"wind\" ở một số cột\n- Wind đẩy agent lên 1-2 ô\n- Start: (3,0), Goal: (3,7)\n- Actions: 4 hướng\n\n**Kết quả SARSA**:\n\n**Các khái niệm quan trọng:**\n- Blackjack là một bài toán ví dụ phổ biến trong Reinforcement Learning, được sử dụng để minh họa các thuật toán như Monte Carlo Control. Trong bài toán này, agent (người chơi) đưa ra các quyết định \"Hit\" (rút thêm bài) hoặc \"Stick\" (dừng). Các trạng thái được định nghĩa bởi tổng điểm hiện tại của người chơi, lá bài ngửa của nhà cái (dealer), và việc có \"usable ace\" (ace có thể sử dụng được) hay không. Mục tiêu của agent là đạt tổng điểm gần 21 nhất mà không vượt quá. Phần thưởng được định nghĩa là +1 khi thắng, -1 khi thua và 0 khi hòa.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nKhởi tạo V(s) arbitrarily, ∀s ∈ S\n\nVới mỗi episode:\n    Khởi tạo S\n    \n    Lặp cho mỗi bước của episode:\n        A ← hành động từ S theo π\n        Thực hiện A, quan sát R, S'\n        \n        V(S) ← V(S) + α[R + γV(S') - V(S)]\n        \n        S ← S'\n    cho đến khi S là terminal\n```\n\n#### 3.4. So sánh MC vs TD(0)\n\n| Tiêu chí | Monte Carlo | TD(0) |\n|----------|-------------|-------|\n| Cập nhật | Cuối episode | Sau mỗi bước |\n| Bootstrap | Không | Có |\n| Target | G_t (actual return) | R + γV(S') |\n| Bias | Unbiased | Biased |\n| Variance | High variance | Low variance |\n| Hội tụ | Chậm | Nhanh hơn |\n| Môi trường | Cần episodic | Cả episodic và continuing |\n\n#### 3.5. Ưu điểm của TD\n\n✅ **Học online**: Cập nhật sau mỗi bước, không cần chờ episode kết thúc\n✅ **Continuing tasks**: Hoạt động với tasks không có điểm kết thúc\n✅ **Lower variance**: Bootstrap giảm variance so với MC\n✅ **Học nhanh hơn**: Thường hội tụ nhanh hơn MC trong thực tế\n✅ **Hiệu quả dữ liệu**: Sử dụng thông tin từ ước lượng hiện tại\n\n#### 3.6. Ví dụ minh họa: Random Walk\n\n**Setup**:\n```\nStates: [A] [B] [C] [D] [E]\nStart: C\nTerminal: Left of A hoặc Right of E\nReward: 0 mọi nơi, +1 khi đến Right of E\nAction: Left hoặc Right (random với p=0.5)\n```\n\n**True values**:\n```\nV^π(A) = 1/6, V^π(B) = 2/6, V^π(C) = 3/6,\nV^π(D) = 4/6, V^π(E) = 5/6\n```\n\n**So sánh MC vs TD**:\n- TD hội tụ nhanh hơn với cùng số episodes\n- TD ít sensitive với khởi tạo\n- MC có RMS error cao hơn trong giai đoạn đầu\n\n### 4. Batch Methods và Certainty Equivalence\n\n#### 4.1. Batch Learning\n**Ý tưởng**: Lặp lại huấn luyện trên cùng một batch experience cho đến hội tụ\n\n```\nCho trước batch experience: \n    {(S₁, A₁, R₁, S'₁), (S₂, A₂, R₂, S'₂), ...}\n\nLặp cho đến hội tụ:\n    Với mỗi experience (S, A, R, S'):\n        Cập nhật V(S) theo MC hoặc TD\n```\n\n#### 4.2. Certainty Equivalence\n\n**MC**: Tìm V^π minimize mean-squared error với observed returns\n```\nV^π = argmin_V Σ_episodes Σ_t (G_t - V(S_t))²\n```\n\n**TD**: Tìm V^π thỏa mãn phương trình Bellman cho MDP ước lượng\n```\nV^π(s) = E[R + γV^π(S') | s]\n```\n(ước lượng từ experience)\n\n**Kết quả**:\n- TD tận dụng cấu trúc Markov\n- MC chỉ minimize error, không exploit Markov property\n- TD thường hiệu quả hơn trong môi trường Markov\n\n#### 4.3. Ví dụ: AB Example\n\n**Experience**:\n```\nA, 0, B, 0\nB, 1 (terminal)\nB, 1 (terminal)\n\n**Các khái niệm quan trọng:**\n- Random Walk là một ví dụ minh họa đơn giản trong Học tăng cường, thường được sử dụng để so sánh hiệu suất của các thuật toán như Monte Carlo và Temporal Difference. Trong ví dụ này, agent di chuyển ngẫu nhiên giữa các trạng thái và nhận phần thưởng khi đạt đến một trạng thái cuối cụ thể.\n- Monte Carlo (MC) là một thuật toán học tăng cường model-free để ước lượng hàm giá trị. Nó cập nhật hàm giá trị chỉ sau khi một episode kết thúc, sử dụng tổng phần thưởng thực tế (actual return G_t) từ episode đó. MC là unbiased nhưng có variance cao. Nó yêu cầu các tasks phải là episodic.\n\n**Mối quan hệ:**\n- Monte Carlo yêu cầu các tasks phải là Episodic để có thể tính toán actual return G_t.\n- Trong Batch Learning, Monte Carlo tìm V^π bằng cách minimize Mean-squared error với observed returns.\n- Monte Carlo có tính chất High variance do phụ thuộc vào toàn bộ chuỗi phần thưởng ngẫu nhiên.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nPhần tiếp theo sẽ mở rộng sang **Model-Free Control**: Không chỉ đánh giá mà còn tìm chính sách tối ưu mà không cần mô hình!\n\n---\n\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Control\n\n#### 1.1. Định nghĩa bài toán\n**Control Problem**: Tìm chính sách tối ưu π* mà không biết trước mô hình môi trường (P, R)\n\n**So sánh với Prediction**:\n- Prediction: Đánh giá V^π cho π cho trước\n- Control: Tối ưu hóa π để maximize V^π\n\n#### 1.2. Thách thức\n- Không biết mô hình → không thể dùng Dynamic Programming\n- Phải học từ interaction với môi trường\n- Cần cân bằng exploration và exploitation\n\n#### 1.3. Ý tưởng chính\nSử dụng **Generalized Policy Iteration (GPI)** framework:\n```\nPolicy Evaluation (Model-Free) → Policy Improvement → Repeat\n```\n\n### 2. Monte Carlo Control\n\n#### 2.1. Từ V(s) sang Q(s,a)\n\n**Vấn đề với V(s)**:\n```\nPolicy Improvement cần:\nπ'(s) = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n                 ↑ Cần biết mô hình!\n```\n\n**Giải pháp**: Sử dụng Q(s,a)\n```\nπ'(s) = argmax_a Q(s,a)  ← Không cần mô hình!\n```\n\n#### 2.2. Monte Carlo Policy Iteration\n\n**Thuật toán**:\n```\n1. Khởi tạo:\n   Q(s,a) = 0, ∀s,a\n   π = chính sách khởi tạo\n\n2. Lặp:\n   a) Policy Evaluation (MC):\n      - Tạo nhiều episodes theo π\n      - Cập nhật Q^π(s,a) bằng MC\n   \n   b) Policy Improvement:\n      π(s) = argmax_a Q(s,a), ∀s\n```\n\n#### 2.3. Vấn đề Exploration\n\n**Greedy Policy**:\n```\nπ(s) = argmax_a Q(s,a)\n```\n→ Chỉ exploit, không explore → Có thể bỏ lỡ chính sách tốt hơn\n\n**Giải pháp 1: ε-Greedy Policy**\n```\nπ(a|s) = {\n    1 - ε + ε/|A|,  nếu a = argmax Q(s,a)\n    ε/|A|,          ngược lại\n}\n```\n\n**Đặc điểm**:\n- Xác suất 1-ε: Chọn hành động tốt nhất (exploit)\n- Xác suất ε: Chọn ngẫu nhiên (explore)\n- ε decay theo thời gian: Explore nhiều lúc đầu, exploit nhiều sau\n\n#### 2.4. ε-Greedy Monte Carlo Control\n\n**Thuật toán**:\n```\nKhởi tạo:\n    Q(s,a) arbitrarily, ∀s,a\n    π = ε-greedy policy dựa trên Q\n    Returns(s,a) = empty list, ∀s,a\n\nLặp forever:\n    1. Tạo episode theo π:\n       S_0, A_0, R_1, ..., S_T\n    \n    2. Với mỗi cặp (s,a) xuất hiện trong episode:\n       G = return sau lần xuất hiện đầu tiên\n       Thêm G vào Returns(s,a)\n       Q(s,a) = average(Returns(s,a))\n    \n    3. Với mỗi s trong episode:\n       π(s) = ε-greedy(Q(s,·))\n```\n\n#### 2.5. Greedy in the Limit of Infinite Exploration (GLIE)\n\n\n**Các khái niệm quan trọng:**\n- Monte Carlo Control là một thuật toán học tăng cường model-free, được sử dụng cho các bài toán episodic để học hàm giá trị hoặc chính sách tối ưu. Nó hoạt động bằng cách tạo ra nhiều episode hoàn chỉnh, tính toán return cho mỗi cặp (trạng thái, hành động) (s,a), và cập nhật Q-function (Q(s,a)) dựa trên giá trị trung bình của các return đó. Thuật toán này có thể là on-policy hoặc off-policy, nổi bật với việc triển khai đơn giản và không yêu cầu mô hình môi trường. Để giải quyết vấn đề exploration, nó thường sử dụng ε-Greedy Policy, trong đó ε-Greedy Monte Carlo Control lặp đi lặp lại việc tạo các episode theo chính sách ε-greedy hiện tại, cập nhật Q(s,a), và cải thiện chính sách. GLIE Monte Carlo Control là một biến thể đảm bảo hội tụ đến chính sách tối ưu π* bằng cách yêu cầu chính sách tuân thủ điều kiện GLIE (Greedy in the Limit with Infinite Exploration)."
        },
        "output_without_context": {
            "question": "Trong phương pháp Monte-Carlo trong học tăng cường, hàm giá trị của một trạng thái được ước tính dựa trên kết quả của các lần chơi thử nghiệm nào?",
            "answer": "Các lần chơi thử nghiệm thực tế như Blackjack."
        }
    },
    {
        "topic_description": "Đánh giá chính sách Monte-Carlo là một kỹ thuật quan trọng trong Học tăng cường. Chủ đề này cung cấp thông tin về hai phương pháp: Đánh giá lần đầu tiên và đánh giá mọi lần, bao gồm các công thức tính toán cụ thể. Học sinh sẽ được đánh giá khả năng phân biệt giữa hai cách tiếp cận này.",
        "output_with_context": {
            "question": "Trong Đánh giá chính sách Monte-Carlo, phương pháp nào cập nhật giá trị của một trạng thái mỗi khi trạng thái đó xuất hiện trong một episode?",
            "answer": "Every-Visit Monte Carlo",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nPhần tiếp theo sẽ mở rộng sang **Model-Free Control**: Không chỉ đánh giá mà còn tìm chính sách tối ưu mà không cần mô hình!\n\n---\n\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Control\n\n#### 1.1. Định nghĩa bài toán\n**Control Problem**: Tìm chính sách tối ưu π* mà không biết trước mô hình môi trường (P, R)\n\n**So sánh với Prediction**:\n- Prediction: Đánh giá V^π cho π cho trước\n- Control: Tối ưu hóa π để maximize V^π\n\n#### 1.2. Thách thức\n- Không biết mô hình → không thể dùng Dynamic Programming\n- Phải học từ interaction với môi trường\n- Cần cân bằng exploration và exploitation\n\n#### 1.3. Ý tưởng chính\nSử dụng **Generalized Policy Iteration (GPI)** framework:\n```\nPolicy Evaluation (Model-Free) → Policy Improvement → Repeat\n```\n\n### 2. Monte Carlo Control\n\n#### 2.1. Từ V(s) sang Q(s,a)\n\n**Vấn đề với V(s)**:\n```\nPolicy Improvement cần:\nπ'(s) = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n                 ↑ Cần biết mô hình!\n```\n\n**Giải pháp**: Sử dụng Q(s,a)\n```\nπ'(s) = argmax_a Q(s,a)  ← Không cần mô hình!\n```\n\n#### 2.2. Monte Carlo Policy Iteration\n\n**Thuật toán**:\n```\n1. Khởi tạo:\n   Q(s,a) = 0, ∀s,a\n   π = chính sách khởi tạo\n\n2. Lặp:\n   a) Policy Evaluation (MC):\n      - Tạo nhiều episodes theo π\n      - Cập nhật Q^π(s,a) bằng MC\n   \n   b) Policy Improvement:\n      π(s) = argmax_a Q(s,a), ∀s\n```\n\n#### 2.3. Vấn đề Exploration\n\n**Greedy Policy**:\n```\nπ(s) = argmax_a Q(s,a)\n```\n→ Chỉ exploit, không explore → Có thể bỏ lỡ chính sách tốt hơn\n\n**Giải pháp 1: ε-Greedy Policy**\n```\nπ(a|s) = {\n    1 - ε + ε/|A|,  nếu a = argmax Q(s,a)\n    ε/|A|,          ngược lại\n}\n```\n\n**Đặc điểm**:\n- Xác suất 1-ε: Chọn hành động tốt nhất (exploit)\n- Xác suất ε: Chọn ngẫu nhiên (explore)\n- ε decay theo thời gian: Explore nhiều lúc đầu, exploit nhiều sau\n\n#### 2.4. ε-Greedy Monte Carlo Control\n\n**Thuật toán**:\n```\nKhởi tạo:\n    Q(s,a) arbitrarily, ∀s,a\n    π = ε-greedy policy dựa trên Q\n    Returns(s,a) = empty list, ∀s,a\n\nLặp forever:\n    1. Tạo episode theo π:\n       S_0, A_0, R_1, ..., S_T\n    \n    2. Với mỗi cặp (s,a) xuất hiện trong episode:\n       G = return sau lần xuất hiện đầu tiên\n       Thêm G vào Returns(s,a)\n       Q(s,a) = average(Returns(s,a))\n    \n    3. Với mỗi s trong episode:\n       π(s) = ε-greedy(Q(s,·))\n```\n\n#### 2.5. Greedy in the Limit of Infinite Exploration (GLIE)\n\n\n**Các khái niệm quan trọng:**\n- Policy Evaluation là quá trình ước lượng hàm giá trị (V^π hoặc Q^π) cho một chính sách π đã cho. Trong Model-Free Control, Policy Evaluation phải được thực hiện mà không cần biết mô hình môi trường, thường thông qua các phương pháp như Monte Carlo hoặc Temporal Difference.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n        Nếu S_t xuất hiện lần đầu tiên trong episode:\n            Thêm G vào Returns(S_t)\n            V(S_t) = average(Returns(S_t))\n```\n\n**Đặc điểm**:\n- Chỉ cập nhật cho lần xuất hiện đầu tiên của trạng thái\n- Không có bias\n- Đảm bảo hội tụ đến V^π(s) khi số episodes → ∞\n\n#### 2.4. Every-Visit Monte Carlo\n\n**Khác biệt**: Cập nhật cho mọi lần xuất hiện của trạng thái trong episode\n\n```\nVới mỗi bước t = T-1, T-2, ..., 0:\n    G = γG + R_{t+1}\n    Thêm G vào Returns(S_t)\n    V(S_t) = average(Returns(S_t))\n```\n\n**So sánh**:\n- Every-visit có variance thấp hơn\n- Cả hai đều hội tụ đến V^π(s)\n- Every-visit thường được sử dụng nhiều hơn\n\n#### 2.5. Incremental Mean Update\n\n**Vấn đề**: Lưu trữ tất cả returns không hiệu quả\n\n**Giải pháp**: Cập nhật incremental\n```\nCông thức tổng quát:\n    μ_k = μ_{k-1} + (1/k)(x_k - μ_{k-1})\n\nÁp dụng cho MC:\n    N(s) = N(s) + 1\n    V(s) = V(s) + (1/N(s))(G - V(s))\n    \nHoặc dùng learning rate α cố định:\n    V(s) = V(s) + α(G - V(s))\n```\n\n**Lợi ích**:\n- Tiết kiệm bộ nhớ\n- Cập nhật online\n- Quên dần các ước lượng cũ (với α cố định)\n\n#### 2.6. Ví dụ: Blackjack\n\n**Mô tả bài toán**:\n- **Trạng thái**: (tổng bài của người chơi, bài úp của dealer, có ace không)\n- **Hành động**: Hit (rút thêm) hoặc Stick (dừng)\n- **Phần thưởng**: +1 thắng, -1 thua, 0 hòa\n- **Chính sách**: Stick nếu tổng ≥ 20, ngược lại Hit\n\n**Ứng dụng MC**:\n1. Chơi nhiều games theo chính sách\n2. Ghi lại returns cho mỗi trạng thái\n3. Tính V^π bằng trung bình returns\n\n**Kết quả**:\n```\nV^π(20, ACE, usable_ace) ≈ 0.8  (rất tốt)\nV^π(12, 2, no_ace) ≈ -0.3       (tệ)\n```\n\n### 3. Temporal-Difference Learning (TD)\n\n#### 3.1. Giới thiệu\nTD Learning kết hợp ý tưởng của MC và DP:\n- Như MC: Học từ kinh nghiệm, không cần mô hình\n- Như DP: Bootstrap từ ước lượng hiện tại, không cần episode hoàn chỉnh\n\n#### 3.2. TD(0) - Temporal Difference cơ bản\n\n**TD Update Rule**:\n```\nV(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]\n```\n\n**Các thành phần**:\n- **TD Target**: R_{t+1} + γV(S_{t+1})\n- **TD Error**: δ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\n- **Update**: V(S_t) ← V(S_t) + α × δ_t\n\n#### 3.3. Thuật toán TD(0)\n\n```\n\n**Các khái niệm quan trọng:**\n- First-Visit Monte Carlo là một thuật toán học tăng cường model-free để ước lượng hàm giá trị V^π(s) của một chính sách π. Thuật toán này tính toán giá trị của một trạng thái bằng cách lấy trung bình các tổng phần thưởng chiết khấu (returns) nhận được sau lần đầu tiên trạng thái đó xuất hiện trong mỗi episode. Nó không có bias và hội tụ đến V^π(s) khi số lượng episode tiến tới vô hạn, mặc dù có thể có variance cao. Nó khác với Every-Visit Monte Carlo, thuật toán cập nhật giá trị của một trạng thái mỗi khi trạng thái đó xuất hiện trong một episode.\n\n**Mối quan hệ:**\n- Every-Visit Monte Carlo ước lượng hàm giá trị V^π(s) bằng cách lấy trung bình các returns mỗi khi trạng thái s xuất hiện trong một episode.\n- Every-Visit Monte Carlo sử dụng tập hợp Returns(S_t) để lưu trữ các tổng phần thưởng chiết khấu cho mỗi lần thăm của trạng thái S_t.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nKhởi tạo V(s) arbitrarily, ∀s ∈ S\n\nVới mỗi episode:\n    Khởi tạo S\n    \n    Lặp cho mỗi bước của episode:\n        A ← hành động từ S theo π\n        Thực hiện A, quan sát R, S'\n        \n        V(S) ← V(S) + α[R + γV(S') - V(S)]\n        \n        S ← S'\n    cho đến khi S là terminal\n```\n\n#### 3.4. So sánh MC vs TD(0)\n\n| Tiêu chí | Monte Carlo | TD(0) |\n|----------|-------------|-------|\n| Cập nhật | Cuối episode | Sau mỗi bước |\n| Bootstrap | Không | Có |\n| Target | G_t (actual return) | R + γV(S') |\n| Bias | Unbiased | Biased |\n| Variance | High variance | Low variance |\n| Hội tụ | Chậm | Nhanh hơn |\n| Môi trường | Cần episodic | Cả episodic và continuing |\n\n#### 3.5. Ưu điểm của TD\n\n✅ **Học online**: Cập nhật sau mỗi bước, không cần chờ episode kết thúc\n✅ **Continuing tasks**: Hoạt động với tasks không có điểm kết thúc\n✅ **Lower variance**: Bootstrap giảm variance so với MC\n✅ **Học nhanh hơn**: Thường hội tụ nhanh hơn MC trong thực tế\n✅ **Hiệu quả dữ liệu**: Sử dụng thông tin từ ước lượng hiện tại\n\n#### 3.6. Ví dụ minh họa: Random Walk\n\n**Setup**:\n```\nStates: [A] [B] [C] [D] [E]\nStart: C\nTerminal: Left of A hoặc Right of E\nReward: 0 mọi nơi, +1 khi đến Right of E\nAction: Left hoặc Right (random với p=0.5)\n```\n\n**True values**:\n```\nV^π(A) = 1/6, V^π(B) = 2/6, V^π(C) = 3/6,\nV^π(D) = 4/6, V^π(E) = 5/6\n```\n\n**So sánh MC vs TD**:\n- TD hội tụ nhanh hơn với cùng số episodes\n- TD ít sensitive với khởi tạo\n- MC có RMS error cao hơn trong giai đoạn đầu\n\n### 4. Batch Methods và Certainty Equivalence\n\n#### 4.1. Batch Learning\n**Ý tưởng**: Lặp lại huấn luyện trên cùng một batch experience cho đến hội tụ\n\n```\nCho trước batch experience: \n    {(S₁, A₁, R₁, S'₁), (S₂, A₂, R₂, S'₂), ...}\n\nLặp cho đến hội tụ:\n    Với mỗi experience (S, A, R, S'):\n        Cập nhật V(S) theo MC hoặc TD\n```\n\n#### 4.2. Certainty Equivalence\n\n**MC**: Tìm V^π minimize mean-squared error với observed returns\n```\nV^π = argmin_V Σ_episodes Σ_t (G_t - V(S_t))²\n```\n\n**TD**: Tìm V^π thỏa mãn phương trình Bellman cho MDP ước lượng\n```\nV^π(s) = E[R + γV^π(S') | s]\n```\n(ước lượng từ experience)\n\n**Kết quả**:\n- TD tận dụng cấu trúc Markov\n- MC chỉ minimize error, không exploit Markov property\n- TD thường hiệu quả hơn trong môi trường Markov\n\n#### 4.3. Ví dụ: AB Example\n\n**Experience**:\n```\nA, 0, B, 0\nB, 1 (terminal)\nB, 1 (terminal)\n\n**Các khái niệm quan trọng:**\n- Monte Carlo (MC) là một thuật toán học tăng cường model-free để ước lượng hàm giá trị. Nó cập nhật hàm giá trị chỉ sau khi một episode kết thúc, sử dụng tổng phần thưởng thực tế (actual return G_t) từ episode đó. MC là unbiased nhưng có variance cao. Nó yêu cầu các tasks phải là episodic.\n\n**Mối quan hệ:**\n- Monte Carlo yêu cầu các tasks phải là Episodic để có thể tính toán actual return G_t.\n- Trong Batch Learning, Monte Carlo tìm V^π bằng cách minimize Mean-squared error với observed returns.\n- Monte Carlo có tính chất High variance do phụ thuộc vào toàn bộ chuỗi phần thưởng ngẫu nhiên."
        },
        "output_without_context": {
            "question": "Trong đánh giá chính sách Monte-Carlo, phương pháp nào được sử dụng để ước lượng giá trị của một chính sách dựa trên các lần thử nghiệm ngẫu nhiên?",
            "answer": "Đánh giá mọi lần"
        }
    },
    {
        "topic_description": "Chủ đề này tập trung vào cơ chế cập nhật giá trị V(s) thông qua các phương pháp Monte-Carlo tăng dần. Học sinh sẽ cần hiểu và áp dụng các công thức liên quan đến quá trình cập nhật trong các vấn đề không dừng.",
        "output_with_context": {
            "question": "Trong phương pháp cập nhật giá trị V(s) bằng Monte Carlo, công thức nào được sử dụng để cập nhật giá trị V(S)?",
            "answer": "V(S) ← V(S) + α[R + γV(S') - V(S)]",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nKhởi tạo V(s) arbitrarily, ∀s ∈ S\n\nVới mỗi episode:\n    Khởi tạo S\n    \n    Lặp cho mỗi bước của episode:\n        A ← hành động từ S theo π\n        Thực hiện A, quan sát R, S'\n        \n        V(S) ← V(S) + α[R + γV(S') - V(S)]\n        \n        S ← S'\n    cho đến khi S là terminal\n```\n\n#### 3.4. So sánh MC vs TD(0)\n\n| Tiêu chí | Monte Carlo | TD(0) |\n|----------|-------------|-------|\n| Cập nhật | Cuối episode | Sau mỗi bước |\n| Bootstrap | Không | Có |\n| Target | G_t (actual return) | R + γV(S') |\n| Bias | Unbiased | Biased |\n| Variance | High variance | Low variance |\n| Hội tụ | Chậm | Nhanh hơn |\n| Môi trường | Cần episodic | Cả episodic và continuing |\n\n#### 3.5. Ưu điểm của TD\n\n✅ **Học online**: Cập nhật sau mỗi bước, không cần chờ episode kết thúc\n✅ **Continuing tasks**: Hoạt động với tasks không có điểm kết thúc\n✅ **Lower variance**: Bootstrap giảm variance so với MC\n✅ **Học nhanh hơn**: Thường hội tụ nhanh hơn MC trong thực tế\n✅ **Hiệu quả dữ liệu**: Sử dụng thông tin từ ước lượng hiện tại\n\n#### 3.6. Ví dụ minh họa: Random Walk\n\n**Setup**:\n```\nStates: [A] [B] [C] [D] [E]\nStart: C\nTerminal: Left of A hoặc Right of E\nReward: 0 mọi nơi, +1 khi đến Right of E\nAction: Left hoặc Right (random với p=0.5)\n```\n\n**True values**:\n```\nV^π(A) = 1/6, V^π(B) = 2/6, V^π(C) = 3/6,\nV^π(D) = 4/6, V^π(E) = 5/6\n```\n\n**So sánh MC vs TD**:\n- TD hội tụ nhanh hơn với cùng số episodes\n- TD ít sensitive với khởi tạo\n- MC có RMS error cao hơn trong giai đoạn đầu\n\n### 4. Batch Methods và Certainty Equivalence\n\n#### 4.1. Batch Learning\n**Ý tưởng**: Lặp lại huấn luyện trên cùng một batch experience cho đến hội tụ\n\n```\nCho trước batch experience: \n    {(S₁, A₁, R₁, S'₁), (S₂, A₂, R₂, S'₂), ...}\n\nLặp cho đến hội tụ:\n    Với mỗi experience (S, A, R, S'):\n        Cập nhật V(S) theo MC hoặc TD\n```\n\n#### 4.2. Certainty Equivalence\n\n**MC**: Tìm V^π minimize mean-squared error với observed returns\n```\nV^π = argmin_V Σ_episodes Σ_t (G_t - V(S_t))²\n```\n\n**TD**: Tìm V^π thỏa mãn phương trình Bellman cho MDP ước lượng\n```\nV^π(s) = E[R + γV^π(S') | s]\n```\n(ước lượng từ experience)\n\n**Kết quả**:\n- TD tận dụng cấu trúc Markov\n- MC chỉ minimize error, không exploit Markov property\n- TD thường hiệu quả hơn trong môi trường Markov\n\n#### 4.3. Ví dụ: AB Example\n\n**Experience**:\n```\nA, 0, B, 0\nB, 1 (terminal)\nB, 1 (terminal)\n\n**Các khái niệm quan trọng:**\n- Monte Carlo (MC) là một thuật toán học tăng cường model-free để ước lượng hàm giá trị. Nó cập nhật hàm giá trị chỉ sau khi một episode kết thúc, sử dụng tổng phần thưởng thực tế (actual return G_t) từ episode đó. MC là unbiased nhưng có variance cao. Nó yêu cầu các tasks phải là episodic.\n\n**Mối quan hệ:**\n- Monte Carlo yêu cầu các tasks phải là Episodic để có thể tính toán actual return G_t.\n- Trong Batch Learning, Monte Carlo tìm V^π bằng cách minimize Mean-squared error với observed returns.\n- Monte Carlo có tính chất High variance do phụ thuộc vào toàn bộ chuỗi phần thưởng ngẫu nhiên.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n     = Σₛ d(s)(V^π(s) - V̂(s; w))²\n```\n- d(s): distribution của states dưới policy π\n\n**Mục tiêu**: Minimize J(w) = ||V^π - V̂_w||²_d\n\n### 3. Stochastic Gradient Descent (SGD)\n\n#### 3.1. Gradient Descent\n\n**Update Rule**:\n```\nw_{t+1} = w_t - (α/2)∇_w J(w_t)\n        = w_t + α E[(V^π(s) - V̂(s; w))∇_w V̂(s; w)]\n```\n\n**Stochastic Gradient Descent**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]∇_w V̂(S_t; w_t)\n```\n\n#### 3.2. Linear SGD\n\n**Gradient của linear function**:\n```\n∇_w V̂(s; w) = ∇_w(w^T φ(s)) = φ(s)\n```\n\n**Update rule**:\n```\nw_{t+1} = w_t + α[V^π(S_t) - V̂(S_t; w_t)]φ(S_t)\n```\n\n**Đặc điểm**:\n- Converge đến local optimum (global cho linear)\n- Learning rate α quan trọng\n- Simple và efficient\n\n#### 3.3. Feature Scaling\n\n**Vấn đề**: Features có scale khác nhau → học không ổn định\n\n**Giải pháp**:\n```\nNormalization: φᵢ = (φᵢ - μᵢ)/σᵢ\nStandardization: φᵢ ∈ [0, 1]\n```\n\n### 4. Incremental Prediction Methods\n\n#### 4.1. Monte Carlo với Function Approximation\n\n**Update**:\n```\nw ← w + α[G_t - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ Target: actual return\n```\n\n**Đặc điểm**:\n- Unbiased target\n- High variance\n- Non-stationary target (G_t khác nhau mỗi episode)\n\n#### 4.2. TD(0) với Function Approximation\n\n**Update**:\n```\nw ← w + α[R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)]∇_w V̂(S_t; w)\n         ↑ TD target\n```\n\n**Semi-gradient**: Không lấy gradient qua V̂(S_{t+1}; w)\n\n**Thuật toán Semi-gradient TD(0)**:\n```\nKhởi tạo w arbitrarily\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        A ← π(S)\n        Thực hiện A, quan sát R, S'\n        \n        w ← w + α[R + γV̂(S'; w) - V̂(S; w)]∇_w V̂(S; w)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 4.3. TD(λ) với Function Approximation\n\n**Eligibility Traces**:\n```\nz_0 = 0\nz_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n```\n\n**Update**:\n```\nδ_t = R_{t+1} + γV̂(S_{t+1}; w) - V̂(S_t; w)\nw ← w + αδ_t z_t\n```\n\n**Accumulating vs Replacing traces**:\n- Accumulating: z_t = γλz_{t-1} + ∇_w V̂(S_t; w)\n- Replacing: Phức tạp hơn, phụ thuộc feature type\n\n### 5. Control với Function Approximation\n\n#### 5.1. Action-Value Function Approximation\n\n**Parameterization**:\n```\nQ̂(s, a; w) = w^T φ(s, a)\n```\n\n\n**Các khái niệm quan trọng:**\n- Monte Carlo với Function Approximation là một phương pháp học tăng cường sử dụng kỹ thuật Monte Carlo để ước lượng hàm giá trị, kết hợp với xấp xỉ hàm giá trị. Nó cập nhật các tham số w dựa trên return thực tế G_t quan sát được từ một episode hoàn chỉnh. Công thức cập nhật là w ← w + α[G_t - V̂(S_t; w)]∇_w V̂(S_t; w). Đặc điểm của nó là mục tiêu (G_t) không thiên vị nhưng có phương sai cao và không dừng (non-stationary).\n\n**Mối quan hệ:**\n- Monte Carlo với Function Approximation sử dụng G_t làm mục tiêu để cập nhật hàm giá trị xấp xỉ.\n- Monte Carlo với Function Approximation cập nhật các tham số w dựa trên return thực tế G_t."
        },
        "output_without_context": {
            "question": "Trong phương pháp cập nhật Monte-Carlo tăng dần, công thức nào được sử dụng để cập nhật giá trị V(s) của trạng thái s?",
            "answer": "V(s) = V(s) + α(R + γV(s') - V(s))"
        }
    },
    {
        "topic_description": "Chủ đề này yêu cầu học sinh so sánh hai phương pháp Học tăng cường: Monte-Carlo và Temporal-Difference. Điều này bao gồm việc hiểu rõ những ưu điểm và nhược điểm của từng phương pháp trong việc học từ các chuỗi khác nhau và ứng dụng của chúng trong các môi trường khác nhau.",
        "output_with_context": {
            "question": "So với phương pháp Monte Carlo, ưu điểm chính của phương pháp Temporal-Difference là gì?",
            "answer": "Temporal-Difference cập nhật giá trị sau mỗi bước, cho phép học online và hội tụ nhanh hơn.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nKhởi tạo V(s) arbitrarily, ∀s ∈ S\n\nVới mỗi episode:\n    Khởi tạo S\n    \n    Lặp cho mỗi bước của episode:\n        A ← hành động từ S theo π\n        Thực hiện A, quan sát R, S'\n        \n        V(S) ← V(S) + α[R + γV(S') - V(S)]\n        \n        S ← S'\n    cho đến khi S là terminal\n```\n\n#### 3.4. So sánh MC vs TD(0)\n\n| Tiêu chí | Monte Carlo | TD(0) |\n|----------|-------------|-------|\n| Cập nhật | Cuối episode | Sau mỗi bước |\n| Bootstrap | Không | Có |\n| Target | G_t (actual return) | R + γV(S') |\n| Bias | Unbiased | Biased |\n| Variance | High variance | Low variance |\n| Hội tụ | Chậm | Nhanh hơn |\n| Môi trường | Cần episodic | Cả episodic và continuing |\n\n#### 3.5. Ưu điểm của TD\n\n✅ **Học online**: Cập nhật sau mỗi bước, không cần chờ episode kết thúc\n✅ **Continuing tasks**: Hoạt động với tasks không có điểm kết thúc\n✅ **Lower variance**: Bootstrap giảm variance so với MC\n✅ **Học nhanh hơn**: Thường hội tụ nhanh hơn MC trong thực tế\n✅ **Hiệu quả dữ liệu**: Sử dụng thông tin từ ước lượng hiện tại\n\n#### 3.6. Ví dụ minh họa: Random Walk\n\n**Setup**:\n```\nStates: [A] [B] [C] [D] [E]\nStart: C\nTerminal: Left of A hoặc Right of E\nReward: 0 mọi nơi, +1 khi đến Right of E\nAction: Left hoặc Right (random với p=0.5)\n```\n\n**True values**:\n```\nV^π(A) = 1/6, V^π(B) = 2/6, V^π(C) = 3/6,\nV^π(D) = 4/6, V^π(E) = 5/6\n```\n\n**So sánh MC vs TD**:\n- TD hội tụ nhanh hơn với cùng số episodes\n- TD ít sensitive với khởi tạo\n- MC có RMS error cao hơn trong giai đoạn đầu\n\n### 4. Batch Methods và Certainty Equivalence\n\n#### 4.1. Batch Learning\n**Ý tưởng**: Lặp lại huấn luyện trên cùng một batch experience cho đến hội tụ\n\n```\nCho trước batch experience: \n    {(S₁, A₁, R₁, S'₁), (S₂, A₂, R₂, S'₂), ...}\n\nLặp cho đến hội tụ:\n    Với mỗi experience (S, A, R, S'):\n        Cập nhật V(S) theo MC hoặc TD\n```\n\n#### 4.2. Certainty Equivalence\n\n**MC**: Tìm V^π minimize mean-squared error với observed returns\n```\nV^π = argmin_V Σ_episodes Σ_t (G_t - V(S_t))²\n```\n\n**TD**: Tìm V^π thỏa mãn phương trình Bellman cho MDP ước lượng\n```\nV^π(s) = E[R + γV^π(S') | s]\n```\n(ước lượng từ experience)\n\n**Kết quả**:\n- TD tận dụng cấu trúc Markov\n- MC chỉ minimize error, không exploit Markov property\n- TD thường hiệu quả hơn trong môi trường Markov\n\n#### 4.3. Ví dụ: AB Example\n\n**Experience**:\n```\nA, 0, B, 0\nB, 1 (terminal)\nB, 1 (terminal)\n\n**Các khái niệm quan trọng:**\n- Random Walk là một ví dụ minh họa đơn giản trong Học tăng cường, thường được sử dụng để so sánh hiệu suất của các thuật toán như Monte Carlo và Temporal Difference. Trong ví dụ này, agent di chuyển ngẫu nhiên giữa các trạng thái và nhận phần thưởng khi đạt đến một trạng thái cuối cụ thể.\n- Monte Carlo (MC) là một thuật toán học tăng cường model-free để ước lượng hàm giá trị. Nó cập nhật hàm giá trị chỉ sau khi một episode kết thúc, sử dụng tổng phần thưởng thực tế (actual return G_t) từ episode đó. MC là unbiased nhưng có variance cao. Nó yêu cầu các tasks phải là episodic.\n\n**Mối quan hệ:**\n- Monte Carlo yêu cầu các tasks phải là Episodic để có thể tính toán actual return G_t.\n- Trong Batch Learning, Monte Carlo tìm V^π bằng cách minimize Mean-squared error với observed returns.\n- Monte Carlo có tính chất High variance do phụ thuộc vào toàn bộ chuỗi phần thưởng ngẫu nhiên.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n- Cần học nhanh\n- Online learning\n\n**TD(λ)**:\n- Khi cần cân bằng bias-variance\n- Credit assignment phức tạp\n- Eligibility traces quan trọng\n\n### 10. Code Implementation\n\n#### 10.1. Monte Carlo First-Visit\n```python\ndef monte_carlo_prediction(env, policy, num_episodes, gamma=0.99):\n    V = defaultdict(float)\n    returns = defaultdict(list)\n    \n    for _ in range(num_episodes):\n        episode = generate_episode(env, policy)\n        G = 0\n        visited = set()\n        \n        # Duyệt ngược từ cuối episode\n        for t in range(len(episode)-1, -1, -1):\n            state, action, reward = episode[t]\n            G = gamma * G + reward\n            \n            if state not in visited:\n                returns[state].append(G)\n                V[state] = np.mean(returns[state])\n                visited.add(state)\n    \n    return V\n```\n\n#### 10.2. TD(0)\n```python\ndef td_prediction(env, policy, num_episodes, alpha=0.1, gamma=0.99):\n    V = defaultdict(float)\n    \n    for _ in range(num_episodes):\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # TD update\n            td_target = reward + gamma * V[next_state]\n            td_error = td_target - V[state]\n            V[state] += alpha * td_error\n            \n            state = next_state\n    \n    return V\n```\n\n#### 10.3. TD(λ) with Eligibility Traces\n```python\ndef td_lambda_prediction(env, policy, num_episodes, \n                        alpha=0.1, gamma=0.99, lambda_=0.9):\n    V = defaultdict(float)\n    \n    for _ in range(num_episodes):\n        E = defaultdict(float)  # Eligibility traces\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # TD error\n            delta = reward + gamma * V[next_state] - V[state]\n            \n            # Update eligibility trace\n            E[state] += 1\n            \n            # Update all states\n            for s in E:\n                V[s] += alpha * delta * E[s]\n                E[s] *= gamma * lambda_\n            \n            state = next_state\n    \n    return V\n```\n\n### 11. Bài tập thực hành\n\n#### 11.1. Bài tập cơ bản\n1. Implement First-Visit MC cho Blackjack\n2. So sánh MC vs TD(0) trên Random Walk\n3. Visualize learning curves với different α\n\n#### 11.2. Bài tập nâng cao\n1. Implement n-step TD với n = 1, 3, 5, 10\n2. Compare TD(λ) với λ = 0, 0.5, 0.9, 1.0\n3. Analyze bias-variance tradeoff empirically\n\n#### 11.3. Dự án\n1. Build Tic-Tac-Toe AI với TD learning\n2. Robot navigation trong gridworld phức tạp\n3. Stock price prediction với TD methods\n\n### 12. Kết luận\n\nModel-Free Prediction giải quyết vấn đề quan trọng: **Đánh giá chính sách mà không cần biết mô hình môi trường**.\n\n**Các phương pháp chính**:\n- **Monte Carlo**: Đơn giản, unbiased, nhưng high variance\n- **TD(0)**: Efficient, online, nhưng biased\n- **TD(λ)**: Cân bằng tốt nhất, flexible\n\n**Key insights**:\n1. Bootstrap (TD) vs Full returns (MC) là tradeoff cơ bản\n2. TD thường hiệu quả hơn trong môi trường Markov\n3. Eligibility traces (TD(λ)) cung cấp spectrum liên tục\n4. Lựa chọn phương pháp phụ thuộc vào đặc điểm bài toán\n\n\n**Các khái niệm quan trọng:**\n- Bias-Variance Tradeoff là một khái niệm quan trọng trong học máy và các thuật toán ước lượng, mô tả sự đánh đổi giữa lỗi do giả định sai (bias) và lỗi do sự nhạy cảm với các biến động nhỏ trong tập huấn luyện (variance). Trong học tăng cường (RL), các phương pháp như Monte Carlo có bias thấp nhưng variance cao, trong khi TD(0) có bias cao hơn nhưng variance thấp hơn. Các phương pháp như n-Step TD và TD(λ) được thiết kế để cân bằng giữa hai yếu tố này.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n        Nếu S_t xuất hiện lần đầu tiên trong episode:\n            Thêm G vào Returns(S_t)\n            V(S_t) = average(Returns(S_t))\n```\n\n**Đặc điểm**:\n- Chỉ cập nhật cho lần xuất hiện đầu tiên của trạng thái\n- Không có bias\n- Đảm bảo hội tụ đến V^π(s) khi số episodes → ∞\n\n#### 2.4. Every-Visit Monte Carlo\n\n**Khác biệt**: Cập nhật cho mọi lần xuất hiện của trạng thái trong episode\n\n```\nVới mỗi bước t = T-1, T-2, ..., 0:\n    G = γG + R_{t+1}\n    Thêm G vào Returns(S_t)\n    V(S_t) = average(Returns(S_t))\n```\n\n**So sánh**:\n- Every-visit có variance thấp hơn\n- Cả hai đều hội tụ đến V^π(s)\n- Every-visit thường được sử dụng nhiều hơn\n\n#### 2.5. Incremental Mean Update\n\n**Vấn đề**: Lưu trữ tất cả returns không hiệu quả\n\n**Giải pháp**: Cập nhật incremental\n```\nCông thức tổng quát:\n    μ_k = μ_{k-1} + (1/k)(x_k - μ_{k-1})\n\nÁp dụng cho MC:\n    N(s) = N(s) + 1\n    V(s) = V(s) + (1/N(s))(G - V(s))\n    \nHoặc dùng learning rate α cố định:\n    V(s) = V(s) + α(G - V(s))\n```\n\n**Lợi ích**:\n- Tiết kiệm bộ nhớ\n- Cập nhật online\n- Quên dần các ước lượng cũ (với α cố định)\n\n#### 2.6. Ví dụ: Blackjack\n\n**Mô tả bài toán**:\n- **Trạng thái**: (tổng bài của người chơi, bài úp của dealer, có ace không)\n- **Hành động**: Hit (rút thêm) hoặc Stick (dừng)\n- **Phần thưởng**: +1 thắng, -1 thua, 0 hòa\n- **Chính sách**: Stick nếu tổng ≥ 20, ngược lại Hit\n\n**Ứng dụng MC**:\n1. Chơi nhiều games theo chính sách\n2. Ghi lại returns cho mỗi trạng thái\n3. Tính V^π bằng trung bình returns\n\n**Kết quả**:\n```\nV^π(20, ACE, usable_ace) ≈ 0.8  (rất tốt)\nV^π(12, 2, no_ace) ≈ -0.3       (tệ)\n```\n\n### 3. Temporal-Difference Learning (TD)\n\n#### 3.1. Giới thiệu\nTD Learning kết hợp ý tưởng của MC và DP:\n- Như MC: Học từ kinh nghiệm, không cần mô hình\n- Như DP: Bootstrap từ ước lượng hiện tại, không cần episode hoàn chỉnh\n\n#### 3.2. TD(0) - Temporal Difference cơ bản\n\n**TD Update Rule**:\n```\nV(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]\n```\n\n**Các thành phần**:\n- **TD Target**: R_{t+1} + γV(S_{t+1})\n- **TD Error**: δ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\n- **Update**: V(S_t) ← V(S_t) + α × δ_t\n\n#### 3.3. Thuật toán TD(0)\n\n```\n\n**Các khái niệm quan trọng:**\n- Temporal-Difference Learning (TD) là một loại thuật toán học tăng cường kết hợp ý tưởng từ Monte Carlo và Dynamic Programming. Giống như Monte Carlo, TD học từ kinh nghiệm mà không cần mô hình môi trường. Giống như Dynamic Programming, TD bootstrap từ các ước lượng giá trị hiện tại của các trạng thái tiếp theo, cho phép cập nhật giá trị mà không cần chờ đến cuối episode.\n\n**Mối quan hệ:**\n- Temporal-Difference Learning (TD) kết hợp ý tưởng bootstrap từ ước lượng hiện tại của Dynamic Programming, cho phép cập nhật mà không cần chờ đến cuối episode.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n#### 7.1. Game Playing\n\n**TD-Gammon** (1992):\n- Backgammon AI\n- Sử dụng TD(λ) với neural network\n- Self-play\n- Đạt world-champion level\n\n**AlphaGo Zero**:\n- Sử dụng TD-style updates\n- Self-play + MCTS\n- Không cần human knowledge\n\n#### 7.2. Robot Navigation\n\n**Task**: Robot tìm đường trong môi trường chưa biết\n- State: Vị trí robot\n- Action: Di chuyển\n- Reward: -1 mỗi bước, +100 khi đến đích\n\n**Ưu điểm TD**:\n- Học online trong quá trình điều hướng\n- Không cần đợi đến đích mới cập nhật\n- Adapt với môi trường thay đổi\n\n#### 7.3. Resource Management\n\n**Elevator Control**:\n- State: Vị trí thang máy, yêu cầu chờ\n- Action: Lên/xuống/đứng yên\n- Reward: -1 × tổng thời gian chờ\n\n**TD Learning**:\n- Học value function cho mỗi trạng thái\n- Online learning từ hoạt động hàng ngày\n- Cải thiện liên tục\n\n### 8. Phân tích lý thuyết\n\n#### 8.1. Tốc độ hội tụ\n\n**Monte Carlo**:\n```\nV_k(s) → V^π(s) với rate O(1/√k)\nk: số episodes\n```\n\n**TD(0)**:\n```\nV_k(s) → V^π(s) nhanh hơn trong thực tế\nKhông có bound lý thuyết chặt chẽ\n```\n\n**Thực nghiệm**: TD thường nhanh hơn MC 2-10 lần\n\n#### 8.2. Điều kiện hội tụ\n\n**Robbins-Monro conditions** cho learning rate α_t:\n```\nΣ_{t=1}^∞ α_t = ∞     (đảm bảo hội tụ)\nΣ_{t=1}^∞ α_t² < ∞    (đảm bảo variance hội tụ về 0)\n```\n\n**Ví dụ**:\n- α_t = 1/t: Thỏa mãn\n- α_t = 0.01: Không thỏa mãn điều kiện 1, nhưng practical\n\n#### 8.3. Bias-Variance Tradeoff\n\n**Monte Carlo**:\n- Bias = 0 (ước lượng không chệch)\n- Variance cao (phụ thuộc vào toàn bộ trajectory)\n\n**TD(0)**:\n- Bias > 0 (phụ thuộc vào V hiện tại)\n- Variance thấp (chỉ phụ thuộc 1 bước)\n\n**n-Step TD**: Cân bằng\n```\nBias giảm khi n tăng\nVariance tăng khi n tăng\n```\n\n### 9. So sánh tổng hợp\n\n#### 9.1. Bảng so sánh đầy đủ\n\n| Tiêu chí | MC | TD(0) | TD(λ) | DP |\n|----------|----|----|-------|-----|\n| Model-free | ✓ | ✓ | ✓ | ✗ |\n| Bootstrap | ✗ | ✓ | ✓ | ✓ |\n| Online | ✗ | ✓ | ✓ | ✓ |\n| Episodic only | ✓ | ✗ | ✗ | ✗ |\n| Bias | Low | High | Medium | Low |\n| Variance | High | Low | Medium | Low |\n| Convergence | Slow | Fast | Medium | Fastest |\n\n#### 9.2. Khi nào dùng phương pháp nào?\n\n**Monte Carlo**:\n- Môi trường không Markov\n- Cần ước lượng unbiased\n- Episodic tasks ngắn\n\n**TD(0)**:\n- Môi trường Markov\n- Continuing tasks\n\n**Các khái niệm quan trọng:**\n- Episodic only là một tính chất của các thuật toán học tăng cường chỉ có thể áp dụng cho các tác vụ có điểm kết thúc rõ ràng (episodes). Monte Carlo là thuật toán episodic only, trong khi TD(0), TD(λ), và DP có thể xử lý cả tác vụ episodic và continuing.\n\n**Mối quan hệ:**\n- Monte Carlo chỉ áp dụng cho các tác vụ episodic.\n- TD(0) không phải là thuật toán episodic only.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\nDynamic Programming là nền tảng quan trọng trong Reinforcement Learning. Mặc dù có hạn chế về yêu cầu biết mô hình và curse of dimensionality, DP cung cấp:\n\n- **Framework lý thuyết**: Hiểu rõ về optimal value functions và policies\n- **Nền tảng thuật toán**: Cơ sở cho các phương pháp model-free\n- **Công cụ thực tế**: Giải quyết được nhiều bài toán thực tế\n\nCác khái niệm từ DP (đặc biệt là GPI và Bellman equations) sẽ xuất hiện xuyên suốt trong các phương pháp học tăng cường tiếp theo, từ Temporal-Difference Learning đến Deep Q-Networks.\n\n---\n\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Learning\n\n#### 1.1. Định nghĩa\nModel-Free Learning là các phương pháp học tăng cường không yêu cầu biết trước mô hình môi trường (hàm chuyển trạng thái P và hàm phần thưởng R). Agent học trực tiếp từ kinh nghiệm tương tác với môi trường.\n\n#### 1.2. So sánh Model-Based vs Model-Free\n\n| Đặc điểm | Model-Based | Model-Free |\n|----------|-------------|------------|\n| Yêu cầu mô hình | Cần biết P và R | Không cần |\n| Học từ | Phương trình Bellman | Kinh nghiệm thực tế |\n| Ưu điểm | Hiệu quả dữ liệu | Linh hoạt, thực tế |\n| Nhược điểm | Khó có mô hình chính xác | Cần nhiều dữ liệu |\n| Ví dụ | DP, Model-Based RL | MC, TD, Q-Learning |\n\n#### 1.3. Bài toán Prediction\n**Mục tiêu**: Đánh giá một chính sách π cho trước\n- Input: Chính sách π\n- Output: Hàm giá trị V^π(s)\n- Không cần biết mô hình môi trường\n\n### 2. Monte Carlo Methods - Phương Pháp Monte Carlo\n\n#### 2.1. Ý tưởng cơ bản\nMonte Carlo (MC) ước lượng giá trị của trạng thái bằng cách lấy trung bình các return thực tế nhận được từ nhiều episodes.\n\n**Nguyên lý**:\n```\nV^π(s) = E_π[G_t | S_t = s]\n       ≈ average of returns từ trạng thái s\n```\n\n#### 2.2. Episode và Return\n\n**Episode**: Một chuỗi hoàn chỉnh từ trạng thái ban đầu đến kết thúc\n```\nS_0, A_0, R_1, S_1, A_1, R_2, ..., S_T\n```\n\n**Return**: Tổng phần thưởng chiết khấu\n```\nG_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... + γ^{T-t-1}R_T\n```\n\n#### 2.3. First-Visit Monte Carlo\n\n**Thuật toán**:\n```\nKhởi tạo:\n    V(s) = 0, ∀s\n    Returns(s) = danh sách rỗng, ∀s\n\nVới mỗi episode:\n    Tạo một episode tuân theo π: S_0, A_0, R_1, ..., S_T\n    G = 0\n    \n    Với mỗi bước t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        \n\n**Các khái niệm quan trọng:**\n- Monte Carlo (MC) là một nhóm các phương pháp model-free, không bootstrap trong Reinforcement Learning, học trực tiếp từ kinh nghiệm mà không yêu cầu biết mô hình môi trường. MC ước lượng giá trị của trạng thái hoặc cặp trạng thái-hành động, hoặc học chính sách, bằng cách lấy trung bình các return (tổng phần thưởng chiết khấu thực tế) nhận được từ nhiều episode hoàn chỉnh (từ trạng thái ban đầu đến trạng thái kết thúc). Việc học chỉ diễn ra sau khi một episode kết thúc. MC có bias thấp nhưng variance cao, chỉ phù hợp với các tác vụ episodic và có thể hội tụ đến local optimum hoặc global optimum với xấp xỉ hàm tuyến tính.\n\n**Mối quan hệ:**\n- Monte Carlo Methods yêu cầu các episode hoàn chỉnh để tính toán return.\n- Monte Carlo Methods ước lượng giá trị của trạng thái V^π(s)."
        },
        "output_without_context": {
            "question": "So sánh ưu điểm nào của phương pháp Monte-Carlo với phương pháp Temporal-Difference trong việc học từ các chuỗi khác nhau?",
            "answer": "Monte-Carlo có khả năng học từ các chuỗi hoàn chỉnh, trong khi Temporal-Difference có thể cập nhật giá trị ngay cả khi chưa có chuỗi hoàn chỉnh."
        }
    },
    {
        "topic_description": "Chủ đề này sẽ yêu cầu học sinh áp dụng các khái niệm lập trình động từ tuần trước vào các tình huống thực tế trong MDPs, cho thấy sự tương tác giữa lập trình động và Học tăng cường không mô hình. Điều này giúp kiểm tra khả năng liên kết kiến thức từ các bài giảng trước.",
        "output_with_context": {
            "question": "Trong quy trình đánh giá chính sách (Policy Evaluation) của thuật toán Policy Iteration, phương trình Bellman cho hàm giá trị trạng thái V^π(s) được định nghĩa như thế nào?",
            "answer": "V^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n#### 13.1. Approximate Dynamic Programming\n- Sử dụng function approximation\n- Neural networks để biểu diễn V hoặc π\n- Trade-off giữa accuracy và scalability\n\n#### 13.2. Model-Free Methods\n- Không cần biết P và R\n- Học từ experience\n- Temporal-Difference Learning, Q-Learning (phần sau)\n\n#### 13.3. Deep Reinforcement Learning\n- Kết hợp DP với deep learning\n- DQN, Actor-Critic, PPO\n- Giải quyết được bài toán phức tạp\n\n### 14. Code Implementation - Ví dụ Python\n\n#### 14.1. Policy Iteration\n```python\ndef policy_iteration(env, gamma=0.9, theta=1e-6):\n    # Khởi tạo\n    V = np.zeros(env.num_states)\n    policy = np.zeros(env.num_states, dtype=int)\n    \n    while True:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.num_states):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([p * (r + gamma * V[s_]) \n                           for p, s_, r in env.transitions(s, a)])\n                delta = max(delta, abs(v - V[s]))\n            if delta < theta:\n                break\n        \n        # Policy Improvement\n        policy_stable = True\n        for s in range(env.num_states):\n            old_action = policy[s]\n            # Tìm hành động tốt nhất\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            policy[s] = np.argmax(action_values)\n            \n            if old_action != policy[s]:\n                policy_stable = False\n        \n        if policy_stable:\n            return V, policy\n```\n\n#### 14.2. Value Iteration\n```python\ndef value_iteration(env, gamma=0.9, theta=1e-6):\n    V = np.zeros(env.num_states)\n    \n    while True:\n        delta = 0\n        for s in range(env.num_states):\n            v = V[s]\n            # Bellman optimality update\n            action_values = []\n            for a in range(env.num_actions):\n                q_value = sum([p * (r + gamma * V[s_]) \n                              for p, s_, r in env.transitions(s, a)])\n                action_values.append(q_value)\n            V[s] = max(action_values)\n            delta = max(delta, abs(v - V[s]))\n        \n        if delta < theta:\n            break\n    \n    # Trích xuất chính sách\n    policy = np.zeros(env.num_states, dtype=int)\n    for s in range(env.num_states):\n        action_values = []\n        for a in range(env.num_actions):\n            q_value = sum([p * (r + gamma * V[s_]) \n                          for p, s_, r in env.transitions(s, a)])\n            action_values.append(q_value)\n        policy[s] = np.argmax(action_values)\n    \n    return V, policy\n```\n\n### 15. Bài tập thực hành\n\n#### 15.1. Bài tập cơ bản\n1. Implement policy evaluation cho Gridworld\n2. So sánh tốc độ hội tụ của in-place và two-array DP\n3. Visualize quá trình hội tụ của value iteration\n\n#### 15.2. Bài tập nâng cao\n1. Giải Gambler's Problem với các giá trị p khác nhau\n2. Implement prioritized sweeping\n3. Modified policy iteration với k=1, k=3, k=∞\n\n#### 15.3. Dự án\n1. Xây dựng AI cho game 2048 bằng DP\n2. Tối ưu hóa việc sạc pin cho robot\n3. Quản lý danh mục đầu tư bằng MDP\n\n### 16. Kết luận\n\n\n**Các khái niệm quan trọng:**\n- Planning by Dynamic Programming (Lập kế hoạch bằng Quy hoạch động) là một phương pháp trong Reinforcement Learning sử dụng các thuật toán quy hoạch động để giải quyết các bài toán MDP khi mô hình môi trường (xác suất chuyển trạng thái P và phần thưởng R) được biết. Các thuật toán như Policy Iteration và Value Iteration thuộc nhóm này, chúng tính toán hàm giá trị và chính sách tối ưu bằng cách lặp đi lặp lại các phép tính dựa trên Bellman Equation.\n- Quản lý danh mục đầu tư bằng MDP là một ứng dụng của Reinforcement Learning trong lĩnh vực tài chính. Bài toán này mô hình hóa quá trình ra quyết định đầu tư như một Markov Decision Process, nơi agent (nhà đầu tư) học cách phân bổ tài sản vào các loại hình đầu tư khác nhau để tối đa hóa lợi nhuận hoặc giảm thiểu rủi ro theo thời gian.\n\n**Mối quan hệ:**\n- Deep Reinforcement Learning kết hợp các ý tưởng của Planning by Dynamic Programming với deep learning.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Integrating Learning and Planning - Tích Hợp Học và Lập Kế Hoạch\n### 8. Ứng dụng thực tế\n\n#### 8.1. Robotics\n\n**Manipulation**:\n- Learn forward model của robot\n- MPC cho grasping\n- Fast adaptation\n\n**Locomotion**:\n- Model-based để bootstrap learning\n- Transfer từ simulation\n\n#### 8.2. Games\n\n**Board Games** (Chess, Go):\n- Perfect models\n- MCTS dominates\n\n**Video Games**:\n- Approximate models\n- World models + RL\n\n#### 8.3. Autonomous Driving\n\n**Prediction Models**:\n- Predict other vehicles behavior\n- Plan safe trajectories\n- Contingency planning\n\n### 9. Code Example: Simple Dyna-Q\n\n```python\nclass DynaQ:\n    def __init__(self, num_states, num_actions, \n                 alpha=0.1, gamma=0.99, planning_steps=5):\n        self.Q = np.zeros((num_states, num_actions))\n        self.model = {}  # (s,a) -> (r, s')\n        self.alpha = alpha\n        self.gamma = gamma\n        self.n = planning_steps\n    \n    def update(self, s, a, r, s_next):\n        # Direct RL update\n        best_next = np.max(self.Q[s_next])\n        self.Q[s, a] += self.alpha * (r + self.gamma * best_next - self.Q[s, a])\n        \n        # Model learning\n        self.model[(s, a)] = (r, s_next)\n        \n        # Planning\n        for _ in range(self.n):\n            # Random previously seen state-action\n            s_sim, a_sim = random.choice(list(self.model.keys()))\n            r_sim, s_next_sim = self.model[(s_sim, a_sim)]\n            \n            # Simulated update\n            best_next_sim = np.max(self.Q[s_next_sim])\n            self.Q[s_sim, a_sim] += self.alpha * (\n                r_sim + self.gamma * best_next_sim - self.Q[s_sim, a_sim])\n```\n\n### 10. Kết luận\n\nIntegrating Learning and Planning kết hợp sức mạnh của cả model-free và model-based RL.\n\n**Key Insights**:\n\n1. **Dyna**: Simple và effective integration\n2. **MCTS**: Powerful search algorithm, basis của AlphaGo/AlphaZero\n3. **MPC**: Optimal control với learned models\n4. **World Models**: Learn và plan in latent space\n5. **Model Errors**: Cần careful handling\n\n**Trade-offs**:\n- Sample efficiency ↔ Model error\n- Planning cost ↔ Better policies\n- Model complexity ↔ Accuracy vs generalization\n\n**Best Practices**:\n- Use short horizon planning\n- Ensemble models cho uncertainty\n- Combine với model-free learning\n- Careful với compounding errors\n\n---\n\n\n**Các khái niệm quan trọng:**\n- Integrating Learning and Planning là một khái niệm trong Học tăng cường, đề cập đến việc kết hợp các phương pháp học (learning) từ kinh nghiệm với các phương pháp lập kế hoạch (planning) sử dụng một mô hình môi trường. Mục tiêu là tận dụng ưu điểm của cả hai cách tiếp cận để đạt được hiệu suất tốt hơn, đặc biệt là về hiệu quả mẫu và khả năng tìm kiếm chính sách tối ưu.\n- Model Learning là một thành phần của Dyna Architecture, đặc biệt là trong thuật toán Dyna-Q, nơi agent học một mô hình của môi trường từ kinh nghiệm thực tế. Mô hình này được cập nhật dựa trên các bộ bốn (trạng thái, hành động, phần thưởng, trạng thái tiếp theo) đã quan sát, lưu trữ cặp (phần thưởng, trạng thái tiếp theo) tương ứng với cặp (trạng thái, hành động) đã thực hiện (ví dụ: self.model[(s, a)] = (r, s_next)). Mô hình đã học này sau đó được sử dụng để lập kế hoạch hoặc tạo ra kinh nghiệm giả lập.\n\n**Mối quan hệ:**\n- Integrating Learning and Planning kết hợp sức mạnh của cả model-free RL (học từ kinh nghiệm) và model-based RL (lập kế hoạch với mô hình).\n- Dyna-Q sử dụng kỹ thuật Model learning để xây dựng và cập nhật mô hình môi trường từ các kinh nghiệm thực tế đã quan sát.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Markov Decision Processes - Quá Trình Quyết Định Markov\n- Tối ưu hóa việc phân bổ tài nguyên\n- Quản lý inventory\n\n#### 11.3. Game AI\n- Tạo ra đối thủ thông minh\n- Cân bằng game\n\n#### 11.4. Tài chính\n- Tối ưu hóa portfolio\n- Quản lý rủi ro\n\n### 12. Thách thức và giới hạn\n\n#### 12.1. Curse of dimensionality\n- Số trạng thái tăng theo cấp số nhân với số chiều\n- Khó giải quyết với không gian trạng thái lớn\n\n#### 12.2. Môi trường không hoàn toàn quan sát được\n- MDP chuẩn giả định biết hoàn toàn trạng thái\n- Thực tế thường chỉ có quan sát một phần (POMDP)\n\n#### 12.3. Mô hình không chính xác\n- Giả định biết P và R\n- Thực tế thường phải học từ tương tác\n\n### 13. Kết luận\n\nMDP cung cấp một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự. Hiểu rõ các khái niệm cơ bản của MDP là nền tảng để nghiên cứu sâu hơn về học tăng cường, bao gồm các phương pháp model-free và deep reinforcement learning.\n\n---\n\n## Planning by Dynamic Programming - Lập Kế Hoạch Bằng Quy Hoạch Động\n\n### 1. Giới thiệu về Dynamic Programming\n\n#### 1.1. Định nghĩa\nDynamic Programming (DP) là một phương pháp giải quyết các bài toán phức tạp bằng cách chia nhỏ thành các bài toán con đơn giản hơn, giải quyết từng bài toán con một lần và lưu trữ kết quả để tái sử dụng.\n\n#### 1.2. Điều kiện áp dụng DP\n- **Optimal Substructure**: Nghiệm tối ưu có thể được xây dựng từ nghiệm tối ưu của các bài toán con\n- **Overlapping Subproblems**: Các bài toán con được giải đi giải lại nhiều lần\n\n#### 1.3. DP trong Reinforcement Learning\n- Yêu cầu biết hoàn toàn mô hình MDP (model-based)\n- Sử dụng phương trình Bellman để tính toán hàm giá trị\n- Làm nền tảng cho các phương pháp học tăng cường khác\n\n### 2. Policy Evaluation - Đánh Giá Chính Sách\n\n#### 2.1. Mục tiêu\nTính toán hàm giá trị trạng thái V^π(s) cho một chính sách π cho trước.\n\n#### 2.2. Phương trình Bellman cho Policy Evaluation\n```\nV^π(s) = Σ_a π(a|s)[R(s,a) + γ Σ_{s'} P(s'|s,a)V^π(s')]\n```\n\n#### 2.3. Thuật toán Iterative Policy Evaluation\n\n**Bước 1: Khởi tạo**\n```\nV(s) = 0, ∀s ∈ S (hoặc giá trị ngẫu nhiên)\n```\n\n**Bước 2: Lặp cho đến khi hội tụ**\n```\nLặp:\n    Δ = 0\n    Với mỗi s ∈ S:\n        v = V(s)\n\n**Các khái niệm quan trọng:**\n- Markov Decision Process (MDP) là một framework toán học mạnh mẽ để mô hình hóa các bài toán ra quyết định tuần tự, nơi một agent tương tác với môi trường để tối đa hóa tổng phần thưởng chiết khấu. Trong MDP, kết quả của các quyết định có một phần ngẫu nhiên và một phần nằm dưới sự kiểm soát của người ra quyết định. MDP cung cấp nền tảng toán học cho hầu hết các bài toán học tăng cường (Reinforcement Learning) và được định nghĩa bởi các thành phần: tập trạng thái S, tập hành động A, hàm phần thưởng R, xác suất chuyển trạng thái P, và discount factor γ.\n\n**Mối quan hệ:**\n- Mô hình không chính xác là một thách thức khi các giả định về P và R trong Markov Decision Processes không được biết trước hoặc không chính xác.\n- Môi trường không hoàn toàn quan sát được là một giới hạn của mô hình Markov Decision Processes chuẩn, dẫn đến việc sử dụng POMDP.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Policy Gradient Methods - Phương Pháp Gradient Chính Sách\n6. **State-of-the-art**: PPO và SAC là go-to choices hiện nay\n\n**Evolution**:\n```\nREINFORCE → Actor-Critic → A3C → TRPO → PPO\n                    ↓\n               DDPG → TD3 → SAC\n```\n\n**Practical Advice**:\n- Start với PPO cho discrete actions\n- Use SAC cho continuous control\n- Tune hyperparameters carefully\n- Use parallel workers khi có thể\n- Monitor training metrics (returns, entropy, losses)\n\nReinforcement Learning đã phát triển mạnh mẽ và Policy Gradient Methods là công cụ quan trọng trong arsenal của RL researcher!\n\n---\n\n## Integrating Learning and Planning - Tích Hợp Học và Lập Kế Hoạch\n\n### 1. Giới thiệu\n\n#### 1.1. Model-Free vs Model-Based RL\n\n**Model-Free RL**:\n- Học trực tiếp từ experience\n- Không build model của environment\n- Ví dụ: Q-Learning, SARSA, Policy Gradient\n\n**Model-Based RL**:\n- Học model của environment\n- Sử dụng model để planning\n- Có thể kết hợp với learning\n\n#### 1.2. Lợi ích của Model-Based RL\n\n✅ **Sample Efficiency**: Model cho phép reuse experience\n✅ **Planning**: Có thể simulate và plan ahead\n✅ **Transfer**: Model có thể transfer sang tasks khác\n✅ **Interpretability**: Hiểu được dynamics của environment\n\n**Trade-offs**:\n❌ Model error có thể compound\n❌ Computational cost của planning\n❌ Complexity trong implementation\n\n### 2. Models trong RL\n\n#### 2.1. Model Representation\n\n**Transition Model**:\n```\nP(s'|s,a) = Probability of next state\nhoặc\ns' = f(s, a) + noise  (deterministic + noise)\n```\n\n**Reward Model**:\n```\nR(s,a) = Expected reward\nhoặc\nr ~ P(r|s,a)\n```\n\n#### 2.2. Learning Models\n\n**Table Lookup** (Discrete):\n```\nCount(s,a,s') = số lần chuyển từ s đến s' với action a\nP̂(s'|s,a) = Count(s,a,s') / Σ_{s''} Count(s,a,s'')\n```\n\n**Function Approximation**:\n```\nNeural Network: s' = NN(s, a; θ)\nGaussian Process: s' ~ GP(s, a)\n```\n\n**Ensemble Models**: Nhiều models để estimate uncertainty\n\n### 3. Dyna Architecture\n\n#### 3.1. Ý tưởng Dyna\n\n**Integration**: Kết hợp direct RL và planning\n\n**Components**:\n1. **Direct RL**: Học từ real experience\n2. **Model Learning**: Học model từ experience\n3. **Planning**: Sử dụng model để generate simulated experience\n\n#### 3.2. Dyna-Q Algorithm\n\n```\nKhởi tạo Q(s,a) và Model(s,a)\nParameters: n (planning steps)\n\nLặp:\n    # (a) Direct RL\n    Observe (S, A, R, S')\n    Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n    \n    # (b) Model Learning\n    Model(S,A) ← (R, S')  # Store observed transition\n    \n    # (c) Planning\n    Lặp n lần:\n        S_sim ← random previously observed state\n        A_sim ← random action from S_sim\n        (R_sim, S'_sim) ← Model(S_sim, A_sim)\n        Q(S_sim, A_sim) ← Q(S_sim, A_sim) + \n                          α[R_sim + γ max_a Q(S'_sim, a) - Q(S_sim, A_sim)]\n```\n\n**Đặc điểm**:\n- Mỗi real experience update cả Q và Model\n- Planning steps tăng sample efficiency\n- Convergence nhanh hơn model-free\n\n#### 3.3. Ví dụ: Dyna Maze\n\n**Setup**:\n- Gridworld maze\n- Goal: Reach target\n- Reward: -1 per step\n\n**Results**:\n- Dyna-Q với n=5: ~15 episodes để solve\n\n**Các khái niệm quan trọng:**\n- Planning trong Dyna Architecture là quá trình sử dụng mô hình môi trường đã học để tạo ra kinh nghiệm giả lập (simulated experience). Kinh nghiệm giả lập này sau đó được sử dụng để cập nhật hàm giá trị hoặc chính sách, tương tự như cách Direct RL sử dụng kinh nghiệm thực tế, giúp tăng hiệu quả mẫu.\n\n**Mối quan hệ:**\n- Dyna Architecture tích hợp Planning để sử dụng model tạo simulated experience."
        },
        "output_without_context": {
            "question": "Trong bối cảnh của MDPs, lập trình động có thể được áp dụng để tối ưu hóa chính sách nào trong việc ra quyết định?",
            "answer": "Chính sách tối ưu"
        }
    },
    {
        "topic_description": "Chủ đề này khai thác phương pháp dự đoán n-Bước và lợi nhuận lambda trong Học tăng cường, cho phép học sinh hiểu sâu về cách thức tính toán lợi nhuận và ứng dụng trong thực tế. Học sinh sẽ cần áp dụng các công thức và lý thuyết liên quan.",
        "output_with_context": {
            "question": "Công thức nào được sử dụng để tính toán λ-Return trong Học tăng cường?",
            "answer": "G_t^λ = (1-λ) Σ_{n=1}^∞ λ^{n-1} G_t^(n)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nB, 1 (terminal)\nB, 1 (terminal)\nB, 1 (terminal)\nB, 0 (terminal)\n```\n\n**MC Estimate**:\n- V(A) = 0 (chỉ có 1 episode từ A, return = 0)\n- V(B) = 5/6 ≈ 0.83\n\n**TD Estimate**:\n- V(B) = 5/6 (từ data)\n- V(A) = 5/6 (vì 100% chuyển đến B)\n\n**TD tốt hơn**: Exploit structure, generalize better\n\n### 5. Unified View: TD(λ)\n\n#### 5.1. n-Step TD\n\n**Ý tưởng**: Thay vì bootstrap sau 1 bước, sử dụng n bước\n\n**n-Step Return**:\n```\nG_t^(n) = R_{t+1} + γR_{t+2} + ... + γ^{n-1}R_{t+n} + γ^n V(S_{t+n})\n```\n\n**Update**:\n```\nV(S_t) ← V(S_t) + α[G_t^(n) - V(S_t)]\n```\n\n**Đặc điểm**:\n- n=1: TD(0)\n- n=∞: Monte Carlo\n- n trung gian: Cân bằng bias và variance\n\n#### 5.2. TD(λ) - Eligibility Traces\n\n**Motivation**: Thay vì chọn một n, kết hợp tất cả n-step returns\n\n**λ-Return**:\n```\nG_t^λ = (1-λ) Σ_{n=1}^∞ λ^{n-1} G_t^(n)\n```\n\n**Eligibility Trace**:\n```\nE_0(s) = 0\nE_t(s) = γλE_{t-1}(s) + 1(S_t = s)\n```\n\n**TD(λ) Update**:\n```\nδ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\nV(s) ← V(s) + αδ_t E_t(s), ∀s\n```\n\n#### 5.3. Forward View vs Backward View\n\n**Forward View**:\n- Nhìn về tương lai\n- Tính G_t^λ từ future returns\n- Offline algorithm\n\n**Backward View**:\n- Nhìn về quá khứ\n- Sử dụng eligibility traces\n- Online algorithm\n- Computationally efficient\n\n**Quan hệ**: Hai view tương đương về mặt toán học\n\n#### 5.4. Chọn λ\n\n| λ | Đặc điểm | Khi nào dùng |\n|---|----------|--------------|\n| 0 | TD(0), low variance | Môi trường noisy |\n| 0.5-0.9 | Cân bằng | Thường dùng nhất |\n| 1 | MC, unbiased | Episodic, deterministic |\n\n### 6. Các biến thể TD nâng cao\n\n#### 6.1. Double Learning\n\n**Vấn đề**: Maximization bias trong TD\n```\nOverestimation: E[max(X₁, X₂)] ≥ max(E[X₁], E[X₂])\n```\n\n**Giải pháp**: Duy trì 2 value functions\n```\nV₁(S) và V₂(S)\n\nUpdate V₁:\n    V₁(S) ← V₁(S) + α[R + γV₂(S') - V₁(S)]\n    \nUpdate V₂:\n    V₂(S) ← V₂(S) + α[R + γV₁(S') - V₂(S)]\n```\n\n#### 6.2. Gradient TD\n\n**Motivation**: TD không theo gradient descent thực sự\n\n**True Gradient TD (TDC)**:\n```\nMinimize: MSBE = ||V - Π_T^π V||²\n\nUpdate:\n    w ← w + α(R + γV(S') - V(S))∇V(S)\n    - αγ(∇V(S'))⊤w ∇V(S)\n```\n\n**Lợi ích**: Hội tụ vững vàng hơn với function approximation\n\n### 7. Ứng dụng thực tế\n\n\n**Các khái niệm quan trọng:**\n- Đây là công thức của λ-Return. G_t^λ là tổng trọng số của tất cả các n-step returns G_t^(n), với trọng số (1-λ)λ^{n-1}. λ là tham số trong khoảng [0,1] kiểm soát mức độ kết hợp các n-step returns. Khi λ=0, G_t^λ tương đương với G_t^(1) (TD(0)). Khi λ=1, G_t^λ tương đương với G_t^(∞) (Monte Carlo).\n- λ-Return (G_t^λ) là một tổng trọng số của tất cả các n-step returns (G_t^(n)) với trọng số giảm dần theo λ. Công thức: G_t^λ = (1-λ) Σ_{n=1}^∞ λ^{n-1} G_t^(n). Nó cung cấp một cách để kết hợp các lợi ích của Monte Carlo (n=∞) và TD(0) (n=1) trong một mục tiêu cập nhật duy nhất, được sử dụng trong thuật toán TD(λ).\n\n**Mối quan hệ:**\n- TD(λ) sử dụng λ-Return làm mục tiêu cập nhật cho hàm giá trị của nó, kết hợp các n-step returns.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nPhần tiếp theo sẽ mở rộng sang **Model-Free Control**: Không chỉ đánh giá mà còn tìm chính sách tối ưu mà không cần mô hình!\n\n---\n\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n\n### 1. Giới thiệu về Model-Free Control\n\n#### 1.1. Định nghĩa bài toán\n**Control Problem**: Tìm chính sách tối ưu π* mà không biết trước mô hình môi trường (P, R)\n\n**So sánh với Prediction**:\n- Prediction: Đánh giá V^π cho π cho trước\n- Control: Tối ưu hóa π để maximize V^π\n\n#### 1.2. Thách thức\n- Không biết mô hình → không thể dùng Dynamic Programming\n- Phải học từ interaction với môi trường\n- Cần cân bằng exploration và exploitation\n\n#### 1.3. Ý tưởng chính\nSử dụng **Generalized Policy Iteration (GPI)** framework:\n```\nPolicy Evaluation (Model-Free) → Policy Improvement → Repeat\n```\n\n### 2. Monte Carlo Control\n\n#### 2.1. Từ V(s) sang Q(s,a)\n\n**Vấn đề với V(s)**:\n```\nPolicy Improvement cần:\nπ'(s) = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a)V(s')]\n                 ↑ Cần biết mô hình!\n```\n\n**Giải pháp**: Sử dụng Q(s,a)\n```\nπ'(s) = argmax_a Q(s,a)  ← Không cần mô hình!\n```\n\n#### 2.2. Monte Carlo Policy Iteration\n\n**Thuật toán**:\n```\n1. Khởi tạo:\n   Q(s,a) = 0, ∀s,a\n   π = chính sách khởi tạo\n\n2. Lặp:\n   a) Policy Evaluation (MC):\n      - Tạo nhiều episodes theo π\n      - Cập nhật Q^π(s,a) bằng MC\n   \n   b) Policy Improvement:\n      π(s) = argmax_a Q(s,a), ∀s\n```\n\n#### 2.3. Vấn đề Exploration\n\n**Greedy Policy**:\n```\nπ(s) = argmax_a Q(s,a)\n```\n→ Chỉ exploit, không explore → Có thể bỏ lỡ chính sách tốt hơn\n\n**Giải pháp 1: ε-Greedy Policy**\n```\nπ(a|s) = {\n    1 - ε + ε/|A|,  nếu a = argmax Q(s,a)\n    ε/|A|,          ngược lại\n}\n```\n\n**Đặc điểm**:\n- Xác suất 1-ε: Chọn hành động tốt nhất (exploit)\n- Xác suất ε: Chọn ngẫu nhiên (explore)\n- ε decay theo thời gian: Explore nhiều lúc đầu, exploit nhiều sau\n\n#### 2.4. ε-Greedy Monte Carlo Control\n\n**Thuật toán**:\n```\nKhởi tạo:\n    Q(s,a) arbitrarily, ∀s,a\n    π = ε-greedy policy dựa trên Q\n    Returns(s,a) = empty list, ∀s,a\n\nLặp forever:\n    1. Tạo episode theo π:\n       S_0, A_0, R_1, ..., S_T\n    \n    2. Với mỗi cặp (s,a) xuất hiện trong episode:\n       G = return sau lần xuất hiện đầu tiên\n       Thêm G vào Returns(s,a)\n       Q(s,a) = average(Returns(s,a))\n    \n    3. Với mỗi s trong episode:\n       π(s) = ε-greedy(Q(s,·))\n```\n\n#### 2.5. Greedy in the Limit of Infinite Exploration (GLIE)\n\n\n**Các khái niệm quan trọng:**\n- Prediction trong Reinforcement Learning là bài toán đánh giá hàm giá trị V^π cho một chính sách π đã cho, tức là ước lượng giá trị kỳ vọng của tổng phần thưởng chiết khấu khi tuân theo chính sách π.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\nBackpropagation là thuật toán cốt lõi để huấn luyện mạng nơ-ron sâu, cho phép tính gradient một cách hiệu quả thông qua quy tắc chuỗi (chain rule).\n\n**Ý tưởng cơ bản:**\n- Tính toán gradient của loss function theo tất cả các tham số (weights và biases)\n- Lan truyền gradient từ output về input qua các lớp\n- Sử dụng quy tắc chuỗi để phân rã gradient phức tạp thành các phần đơn giản\n\n**Quy tắc chuỗi (Chain Rule):**\n$$\frac{\\partial L}{\\partial w^{[l]}} = \frac{\\partial L}{\\partial a^{[l]}} \\cdot \frac{\\partial a^{[l]}}{\\partial z^{[l]}} \\cdot \frac{\\partial z^{[l]}}{\\partial w^{[l]}}$$\n\nTrong đó:\n- $\frac{\\partial L}{\\partial a^{[l]}}$: Gradient của loss theo activation\n- $\frac{\\partial a^{[l]}}{\\partial z^{[l]}}$: Đạo hàm của hàm kích hoạt\n- $\frac{\\partial z^{[l]}}{\\partial w^{[l]}}$: Gradient của pre-activation theo weights\n\n**Các bước chi tiết:**\n\n**1. Forward Pass (Lan truyền xuôi):**\n- Tính toán output của mỗi lớp từ input đến output\n- Lưu trữ tất cả các giá trị $z^{[l]}$ và $a^{[l]}$ (cần cho backward pass)\n\n**2. Tính Loss:**\n- So sánh prediction với ground truth\n- Tính giá trị loss: $L = Loss(y, \\hat{y})$\n\n**3. Backward Pass (Lan truyền ngược):**\n- Bắt đầu từ lớp output, tính gradient của loss theo output\n- Với mỗi lớp từ L về 1:\n  - Tính $\frac{\\partial L}{\\partial z^{[l]}} = \frac{\\partial L}{\\partial a^{[l]}} \\odot \\sigma'(z^{[l]})$ (element-wise product)\n  - Tính $\frac{\\partial L}{\\partial W^{[l]}} = \frac{\\partial L}{\\partial z^{[l]}} \\cdot (a^{[l-1]})^T$\n  - Tính $\frac{\\partial L}{\\partial b^{[l]}} = \frac{\\partial L}{\\partial z^{[l]}}$\n  - Lan truyền về lớp trước: $\frac{\\partial L}{\\partial a^{[l-1]}} = (W^{[l]})^T \\cdot \frac{\\partial L}{\\partial z^{[l]}}$\n\n**4. Cập nhật Weights:**\n- Sử dụng gradient descent hoặc các optimizer khác\n- $W^{[l]} := W^{[l]} - \\alpha \frac{\\partial L}{\\partial W^{[l]}}$\n- $b^{[l]} := b^{[l]} - \\alpha \frac{\\partial L}{\\partial b^{[l]}}$\n\n**Ví dụ minh họa:**\nMạng 2 lớp: Input → Hidden → Output\n- Forward: $a^{[1]} = \\sigma(W^{[1]}x + b^{[1]})$, $\\hat{y} = \\sigma(W^{[2]}a^{[1]} + b^{[2]})$\n- Loss: $L = (y - \\hat{y})^2$\n- Backward:\n  - $\frac{\\partial L}{\\partial \\hat{y}} = -2(y - \\hat{y})$\n  - $\frac{\\partial L}{\\partial W^{[2]}} = \frac{\\partial L}{\\partial \\hat{y}} \\cdot \\sigma'(z^{[2]}) \\cdot a^{[1]}$\n  - Lan truyền về hidden layer tương tự\n\n**Computational Graph:**\n\n**Các khái niệm quan trọng:**\n- Đây là công thức tính gradient của hàm loss $L = (y - \\hat{y})^2$ theo giá trị dự đoán $\\hat{y}$. Nó cho biết mức độ thay đổi của loss khi giá trị dự đoán thay đổi, và được sử dụng làm điểm khởi đầu cho Backward Pass trong ví dụ minh họa.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n**Định nghĩa**: Một dãy chính sách {π_t} thỏa mãn GLIE nếu:\n1. Mọi cặp (s,a) được visit vô hạn lần\n2. Chính sách hội tụ đến greedy policy\n\n**Ví dụ GLIE**:\n```\nε_t = 1/t\n```\n\n**Định lý**: GLIE Monte Carlo Control hội tụ đến π*\n\n#### 2.6. Ví dụ: Blackjack với MC Control\n\n**Setup**:\n- State: (player_sum, dealer_card, usable_ace)\n- Action: Hit hoặc Stick\n- Reward: +1 (win), -1 (lose), 0 (draw)\n\n**Kết quả**:\n```\nChính sách học được:\n- Stick khi player_sum ≥ 20\n- Hit khi player_sum < 12\n- Phức tạp hơn ở vùng 12-19 (phụ thuộc dealer card)\n```\n\n### 3. On-Policy vs Off-Policy Learning\n\n#### 3.1. Định nghĩa\n\n**On-Policy**:\n- Học về chính sách π từ experience generated bởi π\n- Evaluate và improve cùng một chính sách\n- Ví dụ: SARSA, Monte Carlo Control\n\n**Off-Policy**:\n- Học về chính sách π (target) từ experience của μ (behavior)\n- π ≠ μ\n- Ví dụ: Q-Learning, Importance Sampling\n\n#### 3.2. So sánh\n\n| Đặc điểm | On-Policy | Off-Policy |\n|----------|-----------|------------|\n| Chính sách | Một chính sách | Hai chính sách |\n| Sample efficiency | Thấp hơn | Cao hơn |\n| Variance | Thấp | Cao |\n| Converge | Ổn định | Có thể diverge |\n| Use old data | Không | Có thể |\n| Learn optimal | Không (nếu ε-greedy) | Có |\n\n### 4. SARSA - On-Policy TD Control\n\n#### 4.1. Ý tưởng\nÁp dụng TD(0) cho Q(s,a) thay vì V(s)\n\n**TD Update cho Q**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n```\n\n**Tên gọi**: SARSA = (S, A, R, S', A')\n\n#### 4.2. Thuật toán SARSA\n\n```\nKhởi tạo Q(s,a) arbitrarily, ∀s ∈ S, a ∈ A\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    Chọn A từ S theo ε-greedy(Q)\n    \n    Lặp với mỗi bước:\n        Thực hiện A, quan sát R, S'\n        Chọn A' từ S' theo ε-greedy(Q)\n        \n        Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n        \n        S ← S'; A ← A'\n    cho đến S là terminal\n```\n\n#### 4.3. Tính chất của SARSA\n\n**Hội tụ**:\n- GLIE schedule + Robbins-Monro conditions → Q → Q*\n- Trong thực tế, dùng ε nhỏ cố định hoặc decay\n\n**On-Policy**:\n- Học về chính sách ε-greedy đang dùng\n- Safe: Tính đến exploration trong học\n\n#### 4.4. Ví dụ: Windy Gridworld\n\n**Mô tả**:\n- Grid 7×10 với \"wind\" ở một số cột\n- Wind đẩy agent lên 1-2 ô\n- Start: (3,0), Goal: (3,7)\n- Actions: 4 hướng\n\n**Kết quả SARSA**:\n\n**Các khái niệm quan trọng:**\n- On-Policy Learning là một phương pháp học tăng cường trong đó agent học về chính sách π từ kinh nghiệm được tạo ra bởi chính sách π đó, nghĩa là chính sách được sử dụng để tạo dữ liệu (behavior policy) cũng chính là chính sách đang được cải thiện (target policy), với các ví dụ điển hình như SARSA và Monte Carlo Control. Ngược lại, Off-Policy Learning là một phương pháp học tăng cường mà agent học về một chính sách π (target policy) từ kinh nghiệm được tạo ra bởi một chính sách khác μ (behavior policy), trong đó π ≠ μ, cho phép agent học về chính sách tối ưu trong khi vẫn thực hiện các hành động khám phá theo một chính sách khác, với các ví dụ điển hình là Q-Learning và Importance Sampling.\n\n**Mối quan hệ:**\n- SARSA là một ví dụ của thuật toán On-Policy Learning.\n- On-Policy Learning đánh giá và cải thiện cùng một chính sách π.\n- On-Policy Learning học về chính sách π từ kinh nghiệm được tạo ra bởi chính sách π.\n- Monte Carlo Control là một ví dụ của thuật toán On-Policy Learning."
        },
        "output_without_context": {
            "question": "Trong phương pháp dự đoán n-Bước trong Học tăng cường, lợi nhuận lambda được tính toán như thế nào để tối ưu hóa quá trình học?",
            "answer": "Lợi nhuận lambda được tính bằng cách kết hợp lợi nhuận ngắn hạn và dài hạn, thường được biểu diễn dưới dạng một trọng số giữa giá trị hiện tại và giá trị kỳ vọng."
        }
    },
    {
        "topic_description": "Chủ đề này khám phá sự tương đương giữa TD(λ) và Monte-Carlo/TD(0), yêu cầu học sinh nắm vững các khái niệm sâu hơn về Học tăng cường. Điều này sẽ cung cấp cái nhìn tổng quan và phân tích sâu hơn về các thuật toán này.",
        "output_with_context": {
            "question": "Sự tương đương giữa TD(λ) và Monte-Carlo/TD(0) có thể được đánh giá dựa trên yếu tố nào sau đây?",
            "answer": "TD(λ) cung cấp một phổ liên tục giữa Monte Carlo và TD(0), cho phép cập nhật giá trị dựa trên một chuỗi các bước.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Control - Điều Khiển Không Cần Mô Hình\n- Học được đường đi tối ưu sau ~170 episodes\n- Tận dụng wind để di chuyển nhanh hơn\n- An toàn hơn Q-learning (tính đến exploration)\n\n### 5. Q-Learning - Off-Policy TD Control\n\n#### 5.1. Ý tưởng chính\nHọc về chính sách tham lam (greedy) trong khi hành động theo ε-greedy\n\n**Q-Learning Update**:\n```\nQ(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\n                                            ↑ Greedy!\n```\n\n#### 5.2. Thuật toán Q-Learning\n\n```\nKhởi tạo Q(s,a) arbitrarily, ∀s,a\nThiết lập α, ε\n\nLặp với mỗi episode:\n    Khởi tạo S\n    \n    Lặp với mỗi bước:\n        Chọn A từ S theo ε-greedy(Q)  ← Behavior policy\n        Thực hiện A, quan sát R, S'\n        \n        Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n                                   ↑ Target policy (greedy)\n        \n        S ← S'\n    cho đến S là terminal\n```\n\n#### 5.3. So sánh SARSA vs Q-Learning\n\n**SARSA Update**:\n```\nQ(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n                          ↑ A' theo ε-greedy\n```\n\n**Q-Learning Update**:\n```\nQ(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n                          ↑ Greedy choice\n```\n\n**Khác biệt**:\n- SARSA: Conservative, tính đến exploration risk\n- Q-Learning: Optimistic, học optimal ignoring exploration\n\n#### 5.4. Cliff Walking Example\n\n**Setup**:\n```\n[S] [ ] [ ] ... [ ] [G]\n[ ] [ ] [ ] ... [ ] [ ]\n[C] [C] [C] ... [C] [ ]\n```\n- S: Start, G: Goal, C: Cliff (reward = -100)\n- Normal step: reward = -1\n\n**Kết quả**:\n- **Q-Learning**: Học đường ngắn nhất (sát cliff) nhưng thường rơi xuống khi explore\n- **SARSA**: Học đường an toàn (xa cliff) vì tính đến khả năng explore\n- **Performance**: SARSA tốt hơn trong training, Q-Learning tốt hơn nếu greedy\n\n#### 5.5. Tính chất Q-Learning\n\n**Hội tụ**:\n- Đảm bảo hội tụ đến Q* với:\n  - Mọi (s,a) được visit vô hạn lần\n  - Learning rate thỏa mãn Robbins-Monro\n- Không phụ thuộc vào behavior policy!\n\n**Ưu điểm**:\n✅ Học optimal policy\n✅ Có thể tái sử dụng old experience\n✅ Học từ demonstrations\n✅ Simple và popular\n\n**Nhược điểm**:\n❌ Có thể không ổn định\n❌ Overestimation bias\n❌ Higher variance\n\n### 6. Expected SARSA\n\n#### 6.1. Ý tưởng\nThay vì sample A', lấy expectation theo policy\n\n**Update Rule**:\n```\nQ(S,A) ← Q(S,A) + α[R + γ Σ_a π(a|S')Q(S',a) - Q(S,A)]\n                          ↑ Expected value\n```\n\n**Với ε-greedy**:\n```\nQ(S,A) ← Q(S,A) + α[R + γ E_π[Q(S',·)] - Q(S,A)]\n\nE_π[Q(S',·)] = Σ_a π(a|S')Q(S',a)\n\n**Các khái niệm quan trọng:**\n- Temporal Difference (TD) Control là một kỹ thuật trong học tăng cường kết hợp ý tưởng của Monte Carlo và Dynamic Programming. Nó học trực tiếp từ kinh nghiệm mà không cần mô hình môi trường và cập nhật ước lượng giá trị dựa trên các ước lượng giá trị khác (bootstrapping). Q-Learning và SARSA là các thuật toán TD Control.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n#### 7.1. Game Playing\n\n**TD-Gammon** (1992):\n- Backgammon AI\n- Sử dụng TD(λ) với neural network\n- Self-play\n- Đạt world-champion level\n\n**AlphaGo Zero**:\n- Sử dụng TD-style updates\n- Self-play + MCTS\n- Không cần human knowledge\n\n#### 7.2. Robot Navigation\n\n**Task**: Robot tìm đường trong môi trường chưa biết\n- State: Vị trí robot\n- Action: Di chuyển\n- Reward: -1 mỗi bước, +100 khi đến đích\n\n**Ưu điểm TD**:\n- Học online trong quá trình điều hướng\n- Không cần đợi đến đích mới cập nhật\n- Adapt với môi trường thay đổi\n\n#### 7.3. Resource Management\n\n**Elevator Control**:\n- State: Vị trí thang máy, yêu cầu chờ\n- Action: Lên/xuống/đứng yên\n- Reward: -1 × tổng thời gian chờ\n\n**TD Learning**:\n- Học value function cho mỗi trạng thái\n- Online learning từ hoạt động hàng ngày\n- Cải thiện liên tục\n\n### 8. Phân tích lý thuyết\n\n#### 8.1. Tốc độ hội tụ\n\n**Monte Carlo**:\n```\nV_k(s) → V^π(s) với rate O(1/√k)\nk: số episodes\n```\n\n**TD(0)**:\n```\nV_k(s) → V^π(s) nhanh hơn trong thực tế\nKhông có bound lý thuyết chặt chẽ\n```\n\n**Thực nghiệm**: TD thường nhanh hơn MC 2-10 lần\n\n#### 8.2. Điều kiện hội tụ\n\n**Robbins-Monro conditions** cho learning rate α_t:\n```\nΣ_{t=1}^∞ α_t = ∞     (đảm bảo hội tụ)\nΣ_{t=1}^∞ α_t² < ∞    (đảm bảo variance hội tụ về 0)\n```\n\n**Ví dụ**:\n- α_t = 1/t: Thỏa mãn\n- α_t = 0.01: Không thỏa mãn điều kiện 1, nhưng practical\n\n#### 8.3. Bias-Variance Tradeoff\n\n**Monte Carlo**:\n- Bias = 0 (ước lượng không chệch)\n- Variance cao (phụ thuộc vào toàn bộ trajectory)\n\n**TD(0)**:\n- Bias > 0 (phụ thuộc vào V hiện tại)\n- Variance thấp (chỉ phụ thuộc 1 bước)\n\n**n-Step TD**: Cân bằng\n```\nBias giảm khi n tăng\nVariance tăng khi n tăng\n```\n\n### 9. So sánh tổng hợp\n\n#### 9.1. Bảng so sánh đầy đủ\n\n| Tiêu chí | MC | TD(0) | TD(λ) | DP |\n|----------|----|----|-------|-----|\n| Model-free | ✓ | ✓ | ✓ | ✗ |\n| Bootstrap | ✗ | ✓ | ✓ | ✓ |\n| Online | ✗ | ✓ | ✓ | ✓ |\n| Episodic only | ✓ | ✗ | ✗ | ✗ |\n| Bias | Low | High | Medium | Low |\n| Variance | High | Low | Medium | Low |\n| Convergence | Slow | Fast | Medium | Fastest |\n\n#### 9.2. Khi nào dùng phương pháp nào?\n\n**Monte Carlo**:\n- Môi trường không Markov\n- Cần ước lượng unbiased\n- Episodic tasks ngắn\n\n**TD(0)**:\n- Môi trường Markov\n- Continuing tasks\n\n**Các khái niệm quan trọng:**\n- TD(λ) là một thuật toán học tăng cường tiên tiến sử dụng eligibility traces (hoặc λ-return) để cân bằng giữa bias và variance, đồng thời giải quyết vấn đề credit assignment phức tạp. Nó cung cấp một phổ liên tục giữa phương pháp Monte Carlo (khi λ=1, cập nhật dựa trên toàn bộ episode) và TD(0) (khi λ=0, cập nhật chỉ dựa trên một bước), cho phép cập nhật giá trị dựa trên một chuỗi các bước (n-step TD). Thuật toán này có thể được xem xét từ hai góc độ: Forward View (nhìn về tương lai) và Backward View (nhìn về quá khứ), và đã được ứng dụng thành công trong các hệ thống như TD-Gammon.\n- TD(0) (Temporal Difference learning với λ=0) là thuật toán Temporal-Difference Learning cơ bản nhất, một phương pháp học tăng cường model-free, online và bootstrap. Nó học từng bước một từ kinh nghiệm mà không cần đợi đến cuối episode, phù hợp với các tác vụ liên tục. TD(0) cập nhật hàm giá trị V(s) của trạng thái hiện tại (S_t) dựa trên phần thưởng tức thời (R_{t+1}) và ước lượng giá trị của trạng thái tiếp theo (V(S_{t+1})). Công thức cập nhật là V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]. Thuật toán này có bias cao nhưng variance thấp, thường hội tụ nhanh hơn Monte Carlo trong thực tế. SARSA là một ứng dụng của TD(0) cho Q-function.\n\n**Mối quan hệ:**\n- TD(λ) là thuật toán model-free.\n- TD(0) hội tụ đến V^π(s) nhanh hơn Monte Carlo trong thực tế.\n- TD-Gammon sử dụng thuật toán TD(λ) để học cách chơi Backgammon.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n- Cần học nhanh\n- Online learning\n\n**TD(λ)**:\n- Khi cần cân bằng bias-variance\n- Credit assignment phức tạp\n- Eligibility traces quan trọng\n\n### 10. Code Implementation\n\n#### 10.1. Monte Carlo First-Visit\n```python\ndef monte_carlo_prediction(env, policy, num_episodes, gamma=0.99):\n    V = defaultdict(float)\n    returns = defaultdict(list)\n    \n    for _ in range(num_episodes):\n        episode = generate_episode(env, policy)\n        G = 0\n        visited = set()\n        \n        # Duyệt ngược từ cuối episode\n        for t in range(len(episode)-1, -1, -1):\n            state, action, reward = episode[t]\n            G = gamma * G + reward\n            \n            if state not in visited:\n                returns[state].append(G)\n                V[state] = np.mean(returns[state])\n                visited.add(state)\n    \n    return V\n```\n\n#### 10.2. TD(0)\n```python\ndef td_prediction(env, policy, num_episodes, alpha=0.1, gamma=0.99):\n    V = defaultdict(float)\n    \n    for _ in range(num_episodes):\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # TD update\n            td_target = reward + gamma * V[next_state]\n            td_error = td_target - V[state]\n            V[state] += alpha * td_error\n            \n            state = next_state\n    \n    return V\n```\n\n#### 10.3. TD(λ) with Eligibility Traces\n```python\ndef td_lambda_prediction(env, policy, num_episodes, \n                        alpha=0.1, gamma=0.99, lambda_=0.9):\n    V = defaultdict(float)\n    \n    for _ in range(num_episodes):\n        E = defaultdict(float)  # Eligibility traces\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # TD error\n            delta = reward + gamma * V[next_state] - V[state]\n            \n            # Update eligibility trace\n            E[state] += 1\n            \n            # Update all states\n            for s in E:\n                V[s] += alpha * delta * E[s]\n                E[s] *= gamma * lambda_\n            \n            state = next_state\n    \n    return V\n```\n\n### 11. Bài tập thực hành\n\n#### 11.1. Bài tập cơ bản\n1. Implement First-Visit MC cho Blackjack\n2. So sánh MC vs TD(0) trên Random Walk\n3. Visualize learning curves với different α\n\n#### 11.2. Bài tập nâng cao\n1. Implement n-step TD với n = 1, 3, 5, 10\n2. Compare TD(λ) với λ = 0, 0.5, 0.9, 1.0\n3. Analyze bias-variance tradeoff empirically\n\n#### 11.3. Dự án\n1. Build Tic-Tac-Toe AI với TD learning\n2. Robot navigation trong gridworld phức tạp\n3. Stock price prediction với TD methods\n\n### 12. Kết luận\n\nModel-Free Prediction giải quyết vấn đề quan trọng: **Đánh giá chính sách mà không cần biết mô hình môi trường**.\n\n**Các phương pháp chính**:\n- **Monte Carlo**: Đơn giản, unbiased, nhưng high variance\n- **TD(0)**: Efficient, online, nhưng biased\n- **TD(λ)**: Cân bằng tốt nhất, flexible\n\n**Key insights**:\n1. Bootstrap (TD) vs Full returns (MC) là tradeoff cơ bản\n2. TD thường hiệu quả hơn trong môi trường Markov\n3. Eligibility traces (TD(λ)) cung cấp spectrum liên tục\n4. Lựa chọn phương pháp phụ thuộc vào đặc điểm bài toán\n\n\n**Các khái niệm quan trọng:**\n- TD(λ) là một thuật toán học tăng cường tiên tiến sử dụng eligibility traces (hoặc λ-return) để cân bằng giữa bias và variance, đồng thời giải quyết vấn đề credit assignment phức tạp. Nó cung cấp một phổ liên tục giữa phương pháp Monte Carlo (khi λ=1, cập nhật dựa trên toàn bộ episode) và TD(0) (khi λ=0, cập nhật chỉ dựa trên một bước), cho phép cập nhật giá trị dựa trên một chuỗi các bước (n-step TD). Thuật toán này có thể được xem xét từ hai góc độ: Forward View (nhìn về tương lai) và Backward View (nhìn về quá khứ), và đã được ứng dụng thành công trong các hệ thống như TD-Gammon.\n- TD(0) (Temporal Difference learning với λ=0) là thuật toán Temporal-Difference Learning cơ bản nhất, một phương pháp học tăng cường model-free, online và bootstrap. Nó học từng bước một từ kinh nghiệm mà không cần đợi đến cuối episode, phù hợp với các tác vụ liên tục. TD(0) cập nhật hàm giá trị V(s) của trạng thái hiện tại (S_t) dựa trên phần thưởng tức thời (R_{t+1}) và ước lượng giá trị của trạng thái tiếp theo (V(S_{t+1})). Công thức cập nhật là V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]. Thuật toán này có bias cao nhưng variance thấp, thường hội tụ nhanh hơn Monte Carlo trong thực tế. SARSA là một ứng dụng của TD(0) cho Q-function.\n\n**Mối quan hệ:**\n- TD(λ) là thuật toán model-free.\n- TD(0) hội tụ đến V^π(s) nhanh hơn Monte Carlo trong thực tế.\n- TD-Gammon sử dụng thuật toán TD(λ) để học cách chơi Backgammon.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\nB, 1 (terminal)\nB, 1 (terminal)\nB, 1 (terminal)\nB, 0 (terminal)\n```\n\n**MC Estimate**:\n- V(A) = 0 (chỉ có 1 episode từ A, return = 0)\n- V(B) = 5/6 ≈ 0.83\n\n**TD Estimate**:\n- V(B) = 5/6 (từ data)\n- V(A) = 5/6 (vì 100% chuyển đến B)\n\n**TD tốt hơn**: Exploit structure, generalize better\n\n### 5. Unified View: TD(λ)\n\n#### 5.1. n-Step TD\n\n**Ý tưởng**: Thay vì bootstrap sau 1 bước, sử dụng n bước\n\n**n-Step Return**:\n```\nG_t^(n) = R_{t+1} + γR_{t+2} + ... + γ^{n-1}R_{t+n} + γ^n V(S_{t+n})\n```\n\n**Update**:\n```\nV(S_t) ← V(S_t) + α[G_t^(n) - V(S_t)]\n```\n\n**Đặc điểm**:\n- n=1: TD(0)\n- n=∞: Monte Carlo\n- n trung gian: Cân bằng bias và variance\n\n#### 5.2. TD(λ) - Eligibility Traces\n\n**Motivation**: Thay vì chọn một n, kết hợp tất cả n-step returns\n\n**λ-Return**:\n```\nG_t^λ = (1-λ) Σ_{n=1}^∞ λ^{n-1} G_t^(n)\n```\n\n**Eligibility Trace**:\n```\nE_0(s) = 0\nE_t(s) = γλE_{t-1}(s) + 1(S_t = s)\n```\n\n**TD(λ) Update**:\n```\nδ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\nV(s) ← V(s) + αδ_t E_t(s), ∀s\n```\n\n#### 5.3. Forward View vs Backward View\n\n**Forward View**:\n- Nhìn về tương lai\n- Tính G_t^λ từ future returns\n- Offline algorithm\n\n**Backward View**:\n- Nhìn về quá khứ\n- Sử dụng eligibility traces\n- Online algorithm\n- Computationally efficient\n\n**Quan hệ**: Hai view tương đương về mặt toán học\n\n#### 5.4. Chọn λ\n\n| λ | Đặc điểm | Khi nào dùng |\n|---|----------|--------------|\n| 0 | TD(0), low variance | Môi trường noisy |\n| 0.5-0.9 | Cân bằng | Thường dùng nhất |\n| 1 | MC, unbiased | Episodic, deterministic |\n\n### 6. Các biến thể TD nâng cao\n\n#### 6.1. Double Learning\n\n**Vấn đề**: Maximization bias trong TD\n```\nOverestimation: E[max(X₁, X₂)] ≥ max(E[X₁], E[X₂])\n```\n\n**Giải pháp**: Duy trì 2 value functions\n```\nV₁(S) và V₂(S)\n\nUpdate V₁:\n    V₁(S) ← V₁(S) + α[R + γV₂(S') - V₁(S)]\n    \nUpdate V₂:\n    V₂(S) ← V₂(S) + α[R + γV₁(S') - V₂(S)]\n```\n\n#### 6.2. Gradient TD\n\n**Motivation**: TD không theo gradient descent thực sự\n\n**True Gradient TD (TDC)**:\n```\nMinimize: MSBE = ||V - Π_T^π V||²\n\nUpdate:\n    w ← w + α(R + γV(S') - V(S))∇V(S)\n    - αγ(∇V(S'))⊤w ∇V(S)\n```\n\n**Lợi ích**: Hội tụ vững vàng hơn với function approximation\n\n### 7. Ứng dụng thực tế\n\n\n**Các khái niệm quan trọng:**\n- TD(λ) là một thuật toán học tăng cường tiên tiến sử dụng eligibility traces (hoặc λ-return) để cân bằng giữa bias và variance, đồng thời giải quyết vấn đề credit assignment phức tạp. Nó cung cấp một phổ liên tục giữa phương pháp Monte Carlo (khi λ=1, cập nhật dựa trên toàn bộ episode) và TD(0) (khi λ=0, cập nhật chỉ dựa trên một bước), cho phép cập nhật giá trị dựa trên một chuỗi các bước (n-step TD). Thuật toán này có thể được xem xét từ hai góc độ: Forward View (nhìn về tương lai) và Backward View (nhìn về quá khứ), và đã được ứng dụng thành công trong các hệ thống như TD-Gammon.\n\n**Mối quan hệ:**\n- TD(λ) là thuật toán model-free.\n- TD-Gammon sử dụng thuật toán TD(λ) để học cách chơi Backgammon.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Model-Free Prediction - Dự Đoán Không Cần Mô Hình\n        Nếu S_t xuất hiện lần đầu tiên trong episode:\n            Thêm G vào Returns(S_t)\n            V(S_t) = average(Returns(S_t))\n```\n\n**Đặc điểm**:\n- Chỉ cập nhật cho lần xuất hiện đầu tiên của trạng thái\n- Không có bias\n- Đảm bảo hội tụ đến V^π(s) khi số episodes → ∞\n\n#### 2.4. Every-Visit Monte Carlo\n\n**Khác biệt**: Cập nhật cho mọi lần xuất hiện của trạng thái trong episode\n\n```\nVới mỗi bước t = T-1, T-2, ..., 0:\n    G = γG + R_{t+1}\n    Thêm G vào Returns(S_t)\n    V(S_t) = average(Returns(S_t))\n```\n\n**So sánh**:\n- Every-visit có variance thấp hơn\n- Cả hai đều hội tụ đến V^π(s)\n- Every-visit thường được sử dụng nhiều hơn\n\n#### 2.5. Incremental Mean Update\n\n**Vấn đề**: Lưu trữ tất cả returns không hiệu quả\n\n**Giải pháp**: Cập nhật incremental\n```\nCông thức tổng quát:\n    μ_k = μ_{k-1} + (1/k)(x_k - μ_{k-1})\n\nÁp dụng cho MC:\n    N(s) = N(s) + 1\n    V(s) = V(s) + (1/N(s))(G - V(s))\n    \nHoặc dùng learning rate α cố định:\n    V(s) = V(s) + α(G - V(s))\n```\n\n**Lợi ích**:\n- Tiết kiệm bộ nhớ\n- Cập nhật online\n- Quên dần các ước lượng cũ (với α cố định)\n\n#### 2.6. Ví dụ: Blackjack\n\n**Mô tả bài toán**:\n- **Trạng thái**: (tổng bài của người chơi, bài úp của dealer, có ace không)\n- **Hành động**: Hit (rút thêm) hoặc Stick (dừng)\n- **Phần thưởng**: +1 thắng, -1 thua, 0 hòa\n- **Chính sách**: Stick nếu tổng ≥ 20, ngược lại Hit\n\n**Ứng dụng MC**:\n1. Chơi nhiều games theo chính sách\n2. Ghi lại returns cho mỗi trạng thái\n3. Tính V^π bằng trung bình returns\n\n**Kết quả**:\n```\nV^π(20, ACE, usable_ace) ≈ 0.8  (rất tốt)\nV^π(12, 2, no_ace) ≈ -0.3       (tệ)\n```\n\n### 3. Temporal-Difference Learning (TD)\n\n#### 3.1. Giới thiệu\nTD Learning kết hợp ý tưởng của MC và DP:\n- Như MC: Học từ kinh nghiệm, không cần mô hình\n- Như DP: Bootstrap từ ước lượng hiện tại, không cần episode hoàn chỉnh\n\n#### 3.2. TD(0) - Temporal Difference cơ bản\n\n**TD Update Rule**:\n```\nV(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]\n```\n\n**Các thành phần**:\n- **TD Target**: R_{t+1} + γV(S_{t+1})\n- **TD Error**: δ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)\n- **Update**: V(S_t) ← V(S_t) + α × δ_t\n\n#### 3.3. Thuật toán TD(0)\n\n```\n\n**Các khái niệm quan trọng:**\n- TD(0) (Temporal Difference learning với λ=0) là thuật toán Temporal-Difference Learning cơ bản nhất, một phương pháp học tăng cường model-free, online và bootstrap. Nó học từng bước một từ kinh nghiệm mà không cần đợi đến cuối episode, phù hợp với các tác vụ liên tục. TD(0) cập nhật hàm giá trị V(s) của trạng thái hiện tại (S_t) dựa trên phần thưởng tức thời (R_{t+1}) và ước lượng giá trị của trạng thái tiếp theo (V(S_{t+1})). Công thức cập nhật là V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]. Thuật toán này có bias cao nhưng variance thấp, thường hội tụ nhanh hơn Monte Carlo trong thực tế. SARSA là một ứng dụng của TD(0) cho Q-function.\n\n**Mối quan hệ:**\n- TD(0) hội tụ đến V^π(s) nhanh hơn Monte Carlo trong thực tế."
        },
        "output_without_context": {
            "question": "Sự tương đương giữa TD(λ) và Monte-Carlo/TD(0) chủ yếu dựa trên yếu tố nào trong học tăng cường?",
            "answer": "Cách thức cập nhật giá trị trạng thái dựa trên thông tin từ các bước trước đó."
        }
    }
]