[
    {
        "topic_description": "Chủ đề này tập trung vào mô hình Perceptron, nơi sinh viên sẽ tìm hiểu về cách thức hoạt động của siêu phẳng phân tách dữ liệu và các khái niệm liên quan như biên độ và trọng số. Câu hỏi có thể kiểm tra các định nghĩa, công thức và ứng dụng của Perceptron. Kết nối với tuần 4.",
        "output_with_context": {
            "question": "Công thức nào mô tả cách tính tổng có trọng số trong mô hình Perceptron?",
            "answer": "z = w^Tx + b = ∑ᵢwᵢxᵢ + b",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n- Matrix factorization (NMF, SVD)\n\n**5. Topic Modeling:**\n- Document clustering\n- Automatic tagging\n- Content organization\n- Trend detection\n\n**6. Gene Expression Analysis:**\n- Group similar genes\n- Identify cancer subtypes\n- Drug discovery\n- Understanding diseases\n\n**7. Social Network Analysis:**\n- Community detection\n- Influencer identification\n- Link prediction\n- Recommendation\n\n**8. Data Preprocessing:**\n- Feature extraction (PCA, ICA)\n- Noise reduction (autoencoders)\n- Data compression\n- Dimensionality reduction\n\n**9. Market Basket Analysis:**\n- Product recommendations\n- Store layout\n- Promotions\n- Cross-selling\n\n**10. Image Segmentation:**\n- Medical imaging\n- Object detection preparation\n- Video processing\n- Computer vision preprocessing\n\n---\n\n## Học Sâu (Deep Learning)\n\n### Giới Thiệu về Học Sâu\n\nHọc sâu (Deep Learning) là một nhánh con của học máy sử dụng mạng nơ-ron nhân tạo với nhiều lớp ẩn để học các biểu diễn phân cấp của dữ liệu. Khác với các phương pháp học máy truyền thống, học sâu có khả năng tự động trích xuất đặc trưng từ dữ liệu thô mà không cần kỹ thuật đặc trưng thủ công.\n\n**Đặc điểm chính:**\n- **Học biểu diễn phân cấp:** Các lớp đầu học các đặc trưng cấp thấp (cạnh, góc), các lớp sau học đặc trưng cấp cao hơn (hình dạng, đối tượng)\n- **Khả năng xử lý dữ liệu lớn:** Hiệu suất tăng theo lượng dữ liệu\n- **End-to-end learning:** Học trực tiếp từ đầu vào thô đến đầu ra mong muốn\n- **Tự động trích xuất đặc trưng:** Không cần thiết kế đặc trưng thủ công\n\n**Ứng dụng đã cách mạng hóa:**\n- Thị giác máy tính (nhận dạng ảnh, phát hiện đối tượng)\n- Xử lý ngôn ngữ tự nhiên (dịch máy, chatbot, sinh văn bản)\n- Nhận dạng giọng nói (trợ lý ảo, chuyển đổi giọng nói thành văn bản)\n- Y tế (chẩn đoán hình ảnh, phát triển thuốc)\n- Tự động hóa (xe tự lái, robot)\n\n### Mạng Nơ-ron Nhân Tạo (Artificial Neural Networks - ANN)\n\nMạng nơ-ron nhân tạo được lấy cảm hứng từ cách thức hoạt động của não người, trong đó các nơ-ron sinh học truyền tín hiệu cho nhau thông qua các synapse.\n\n### Perceptron - Đơn Vị Cơ Bản\n\nPerceptron là đơn vị mạng nơ-ron đơn giản nhất, được phát minh bởi Frank Rosenblatt năm 1958.\n\n**Công thức:**\n$$y = \\sigma(w^Tx + b)$$\n\nTrong đó:\n- $x = [x_1, x_2, ..., x_n]^T$: Vector đầu vào (các đặc trưng)\n- $w = [w_1, w_2, ..., w_n]^T$: Vector trọng số (weights)\n- $b$: Hệ số điều chỉnh (bias) - cho phép dịch chuyển hàm quyết định\n- $\\sigma$: Hàm kích hoạt (activation function)\n- $y$: Đầu ra dự đoán\n\n\n**Các khái niệm quan trọng:**\n- Perceptron là một mô hình học máy cơ bản, chỉ có thể học các mẫu phân tách tuyến tính. Nó tính tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào, sau đó áp dụng hàm kích hoạt để tạo đầu ra. Perceptron không thể giải quyết bài toán XOR hoặc mô hình hóa các quan hệ phi tuyến phức tạp.\n- Perceptron là đơn vị mạng nơ-ron đơn giản nhất, được phát minh bởi Frank Rosenblatt năm 1958. Nó nhận một vector đầu vào, tính tổng có trọng số và áp dụng hàm kích hoạt để tạo ra đầu ra dự đoán.\n\n**Mối quan hệ:**\n- Perceptron tính toán tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào.\n- Perceptron chỉ có thể học các mẫu phân tách tuyến tính.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**Cách hoạt động:**\n1. Nhận đầu vào từ các đặc trưng\n2. Tính tổng có trọng số: $z = w^Tx + b = \\sum_{i=1}^{n}w_i x_i + b$\n3. Áp dụng hàm kích hoạt để tạo đầu ra\n\n**Hạn chế quan trọng:** \n- Chỉ có thể học các mẫu phân tách tuyến tính (linearly separable)\n- Không thể giải quyết bài toán XOR\n- Không thể mô hình hóa các quan hệ phi tuyến phức tạp\n\n**Ví dụ:** Một perceptron có thể phân loại điểm nằm phía trên hay dưới một đường thẳng, nhưng không thể phân loại các điểm trong bài toán XOR (cần đường cong để phân tách).\n\n### Perceptron Đa Lớp (Multi-Layer Perceptron - MLP)\n\nMLP khắc phục hạn chế của perceptron đơn bằng cách xếp chồng nhiều lớp, cho phép học các hàm phi tuyến phức tạp.\n\n**Kiến trúc:**\n\n**1. Lớp đầu vào (Input Layer):**\n- Nhận dữ liệu thô\n- Số nơ-ron = số đặc trưng đầu vào\n- Không có phép biến đổi, chỉ truyền dữ liệu\n\n**2. Lớp ẩn (Hidden Layers):**\n- Thực hiện các phép biến đổi phi tuyến\n- Số lượng có thể từ 1 đến hàng trăm lớp\n- Mỗi lớp học biểu diễn trừu tượng hơn\n- Số nơ-ron trong mỗi lớp là hyperparameter\n\n**3. Lớp đầu ra (Output Layer):**\n- Tạo dự đoán cuối cùng\n- Số nơ-ron phụ thuộc vào bài toán:\n  - Hồi quy: 1 nơ-ron\n  - Phân loại nhị phân: 1 nơ-ron (với sigmoid) hoặc 2 (với softmax)\n  - Phân loại đa lớp: K nơ-ron (K = số lớp)\n\n**Lan truyền xuôi (Forward Propagation):**\n\nQuá trình tính toán từ đầu vào đến đầu ra qua các lớp:\n\nVới lớp $l$:\n$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n$$a^{[l]} = \\sigma(z^{[l]})$$\n\nTrong đó:\n- $W^{[l]}$: Ma trận trọng số của lớp $l$ (kích thước $n^{[l]} \times n^{[l-1]}$)\n- $b^{[l]}$: Vector bias của lớp $l$ (kích thước $n^{[l]} \times 1$)\n- $a^{[l-1]}$: Activation của lớp trước (đầu vào cho lớp $l$)\n- $z^{[l]}$: Pre-activation (trước khi áp dụng hàm kích hoạt)\n- $a^{[l]}$: Activation (sau khi áp dụng hàm kích hoạt)\n- $\\sigma$: Hàm kích hoạt\n\n**Ví dụ minh họa:**\n- Lớp 1: 3 nơ-ron, nhận input 784 chiều (ảnh 28×28) → $W^{[1]}$: 3×784\n- Lớp 2: 2 nơ-ron, nhận từ lớp 1 → $W^{[2]}$: 2×3\n\n**Các khái niệm quan trọng:**\n- Perceptron là một mô hình học máy cơ bản, chỉ có thể học các mẫu phân tách tuyến tính. Nó tính tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào, sau đó áp dụng hàm kích hoạt để tạo đầu ra. Perceptron không thể giải quyết bài toán XOR hoặc mô hình hóa các quan hệ phi tuyến phức tạp.\n- Công thức z = w^Tx + b = ∑ᵢwᵢxᵢ + b tính tổng có trọng số của các đặc trưng đầu vào xᵢ với các trọng số wᵢ và thêm bias b. Đây là phép biến đổi tuyến tính cơ bản trong perceptron và các nơ-ron của mạng nơ-ron, đại diện cho pre-activation trước khi áp dụng hàm kích hoạt.\n- Perceptron là đơn vị mạng nơ-ron đơn giản nhất, được phát minh bởi Frank Rosenblatt năm 1958. Nó nhận một vector đầu vào, tính tổng có trọng số và áp dụng hàm kích hoạt để tạo ra đầu ra dự đoán.\n- Linearly separable (phân tách tuyến tính) là một vấn đề trong đó dữ liệu có thể được phân chia thành các lớp bằng một đường thẳng (trong 2D), mặt phẳng (trong 3D) hoặc siêu phẳng (trong không gian nhiều chiều). Perceptron chỉ có thể học các mẫu phân tách tuyến tính.\n\n**Mối quan hệ:**\n- Perceptron tính toán tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào.\n- Perceptron chỉ có thể học các mẫu phân tách tuyến tính."
        },
        "output_without_context": {
            "question": "Mô hình Perceptron sử dụng siêu phẳng nào để phân tách dữ liệu?",
            "answer": "Siêu phẳng phân tách."
        }
    },
    {
        "topic_description": "Chủ đề này tìm hiểu về quy trình hoạt động của thuật toán Perceptron, cách cập nhật các trọng số và độ chính xác trong phân loại. Câu hỏi có thể tập trung vào công thức và các bước thực hiện trong thuật toán. Kết nối với tuần 4.",
        "output_with_context": {
            "question": "Công thức nào được sử dụng để cập nhật trọng số trong thuật toán Perceptron?",
            "answer": "w := w - α * (∂L/∂w)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\nBackpropagation có thể được hiểu thông qua đồ thị tính toán (computational graph), trong đó:\n- Mỗi node là một operation\n- Edges mang giá trị và gradients\n- Forward pass tính giá trị, backward pass tính gradients\n\n**Vấn đề Vanishing/Exploding Gradients:**\n- **Vanishing:** Gradient giảm dần khi lan truyền về các lớp đầu → các lớp đầu học chậm\n  - Nguyên nhân: Hàm kích hoạt có đạo hàm nhỏ (sigmoid, tanh)\n  - Giải pháp: ReLU, batch normalization, residual connections\n- **Exploding:** Gradient tăng dần → weights cập nhật quá mạnh, không ổn định\n  - Giải pháp: Gradient clipping, proper weight initialization\n\n**Lưu ý về hiệu suất:**\n- Độ phức tạp tính toán của backpropagation tương đương forward pass\n- Matrix operations có thể vectorize → tính toán hiệu quả trên GPU\n- Cần lưu trữ activations từ forward pass → tốn memory\n\n### Thuật Toán Tối Ưu (Optimization Algorithms)\n\nCác thuật toán tối ưu quyết định cách cập nhật weights để minimize loss function.\n\n**1. Gradient Descent (Hạ Gradient):**\n$$w := w - \\alpha\frac{\\partial L}{\\partial w}$$\n\n**Đặc điểm:**\n- $\\alpha$ (learning rate): Hyperparameter quan trọng nhất\n- Cập nhật dựa trên toàn bộ training set (batch gradient descent)\n\n**Ưu điểm:**\n- Đơn giản, dễ hiểu\n- Hội tụ ổn định với learning rate phù hợp\n- Đảm bảo tìm được local minimum với hàm convex\n\n**Nhược điểm:**\n- Chậm với dữ liệu lớn (phải xử lý toàn bộ dataset mỗi iteration)\n- Có thể bị kẹt ở local minima hoặc saddle points\n- Learning rate cố định không phù hợp mọi giai đoạn training\n\n**2. Stochastic Gradient Descent (SGD):**\n$$w := w - \\alpha\frac{\\partial L_i}{\\partial w}$$\n\n**Đặc điểm:**\n- Cập nhật sau **mỗi** mẫu dữ liệu (sample)\n- Gradient ước lượng từ 1 sample → noisy nhưng nhanh\n\n**Ưu điểm:**\n- Rất nhanh, có thể train trên dữ liệu lớn\n- Noise giúp thoát khỏi local minima\n- Có thể train online (dữ liệu đến liên tục)\n\n**Nhược điểm:**\n- Quá trình hội tụ không ổn định, dao động mạnh\n- Có thể không hội tụ chính xác đến minimum\n- Khó song song hóa (sequential updates)\n\n**3. Mini-batch Gradient Descent:**\n$$w := w - \\alpha\frac{1}{m}\\sum_{i=1}^{m}\frac{\\partial L_i}{\\partial w}$$\n\n**Đặc điểm:**\n- Cập nhật sau một **batch nhỏ** (thường 32, 64, 128, 256)\n- Kết hợp ưu điểm của batch GD và SGD\n- **Là phương pháp được sử dụng phổ biến nhất trong thực tế**\n\n**Ưu điểm:**\n- Tốc độ nhanh, ổn định hơn SGD\n- Có thể vectorize, tận dụng GPU hiệu quả\n- Gradient ổn định hơn SGD nhưng vẫn có noise tốt\n- Batch size là hyperparameter điều chỉnh được\n\n**Lựa chọn batch size:**\n\n**Các khái niệm quan trọng:**\n- Weights (trọng số) là các tham số có thể học được trong mô hình Machine Learning, đặc biệt là trong mạng nơ-ron, đại diện cho sức mạnh của kết nối giữa các nơ-ron. Trong công thức $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$, $W^{[l]}$ là ma trận trọng số của lớp thứ $l$. Trong quá trình training, weights được cập nhật liên tục bởi các thuật toán tối ưu để cực tiểu hóa (minimize) hàm loss. Công thức cập nhật chung là w := w - α * (∂L/∂w).\n- Optimization Algorithms là các thuật toán quyết định cách cập nhật weights của mô hình để minimize loss function. Chúng bao gồm Gradient Descent, SGD, Mini-batch Gradient Descent và các biến thể nâng cao hơn như Adam, RMSprop. Mục tiêu là tìm ra tập hợp weights tối ưu để mô hình hoạt động tốt nhất.\n\n**Mối quan hệ:**\n- Các thuật toán tối ưu quyết định cách cập nhật weights để minimize loss function.\n- Stochastic Gradient Descent cập nhật weights sau mỗi mẫu dữ liệu theo công thức w := w - α * (∂L_i/∂w).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\nBackpropagation là thuật toán cốt lõi để huấn luyện mạng nơ-ron sâu, cho phép tính gradient một cách hiệu quả thông qua quy tắc chuỗi (chain rule).\n\n**Ý tưởng cơ bản:**\n- Tính toán gradient của loss function theo tất cả các tham số (weights và biases)\n- Lan truyền gradient từ output về input qua các lớp\n- Sử dụng quy tắc chuỗi để phân rã gradient phức tạp thành các phần đơn giản\n\n**Quy tắc chuỗi (Chain Rule):**\n$$\frac{\\partial L}{\\partial w^{[l]}} = \frac{\\partial L}{\\partial a^{[l]}} \\cdot \frac{\\partial a^{[l]}}{\\partial z^{[l]}} \\cdot \frac{\\partial z^{[l]}}{\\partial w^{[l]}}$$\n\nTrong đó:\n- $\frac{\\partial L}{\\partial a^{[l]}}$: Gradient của loss theo activation\n- $\frac{\\partial a^{[l]}}{\\partial z^{[l]}}$: Đạo hàm của hàm kích hoạt\n- $\frac{\\partial z^{[l]}}{\\partial w^{[l]}}$: Gradient của pre-activation theo weights\n\n**Các bước chi tiết:**\n\n**1. Forward Pass (Lan truyền xuôi):**\n- Tính toán output của mỗi lớp từ input đến output\n- Lưu trữ tất cả các giá trị $z^{[l]}$ và $a^{[l]}$ (cần cho backward pass)\n\n**2. Tính Loss:**\n- So sánh prediction với ground truth\n- Tính giá trị loss: $L = Loss(y, \\hat{y})$\n\n**3. Backward Pass (Lan truyền ngược):**\n- Bắt đầu từ lớp output, tính gradient của loss theo output\n- Với mỗi lớp từ L về 1:\n  - Tính $\frac{\\partial L}{\\partial z^{[l]}} = \frac{\\partial L}{\\partial a^{[l]}} \\odot \\sigma'(z^{[l]})$ (element-wise product)\n  - Tính $\frac{\\partial L}{\\partial W^{[l]}} = \frac{\\partial L}{\\partial z^{[l]}} \\cdot (a^{[l-1]})^T$\n  - Tính $\frac{\\partial L}{\\partial b^{[l]}} = \frac{\\partial L}{\\partial z^{[l]}}$\n  - Lan truyền về lớp trước: $\frac{\\partial L}{\\partial a^{[l-1]}} = (W^{[l]})^T \\cdot \frac{\\partial L}{\\partial z^{[l]}}$\n\n**4. Cập nhật Weights:**\n- Sử dụng gradient descent hoặc các optimizer khác\n- $W^{[l]} := W^{[l]} - \\alpha \frac{\\partial L}{\\partial W^{[l]}}$\n- $b^{[l]} := b^{[l]} - \\alpha \frac{\\partial L}{\\partial b^{[l]}}$\n\n**Ví dụ minh họa:**\nMạng 2 lớp: Input → Hidden → Output\n- Forward: $a^{[1]} = \\sigma(W^{[1]}x + b^{[1]})$, $\\hat{y} = \\sigma(W^{[2]}a^{[1]} + b^{[2]})$\n- Loss: $L = (y - \\hat{y})^2$\n- Backward:\n  - $\frac{\\partial L}{\\partial \\hat{y}} = -2(y - \\hat{y})$\n  - $\frac{\\partial L}{\\partial W^{[2]}} = \frac{\\partial L}{\\partial \\hat{y}} \\cdot \\sigma'(z^{[2]}) \\cdot a^{[1]}$\n  - Lan truyền về hidden layer tương tự\n\n**Computational Graph:**\n\n**Các khái niệm quan trọng:**\n- Weights (trọng số) là các tham số có thể học được trong mô hình Machine Learning, đặc biệt là trong mạng nơ-ron, đại diện cho sức mạnh của kết nối giữa các nơ-ron. Trong công thức $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$, $W^{[l]}$ là ma trận trọng số của lớp thứ $l$. Trong quá trình training, weights được cập nhật liên tục bởi các thuật toán tối ưu để cực tiểu hóa (minimize) hàm loss. Công thức cập nhật chung là w := w - α * (∂L/∂w).\n\n**Mối quan hệ:**\n- Các thuật toán tối ưu quyết định cách cập nhật weights để minimize loss function.\n- Stochastic Gradient Descent cập nhật weights sau mỗi mẫu dữ liệu theo công thức w := w - α * (∂L_i/∂w).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**Cách hoạt động:**\n1. Nhận đầu vào từ các đặc trưng\n2. Tính tổng có trọng số: $z = w^Tx + b = \\sum_{i=1}^{n}w_i x_i + b$\n3. Áp dụng hàm kích hoạt để tạo đầu ra\n\n**Hạn chế quan trọng:** \n- Chỉ có thể học các mẫu phân tách tuyến tính (linearly separable)\n- Không thể giải quyết bài toán XOR\n- Không thể mô hình hóa các quan hệ phi tuyến phức tạp\n\n**Ví dụ:** Một perceptron có thể phân loại điểm nằm phía trên hay dưới một đường thẳng, nhưng không thể phân loại các điểm trong bài toán XOR (cần đường cong để phân tách).\n\n### Perceptron Đa Lớp (Multi-Layer Perceptron - MLP)\n\nMLP khắc phục hạn chế của perceptron đơn bằng cách xếp chồng nhiều lớp, cho phép học các hàm phi tuyến phức tạp.\n\n**Kiến trúc:**\n\n**1. Lớp đầu vào (Input Layer):**\n- Nhận dữ liệu thô\n- Số nơ-ron = số đặc trưng đầu vào\n- Không có phép biến đổi, chỉ truyền dữ liệu\n\n**2. Lớp ẩn (Hidden Layers):**\n- Thực hiện các phép biến đổi phi tuyến\n- Số lượng có thể từ 1 đến hàng trăm lớp\n- Mỗi lớp học biểu diễn trừu tượng hơn\n- Số nơ-ron trong mỗi lớp là hyperparameter\n\n**3. Lớp đầu ra (Output Layer):**\n- Tạo dự đoán cuối cùng\n- Số nơ-ron phụ thuộc vào bài toán:\n  - Hồi quy: 1 nơ-ron\n  - Phân loại nhị phân: 1 nơ-ron (với sigmoid) hoặc 2 (với softmax)\n  - Phân loại đa lớp: K nơ-ron (K = số lớp)\n\n**Lan truyền xuôi (Forward Propagation):**\n\nQuá trình tính toán từ đầu vào đến đầu ra qua các lớp:\n\nVới lớp $l$:\n$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n$$a^{[l]} = \\sigma(z^{[l]})$$\n\nTrong đó:\n- $W^{[l]}$: Ma trận trọng số của lớp $l$ (kích thước $n^{[l]} \times n^{[l-1]}$)\n- $b^{[l]}$: Vector bias của lớp $l$ (kích thước $n^{[l]} \times 1$)\n- $a^{[l-1]}$: Activation của lớp trước (đầu vào cho lớp $l$)\n- $z^{[l]}$: Pre-activation (trước khi áp dụng hàm kích hoạt)\n- $a^{[l]}$: Activation (sau khi áp dụng hàm kích hoạt)\n- $\\sigma$: Hàm kích hoạt\n\n**Ví dụ minh họa:**\n- Lớp 1: 3 nơ-ron, nhận input 784 chiều (ảnh 28×28) → $W^{[1]}$: 3×784\n- Lớp 2: 2 nơ-ron, nhận từ lớp 1 → $W^{[2]}$: 2×3\n\n**Các khái niệm quan trọng:**\n- Perceptron là một mô hình học máy cơ bản, chỉ có thể học các mẫu phân tách tuyến tính. Nó tính tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào, sau đó áp dụng hàm kích hoạt để tạo đầu ra. Perceptron không thể giải quyết bài toán XOR hoặc mô hình hóa các quan hệ phi tuyến phức tạp.\n- Perceptron là đơn vị mạng nơ-ron đơn giản nhất, được phát minh bởi Frank Rosenblatt năm 1958. Nó nhận một vector đầu vào, tính tổng có trọng số và áp dụng hàm kích hoạt để tạo ra đầu ra dự đoán.\n- Công thức z = w^Tx + b = ∑ᵢwᵢxᵢ + b tính tổng có trọng số của các đặc trưng đầu vào xᵢ với các trọng số wᵢ và thêm bias b. Đây là phép biến đổi tuyến tính cơ bản trong perceptron và các nơ-ron của mạng nơ-ron, đại diện cho pre-activation trước khi áp dụng hàm kích hoạt.\n\n**Mối quan hệ:**\n- Perceptron tính toán tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n- Matrix factorization (NMF, SVD)\n\n**5. Topic Modeling:**\n- Document clustering\n- Automatic tagging\n- Content organization\n- Trend detection\n\n**6. Gene Expression Analysis:**\n- Group similar genes\n- Identify cancer subtypes\n- Drug discovery\n- Understanding diseases\n\n**7. Social Network Analysis:**\n- Community detection\n- Influencer identification\n- Link prediction\n- Recommendation\n\n**8. Data Preprocessing:**\n- Feature extraction (PCA, ICA)\n- Noise reduction (autoencoders)\n- Data compression\n- Dimensionality reduction\n\n**9. Market Basket Analysis:**\n- Product recommendations\n- Store layout\n- Promotions\n- Cross-selling\n\n**10. Image Segmentation:**\n- Medical imaging\n- Object detection preparation\n- Video processing\n- Computer vision preprocessing\n\n---\n\n## Học Sâu (Deep Learning)\n\n### Giới Thiệu về Học Sâu\n\nHọc sâu (Deep Learning) là một nhánh con của học máy sử dụng mạng nơ-ron nhân tạo với nhiều lớp ẩn để học các biểu diễn phân cấp của dữ liệu. Khác với các phương pháp học máy truyền thống, học sâu có khả năng tự động trích xuất đặc trưng từ dữ liệu thô mà không cần kỹ thuật đặc trưng thủ công.\n\n**Đặc điểm chính:**\n- **Học biểu diễn phân cấp:** Các lớp đầu học các đặc trưng cấp thấp (cạnh, góc), các lớp sau học đặc trưng cấp cao hơn (hình dạng, đối tượng)\n- **Khả năng xử lý dữ liệu lớn:** Hiệu suất tăng theo lượng dữ liệu\n- **End-to-end learning:** Học trực tiếp từ đầu vào thô đến đầu ra mong muốn\n- **Tự động trích xuất đặc trưng:** Không cần thiết kế đặc trưng thủ công\n\n**Ứng dụng đã cách mạng hóa:**\n- Thị giác máy tính (nhận dạng ảnh, phát hiện đối tượng)\n- Xử lý ngôn ngữ tự nhiên (dịch máy, chatbot, sinh văn bản)\n- Nhận dạng giọng nói (trợ lý ảo, chuyển đổi giọng nói thành văn bản)\n- Y tế (chẩn đoán hình ảnh, phát triển thuốc)\n- Tự động hóa (xe tự lái, robot)\n\n### Mạng Nơ-ron Nhân Tạo (Artificial Neural Networks - ANN)\n\nMạng nơ-ron nhân tạo được lấy cảm hứng từ cách thức hoạt động của não người, trong đó các nơ-ron sinh học truyền tín hiệu cho nhau thông qua các synapse.\n\n### Perceptron - Đơn Vị Cơ Bản\n\nPerceptron là đơn vị mạng nơ-ron đơn giản nhất, được phát minh bởi Frank Rosenblatt năm 1958.\n\n**Công thức:**\n$$y = \\sigma(w^Tx + b)$$\n\nTrong đó:\n- $x = [x_1, x_2, ..., x_n]^T$: Vector đầu vào (các đặc trưng)\n- $w = [w_1, w_2, ..., w_n]^T$: Vector trọng số (weights)\n- $b$: Hệ số điều chỉnh (bias) - cho phép dịch chuyển hàm quyết định\n- $\\sigma$: Hàm kích hoạt (activation function)\n- $y$: Đầu ra dự đoán\n\n\n**Các khái niệm quan trọng:**\n- Perceptron là một mô hình học máy cơ bản, chỉ có thể học các mẫu phân tách tuyến tính. Nó tính tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào, sau đó áp dụng hàm kích hoạt để tạo đầu ra. Perceptron không thể giải quyết bài toán XOR hoặc mô hình hóa các quan hệ phi tuyến phức tạp.\n- Perceptron là đơn vị mạng nơ-ron đơn giản nhất, được phát minh bởi Frank Rosenblatt năm 1958. Nó nhận một vector đầu vào, tính tổng có trọng số và áp dụng hàm kích hoạt để tạo ra đầu ra dự đoán.\n\n**Mối quan hệ:**\n- Perceptron tính toán tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào."
        },
        "output_without_context": {
            "question": "Trong thuật toán Perceptron, công thức nào được sử dụng để cập nhật trọng số sau mỗi lần dự đoán sai?",
            "answer": "w = w + η(y - ŷ)x"
        }
    },
    {
        "topic_description": "Chủ đề này tập trung vào giới hạn số lần cập nhật của thuật toán Perceptron, cùng với các công thức liên quan. Sinh viên sẽ được đánh giá về sự hiểu biết của họ về lý thuyết này và tính toán kết quả. Kết nối với tuần 4.",
        "output_with_context": {
            "question": "Trong thuật toán Perceptron, công thức nào được sử dụng để tính toán đầu ra dự đoán?",
            "answer": "y = σ(w^Tx + b)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n- Matrix factorization (NMF, SVD)\n\n**5. Topic Modeling:**\n- Document clustering\n- Automatic tagging\n- Content organization\n- Trend detection\n\n**6. Gene Expression Analysis:**\n- Group similar genes\n- Identify cancer subtypes\n- Drug discovery\n- Understanding diseases\n\n**7. Social Network Analysis:**\n- Community detection\n- Influencer identification\n- Link prediction\n- Recommendation\n\n**8. Data Preprocessing:**\n- Feature extraction (PCA, ICA)\n- Noise reduction (autoencoders)\n- Data compression\n- Dimensionality reduction\n\n**9. Market Basket Analysis:**\n- Product recommendations\n- Store layout\n- Promotions\n- Cross-selling\n\n**10. Image Segmentation:**\n- Medical imaging\n- Object detection preparation\n- Video processing\n- Computer vision preprocessing\n\n---\n\n## Học Sâu (Deep Learning)\n\n### Giới Thiệu về Học Sâu\n\nHọc sâu (Deep Learning) là một nhánh con của học máy sử dụng mạng nơ-ron nhân tạo với nhiều lớp ẩn để học các biểu diễn phân cấp của dữ liệu. Khác với các phương pháp học máy truyền thống, học sâu có khả năng tự động trích xuất đặc trưng từ dữ liệu thô mà không cần kỹ thuật đặc trưng thủ công.\n\n**Đặc điểm chính:**\n- **Học biểu diễn phân cấp:** Các lớp đầu học các đặc trưng cấp thấp (cạnh, góc), các lớp sau học đặc trưng cấp cao hơn (hình dạng, đối tượng)\n- **Khả năng xử lý dữ liệu lớn:** Hiệu suất tăng theo lượng dữ liệu\n- **End-to-end learning:** Học trực tiếp từ đầu vào thô đến đầu ra mong muốn\n- **Tự động trích xuất đặc trưng:** Không cần thiết kế đặc trưng thủ công\n\n**Ứng dụng đã cách mạng hóa:**\n- Thị giác máy tính (nhận dạng ảnh, phát hiện đối tượng)\n- Xử lý ngôn ngữ tự nhiên (dịch máy, chatbot, sinh văn bản)\n- Nhận dạng giọng nói (trợ lý ảo, chuyển đổi giọng nói thành văn bản)\n- Y tế (chẩn đoán hình ảnh, phát triển thuốc)\n- Tự động hóa (xe tự lái, robot)\n\n### Mạng Nơ-ron Nhân Tạo (Artificial Neural Networks - ANN)\n\nMạng nơ-ron nhân tạo được lấy cảm hứng từ cách thức hoạt động của não người, trong đó các nơ-ron sinh học truyền tín hiệu cho nhau thông qua các synapse.\n\n### Perceptron - Đơn Vị Cơ Bản\n\nPerceptron là đơn vị mạng nơ-ron đơn giản nhất, được phát minh bởi Frank Rosenblatt năm 1958.\n\n**Công thức:**\n$$y = \\sigma(w^Tx + b)$$\n\nTrong đó:\n- $x = [x_1, x_2, ..., x_n]^T$: Vector đầu vào (các đặc trưng)\n- $w = [w_1, w_2, ..., w_n]^T$: Vector trọng số (weights)\n- $b$: Hệ số điều chỉnh (bias) - cho phép dịch chuyển hàm quyết định\n- $\\sigma$: Hàm kích hoạt (activation function)\n- $y$: Đầu ra dự đoán\n\n\n**Các khái niệm quan trọng:**\n- Perceptron là một mô hình học máy cơ bản, chỉ có thể học các mẫu phân tách tuyến tính. Nó tính tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào, sau đó áp dụng hàm kích hoạt để tạo đầu ra. Perceptron không thể giải quyết bài toán XOR hoặc mô hình hóa các quan hệ phi tuyến phức tạp.\n- Công thức $y = \\sigma(w^Tx + b)$ mô tả hoạt động của một Perceptron, trong đó $w^Tx + b$ là tổng có trọng số của đầu vào cộng với bias, và $\\sigma$ là hàm kích hoạt áp dụng cho tổng này để tạo ra đầu ra dự đoán $y$.\n\n**Mối quan hệ:**\n- Perceptron tính toán tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào.\n- Perceptron chỉ có thể học các mẫu phân tách tuyến tính.\n- Perceptron tính toán đầu ra dự đoán bằng công thức $y = \\sigma(w^Tx + b)$.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**Cách hoạt động:**\n1. Nhận đầu vào từ các đặc trưng\n2. Tính tổng có trọng số: $z = w^Tx + b = \\sum_{i=1}^{n}w_i x_i + b$\n3. Áp dụng hàm kích hoạt để tạo đầu ra\n\n**Hạn chế quan trọng:** \n- Chỉ có thể học các mẫu phân tách tuyến tính (linearly separable)\n- Không thể giải quyết bài toán XOR\n- Không thể mô hình hóa các quan hệ phi tuyến phức tạp\n\n**Ví dụ:** Một perceptron có thể phân loại điểm nằm phía trên hay dưới một đường thẳng, nhưng không thể phân loại các điểm trong bài toán XOR (cần đường cong để phân tách).\n\n### Perceptron Đa Lớp (Multi-Layer Perceptron - MLP)\n\nMLP khắc phục hạn chế của perceptron đơn bằng cách xếp chồng nhiều lớp, cho phép học các hàm phi tuyến phức tạp.\n\n**Kiến trúc:**\n\n**1. Lớp đầu vào (Input Layer):**\n- Nhận dữ liệu thô\n- Số nơ-ron = số đặc trưng đầu vào\n- Không có phép biến đổi, chỉ truyền dữ liệu\n\n**2. Lớp ẩn (Hidden Layers):**\n- Thực hiện các phép biến đổi phi tuyến\n- Số lượng có thể từ 1 đến hàng trăm lớp\n- Mỗi lớp học biểu diễn trừu tượng hơn\n- Số nơ-ron trong mỗi lớp là hyperparameter\n\n**3. Lớp đầu ra (Output Layer):**\n- Tạo dự đoán cuối cùng\n- Số nơ-ron phụ thuộc vào bài toán:\n  - Hồi quy: 1 nơ-ron\n  - Phân loại nhị phân: 1 nơ-ron (với sigmoid) hoặc 2 (với softmax)\n  - Phân loại đa lớp: K nơ-ron (K = số lớp)\n\n**Lan truyền xuôi (Forward Propagation):**\n\nQuá trình tính toán từ đầu vào đến đầu ra qua các lớp:\n\nVới lớp $l$:\n$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n$$a^{[l]} = \\sigma(z^{[l]})$$\n\nTrong đó:\n- $W^{[l]}$: Ma trận trọng số của lớp $l$ (kích thước $n^{[l]} \times n^{[l-1]}$)\n- $b^{[l]}$: Vector bias của lớp $l$ (kích thước $n^{[l]} \times 1$)\n- $a^{[l-1]}$: Activation của lớp trước (đầu vào cho lớp $l$)\n- $z^{[l]}$: Pre-activation (trước khi áp dụng hàm kích hoạt)\n- $a^{[l]}$: Activation (sau khi áp dụng hàm kích hoạt)\n- $\\sigma$: Hàm kích hoạt\n\n**Ví dụ minh họa:**\n- Lớp 1: 3 nơ-ron, nhận input 784 chiều (ảnh 28×28) → $W^{[1]}$: 3×784\n- Lớp 2: 2 nơ-ron, nhận từ lớp 1 → $W^{[2]}$: 2×3\n\n**Các khái niệm quan trọng:**\n- Perceptron là một mô hình học máy cơ bản, chỉ có thể học các mẫu phân tách tuyến tính. Nó tính tổng có trọng số z = w^Tx + b = ∑ᵢwᵢxᵢ + b từ các đặc trưng đầu vào, sau đó áp dụng hàm kích hoạt để tạo đầu ra. Perceptron không thể giải quyết bài toán XOR hoặc mô hình hóa các quan hệ phi tuyến phức tạp.\n\n**Mối quan hệ:**\n- Perceptron chỉ có thể học các mẫu phân tách tuyến tính.\n- Perceptron tính toán đầu ra dự đoán bằng công thức $y = \\sigma(w^Tx + b)$.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\nBackpropagation có thể được hiểu thông qua đồ thị tính toán (computational graph), trong đó:\n- Mỗi node là một operation\n- Edges mang giá trị và gradients\n- Forward pass tính giá trị, backward pass tính gradients\n\n**Vấn đề Vanishing/Exploding Gradients:**\n- **Vanishing:** Gradient giảm dần khi lan truyền về các lớp đầu → các lớp đầu học chậm\n  - Nguyên nhân: Hàm kích hoạt có đạo hàm nhỏ (sigmoid, tanh)\n  - Giải pháp: ReLU, batch normalization, residual connections\n- **Exploding:** Gradient tăng dần → weights cập nhật quá mạnh, không ổn định\n  - Giải pháp: Gradient clipping, proper weight initialization\n\n**Lưu ý về hiệu suất:**\n- Độ phức tạp tính toán của backpropagation tương đương forward pass\n- Matrix operations có thể vectorize → tính toán hiệu quả trên GPU\n- Cần lưu trữ activations từ forward pass → tốn memory\n\n### Thuật Toán Tối Ưu (Optimization Algorithms)\n\nCác thuật toán tối ưu quyết định cách cập nhật weights để minimize loss function.\n\n**1. Gradient Descent (Hạ Gradient):**\n$$w := w - \\alpha\frac{\\partial L}{\\partial w}$$\n\n**Đặc điểm:**\n- $\\alpha$ (learning rate): Hyperparameter quan trọng nhất\n- Cập nhật dựa trên toàn bộ training set (batch gradient descent)\n\n**Ưu điểm:**\n- Đơn giản, dễ hiểu\n- Hội tụ ổn định với learning rate phù hợp\n- Đảm bảo tìm được local minimum với hàm convex\n\n**Nhược điểm:**\n- Chậm với dữ liệu lớn (phải xử lý toàn bộ dataset mỗi iteration)\n- Có thể bị kẹt ở local minima hoặc saddle points\n- Learning rate cố định không phù hợp mọi giai đoạn training\n\n**2. Stochastic Gradient Descent (SGD):**\n$$w := w - \\alpha\frac{\\partial L_i}{\\partial w}$$\n\n**Đặc điểm:**\n- Cập nhật sau **mỗi** mẫu dữ liệu (sample)\n- Gradient ước lượng từ 1 sample → noisy nhưng nhanh\n\n**Ưu điểm:**\n- Rất nhanh, có thể train trên dữ liệu lớn\n- Noise giúp thoát khỏi local minima\n- Có thể train online (dữ liệu đến liên tục)\n\n**Nhược điểm:**\n- Quá trình hội tụ không ổn định, dao động mạnh\n- Có thể không hội tụ chính xác đến minimum\n- Khó song song hóa (sequential updates)\n\n**3. Mini-batch Gradient Descent:**\n$$w := w - \\alpha\frac{1}{m}\\sum_{i=1}^{m}\frac{\\partial L_i}{\\partial w}$$\n\n**Đặc điểm:**\n- Cập nhật sau một **batch nhỏ** (thường 32, 64, 128, 256)\n- Kết hợp ưu điểm của batch GD và SGD\n- **Là phương pháp được sử dụng phổ biến nhất trong thực tế**\n\n**Ưu điểm:**\n- Tốc độ nhanh, ổn định hơn SGD\n- Có thể vectorize, tận dụng GPU hiệu quả\n- Gradient ổn định hơn SGD nhưng vẫn có noise tốt\n- Batch size là hyperparameter điều chỉnh được\n\n**Lựa chọn batch size:**\n\n**Các khái niệm quan trọng:**\n- Weights (trọng số) là các tham số có thể học được trong mô hình Machine Learning, đặc biệt là trong mạng nơ-ron, đại diện cho sức mạnh của kết nối giữa các nơ-ron. Trong công thức $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$, $W^{[l]}$ là ma trận trọng số của lớp thứ $l$. Trong quá trình training, weights được cập nhật liên tục bởi các thuật toán tối ưu để cực tiểu hóa (minimize) hàm loss. Công thức cập nhật chung là w := w - α * (∂L/∂w).\n\n**Mối quan hệ:**\n- Các thuật toán tối ưu quyết định cách cập nhật weights để minimize loss function.\n- Stochastic Gradient Descent cập nhật weights sau mỗi mẫu dữ liệu theo công thức w := w - α * (∂L_i/∂w).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\nBackpropagation là thuật toán cốt lõi để huấn luyện mạng nơ-ron sâu, cho phép tính gradient một cách hiệu quả thông qua quy tắc chuỗi (chain rule).\n\n**Ý tưởng cơ bản:**\n- Tính toán gradient của loss function theo tất cả các tham số (weights và biases)\n- Lan truyền gradient từ output về input qua các lớp\n- Sử dụng quy tắc chuỗi để phân rã gradient phức tạp thành các phần đơn giản\n\n**Quy tắc chuỗi (Chain Rule):**\n$$\frac{\\partial L}{\\partial w^{[l]}} = \frac{\\partial L}{\\partial a^{[l]}} \\cdot \frac{\\partial a^{[l]}}{\\partial z^{[l]}} \\cdot \frac{\\partial z^{[l]}}{\\partial w^{[l]}}$$\n\nTrong đó:\n- $\frac{\\partial L}{\\partial a^{[l]}}$: Gradient của loss theo activation\n- $\frac{\\partial a^{[l]}}{\\partial z^{[l]}}$: Đạo hàm của hàm kích hoạt\n- $\frac{\\partial z^{[l]}}{\\partial w^{[l]}}$: Gradient của pre-activation theo weights\n\n**Các bước chi tiết:**\n\n**1. Forward Pass (Lan truyền xuôi):**\n- Tính toán output của mỗi lớp từ input đến output\n- Lưu trữ tất cả các giá trị $z^{[l]}$ và $a^{[l]}$ (cần cho backward pass)\n\n**2. Tính Loss:**\n- So sánh prediction với ground truth\n- Tính giá trị loss: $L = Loss(y, \\hat{y})$\n\n**3. Backward Pass (Lan truyền ngược):**\n- Bắt đầu từ lớp output, tính gradient của loss theo output\n- Với mỗi lớp từ L về 1:\n  - Tính $\frac{\\partial L}{\\partial z^{[l]}} = \frac{\\partial L}{\\partial a^{[l]}} \\odot \\sigma'(z^{[l]})$ (element-wise product)\n  - Tính $\frac{\\partial L}{\\partial W^{[l]}} = \frac{\\partial L}{\\partial z^{[l]}} \\cdot (a^{[l-1]})^T$\n  - Tính $\frac{\\partial L}{\\partial b^{[l]}} = \frac{\\partial L}{\\partial z^{[l]}}$\n  - Lan truyền về lớp trước: $\frac{\\partial L}{\\partial a^{[l-1]}} = (W^{[l]})^T \\cdot \frac{\\partial L}{\\partial z^{[l]}}$\n\n**4. Cập nhật Weights:**\n- Sử dụng gradient descent hoặc các optimizer khác\n- $W^{[l]} := W^{[l]} - \\alpha \frac{\\partial L}{\\partial W^{[l]}}$\n- $b^{[l]} := b^{[l]} - \\alpha \frac{\\partial L}{\\partial b^{[l]}}$\n\n**Ví dụ minh họa:**\nMạng 2 lớp: Input → Hidden → Output\n- Forward: $a^{[1]} = \\sigma(W^{[1]}x + b^{[1]})$, $\\hat{y} = \\sigma(W^{[2]}a^{[1]} + b^{[2]})$\n- Loss: $L = (y - \\hat{y})^2$\n- Backward:\n  - $\frac{\\partial L}{\\partial \\hat{y}} = -2(y - \\hat{y})$\n  - $\frac{\\partial L}{\\partial W^{[2]}} = \frac{\\partial L}{\\partial \\hat{y}} \\cdot \\sigma'(z^{[2]}) \\cdot a^{[1]}$\n  - Lan truyền về hidden layer tương tự\n\n**Computational Graph:**\n\n**Các khái niệm quan trọng:**\n- Weights (trọng số) là các tham số có thể học được trong mô hình Machine Learning, đặc biệt là trong mạng nơ-ron, đại diện cho sức mạnh của kết nối giữa các nơ-ron. Trong công thức $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$, $W^{[l]}$ là ma trận trọng số của lớp thứ $l$. Trong quá trình training, weights được cập nhật liên tục bởi các thuật toán tối ưu để cực tiểu hóa (minimize) hàm loss. Công thức cập nhật chung là w := w - α * (∂L/∂w).\n\n**Mối quan hệ:**\n- Các thuật toán tối ưu quyết định cách cập nhật weights để minimize loss function.\n- Stochastic Gradient Descent cập nhật weights sau mỗi mẫu dữ liệu theo công thức w := w - α * (∂L_i/∂w)."
        },
        "output_without_context": {
            "question": "Trong lý thuyết Perceptron, số lần cập nhật tối đa cần thiết để đạt được một quyết định chính xác là bao nhiêu lần nếu dữ liệu có thể phân tách được?",
            "answer": "Số lần cập nhật tối đa là O(1/ε), với ε là khoảng cách giữa các lớp."
        }
    },
    {
        "topic_description": "Chủ đề này khám phá khái niệm về SVM, cách tìm siêu phẳng phân tách và định nghĩa biên độ tối đa. Câu hỏi có thể yêu cầu sinh viên hiểu rõ về thuật toán SVM và sự khác biệt giữa các loại. Kết nối đồng thời với tuần 3 và tuần 4.",
        "output_with_context": {
            "question": "Trong thuật toán SVM, hàm kernel nào được sử dụng để ánh xạ dữ liệu sang không gian đặc trưng cao hơn nhằm phân tách tuyến tính các lớp dữ liệu không phân tách tuyến tính trong không gian gốc?",
            "answer": "Kernel Trick",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Nhiều bài toán không phân tách tuyến tính trong không gian gốc\n- Ví dụ: XOR problem, dữ liệu phân bố hình tròn\n\n**Giải pháp:**\n- Ánh xạ dữ liệu sang không gian đặc trưng (feature space) nhiều chiều hơn\n- Trong không gian mới, dữ liệu có thể phân tách tuyến tính\n- SVM tuyến tính trong không gian mới = SVM phi tuyến trong không gian gốc\n\n**Hàm Kernel:**\n$$K(x_i, x_j) = \\phi(x_i)^T\\phi(x_j)$$\n\nTrong đó:\n- $\\phi(x)$ là hàm ánh xạ (không cần tính tường minh)\n- $K(x_i, x_j)$ tính trực tiếp inner product trong không gian đặc trưng\n\n**Ưu điểm Kernel Trick:**\n- Không cần tính $\\phi(x)$ tường minh\n- Không cần lưu trữ trong không gian nhiều chiều\n- Chỉ cần tính $K(x_i, x_j)$\n- Hiệu quả tính toán\n\n**Các Kernel Phổ Biến:**\n\n**1. Linear Kernel (Kernel Tuyến Tính):**\n$$K(x_i, x_j) = x_i^Tx_j$$\n\n- Không ánh xạ, chỉ là inner product thông thường\n- Sử dụng khi dữ liệu đã phân tách tuyến tính\n- Nhanh nhất\n- Tốt cho high-dimensional sparse data (text)\n- Dễ diễn giải\n\n**Khi nào dùng:**\n- Số đặc trưng >> số mẫu\n- Dữ liệu văn bản\n- Cần tốc độ và interpretability\n\n**2. Polynomial Kernel (Kernel Đa Thức):**\n$$K(x_i, x_j) = (x_i^Tx_j + c)^d$$\n\nTrong đó:\n- $d$ là degree (bậc) của polynomial\n- $c$ là constant (thường = 0 hoặc 1)\n\n**Đặc điểm:**\n- Tạo đặc trưng polynomial\n- $d=2$: Bao gồm tương tác cặp\n- $d$ lớn: Flexibility cao nhưng dễ overfit\n- Có thể bùng nổ tính toán với $d$ lớn\n\n**Khi nào dùng:**\n- Quan hệ polynomial giữa features\n- Image processing\n- $d=2$ hoặc $d=3$ thường đủ\n\n**3. RBF Kernel (Radial Basis Function / Gaussian Kernel):**\n$$K(x_i, x_j) = \\exp(-\\gamma||x_i - x_j||^2)$$\n\nTrong đó:\n- $\\gamma$ (gamma) kiểm soát ảnh hưởng của single training example\n- $\\gamma = \frac{1}{2\\sigma^2}$ trong phân phối Gaussian\n\n**$\\gamma$ nhỏ:**\n- Ảnh hưởng lan rộng\n- Decision boundary mượt\n- Low variance, high bias (underfitting)\n\n**$\\gamma$ lớn:**\n- Ảnh hưởng hẹp\n- Decision boundary phức tạp\n- High variance, low bias (overfitting)\n\n**Đặc điểm:**\n- Phổ biến nhất\n- Ánh xạ sang không gian vô hạn chiều\n- Linh hoạt, xử lý được nhiều dạng data\n- Giá trị trong [0, 1]\n\n**Khi nào dùng:**\n- Default choice khi không biết kernel nào\n- Dữ liệu không có structure rõ ràng\n- Hiệu quả với nhiều loại bài toán\n\n**Lựa chọn $\\gamma$:**\n- Cross-validation\n\n**Các khái niệm quan trọng:**\n- Support Vector Machine (SVM) là một thuật toán học có giám sát mạnh mẽ, được sử dụng rộng rãi cho cả bài toán phân loại và hồi quy (SVR). Mục tiêu chính của SVM là tìm một siêu phẳng tối ưu trong không gian nhiều chiều để phân tách các lớp dữ liệu, tối đa hóa khoảng cách (margin) giữa siêu phẳng này và các điểm dữ liệu gần nhất (được gọi là support vectors). SVM có khả năng xử lý hiệu quả dữ liệu trong không gian chiều cao, ngay cả khi số lượng chiều lớn hơn số lượng mẫu.\n\nĐể xử lý các bài toán không phân tách tuyến tính trong không gian gốc, SVM sử dụng Kernel Trick để ánh xạ dữ liệu sang một không gian đặc trưng có chiều cao hơn, nơi dữ liệu có thể phân tách tuyến tính. Điều này cho phép SVM hoạt động như một mô hình phi tuyến trong không gian gốc thông qua các hàm kernel linh hoạt. SVM có hai dạng chính là Hard Margin SVM (khi dữ liệu phân tách hoàn toàn) và Soft Margin SVM (khi có thể chấp nhận một số lỗi phân loại để đạt được biên độ lớn hơn), đồng thời tích hợp cơ chế regularization thông qua tham số C để kiểm soát overfitting.\n\nMô hình này được tối ưu hóa thông qua bài toán đối ngẫu để tìm các Lagrange multipliers, từ đó xác định các support vectors quan trọng. Hàm mục tiêu của SVM là một bài toán tối ưu lồi, đảm bảo tìm được cực tiểu toàn cục.\n\nCác ứng dụng của SVM rất đa dạng, bao gồm phân loại văn bản, nhận dạng chữ viết tay, nhận dạng khuôn mặt, phân loại hình ảnh, phân tích sinh học, xác minh chữ ký và dự đoán chuỗi thời gian.\n\n**Mối quan hệ:**\n- Support Vector Machine hoạt động bằng cách tìm siêu phẳng tối ưu để phân tách các lớp dữ liệu.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n- Content filtering\n\n---\n\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n\n### Giới Thiệu Về SVM\n\nSupport Vector Machine (SVM) là thuật toán học có giám sát mạnh mẽ cho phân loại và hồi quy. Chúng hoạt động bằng cách tìm siêu phẳng (hyperplane) tối ưu phân tách tối đa các lớp trong không gian nhiều chiều.\n\n**Ý tưởng cốt lõi:**\n- Tìm ranh giới quyết định tốt nhất giữa các lớp\n- Tối đa hóa khoảng cách (margin) giữa các lớp\n- Chỉ dựa vào các điểm dữ liệu quan trọng nhất (support vectors)\n\n**Ứng dụng:**\n- Phân loại văn bản (spam detection, sentiment analysis)\n- Nhận dạng chữ viết tay\n- Nhận dạng khuôn mặt\n- Phân loại hình ảnh\n- Phân tích sinh học (protein classification)\n- Dự đoán chuỗi thời gian\n\n**Ưu điểm chính:**\n- Hiệu quả trong không gian nhiều chiều\n- Hoạt động tốt khi có ranh giới rõ ràng\n- Tiết kiệm bộ nhớ (chỉ lưu support vectors)\n- Linh hoạt với nhiều kernel functions\n\n### SVM Tuyến Tính (Linear SVM)\n\n**Mục tiêu:** Tìm siêu phẳng có margin tối đa giữa các lớp.\n\n**Siêu Phẳng (Hyperplane):**\n\nTrong không gian n chiều, siêu phẳng là không gian con (n-1) chiều chia không gian thành hai nửa.\n\n**Phương trình siêu phẳng:**\n$$w^Tx + b = 0$$\n\nTrong đó:\n- $w$ là vector trọng số (weights) - vector pháp tuyến của siêu phẳng\n- $x$ là vector đặc trưng\n- $b$ là bias (hệ số chặn)\n\n**Ví dụ:**\n- 2D: $w_1x_1 + w_2x_2 + b = 0$ (đường thẳng)\n- 3D: $w_1x_1 + w_2x_2 + w_3x_3 + b = 0$ (mặt phẳng)\n\n**Hàm Quyết Định:**\n$$f(x) = sign(w^Tx + b)$$\n\n- Nếu $w^Tx + b > 0$: Dự đoán lớp +1\n- Nếu $w^Tx + b < 0$: Dự đoán lớp -1\n- Nếu $w^Tx + b = 0$: Điểm nằm trên siêu phẳng\n\n**Margin (Lề):**\n\nMargin là khoảng cách từ siêu phẳng đến điểm dữ liệu gần nhất.\n\n**Công thức:**\n$$margin = \frac{2}{||w||}$$\n\n**Giải thích:**\n- Khoảng cách từ điểm $x_i$ đến siêu phẳng: $\frac{|w^Tx_i + b|}{||w||}$\n- Điểm support vector thỏa: $|w^Tx_i + b| = 1$\n- Margin = khoảng cách từ support vector này đến support vector bên kia = $\frac{2}{||w||}$\n\n**Tối đa hóa margin:**\n- Margin lớn → Generalization tốt hơn\n- Mô hình ổn định hơn với noise\n- Tăng khả năng phân loại đúng trên dữ liệu mới\n\n\n**Các khái niệm quan trọng:**\n- Support Vector Machine (SVM) là một thuật toán học có giám sát mạnh mẽ, được sử dụng rộng rãi cho cả bài toán phân loại và hồi quy (SVR). Mục tiêu chính của SVM là tìm một siêu phẳng tối ưu trong không gian nhiều chiều để phân tách các lớp dữ liệu, tối đa hóa khoảng cách (margin) giữa siêu phẳng này và các điểm dữ liệu gần nhất (được gọi là support vectors). SVM có khả năng xử lý hiệu quả dữ liệu trong không gian chiều cao, ngay cả khi số lượng chiều lớn hơn số lượng mẫu.\n\nĐể xử lý các bài toán không phân tách tuyến tính trong không gian gốc, SVM sử dụng Kernel Trick để ánh xạ dữ liệu sang một không gian đặc trưng có chiều cao hơn, nơi dữ liệu có thể phân tách tuyến tính. Điều này cho phép SVM hoạt động như một mô hình phi tuyến trong không gian gốc thông qua các hàm kernel linh hoạt. SVM có hai dạng chính là Hard Margin SVM (khi dữ liệu phân tách hoàn toàn) và Soft Margin SVM (khi có thể chấp nhận một số lỗi phân loại để đạt được biên độ lớn hơn), đồng thời tích hợp cơ chế regularization thông qua tham số C để kiểm soát overfitting.\n\nMô hình này được tối ưu hóa thông qua bài toán đối ngẫu để tìm các Lagrange multipliers, từ đó xác định các support vectors quan trọng. Hàm mục tiêu của SVM là một bài toán tối ưu lồi, đảm bảo tìm được cực tiểu toàn cục.\n\nCác ứng dụng của SVM rất đa dạng, bao gồm phân loại văn bản, nhận dạng chữ viết tay, nhận dạng khuôn mặt, phân loại hình ảnh, phân tích sinh học, xác minh chữ ký và dự đoán chuỗi thời gian.\n- Siêu phẳng là một không gian con (n-1) chiều trong không gian n chiều, có chức năng chia không gian thành hai nửa. Trong SVM, siêu phẳng là ranh giới quyết định phân tách các lớp dữ liệu. Phương trình của siêu phẳng là $w^Tx + b = 0$, trong đó $w$ là vector trọng số, $x$ là vector đặc trưng và $b$ là bias.\n\n**Mối quan hệ:**\n- Support Vector Machine hoạt động bằng cách tìm siêu phẳng tối ưu để phân tách các lớp dữ liệu.\n- Support Vector Machine có thể sử dụng thuật toán Crammer & Singer để giải trực tiếp bài toán phân loại đa lớp thông qua một bài toán tối ưu duy nhất.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n### Hard Margin SVM\n\nDành cho dữ liệu **linearly separable** (phân tách tuyến tính hoàn toàn).\n\n**Bài toán tối ưu:**\n$$\\min_{w,b} \frac{1}{2}||w||^2$$\n\n**Ràng buộc:** $y_i(w^Tx_i + b) \\geq 1, \forall i$\n\n**Giải thích:**\n- Mục tiêu: Tối thiểu hóa $||w||^2$ (tương đương tối đa hóa margin $\frac{2}{||w||}$)\n- Ràng buộc: Tất cả điểm phải được phân loại đúng\n- $y_i \\in \\{-1, +1\\}$: Nhãn lớp\n- $y_i(w^Tx_i + b) \\geq 1$: Điểm nằm đúng phía và cách siêu phẳng ít nhất 1 đơn vị\n\n**Tại sao dùng $\frac{1}{2}||w||^2$:**\n- Đạo hàm đẹp hơn (mất $\frac{1}{2}$ khi lấy đạo hàm)\n- Bài toán convex quadratic programming\n- Dễ giải với Lagrange multipliers\n\n**Hạn chế:**\n- Yêu cầu dữ liệu phân tách tuyến tính hoàn toàn\n- Không tolerant với outliers\n- Hiếm khi áp dụng trong thực tế (dữ liệu thường có noise)\n\n### Soft Margin SVM\n\nDành cho dữ liệu **không phân tách tuyến tính hoàn toàn** (có overlap).\n\n**Giới thiệu Slack Variables $\\xi_i$:**\n- Cho phép một số điểm vi phạm margin\n- $\\xi_i$ đo lường mức độ vi phạm của điểm $i$\n- $\\xi_i = 0$: Điểm được phân loại đúng, nằm ngoài margin\n- $0 < \\xi_i < 1$: Điểm nằm trong margin nhưng phân loại đúng\n- $\\xi_i \\geq 1$: Điểm bị misclassified\n\n**Bài toán tối ưu:**\n$$\\min_{w,b,\\xi} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}\\xi_i$$\n\n**Ràng buộc:**\n- $y_i(w^Tx_i + b) \\geq 1 - \\xi_i$\n- $\\xi_i \\geq 0, \forall i$\n\n**Tham số C (Regularization Parameter):**\n\nC điều khiển trade-off giữa margin rộng và số lượng vi phạm.\n\n**C lớn (C → ∞):**\n- Penalty cao cho vi phạm\n- Margin nhỏ hơn\n- Ít misclassifications\n- Low bias, high variance (overfitting)\n- Gần với hard margin SVM\n\n**C nhỏ:**\n- Penalty thấp cho vi phạm\n- Margin lớn hơn\n- Nhiều misclassifications hơn\n- High bias, low variance (underfitting)\n- Mô hình đơn giản hơn\n\n**Lựa chọn C:**\n- Cross-validation\n- Thường thử: 0.01, 0.1, 1, 10, 100\n- Phụ thuộc vào scale của dữ liệu\n\n**Diễn giải hàm mục tiêu:**\n- $\frac{1}{2}||w||^2$: Tối đa hóa margin\n- $C\\sum_{i=1}^{m}\\xi_i$: Tối thiểu hóa vi phạm\n- C cân bằng hai mục tiêu này\n\n### Kernel Trick (Mẹo Kernel)\n\nÁnh xạ dữ liệu sang không gian nhiều chiều hơn nơi nó trở nên phân tách tuyến tính.\n\n**Vấn đề:**\n\n**Các khái niệm quan trọng:**\n- Support Vector Machine (SVM) là một thuật toán học có giám sát mạnh mẽ, được sử dụng rộng rãi cho cả bài toán phân loại và hồi quy (SVR). Mục tiêu chính của SVM là tìm một siêu phẳng tối ưu trong không gian nhiều chiều để phân tách các lớp dữ liệu, tối đa hóa khoảng cách (margin) giữa siêu phẳng này và các điểm dữ liệu gần nhất (được gọi là support vectors). SVM có khả năng xử lý hiệu quả dữ liệu trong không gian chiều cao, ngay cả khi số lượng chiều lớn hơn số lượng mẫu.\n\nĐể xử lý các bài toán không phân tách tuyến tính trong không gian gốc, SVM sử dụng Kernel Trick để ánh xạ dữ liệu sang một không gian đặc trưng có chiều cao hơn, nơi dữ liệu có thể phân tách tuyến tính. Điều này cho phép SVM hoạt động như một mô hình phi tuyến trong không gian gốc thông qua các hàm kernel linh hoạt. SVM có hai dạng chính là Hard Margin SVM (khi dữ liệu phân tách hoàn toàn) và Soft Margin SVM (khi có thể chấp nhận một số lỗi phân loại để đạt được biên độ lớn hơn), đồng thời tích hợp cơ chế regularization thông qua tham số C để kiểm soát overfitting.\n\nMô hình này được tối ưu hóa thông qua bài toán đối ngẫu để tìm các Lagrange multipliers, từ đó xác định các support vectors quan trọng. Hàm mục tiêu của SVM là một bài toán tối ưu lồi, đảm bảo tìm được cực tiểu toàn cục.\n\nCác ứng dụng của SVM rất đa dạng, bao gồm phân loại văn bản, nhận dạng chữ viết tay, nhận dạng khuôn mặt, phân loại hình ảnh, phân tích sinh học, xác minh chữ ký và dự đoán chuỗi thời gian.\n- Linearly separable là thuộc tính của tập dữ liệu cho phép tìm thấy một siêu phẳng (đường thẳng trong 2D, mặt phẳng trong 3D) để phân tách hoàn toàn các lớp dữ liệu khác nhau mà không có sự chồng chéo, là yêu cầu của Hard Margin SVM. Ngược lại, non-linearly separable là thuộc tính của tập dữ liệu khi không thể tìm thấy một siêu phẳng duy nhất để phân tách hoàn toàn các lớp dữ liệu mà không có sự chồng chéo hoặc lỗi, và được xử lý bằng Soft Margin SVM và Kernel Trick.\n\n**Mối quan hệ:**\n- Support Vector Machine hoạt động bằng cách tìm siêu phẳng tối ưu để phân tách các lớp dữ liệu.\n- Support Vector Machine có thể sử dụng thuật toán Crammer & Singer để giải trực tiếp bài toán phân loại đa lớp thông qua một bài toán tối ưu duy nhất.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Kiểm soát trade-off giữa margin và errors\n- Range: 0.01 đến 1000\n- Grid search: [0.01, 0.1, 1, 10, 100]\n\n**2. Kernel type:**\n- Lựa chọn hàm kernel\n- Try: 'linear', 'rbf', 'poly'\n- Default: 'rbf'\n\n**3. Gamma (γ) - cho RBF kernel:**\n- Định nghĩa bán kính ảnh hưởng\n- Range: 0.0001 đến 1\n- Grid search: [0.001, 0.01, 0.1, 1]\n- 'scale': $\frac{1}{n \times var(X)}$\n\n**4. Degree (d) - cho Polynomial kernel:**\n- Bậc của polynomial\n- Thường: 2, 3, 4\n- Tránh quá cao (overfitting, computational cost)\n\n**5. Epsilon (ε) - cho SVR:**\n- Độ rộng tube\n- Range: 0.01 đến 1\n- Phụ thuộc scale của target\n\n**Chiến lược Tuning:**\n\n**1. Coarse Grid Search:**\n- Tìm vùng tốt với grid thô\n- C: [0.1, 1, 10, 100]\n- gamma: [0.001, 0.01, 0.1, 1]\n\n**2. Fine Grid Search:**\n- Zoom vào vùng tốt\n- Grid mịn hơn\n\n**3. Randomized Search:**\n- Nhanh hơn grid search\n- Sample ngẫu nhiên từ distributions\n\n**4. Bayesian Optimization:**\n- Thông minh, adaptive\n- Ít iterations hơn\n\n**Tips:**\n- Luôn feature scaling trước\n- Cross-validation (5-fold hoặc 10-fold)\n- Monitor training time\n- Tune C và gamma cùng lúc (interdependent)\n\n### Ưu Điểm Của SVM\n\n**1. Hiệu quả trong không gian nhiều chiều:**\n- Hoạt động tốt khi $n_{features} > n_{samples}$\n- Phù hợp cho text, genomics, high-dimensional data\n\n**2. Tiết kiệm bộ nhớ:**\n- Chỉ lưu support vectors\n- Không cần lưu toàn bộ training data\n- Sparse representation\n\n**3. Linh hoạt với kernel functions:**\n- Nhiều kernel có sẵn\n- Có thể define custom kernel\n- Xử lý được non-linear relationships\n\n**4. Hoạt động tốt với margin rõ ràng:**\n- Khi các lớp separated tốt\n- Decision boundary rõ ràng\n\n**5. Robust to outliers (soft margin):**\n- Slack variables cho phép một số outliers\n- Không bị ảnh hưởng nhiều bởi outliers xa\n\n**6. Cơ sở toán học vững chắc:**\n- Convex optimization problem\n- Global optimum guaranteed\n- Không bị stuck ở local minima\n\n**7. Regularization tích hợp:**\n- Tham số C control overfitting\n- Không cần regularization bên ngoài\n\n### Nhược Điểm\n\n**1. Chi phí tính toán cao cho dữ liệu lớn:**\n- Training complexity: O($n^2$) đến O($n^3$)\n- Không scale tốt với >10,000 mẫu\n- Memory intensive\n\n**2. Nhạy cảm với feature scaling:**\n- **Bắt buộc** phải scaling/normalization\n- Đặc trưng có scale lớn sẽ dominate\n- Ảnh hưởng đến kernel calculations\n\n**3. Khó diễn giải (với kernels):**\n- Đặc biệt với RBF kernel\n- Không thấy được feature importance trực tiếp\n- Black box (so với linear models, trees)\n\n\n**Các khái niệm quan trọng:**\n- Support Vector Machine (SVM) là một thuật toán học có giám sát mạnh mẽ, được sử dụng rộng rãi cho cả bài toán phân loại và hồi quy (SVR). Mục tiêu chính của SVM là tìm một siêu phẳng tối ưu trong không gian nhiều chiều để phân tách các lớp dữ liệu, tối đa hóa khoảng cách (margin) giữa siêu phẳng này và các điểm dữ liệu gần nhất (được gọi là support vectors). SVM có khả năng xử lý hiệu quả dữ liệu trong không gian chiều cao, ngay cả khi số lượng chiều lớn hơn số lượng mẫu.\n\nĐể xử lý các bài toán không phân tách tuyến tính trong không gian gốc, SVM sử dụng Kernel Trick để ánh xạ dữ liệu sang một không gian đặc trưng có chiều cao hơn, nơi dữ liệu có thể phân tách tuyến tính. Điều này cho phép SVM hoạt động như một mô hình phi tuyến trong không gian gốc thông qua các hàm kernel linh hoạt. SVM có hai dạng chính là Hard Margin SVM (khi dữ liệu phân tách hoàn toàn) và Soft Margin SVM (khi có thể chấp nhận một số lỗi phân loại để đạt được biên độ lớn hơn), đồng thời tích hợp cơ chế regularization thông qua tham số C để kiểm soát overfitting.\n\nMô hình này được tối ưu hóa thông qua bài toán đối ngẫu để tìm các Lagrange multipliers, từ đó xác định các support vectors quan trọng. Hàm mục tiêu của SVM là một bài toán tối ưu lồi, đảm bảo tìm được cực tiểu toàn cục.\n\nCác ứng dụng của SVM rất đa dạng, bao gồm phân loại văn bản, nhận dạng chữ viết tay, nhận dạng khuôn mặt, phân loại hình ảnh, phân tích sinh học, xác minh chữ ký và dự đoán chuỗi thời gian.\n\n**Mối quan hệ:**\n- Support Vector Machine hoạt động bằng cách tìm siêu phẳng tối ưu để phân tách các lớp dữ liệu.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Biên giới giữa các lớp\n\n### SVM Đa Lớp (Multi-class SVM)\n\nSVM ban đầu cho binary classification. Mở rộng cho multi-class:\n\n**1. One-vs-One (OvO):**\n- Huấn luyện $\frac{K(K-1)}{2}$ bộ phân loại nhị phân\n- Mỗi cặp lớp có một SVM\n- Dự đoán: Voting - lớp thắng nhiều nhất\n\n**Ưu điểm:**\n- Mỗi SVM đơn giản hơn (chỉ 2 lớp)\n- Training nhanh cho mỗi SVM\n- Tốt cho SVM với kernel\n\n**Nhược điểm:**\n- Nhiều mô hình khi K lớn\n- Voting có thể tie\n\n**2. One-vs-Rest (OvR / One-vs-All):**\n- Huấn luyện K bộ phân loại nhị phân\n- Mỗi SVM: Một lớp vs tất cả lớp khác\n- Dự đoán: Lớp có decision function score cao nhất\n\n**Ưu điểm:**\n- Ít mô hình hơn (K so với $\frac{K(K-1)}{2}$)\n- Đơn giản\n\n**Nhược điểm:**\n- Imbalanced training sets\n- Scores không comparable trực tiếp\n\n**3. Crammer & Singer:**\n- Giải trực tiếp multi-class SVM\n- Một bài toán tối ưu duy nhất\n- Phức tạp tính toán\n\n**Sklearn default:** OvR cho hầu hết, OvO cho `SVC`\n\n### SVM cho Hồi Quy (SVR - Support Vector Regression)\n\nThay vì tối đa hóa margin, SVR tìm một \"ống\" (tube) có độ rộng $\\epsilon$ chứa hầu hết các điểm dữ liệu.\n\n**Ý tưởng:**\n- Cho phép sai số trong khoảng $\\epsilon$\n- Penalty cho điểm ngoài ống\n- Cân bằng giữa flatness và tolerance\n\n**Bài toán tối ưu:**\n$$\\min_{w,b} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}(\\xi_i + \\xi_i^*)$$\n\n**Ràng buộc:**\n- $y_i - (w^Tx_i + b) \\leq \\epsilon + \\xi_i$\n- $(w^Tx_i + b) - y_i \\leq \\epsilon + \\xi_i^*$\n- $\\xi_i, \\xi_i^* \\geq 0$\n\n**Trong đó:**\n- $\\epsilon$ là độ rộng của ống (epsilon-insensitive tube)\n- $\\xi_i, \\xi_i^*$ là slack variables (trên và dưới)\n- Điểm trong ống: không penalty\n- Điểm ngoài ống: penalty tỷ lệ với khoảng cách\n\n**Tham số:**\n\n**1. Epsilon ($\\epsilon$):**\n- Độ rộng tube\n- $\\epsilon$ lớn: Ống rộng, ít support vectors, underfitting\n- $\\epsilon$ nhỏ: Ống hẹp, nhiều support vectors, overfitting\n- Thường: 0.01, 0.1, 0.5\n\n**2. C:**\n- Trade-off margin vs vi phạm\n- Giống như trong classification\n\n**3. Kernel:**\n- RBF, Linear, Polynomial\n- Giống như classification\n\n**Support Vectors trong SVR:**\n- Điểm nằm trên hoặc ngoài biên ống\n- Điểm trong ống không đóng góp\n\n**Ứng dụng:**\n- Time series forecasting\n- Stock price prediction\n- Weather prediction\n- Regression với outliers\n\n### Điều Chỉnh Hyperparameters\n\n**Các tham số chính:**\n\n**1. C (Regularization):**\n\n**Các khái niệm quan trọng:**\n- Support Vector Machine (SVM) là một thuật toán học có giám sát mạnh mẽ, được sử dụng rộng rãi cho cả bài toán phân loại và hồi quy (SVR). Mục tiêu chính của SVM là tìm một siêu phẳng tối ưu trong không gian nhiều chiều để phân tách các lớp dữ liệu, tối đa hóa khoảng cách (margin) giữa siêu phẳng này và các điểm dữ liệu gần nhất (được gọi là support vectors). SVM có khả năng xử lý hiệu quả dữ liệu trong không gian chiều cao, ngay cả khi số lượng chiều lớn hơn số lượng mẫu.\n\nĐể xử lý các bài toán không phân tách tuyến tính trong không gian gốc, SVM sử dụng Kernel Trick để ánh xạ dữ liệu sang một không gian đặc trưng có chiều cao hơn, nơi dữ liệu có thể phân tách tuyến tính. Điều này cho phép SVM hoạt động như một mô hình phi tuyến trong không gian gốc thông qua các hàm kernel linh hoạt. SVM có hai dạng chính là Hard Margin SVM (khi dữ liệu phân tách hoàn toàn) và Soft Margin SVM (khi có thể chấp nhận một số lỗi phân loại để đạt được biên độ lớn hơn), đồng thời tích hợp cơ chế regularization thông qua tham số C để kiểm soát overfitting.\n\nMô hình này được tối ưu hóa thông qua bài toán đối ngẫu để tìm các Lagrange multipliers, từ đó xác định các support vectors quan trọng. Hàm mục tiêu của SVM là một bài toán tối ưu lồi, đảm bảo tìm được cực tiểu toàn cục.\n\nCác ứng dụng của SVM rất đa dạng, bao gồm phân loại văn bản, nhận dạng chữ viết tay, nhận dạng khuôn mặt, phân loại hình ảnh, phân tích sinh học, xác minh chữ ký và dự đoán chuỗi thời gian.\n\n**Mối quan hệ:**\n- Support Vector Machine hoạt động bằng cách tìm siêu phẳng tối ưu để phân tách các lớp dữ liệu.\n- Support Vector Machine có thể sử dụng thuật toán Crammer & Singer để giải trực tiếp bài toán phân loại đa lớp thông qua một bài toán tối ưu duy nhất."
        },
        "output_without_context": {
            "question": "Trong thuật toán Máy Vector Hỗ trợ (SVM), siêu phẳng phân tách được xác định như thế nào?",
            "answer": "Bằng cách tối đa hóa biên độ giữa các điểm dữ liệu gần nhất của các lớp khác nhau."
        }
    },
    {
        "topic_description": "Chủ đề này tìm hiểu về SVM biên độ mềm và cách giải quyết các tình huống không thể phân tách. Kiến thức về các biến slack và tham số C sẽ được kiểm tra. Kết nối liền mạch với tuần 4 về việc nới lỏng các ràng buộc.",
        "output_with_context": {
            "question": "Trong Soft Margin SVM, tham số C điều khiển điều gì trong bài toán tối ưu hóa?",
            "answer": "C điều khiển trade-off giữa độ rộng của margin và số lượng vi phạm của các điểm dữ liệu.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n### Hard Margin SVM\n\nDành cho dữ liệu **linearly separable** (phân tách tuyến tính hoàn toàn).\n\n**Bài toán tối ưu:**\n$$\\min_{w,b} \frac{1}{2}||w||^2$$\n\n**Ràng buộc:** $y_i(w^Tx_i + b) \\geq 1, \forall i$\n\n**Giải thích:**\n- Mục tiêu: Tối thiểu hóa $||w||^2$ (tương đương tối đa hóa margin $\frac{2}{||w||}$)\n- Ràng buộc: Tất cả điểm phải được phân loại đúng\n- $y_i \\in \\{-1, +1\\}$: Nhãn lớp\n- $y_i(w^Tx_i + b) \\geq 1$: Điểm nằm đúng phía và cách siêu phẳng ít nhất 1 đơn vị\n\n**Tại sao dùng $\frac{1}{2}||w||^2$:**\n- Đạo hàm đẹp hơn (mất $\frac{1}{2}$ khi lấy đạo hàm)\n- Bài toán convex quadratic programming\n- Dễ giải với Lagrange multipliers\n\n**Hạn chế:**\n- Yêu cầu dữ liệu phân tách tuyến tính hoàn toàn\n- Không tolerant với outliers\n- Hiếm khi áp dụng trong thực tế (dữ liệu thường có noise)\n\n### Soft Margin SVM\n\nDành cho dữ liệu **không phân tách tuyến tính hoàn toàn** (có overlap).\n\n**Giới thiệu Slack Variables $\\xi_i$:**\n- Cho phép một số điểm vi phạm margin\n- $\\xi_i$ đo lường mức độ vi phạm của điểm $i$\n- $\\xi_i = 0$: Điểm được phân loại đúng, nằm ngoài margin\n- $0 < \\xi_i < 1$: Điểm nằm trong margin nhưng phân loại đúng\n- $\\xi_i \\geq 1$: Điểm bị misclassified\n\n**Bài toán tối ưu:**\n$$\\min_{w,b,\\xi} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}\\xi_i$$\n\n**Ràng buộc:**\n- $y_i(w^Tx_i + b) \\geq 1 - \\xi_i$\n- $\\xi_i \\geq 0, \forall i$\n\n**Tham số C (Regularization Parameter):**\n\nC điều khiển trade-off giữa margin rộng và số lượng vi phạm.\n\n**C lớn (C → ∞):**\n- Penalty cao cho vi phạm\n- Margin nhỏ hơn\n- Ít misclassifications\n- Low bias, high variance (overfitting)\n- Gần với hard margin SVM\n\n**C nhỏ:**\n- Penalty thấp cho vi phạm\n- Margin lớn hơn\n- Nhiều misclassifications hơn\n- High bias, low variance (underfitting)\n- Mô hình đơn giản hơn\n\n**Lựa chọn C:**\n- Cross-validation\n- Thường thử: 0.01, 0.1, 1, 10, 100\n- Phụ thuộc vào scale của dữ liệu\n\n**Diễn giải hàm mục tiêu:**\n- $\frac{1}{2}||w||^2$: Tối đa hóa margin\n- $C\\sum_{i=1}^{m}\\xi_i$: Tối thiểu hóa vi phạm\n- C cân bằng hai mục tiêu này\n\n### Kernel Trick (Mẹo Kernel)\n\nÁnh xạ dữ liệu sang không gian nhiều chiều hơn nơi nó trở nên phân tách tuyến tính.\n\n**Vấn đề:**\n\n**Các khái niệm quan trọng:**\n- Outliers (dữ liệu ngoại lai) là các điểm dữ liệu nằm xa bất thường so với phần lớn các điểm dữ liệu khác. Hard Margin SVM rất nhạy cảm với outliers vì chúng có thể ảnh hưởng đáng kể đến vị trí của siêu phẳng phân tách, làm giảm khả năng tổng quát hóa của mô hình. Soft Margin SVM được thiết kế để tolerant hơn với outliers.\n- Đây là ràng buộc cho slack variables $\\xi_i$ trong Soft Margin SVM, đảm bảo rằng giá trị của chúng luôn không âm. Điều này có nghĩa là mức độ vi phạm margin không thể là số âm, phản ánh đúng bản chất của việc đo lường mức độ vi phạm.\n- Overfitting (quá khớp) là một vấn đề trong học máy khi mô hình học quá sát với dữ liệu huấn luyện, bao gồm cả nhiễu, dẫn đến hiệu suất kém trên dữ liệu mới, chưa từng thấy. Trong Soft Margin SVM, C lớn có thể dẫn đến overfitting.\n- Đây là ràng buộc của Hard Margin SVM, đảm bảo rằng tất cả các điểm dữ liệu $x_i$ được phân loại đúng và nằm cách siêu phẳng $w^Tx + b = 0$ ít nhất 1 đơn vị. $y_i \\in \\{-1, +1\\}$ là nhãn lớp của điểm $x_i$. Nếu $y_i=1$, thì $w^Tx_i + b \\geq 1$; nếu $y_i=-1$, thì $w^Tx_i + b \\leq -1$.\n- Lagrange multipliers là một phương pháp toán học để tìm cực trị của một hàm số dưới các ràng buộc. Trong SVM, phương pháp này được sử dụng để giải bài toán tối ưu hóa có ràng buộc, chuyển bài toán gốc thành bài toán đối ngẫu (dual problem) thường dễ giải hơn, đặc biệt khi áp dụng Kernel Trick.\n- Linearly separable là thuộc tính của tập dữ liệu cho phép tìm thấy một siêu phẳng (đường thẳng trong 2D, mặt phẳng trong 3D) để phân tách hoàn toàn các lớp dữ liệu khác nhau mà không có sự chồng chéo, là yêu cầu của Hard Margin SVM. Ngược lại, non-linearly separable là thuộc tính của tập dữ liệu khi không thể tìm thấy một siêu phẳng duy nhất để phân tách hoàn toàn các lớp dữ liệu mà không có sự chồng chéo hoặc lỗi, và được xử lý bằng Soft Margin SVM và Kernel Trick.\n\n**Mối quan hệ:**\n- Kernel Trick giải quyết vấn đề dữ liệu không phân tách tuyến tính hoàn toàn bằng cách ánh xạ chúng sang không gian chiều cao hơn.\n- C nhỏ ngăn chặn overfitting bằng cách cho phép margin rộng hơn và chấp nhận nhiều vi phạm hơn, làm cho mô hình đơn giản hơn.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Biên giới giữa các lớp\n\n### SVM Đa Lớp (Multi-class SVM)\n\nSVM ban đầu cho binary classification. Mở rộng cho multi-class:\n\n**1. One-vs-One (OvO):**\n- Huấn luyện $\frac{K(K-1)}{2}$ bộ phân loại nhị phân\n- Mỗi cặp lớp có một SVM\n- Dự đoán: Voting - lớp thắng nhiều nhất\n\n**Ưu điểm:**\n- Mỗi SVM đơn giản hơn (chỉ 2 lớp)\n- Training nhanh cho mỗi SVM\n- Tốt cho SVM với kernel\n\n**Nhược điểm:**\n- Nhiều mô hình khi K lớn\n- Voting có thể tie\n\n**2. One-vs-Rest (OvR / One-vs-All):**\n- Huấn luyện K bộ phân loại nhị phân\n- Mỗi SVM: Một lớp vs tất cả lớp khác\n- Dự đoán: Lớp có decision function score cao nhất\n\n**Ưu điểm:**\n- Ít mô hình hơn (K so với $\frac{K(K-1)}{2}$)\n- Đơn giản\n\n**Nhược điểm:**\n- Imbalanced training sets\n- Scores không comparable trực tiếp\n\n**3. Crammer & Singer:**\n- Giải trực tiếp multi-class SVM\n- Một bài toán tối ưu duy nhất\n- Phức tạp tính toán\n\n**Sklearn default:** OvR cho hầu hết, OvO cho `SVC`\n\n### SVM cho Hồi Quy (SVR - Support Vector Regression)\n\nThay vì tối đa hóa margin, SVR tìm một \"ống\" (tube) có độ rộng $\\epsilon$ chứa hầu hết các điểm dữ liệu.\n\n**Ý tưởng:**\n- Cho phép sai số trong khoảng $\\epsilon$\n- Penalty cho điểm ngoài ống\n- Cân bằng giữa flatness và tolerance\n\n**Bài toán tối ưu:**\n$$\\min_{w,b} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}(\\xi_i + \\xi_i^*)$$\n\n**Ràng buộc:**\n- $y_i - (w^Tx_i + b) \\leq \\epsilon + \\xi_i$\n- $(w^Tx_i + b) - y_i \\leq \\epsilon + \\xi_i^*$\n- $\\xi_i, \\xi_i^* \\geq 0$\n\n**Trong đó:**\n- $\\epsilon$ là độ rộng của ống (epsilon-insensitive tube)\n- $\\xi_i, \\xi_i^*$ là slack variables (trên và dưới)\n- Điểm trong ống: không penalty\n- Điểm ngoài ống: penalty tỷ lệ với khoảng cách\n\n**Tham số:**\n\n**1. Epsilon ($\\epsilon$):**\n- Độ rộng tube\n- $\\epsilon$ lớn: Ống rộng, ít support vectors, underfitting\n- $\\epsilon$ nhỏ: Ống hẹp, nhiều support vectors, overfitting\n- Thường: 0.01, 0.1, 0.5\n\n**2. C:**\n- Trade-off margin vs vi phạm\n- Giống như trong classification\n\n**3. Kernel:**\n- RBF, Linear, Polynomial\n- Giống như classification\n\n**Support Vectors trong SVR:**\n- Điểm nằm trên hoặc ngoài biên ống\n- Điểm trong ống không đóng góp\n\n**Ứng dụng:**\n- Time series forecasting\n- Stock price prediction\n- Weather prediction\n- Regression với outliers\n\n### Điều Chỉnh Hyperparameters\n\n**Các tham số chính:**\n\n**1. C (Regularization):**\n\n**Các khái niệm quan trọng:**\n- $\\xi_i$ và $\\xi_i^*$ là các slack variables (biến chùng) trong Support Vector Regression (SVR). Chúng được sử dụng để đo lường mức độ mà một điểm dữ liệu vi phạm các ràng buộc của \"ống\" epsilon-insensitive. Cụ thể, $\\xi_i$ đo lường mức độ một điểm nằm trên biên trên của ống, và $\\xi_i^*$ đo lường mức độ một điểm nằm dưới biên dưới của ống. Các giá trị này phải không âm ($\\xi_i, \\xi_i^* \\geq 0$). Các điểm nằm trong ống có $\\xi_i = \\xi_i^* = 0$ và không bị phạt. Các điểm nằm ngoài ống sẽ có giá trị $\\xi_i$ hoặc $\\xi_i^*$ dương và bị phạt tỷ lệ với khoảng cách của chúng đến ống.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Character recognition\n- Signature verification\n\n**5. Face Detection:**\n- Detect faces trong images\n- Feature-based classification\n\n**6. Time Series Prediction:**\n- Stock market prediction (SVR)\n- Weather forecasting\n- Energy consumption prediction\n\n**Best Practices:**\n\n1. **Luôn scale features**\n2. **Bắt đầu với linear kernel**\n3. **Use cross-validation cho tuning**\n4. **Monitor training time**\n5. **Consider LinearSVC cho large datasets**\n6. **Check support vector ratio** (nếu >50% có thể có vấn đề)\n7. **Combine với ensemble methods** nếu cần\n\n**Directed Acyclic Graph SVM (DAGSVM):** Efficient OvO approach\n\n### SVM for Regression (SVR)\n\nInstead of maximizing margin, SVR finds a tube of width $\\epsilon$ that contains most data points.\n\n**Objective:**\n$$\\min_{w,b} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}(\\xi_i + \\xi_i^*)$$\n\nSubject to:\n- $y_i - (w^Tx_i + b) \\leq \\epsilon + \\xi_i$\n- $(w^Tx_i + b) - y_i \\leq \\epsilon + \\xi_i^*$\n- $\\xi_i, \\xi_i^* \\geq 0$\n\n### Hyperparameter Tuning\n\n**Key Parameters:**\n1. **C (Regularization):** Controls trade-off between margin and errors\n2. **Kernel type:** Choice of kernel function\n3. **Gamma (γ):** For RBF kernel, defines influence radius\n4. **Degree (d):** For polynomial kernel\n5. **Epsilon (ε):** For SVR, width of tube\n\nUse Grid Search or Randomized Search with cross-validation.\n\n### Advantages of SVM\n\n1. Effective in high-dimensional spaces\n2. Memory efficient (uses subset of training points)\n3. Versatile through different kernel functions\n4. Works well with clear margin of separation\n5. Robust to outliers (soft margin)\n\n### Disadvantages\n\n1. Computationally expensive for large datasets (O($n^2$) to O($n^3$))\n2. Sensitive to feature scaling\n3. Difficult to interpret (especially with kernels)\n4. No probabilistic interpretation (requires additional calibration)\n5. Choosing right kernel and parameters is challenging\n\n### Practical Considerations\n\n- **Feature Scaling:** Always standardize features\n- **Class Imbalance:** Adjust class weights\n- **Large Datasets:** Use SGD-based linear SVM\n- **Probability Estimates:** Use Platt scaling or isotonic regression\n\n### Applications\n\n- Text classification\n- Image recognition\n- Bioinformatics (protein classification)\n- Handwriting recognition\n- Face detection\n- Time series prediction\n\n---\n\n\n**Các khái niệm quan trọng:**\n- Robust to outliers là một đặc điểm của SVM (với soft margin), nghĩa là nó ít bị ảnh hưởng bởi các điểm dữ liệu ngoại lai. Soft margin cho phép một số điểm dữ liệu vi phạm biên độ hoặc nằm sai phía của siêu phẳng, giúp mô hình tổng quát hóa tốt hơn trên dữ liệu nhiễu.\n- Clear margin of separation là một đặc điểm của dữ liệu, nơi các lớp có thể được phân tách rõ ràng bởi một siêu phẳng với một khoảng cách đáng kể. SVM hoạt động tốt nhất khi có một biên độ phân tách rõ ràng giữa các lớp."
        },
        "output_without_context": {
            "question": "Trong SVM biên độ mềm, biến slack được sử dụng để làm gì trong bối cảnh phân tách dữ liệu không thể phân tách?",
            "answer": "Biến slack được sử dụng để cho phép một số điểm dữ liệu nằm bên trong biên phân cách."
        }
    },
    {
        "topic_description": "Chủ đề này đi sâu vào quy trình tối ưu hóa SVM và các công thức liên quan đến bài toán quy hoạch bậc hai. Sinh viên sẽ được đánh giá về khả năng áp dụng công thức và hiểu biết về các ràng buộc đặt ra trong SVM.",
        "output_with_context": {
            "question": "Trong bài toán tối ưu hóa SVM, công thức nào được sử dụng để xác định các Lagrange multipliers tối ưu trong bài toán đối ngẫu?",
            "answer": "\\max_\\alpha \\sum_{i=1}^{m}\\alpha_i - \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_jK(x_i, x_j)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- `gamma='scale'`: $\\gamma = \frac{1}{n_{features} \times Var(X)}$\n- `gamma='auto'`: $\\gamma = \frac{1}{n_{features}}$\n- Thử: 0.001, 0.01, 0.1, 1\n\n**4. Sigmoid Kernel:**\n$$K(x_i, x_j) = \tanh(\\alpha x_i^Tx_j + c)$$\n\n- Giống activation function trong neural networks\n- Không phải luôn positive semi-definite\n- Ít được sử dụng\n- Có thể không converge\n\n**So sánh các Kernel:**\n\n| Kernel | Khi nào dùng | Ưu điểm | Nhược điểm |\n|--------|-------------|---------|------------|\n| Linear | High-dim sparse data, text | Nhanh, diễn giải được | Chỉ tuyến tính |\n| Polynomial | Quan hệ polynomial | Flexible | Tốn tính toán, nhiều params |\n| RBF | Default, unknown structure | Rất flexible | Cần tune $\\gamma$, dễ overfit |\n| Sigmoid | Rare | Giống neural net | Không stable |\n\n### Formulation Đối Ngẫu (Dual Formulation)\n\nBài toán tối ưu có thể được công thức hóa dưới dạng đối ngẫu sử dụng Lagrange multipliers $\\alpha_i$.\n\n**Primal Problem:**\n$$\\min_{w,b,\\xi} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}\\xi_i$$\n\n**Dual Problem:**\n$$\\max_\\alpha \\sum_{i=1}^{m}\\alpha_i - \frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_jK(x_i, x_j)$$\n\n**Ràng buộc:**\n- $0 \\leq \\alpha_i \\leq C, \forall i$\n- $\\sum_{i=1}^{m}\\alpha_iy_i = 0$\n\n**Tại sao dùng Dual:**\n- Dễ tính toán hơn với kernel\n- Chỉ cần kernel function $K(x_i, x_j)$\n- Không cần tính $\\phi(x)$ tường minh\n- Quadratic programming problem (well-studied)\n\n**Dự Đoán:**\n$$f(x) = sign\\left(\\sum_{i=1}^{m}\\alpha_iy_iK(x_i, x) + b\right)$$\n\n**Lưu ý:**\n- Chỉ cần tính với support vectors ($\\alpha_i > 0$)\n- Hầu hết $\\alpha_i = 0$\n\n### Support Vectors\n\nCác điểm dữ liệu có $\\alpha_i > 0$ được gọi là support vectors.\n\n**Đây là các điểm quan trọng:**\n\n**1. Nằm trên margin boundary ($\\alpha_i < C$):**\n- $y_i(w^Tx_i + b) = 1$\n- Ảnh hưởng trực tiếp đến vị trí siêu phẳng\n- Điểm \"support\" (hỗ trợ) siêu phẳng\n\n**2. Bị misclassified hoặc trong margin ($\\alpha_i = C$):**\n- $y_i(w^Tx_i + b) < 1$\n- Có slack variable $\\xi_i > 0$\n\n**Đặc điểm:**\n- Chỉ support vectors đóng góp vào decision function\n- Các điểm khác không ảnh hưởng\n- Xóa non-support vectors không thay đổi mô hình\n- Thường chỉ có 10-30% điểm là support vectors\n\n**Tầm quan trọng:**\n- Giảm memory (chỉ lưu support vectors)\n- Dự đoán nhanh hơn\n- Robust to outliers (điểm xa không ảnh hưởng)\n\n**Visualize:**\n- Support vectors thường là các điểm gần decision boundary\n- Điểm khó phân loại\n\n**Các khái niệm quan trọng:**\n- Dual Problem là công thức đối ngẫu của bài toán tối ưu trong SVM, được định nghĩa là $\\max_\\alpha \\sum_{i=1}^{m}\\alpha_i - \frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_jK(x_i, x_j)$. Bài toán này được giải để tìm các Lagrange multipliers $\\alpha_i$ dưới các ràng buộc $0 \\leq \\alpha_i \\leq C$ và $\\sum_{i=1}^{m}\\alpha_iy_i = 0$. Việc giải Dual Problem thường dễ dàng hơn và cho phép sử dụng kernel trick.\n- Đây là công thức toán học để dự đoán nhãn của một điểm dữ liệu mới $x$ trong SVM. Hàm này tính tổng có trọng số của các giá trị kernel giữa $x$ và các support vectors $x_i$, nhân với các Lagrange multipliers $\\alpha_i$ và nhãn $y_i$ tương ứng, sau đó cộng với bias $b$. Hàm sign() trả về +1 hoặc -1 để phân loại điểm dữ liệu.\n- Quadratic programming problem là một loại bài toán tối ưu hóa trong đó hàm mục tiêu là một hàm bậc hai và các ràng buộc là tuyến tính. Dual Problem của SVM là một ví dụ điển hình của bài toán quy hoạch bậc hai, cho phép sử dụng các thuật toán tối ưu hóa đã được phát triển tốt để giải quyết.\n- Dual Formulation là một kỹ thuật trong SVM cho phép công thức hóa bài toán tối ưu dưới dạng đối ngẫu sử dụng Lagrange multipliers $\\alpha_i$. Kỹ thuật này giúp việc tính toán dễ dàng hơn với các hàm kernel, không yêu cầu tính toán tường minh hàm ánh xạ $\\phi(x)$, và là một bài toán quy hoạch bậc hai đã được nghiên cứu kỹ.\n\n**Mối quan hệ:**\n- Dual Problem được tính toán bằng công thức $\\max_\\alpha \\sum_{i=1}^{m}\\alpha_i - \frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_jK(x_i, x_j)$ để tìm các Lagrange multipliers tối ưu.\n- Dual Problem của SVM dẫn xuất từ một Quadratic programming problem, cho phép sử dụng các phương pháp giải quyết đã có.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Kiểm soát trade-off giữa margin và errors\n- Range: 0.01 đến 1000\n- Grid search: [0.01, 0.1, 1, 10, 100]\n\n**2. Kernel type:**\n- Lựa chọn hàm kernel\n- Try: 'linear', 'rbf', 'poly'\n- Default: 'rbf'\n\n**3. Gamma (γ) - cho RBF kernel:**\n- Định nghĩa bán kính ảnh hưởng\n- Range: 0.0001 đến 1\n- Grid search: [0.001, 0.01, 0.1, 1]\n- 'scale': $\frac{1}{n \times var(X)}$\n\n**4. Degree (d) - cho Polynomial kernel:**\n- Bậc của polynomial\n- Thường: 2, 3, 4\n- Tránh quá cao (overfitting, computational cost)\n\n**5. Epsilon (ε) - cho SVR:**\n- Độ rộng tube\n- Range: 0.01 đến 1\n- Phụ thuộc scale của target\n\n**Chiến lược Tuning:**\n\n**1. Coarse Grid Search:**\n- Tìm vùng tốt với grid thô\n- C: [0.1, 1, 10, 100]\n- gamma: [0.001, 0.01, 0.1, 1]\n\n**2. Fine Grid Search:**\n- Zoom vào vùng tốt\n- Grid mịn hơn\n\n**3. Randomized Search:**\n- Nhanh hơn grid search\n- Sample ngẫu nhiên từ distributions\n\n**4. Bayesian Optimization:**\n- Thông minh, adaptive\n- Ít iterations hơn\n\n**Tips:**\n- Luôn feature scaling trước\n- Cross-validation (5-fold hoặc 10-fold)\n- Monitor training time\n- Tune C và gamma cùng lúc (interdependent)\n\n### Ưu Điểm Của SVM\n\n**1. Hiệu quả trong không gian nhiều chiều:**\n- Hoạt động tốt khi $n_{features} > n_{samples}$\n- Phù hợp cho text, genomics, high-dimensional data\n\n**2. Tiết kiệm bộ nhớ:**\n- Chỉ lưu support vectors\n- Không cần lưu toàn bộ training data\n- Sparse representation\n\n**3. Linh hoạt với kernel functions:**\n- Nhiều kernel có sẵn\n- Có thể define custom kernel\n- Xử lý được non-linear relationships\n\n**4. Hoạt động tốt với margin rõ ràng:**\n- Khi các lớp separated tốt\n- Decision boundary rõ ràng\n\n**5. Robust to outliers (soft margin):**\n- Slack variables cho phép một số outliers\n- Không bị ảnh hưởng nhiều bởi outliers xa\n\n**6. Cơ sở toán học vững chắc:**\n- Convex optimization problem\n- Global optimum guaranteed\n- Không bị stuck ở local minima\n\n**7. Regularization tích hợp:**\n- Tham số C control overfitting\n- Không cần regularization bên ngoài\n\n### Nhược Điểm\n\n**1. Chi phí tính toán cao cho dữ liệu lớn:**\n- Training complexity: O($n^2$) đến O($n^3$)\n- Không scale tốt với >10,000 mẫu\n- Memory intensive\n\n**2. Nhạy cảm với feature scaling:**\n- **Bắt buộc** phải scaling/normalization\n- Đặc trưng có scale lớn sẽ dominate\n- Ảnh hưởng đến kernel calculations\n\n**3. Khó diễn giải (với kernels):**\n- Đặc biệt với RBF kernel\n- Không thấy được feature importance trực tiếp\n- Black box (so với linear models, trees)\n\n\n**Các khái niệm quan trọng:**\n- Convex optimization problem là một loại bài toán tối ưu trong đó hàm mục tiêu là hàm lồi và tập hợp các ràng buộc tạo thành một tập hợp lồi. Đặc điểm này đảm bảo rằng bất kỳ cực tiểu cục bộ nào tìm được cũng là cực tiểu toàn cục. SVM được xây dựng dựa trên một bài toán tối ưu lồi, đảm bảo rằng thuật toán huấn luyện sẽ luôn hội tụ về một giải pháp tối ưu toàn cục, không bị mắc kẹt ở các cực tiểu cục bộ.\n- Global optimum là điểm trong không gian tham số mà tại đó hàm mục tiêu đạt giá trị nhỏ nhất (hoặc lớn nhất) trên toàn bộ miền xác định. Trong bối cảnh SVM, việc đạt được global optimum có nghĩa là mô hình tìm thấy siêu phẳng phân tách tốt nhất có thể, đảm bảo hiệu suất tối ưu. Đặc tính của SVM là một bài toán tối ưu lồi đảm bảo rằng nó luôn tìm được global optimum.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n### Hard Margin SVM\n\nDành cho dữ liệu **linearly separable** (phân tách tuyến tính hoàn toàn).\n\n**Bài toán tối ưu:**\n$$\\min_{w,b} \frac{1}{2}||w||^2$$\n\n**Ràng buộc:** $y_i(w^Tx_i + b) \\geq 1, \forall i$\n\n**Giải thích:**\n- Mục tiêu: Tối thiểu hóa $||w||^2$ (tương đương tối đa hóa margin $\frac{2}{||w||}$)\n- Ràng buộc: Tất cả điểm phải được phân loại đúng\n- $y_i \\in \\{-1, +1\\}$: Nhãn lớp\n- $y_i(w^Tx_i + b) \\geq 1$: Điểm nằm đúng phía và cách siêu phẳng ít nhất 1 đơn vị\n\n**Tại sao dùng $\frac{1}{2}||w||^2$:**\n- Đạo hàm đẹp hơn (mất $\frac{1}{2}$ khi lấy đạo hàm)\n- Bài toán convex quadratic programming\n- Dễ giải với Lagrange multipliers\n\n**Hạn chế:**\n- Yêu cầu dữ liệu phân tách tuyến tính hoàn toàn\n- Không tolerant với outliers\n- Hiếm khi áp dụng trong thực tế (dữ liệu thường có noise)\n\n### Soft Margin SVM\n\nDành cho dữ liệu **không phân tách tuyến tính hoàn toàn** (có overlap).\n\n**Giới thiệu Slack Variables $\\xi_i$:**\n- Cho phép một số điểm vi phạm margin\n- $\\xi_i$ đo lường mức độ vi phạm của điểm $i$\n- $\\xi_i = 0$: Điểm được phân loại đúng, nằm ngoài margin\n- $0 < \\xi_i < 1$: Điểm nằm trong margin nhưng phân loại đúng\n- $\\xi_i \\geq 1$: Điểm bị misclassified\n\n**Bài toán tối ưu:**\n$$\\min_{w,b,\\xi} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}\\xi_i$$\n\n**Ràng buộc:**\n- $y_i(w^Tx_i + b) \\geq 1 - \\xi_i$\n- $\\xi_i \\geq 0, \forall i$\n\n**Tham số C (Regularization Parameter):**\n\nC điều khiển trade-off giữa margin rộng và số lượng vi phạm.\n\n**C lớn (C → ∞):**\n- Penalty cao cho vi phạm\n- Margin nhỏ hơn\n- Ít misclassifications\n- Low bias, high variance (overfitting)\n- Gần với hard margin SVM\n\n**C nhỏ:**\n- Penalty thấp cho vi phạm\n- Margin lớn hơn\n- Nhiều misclassifications hơn\n- High bias, low variance (underfitting)\n- Mô hình đơn giản hơn\n\n**Lựa chọn C:**\n- Cross-validation\n- Thường thử: 0.01, 0.1, 1, 10, 100\n- Phụ thuộc vào scale của dữ liệu\n\n**Diễn giải hàm mục tiêu:**\n- $\frac{1}{2}||w||^2$: Tối đa hóa margin\n- $C\\sum_{i=1}^{m}\\xi_i$: Tối thiểu hóa vi phạm\n- C cân bằng hai mục tiêu này\n\n### Kernel Trick (Mẹo Kernel)\n\nÁnh xạ dữ liệu sang không gian nhiều chiều hơn nơi nó trở nên phân tách tuyến tính.\n\n**Vấn đề:**\n\n**Các khái niệm quan trọng:**\n- Lagrange multipliers là một phương pháp toán học để tìm cực trị của một hàm số dưới các ràng buộc. Trong SVM, phương pháp này được sử dụng để giải bài toán tối ưu hóa có ràng buộc, chuyển bài toán gốc thành bài toán đối ngẫu (dual problem) thường dễ giải hơn, đặc biệt khi áp dụng Kernel Trick.\n- Convex quadratic programming là một loại bài toán tối ưu hóa trong đó hàm mục tiêu là một hàm bậc hai lồi và các ràng buộc là tuyến tính. Bài toán tối ưu của Hard Margin SVM là một ví dụ của convex quadratic programming, đảm bảo rằng có thể tìm thấy một nghiệm tối ưu toàn cục.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Character recognition\n- Signature verification\n\n**5. Face Detection:**\n- Detect faces trong images\n- Feature-based classification\n\n**6. Time Series Prediction:**\n- Stock market prediction (SVR)\n- Weather forecasting\n- Energy consumption prediction\n\n**Best Practices:**\n\n1. **Luôn scale features**\n2. **Bắt đầu với linear kernel**\n3. **Use cross-validation cho tuning**\n4. **Monitor training time**\n5. **Consider LinearSVC cho large datasets**\n6. **Check support vector ratio** (nếu >50% có thể có vấn đề)\n7. **Combine với ensemble methods** nếu cần\n\n**Directed Acyclic Graph SVM (DAGSVM):** Efficient OvO approach\n\n### SVM for Regression (SVR)\n\nInstead of maximizing margin, SVR finds a tube of width $\\epsilon$ that contains most data points.\n\n**Objective:**\n$$\\min_{w,b} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}(\\xi_i + \\xi_i^*)$$\n\nSubject to:\n- $y_i - (w^Tx_i + b) \\leq \\epsilon + \\xi_i$\n- $(w^Tx_i + b) - y_i \\leq \\epsilon + \\xi_i^*$\n- $\\xi_i, \\xi_i^* \\geq 0$\n\n### Hyperparameter Tuning\n\n**Key Parameters:**\n1. **C (Regularization):** Controls trade-off between margin and errors\n2. **Kernel type:** Choice of kernel function\n3. **Gamma (γ):** For RBF kernel, defines influence radius\n4. **Degree (d):** For polynomial kernel\n5. **Epsilon (ε):** For SVR, width of tube\n\nUse Grid Search or Randomized Search with cross-validation.\n\n### Advantages of SVM\n\n1. Effective in high-dimensional spaces\n2. Memory efficient (uses subset of training points)\n3. Versatile through different kernel functions\n4. Works well with clear margin of separation\n5. Robust to outliers (soft margin)\n\n### Disadvantages\n\n1. Computationally expensive for large datasets (O($n^2$) to O($n^3$))\n2. Sensitive to feature scaling\n3. Difficult to interpret (especially with kernels)\n4. No probabilistic interpretation (requires additional calibration)\n5. Choosing right kernel and parameters is challenging\n\n### Practical Considerations\n\n- **Feature Scaling:** Always standardize features\n- **Class Imbalance:** Adjust class weights\n- **Large Datasets:** Use SGD-based linear SVM\n- **Probability Estimates:** Use Platt scaling or isotonic regression\n\n### Applications\n\n- Text classification\n- Image recognition\n- Bioinformatics (protein classification)\n- Handwriting recognition\n- Face detection\n- Time series prediction\n\n---\n\n\n**Các khái niệm quan trọng:**\n- Computationally expensive là một nhược điểm của SVM, đặc biệt đối với các tập dữ liệu lớn. Độ phức tạp tính toán của SVM thường là O($n^2$) đến O($n^3$), trong đó $n$ là số lượng điểm dữ liệu, làm cho việc huấn luyện trở nên chậm chạp khi $n$ lớn."
        },
        "output_without_context": {
            "question": "Trong bài toán tối ưu hóa SVM, điều gì là cần thiết để đảm bảo rằng các ràng buộc không bị vi phạm trong quy hoạch bậc hai?",
            "answer": "Các ràng buộc phải được xác định rõ ràng và đảm bảo rằng mọi điểm dữ liệu đều nằm ở phía đúng của siêu phẳng."
        }
    },
    {
        "topic_description": "Chủ đề này giới thiệu các hàm kernel và tác dụng của chúng trong việc phân loại dữ liệu phi tuyến tính. Kiến thức về các hàm kernel khác nhau sẽ được kiểm tra. Kết nối với tuần 4 qua việc chuyển đổi không gian đặc trưng.",
        "output_with_context": {
            "question": "Khi nào nên sử dụng Polynomial kernel trong SVM và bậc nào thường được khuyến nghị để tránh overfitting?",
            "answer": "Sử dụng Polynomial kernel khi có quan hệ đa thức giữa các đặc trưng, với bậc thường là 2 hoặc 3.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Kiểm soát trade-off giữa margin và errors\n- Range: 0.01 đến 1000\n- Grid search: [0.01, 0.1, 1, 10, 100]\n\n**2. Kernel type:**\n- Lựa chọn hàm kernel\n- Try: 'linear', 'rbf', 'poly'\n- Default: 'rbf'\n\n**3. Gamma (γ) - cho RBF kernel:**\n- Định nghĩa bán kính ảnh hưởng\n- Range: 0.0001 đến 1\n- Grid search: [0.001, 0.01, 0.1, 1]\n- 'scale': $\frac{1}{n \times var(X)}$\n\n**4. Degree (d) - cho Polynomial kernel:**\n- Bậc của polynomial\n- Thường: 2, 3, 4\n- Tránh quá cao (overfitting, computational cost)\n\n**5. Epsilon (ε) - cho SVR:**\n- Độ rộng tube\n- Range: 0.01 đến 1\n- Phụ thuộc scale của target\n\n**Chiến lược Tuning:**\n\n**1. Coarse Grid Search:**\n- Tìm vùng tốt với grid thô\n- C: [0.1, 1, 10, 100]\n- gamma: [0.001, 0.01, 0.1, 1]\n\n**2. Fine Grid Search:**\n- Zoom vào vùng tốt\n- Grid mịn hơn\n\n**3. Randomized Search:**\n- Nhanh hơn grid search\n- Sample ngẫu nhiên từ distributions\n\n**4. Bayesian Optimization:**\n- Thông minh, adaptive\n- Ít iterations hơn\n\n**Tips:**\n- Luôn feature scaling trước\n- Cross-validation (5-fold hoặc 10-fold)\n- Monitor training time\n- Tune C và gamma cùng lúc (interdependent)\n\n### Ưu Điểm Của SVM\n\n**1. Hiệu quả trong không gian nhiều chiều:**\n- Hoạt động tốt khi $n_{features} > n_{samples}$\n- Phù hợp cho text, genomics, high-dimensional data\n\n**2. Tiết kiệm bộ nhớ:**\n- Chỉ lưu support vectors\n- Không cần lưu toàn bộ training data\n- Sparse representation\n\n**3. Linh hoạt với kernel functions:**\n- Nhiều kernel có sẵn\n- Có thể define custom kernel\n- Xử lý được non-linear relationships\n\n**4. Hoạt động tốt với margin rõ ràng:**\n- Khi các lớp separated tốt\n- Decision boundary rõ ràng\n\n**5. Robust to outliers (soft margin):**\n- Slack variables cho phép một số outliers\n- Không bị ảnh hưởng nhiều bởi outliers xa\n\n**6. Cơ sở toán học vững chắc:**\n- Convex optimization problem\n- Global optimum guaranteed\n- Không bị stuck ở local minima\n\n**7. Regularization tích hợp:**\n- Tham số C control overfitting\n- Không cần regularization bên ngoài\n\n### Nhược Điểm\n\n**1. Chi phí tính toán cao cho dữ liệu lớn:**\n- Training complexity: O($n^2$) đến O($n^3$)\n- Không scale tốt với >10,000 mẫu\n- Memory intensive\n\n**2. Nhạy cảm với feature scaling:**\n- **Bắt buộc** phải scaling/normalization\n- Đặc trưng có scale lớn sẽ dominate\n- Ảnh hưởng đến kernel calculations\n\n**3. Khó diễn giải (với kernels):**\n- Đặc biệt với RBF kernel\n- Không thấy được feature importance trực tiếp\n- Black box (so với linear models, trees)\n\n\n**Các khái niệm quan trọng:**\n- Kernel là một thành phần quan trọng trong SVM và SVR, là các hàm toán học được sử dụng để biến đổi (ánh xạ) dữ liệu từ không gian đầu vào ban đầu sang một không gian đặc trưng có chiều cao hơn. Trong không gian mới này, dữ liệu có thể phân tách tuyến tính (hoặc hồi quy tuyến tính), cho phép SVM/SVR xử lý hiệu quả các mối quan hệ phi tuyến tính trong dữ liệu. Việc sử dụng kernel giúp tránh việc tính toán rõ ràng các tọa độ trong không gian chiều cao hơn, thay vào đó chỉ cần tính toán tích vô hướng trong không gian đó. Các loại kernel phổ biến bao gồm RBF (Radial Basis Function), Linear, Polynomial và Sigmoid.\n\n**Mối quan hệ:**\n- Support Vector Machine sử dụng kernel functions để xử lý các mối quan hệ phi tuyến tính.\n- Kernel type xác định loại kernel functions được sử dụng trong SVM.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Character recognition\n- Signature verification\n\n**5. Face Detection:**\n- Detect faces trong images\n- Feature-based classification\n\n**6. Time Series Prediction:**\n- Stock market prediction (SVR)\n- Weather forecasting\n- Energy consumption prediction\n\n**Best Practices:**\n\n1. **Luôn scale features**\n2. **Bắt đầu với linear kernel**\n3. **Use cross-validation cho tuning**\n4. **Monitor training time**\n5. **Consider LinearSVC cho large datasets**\n6. **Check support vector ratio** (nếu >50% có thể có vấn đề)\n7. **Combine với ensemble methods** nếu cần\n\n**Directed Acyclic Graph SVM (DAGSVM):** Efficient OvO approach\n\n### SVM for Regression (SVR)\n\nInstead of maximizing margin, SVR finds a tube of width $\\epsilon$ that contains most data points.\n\n**Objective:**\n$$\\min_{w,b} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}(\\xi_i + \\xi_i^*)$$\n\nSubject to:\n- $y_i - (w^Tx_i + b) \\leq \\epsilon + \\xi_i$\n- $(w^Tx_i + b) - y_i \\leq \\epsilon + \\xi_i^*$\n- $\\xi_i, \\xi_i^* \\geq 0$\n\n### Hyperparameter Tuning\n\n**Key Parameters:**\n1. **C (Regularization):** Controls trade-off between margin and errors\n2. **Kernel type:** Choice of kernel function\n3. **Gamma (γ):** For RBF kernel, defines influence radius\n4. **Degree (d):** For polynomial kernel\n5. **Epsilon (ε):** For SVR, width of tube\n\nUse Grid Search or Randomized Search with cross-validation.\n\n### Advantages of SVM\n\n1. Effective in high-dimensional spaces\n2. Memory efficient (uses subset of training points)\n3. Versatile through different kernel functions\n4. Works well with clear margin of separation\n5. Robust to outliers (soft margin)\n\n### Disadvantages\n\n1. Computationally expensive for large datasets (O($n^2$) to O($n^3$))\n2. Sensitive to feature scaling\n3. Difficult to interpret (especially with kernels)\n4. No probabilistic interpretation (requires additional calibration)\n5. Choosing right kernel and parameters is challenging\n\n### Practical Considerations\n\n- **Feature Scaling:** Always standardize features\n- **Class Imbalance:** Adjust class weights\n- **Large Datasets:** Use SGD-based linear SVM\n- **Probability Estimates:** Use Platt scaling or isotonic regression\n\n### Applications\n\n- Text classification\n- Image recognition\n- Bioinformatics (protein classification)\n- Handwriting recognition\n- Face detection\n- Time series prediction\n\n---\n\n\n**Các khái niệm quan trọng:**\n- Kernel là một thành phần quan trọng trong SVM và SVR, là các hàm toán học được sử dụng để biến đổi (ánh xạ) dữ liệu từ không gian đầu vào ban đầu sang một không gian đặc trưng có chiều cao hơn. Trong không gian mới này, dữ liệu có thể phân tách tuyến tính (hoặc hồi quy tuyến tính), cho phép SVM/SVR xử lý hiệu quả các mối quan hệ phi tuyến tính trong dữ liệu. Việc sử dụng kernel giúp tránh việc tính toán rõ ràng các tọa độ trong không gian chiều cao hơn, thay vào đó chỉ cần tính toán tích vô hướng trong không gian đó. Các loại kernel phổ biến bao gồm RBF (Radial Basis Function), Linear, Polynomial và Sigmoid.\n\n**Mối quan hệ:**\n- Support Vector Machine sử dụng kernel functions để xử lý các mối quan hệ phi tuyến tính.\n- Kernel type xác định loại kernel functions được sử dụng trong SVM.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Nhiều bài toán không phân tách tuyến tính trong không gian gốc\n- Ví dụ: XOR problem, dữ liệu phân bố hình tròn\n\n**Giải pháp:**\n- Ánh xạ dữ liệu sang không gian đặc trưng (feature space) nhiều chiều hơn\n- Trong không gian mới, dữ liệu có thể phân tách tuyến tính\n- SVM tuyến tính trong không gian mới = SVM phi tuyến trong không gian gốc\n\n**Hàm Kernel:**\n$$K(x_i, x_j) = \\phi(x_i)^T\\phi(x_j)$$\n\nTrong đó:\n- $\\phi(x)$ là hàm ánh xạ (không cần tính tường minh)\n- $K(x_i, x_j)$ tính trực tiếp inner product trong không gian đặc trưng\n\n**Ưu điểm Kernel Trick:**\n- Không cần tính $\\phi(x)$ tường minh\n- Không cần lưu trữ trong không gian nhiều chiều\n- Chỉ cần tính $K(x_i, x_j)$\n- Hiệu quả tính toán\n\n**Các Kernel Phổ Biến:**\n\n**1. Linear Kernel (Kernel Tuyến Tính):**\n$$K(x_i, x_j) = x_i^Tx_j$$\n\n- Không ánh xạ, chỉ là inner product thông thường\n- Sử dụng khi dữ liệu đã phân tách tuyến tính\n- Nhanh nhất\n- Tốt cho high-dimensional sparse data (text)\n- Dễ diễn giải\n\n**Khi nào dùng:**\n- Số đặc trưng >> số mẫu\n- Dữ liệu văn bản\n- Cần tốc độ và interpretability\n\n**2. Polynomial Kernel (Kernel Đa Thức):**\n$$K(x_i, x_j) = (x_i^Tx_j + c)^d$$\n\nTrong đó:\n- $d$ là degree (bậc) của polynomial\n- $c$ là constant (thường = 0 hoặc 1)\n\n**Đặc điểm:**\n- Tạo đặc trưng polynomial\n- $d=2$: Bao gồm tương tác cặp\n- $d$ lớn: Flexibility cao nhưng dễ overfit\n- Có thể bùng nổ tính toán với $d$ lớn\n\n**Khi nào dùng:**\n- Quan hệ polynomial giữa features\n- Image processing\n- $d=2$ hoặc $d=3$ thường đủ\n\n**3. RBF Kernel (Radial Basis Function / Gaussian Kernel):**\n$$K(x_i, x_j) = \\exp(-\\gamma||x_i - x_j||^2)$$\n\nTrong đó:\n- $\\gamma$ (gamma) kiểm soát ảnh hưởng của single training example\n- $\\gamma = \frac{1}{2\\sigma^2}$ trong phân phối Gaussian\n\n**$\\gamma$ nhỏ:**\n- Ảnh hưởng lan rộng\n- Decision boundary mượt\n- Low variance, high bias (underfitting)\n\n**$\\gamma$ lớn:**\n- Ảnh hưởng hẹp\n- Decision boundary phức tạp\n- High variance, low bias (overfitting)\n\n**Đặc điểm:**\n- Phổ biến nhất\n- Ánh xạ sang không gian vô hạn chiều\n- Linh hoạt, xử lý được nhiều dạng data\n- Giá trị trong [0, 1]\n\n**Khi nào dùng:**\n- Default choice khi không biết kernel nào\n- Dữ liệu không có structure rõ ràng\n- Hiệu quả với nhiều loại bài toán\n\n**Lựa chọn $\\gamma$:**\n- Cross-validation\n\n**Các khái niệm quan trọng:**\n- Polynomial Kernel là một hàm kernel đa thức có công thức $K(x_i, x_j) = (x_i^Tx_j + c)^d$. Trong đó, $d$ là bậc của đa thức và $c$ là một hằng số (thường là 0 hoặc 1). Kernel này tạo ra các đặc trưng đa thức, cho phép mô hình học các mối quan hệ phi tuyến. Bậc $d$ càng lớn, mô hình càng linh hoạt nhưng càng dễ overfit. Polynomial Kernel thường được dùng khi có quan hệ đa thức giữa các đặc trưng, ví dụ trong xử lý ảnh, với $d=2$ hoặc $d=3$ là phổ biến.\n- Linear Kernel là một hàm kernel tuyến tính có công thức $K(x_i, x_j) = x_i^Tx_j$. Nó không thực hiện ánh xạ dữ liệu sang không gian đặc trưng mới mà chỉ tính tích vô hướng thông thường trong không gian gốc. Linear Kernel phù hợp khi dữ liệu đã phân tách tuyến tính, nhanh nhất và dễ diễn giải, đặc biệt hiệu quả với dữ liệu thưa có số đặc trưng lớn (ví dụ: dữ liệu văn bản).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n**4. Không có xác suất trực tiếp:**\n- Decision function cho distance, không phải probability\n- Cần calibration (Platt scaling, isotonic regression)\n- `predict_proba()` chậm hơn và ít reliable\n\n**5. Khó chọn kernel và parameters:**\n- Nhiều choices: kernel type, C, gamma, ...\n- Cần extensive tuning\n- Cross-validation tốn thời gian\n\n**6. Không xử lý trực tiếp missing values:**\n- Cần imputation trước\n- Không như tree-based methods\n\n**7. Không cung cấp feature importance:**\n- Khác với trees, linear models\n- Cần phương pháp bên ngoài (permutation importance)\n\n### Cân Nhắc Thực Tế\n\n**1. Feature Scaling - BẮT BUỘC:**\n\n**Tại sao:**\n- SVM dựa trên khoảng cách (Euclidean distance trong kernel)\n- Đặc trưng có scale lớn sẽ dominate\n- Ảnh hưởng đến optimization\n\n**Phương pháp:**\n- StandardScaler: $(x - \\mu) / \\sigma$\n- MinMaxScaler: Scale về [0, 1] hoặc [-1, 1]\n- RobustScaler: Dùng median, robust to outliers\n\n**Lưu ý:**\n- Scale trên training set\n- Apply cùng transformation cho test set\n- Không scale lại test set độc lập\n\n**2. Class Imbalance:**\n\n**Giải pháp:**\n- `class_weight='balanced'`: Tự động điều chỉnh\n- `class_weight={0: w0, 1: w1}`: Custom weights\n- SMOTE trước khi train\n- Adjust decision threshold\n\n**3. Large Datasets:**\n\n**Vấn đề:**\n- Standard SVM không scale tốt\n- Training chậm với >10,000 mẫu\n\n**Giải pháp:**\n- **LinearSVC:** SGD-based, linear SVM nhanh hơn\n- **SGDClassifier với loss='hinge':** Online learning\n- Sample subset cho training\n- Approximate methods\n\n**4. Probability Estimates:**\n\n**Vấn đề:**\n- SVM cho decision values, không phải probabilities\n- Cần probabilities cho many applications\n\n**Giải pháp:**\n- Platt Scaling: Fit sigmoid function\n- Isotonic Regression: Non-parametric\n- `probability=True` trong `SVC` (chậm hơn)\n\n**Lưu ý:**\n- Probabilities không chính xác như Logistic Regression\n- Dùng cross-validation để calibrate\n\n**5. Multi-class:**\n- Sklearn tự động xử lý\n- `decision_function_shape='ovr'` hoặc `'ovo'`\n- Chọn dựa trên yêu cầu\n\n**6. Kernel Selection:**\n\n**Quy trình:**\n1. Bắt đầu với Linear kernel (nhanh, baseline)\n2. Nếu không tốt, thử RBF\n3. Nếu biết structure, thử Polynomial\n4. So sánh bằng cross-validation\n\n**7. Early Stopping:**\n- Không có early stopping trong standard SVM\n- Training đến khi converge\n- Có thể set `max_iter` để limit\n\n### Ứng Dụng\n\n**1. Text Classification:**\n- Spam detection\n- Sentiment analysis\n- Document categorization\n- Topic labeling\n- Language detection\n\n**Tại sao tốt:**\n- High-dimensional sparse data\n- Linear kernel hiệu quả\n- Good generalization\n\n**2. Image Recognition:**\n- Face detection và recognition\n- Object classification\n- Handwriting recognition (MNIST)\n- Medical image analysis\n\n**Tại sao tốt:**\n- RBF kernel bắt được patterns\n- Robust to variations\n\n**3. Bioinformatics:**\n- Protein classification\n- Gene classification\n- Cancer classification từ gene expression\n- Drug discovery\n\n**Tại sao tốt:**\n- High-dimensional data\n- Small sample size\n- Good with complex patterns\n\n**4. Handwriting Recognition:**\n- Digit recognition\n\n**Các khái niệm quan trọng:**\n- Polynomial kernel là một loại hàm kernel trong SVM, được định nghĩa là $K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d$, trong đó $d$ là bậc của đa thức và $r$ là một hằng số. Nó cho phép SVM tạo ra các ranh giới quyết định phi tuyến tính bằng cách sử dụng các tổ hợp đa thức của các đặc trưng. Polynomial kernel hữu ích khi có kiến thức về cấu trúc đa thức trong dữ liệu.\n- Linear kernel là một loại hàm kernel trong SVM, được định nghĩa là $K(x_i, x_j) = x_i^T x_j$. Nó tạo ra một ranh giới quyết định tuyến tính hoặc một siêu phẳng phân tách tuyến tính trong không gian đặc trưng ban đầu. Linear kernel nhanh chóng, hiệu quả và thường được sử dụng làm điểm khởi đầu (baseline) hoặc lựa chọn tốt để bắt đầu. Nó rất hiệu quả cho dữ liệu có chiều cao và thưa thớt như trong phân loại văn bản, cũng như cho các tập dữ liệu lớn (khi sử dụng LinearSVC).\n\n**Mối quan hệ:**\n- Support Vector Machine sử dụng Polynomial kernel khi có kiến thức về cấu trúc đa thức trong dữ liệu.\n- Support Vector Machine sử dụng Linear kernel làm điểm khởi đầu và cho dữ liệu có chiều cao, thưa thớt."
        },
        "output_without_context": {
            "question": "Trong Support Vector Machine (SVM), hàm kernel nào được sử dụng để biến đổi không gian đặc trưng nhằm phân loại dữ liệu phi tuyến tính?",
            "answer": "Hàm kernel phi tuyến (như RBF kernel)"
        }
    },
    {
        "topic_description": "Chủ đề này tìm hiểu về cách mở rộng mô hình SVM cho phân loại đa lớp. Các phương pháp One-vs-Rest và One-vs-One sẽ được thảo luận. Câu hỏi sẽ tập trung vào việc áp dụng các khái niệm này và so sánh các phương pháp liên quan.",
        "output_with_context": {
            "question": "Trong phân loại đa lớp với SVM, phương pháp nào huấn luyện K bộ phân loại nhị phân, mỗi bộ phân loại phân biệt một lớp cụ thể với tất cả các lớp còn lại?",
            "answer": "One-vs-Rest (OvR)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n**4. Không có xác suất trực tiếp:**\n- Decision function cho distance, không phải probability\n- Cần calibration (Platt scaling, isotonic regression)\n- `predict_proba()` chậm hơn và ít reliable\n\n**5. Khó chọn kernel và parameters:**\n- Nhiều choices: kernel type, C, gamma, ...\n- Cần extensive tuning\n- Cross-validation tốn thời gian\n\n**6. Không xử lý trực tiếp missing values:**\n- Cần imputation trước\n- Không như tree-based methods\n\n**7. Không cung cấp feature importance:**\n- Khác với trees, linear models\n- Cần phương pháp bên ngoài (permutation importance)\n\n### Cân Nhắc Thực Tế\n\n**1. Feature Scaling - BẮT BUỘC:**\n\n**Tại sao:**\n- SVM dựa trên khoảng cách (Euclidean distance trong kernel)\n- Đặc trưng có scale lớn sẽ dominate\n- Ảnh hưởng đến optimization\n\n**Phương pháp:**\n- StandardScaler: $(x - \\mu) / \\sigma$\n- MinMaxScaler: Scale về [0, 1] hoặc [-1, 1]\n- RobustScaler: Dùng median, robust to outliers\n\n**Lưu ý:**\n- Scale trên training set\n- Apply cùng transformation cho test set\n- Không scale lại test set độc lập\n\n**2. Class Imbalance:**\n\n**Giải pháp:**\n- `class_weight='balanced'`: Tự động điều chỉnh\n- `class_weight={0: w0, 1: w1}`: Custom weights\n- SMOTE trước khi train\n- Adjust decision threshold\n\n**3. Large Datasets:**\n\n**Vấn đề:**\n- Standard SVM không scale tốt\n- Training chậm với >10,000 mẫu\n\n**Giải pháp:**\n- **LinearSVC:** SGD-based, linear SVM nhanh hơn\n- **SGDClassifier với loss='hinge':** Online learning\n- Sample subset cho training\n- Approximate methods\n\n**4. Probability Estimates:**\n\n**Vấn đề:**\n- SVM cho decision values, không phải probabilities\n- Cần probabilities cho many applications\n\n**Giải pháp:**\n- Platt Scaling: Fit sigmoid function\n- Isotonic Regression: Non-parametric\n- `probability=True` trong `SVC` (chậm hơn)\n\n**Lưu ý:**\n- Probabilities không chính xác như Logistic Regression\n- Dùng cross-validation để calibrate\n\n**5. Multi-class:**\n- Sklearn tự động xử lý\n- `decision_function_shape='ovr'` hoặc `'ovo'`\n- Chọn dựa trên yêu cầu\n\n**6. Kernel Selection:**\n\n**Quy trình:**\n1. Bắt đầu với Linear kernel (nhanh, baseline)\n2. Nếu không tốt, thử RBF\n3. Nếu biết structure, thử Polynomial\n4. So sánh bằng cross-validation\n\n**7. Early Stopping:**\n- Không có early stopping trong standard SVM\n- Training đến khi converge\n- Có thể set `max_iter` để limit\n\n### Ứng Dụng\n\n**1. Text Classification:**\n- Spam detection\n- Sentiment analysis\n- Document categorization\n- Topic labeling\n- Language detection\n\n**Tại sao tốt:**\n- High-dimensional sparse data\n- Linear kernel hiệu quả\n- Good generalization\n\n**2. Image Recognition:**\n- Face detection và recognition\n- Object classification\n- Handwriting recognition (MNIST)\n- Medical image analysis\n\n**Tại sao tốt:**\n- RBF kernel bắt được patterns\n- Robust to variations\n\n**3. Bioinformatics:**\n- Protein classification\n- Gene classification\n- Cancer classification từ gene expression\n- Drug discovery\n\n**Tại sao tốt:**\n- High-dimensional data\n- Small sample size\n- Good with complex patterns\n\n**4. Handwriting Recognition:**\n- Digit recognition\n\n**Các khái niệm quan trọng:**\n- Multi-class là một loại bài toán phân loại trong đó có nhiều hơn hai lớp đầu ra. Sklearn tự động xử lý các bài toán multi-class cho SVM bằng cách sử dụng các chiến lược như \"One-vs-Rest\" (OvR) hoặc \"One-vs-One\" (OvO). Người dùng có thể chọn chiến lược phù hợp thông qua tham số `decision_function_shape` trong `SVC`.\n\n**Mối quan hệ:**\n- Support Vector Machine xử lý các bài toán Multi-class bằng cách sử dụng các chiến lược như \"One-vs-Rest\" hoặc \"One-vs-One\".\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Biên giới giữa các lớp\n\n### SVM Đa Lớp (Multi-class SVM)\n\nSVM ban đầu cho binary classification. Mở rộng cho multi-class:\n\n**1. One-vs-One (OvO):**\n- Huấn luyện $\frac{K(K-1)}{2}$ bộ phân loại nhị phân\n- Mỗi cặp lớp có một SVM\n- Dự đoán: Voting - lớp thắng nhiều nhất\n\n**Ưu điểm:**\n- Mỗi SVM đơn giản hơn (chỉ 2 lớp)\n- Training nhanh cho mỗi SVM\n- Tốt cho SVM với kernel\n\n**Nhược điểm:**\n- Nhiều mô hình khi K lớn\n- Voting có thể tie\n\n**2. One-vs-Rest (OvR / One-vs-All):**\n- Huấn luyện K bộ phân loại nhị phân\n- Mỗi SVM: Một lớp vs tất cả lớp khác\n- Dự đoán: Lớp có decision function score cao nhất\n\n**Ưu điểm:**\n- Ít mô hình hơn (K so với $\frac{K(K-1)}{2}$)\n- Đơn giản\n\n**Nhược điểm:**\n- Imbalanced training sets\n- Scores không comparable trực tiếp\n\n**3. Crammer & Singer:**\n- Giải trực tiếp multi-class SVM\n- Một bài toán tối ưu duy nhất\n- Phức tạp tính toán\n\n**Sklearn default:** OvR cho hầu hết, OvO cho `SVC`\n\n### SVM cho Hồi Quy (SVR - Support Vector Regression)\n\nThay vì tối đa hóa margin, SVR tìm một \"ống\" (tube) có độ rộng $\\epsilon$ chứa hầu hết các điểm dữ liệu.\n\n**Ý tưởng:**\n- Cho phép sai số trong khoảng $\\epsilon$\n- Penalty cho điểm ngoài ống\n- Cân bằng giữa flatness và tolerance\n\n**Bài toán tối ưu:**\n$$\\min_{w,b} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}(\\xi_i + \\xi_i^*)$$\n\n**Ràng buộc:**\n- $y_i - (w^Tx_i + b) \\leq \\epsilon + \\xi_i$\n- $(w^Tx_i + b) - y_i \\leq \\epsilon + \\xi_i^*$\n- $\\xi_i, \\xi_i^* \\geq 0$\n\n**Trong đó:**\n- $\\epsilon$ là độ rộng của ống (epsilon-insensitive tube)\n- $\\xi_i, \\xi_i^*$ là slack variables (trên và dưới)\n- Điểm trong ống: không penalty\n- Điểm ngoài ống: penalty tỷ lệ với khoảng cách\n\n**Tham số:**\n\n**1. Epsilon ($\\epsilon$):**\n- Độ rộng tube\n- $\\epsilon$ lớn: Ống rộng, ít support vectors, underfitting\n- $\\epsilon$ nhỏ: Ống hẹp, nhiều support vectors, overfitting\n- Thường: 0.01, 0.1, 0.5\n\n**2. C:**\n- Trade-off margin vs vi phạm\n- Giống như trong classification\n\n**3. Kernel:**\n- RBF, Linear, Polynomial\n- Giống như classification\n\n**Support Vectors trong SVR:**\n- Điểm nằm trên hoặc ngoài biên ống\n- Điểm trong ống không đóng góp\n\n**Ứng dụng:**\n- Time series forecasting\n- Stock price prediction\n- Weather prediction\n- Regression với outliers\n\n### Điều Chỉnh Hyperparameters\n\n**Các tham số chính:**\n\n**1. C (Regularization):**\n\n**Các khái niệm quan trọng:**\n- SVC (Support Vector Classification) là một triển khai của Support Vector Machine cho bài toán phân loại. Trong thư viện Sklearn, SVC mặc định sử dụng chiến lược One-vs-One (OvO) để xử lý các bài toán phân loại đa lớp.\n- One-vs-Rest (OvR), còn gọi là One-vs-All (OvA), và One-vs-One (OvO) là hai kỹ thuật chính để mở rộng các bộ phân loại nhị phân (như SVM) sang bài toán phân loại đa lớp.\n\n**One-vs-Rest (OvR / One-vs-All - OvA)**\nKỹ thuật này huấn luyện K bộ phân loại nhị phân độc lập, trong đó mỗi bộ phân loại được huấn luyện để phân biệt một lớp cụ thể với tất cả các lớp còn lại. Khi dự đoán, mô hình chọn lớp có xác suất, điểm tự tin, hoặc điểm hàm quyết định (decision function score) cao nhất từ các bộ phân loại. OvR đơn giản, hiệu quả với số lượng lớp K lớn và cần ít mô hình hơn so với OvO. Tuy nhiên, nó có thể gặp vấn đề mất cân bằng lớp (imbalanced training sets) và các điểm số từ các bộ phân loại có thể không so sánh trực tiếp được.\n\n**One-vs-One (OvO)**\nKỹ thuật này huấn luyện K(K-1)/2 bộ phân loại nhị phân, mỗi bộ phân loại được huấn luyện để phân biệt một cặp lớp cụ thể. Dự đoán được thực hiện bằng cách sử dụng cơ chế bỏ phiếu (voting), trong đó lớp nhận được nhiều phiếu nhất từ các bộ phân loại con sẽ là kết quả cuối cùng. OvO đặc biệt tốt cho các mô hình như SVM với kernel vì mỗi SVM đơn giản hơn và training nhanh. Nó hiệu quả với số lượng lớp K nhỏ hoặc trung bình. Nhược điểm là cần nhiều mô hình khi số lớp K lớn và có thể xảy ra trường hợp bỏ phiếu hòa (tie).\n\n**Mối quan hệ:**\n- SVC (Support Vector Classification) trong Sklearn sử dụng chiến lược One-vs-One (OvO) cho các trường hợp cụ thể, như khi sử dụng kernel.\n- SVC (Support Vector Classification) trong Sklearn mặc định sử dụng chiến lược One-vs-Rest (OvR) cho hầu hết các trường hợp phân loại đa lớp.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Máy Vector Hỗ Trợ (Support Vector Machine - SVM)\n- Character recognition\n- Signature verification\n\n**5. Face Detection:**\n- Detect faces trong images\n- Feature-based classification\n\n**6. Time Series Prediction:**\n- Stock market prediction (SVR)\n- Weather forecasting\n- Energy consumption prediction\n\n**Best Practices:**\n\n1. **Luôn scale features**\n2. **Bắt đầu với linear kernel**\n3. **Use cross-validation cho tuning**\n4. **Monitor training time**\n5. **Consider LinearSVC cho large datasets**\n6. **Check support vector ratio** (nếu >50% có thể có vấn đề)\n7. **Combine với ensemble methods** nếu cần\n\n**Directed Acyclic Graph SVM (DAGSVM):** Efficient OvO approach\n\n### SVM for Regression (SVR)\n\nInstead of maximizing margin, SVR finds a tube of width $\\epsilon$ that contains most data points.\n\n**Objective:**\n$$\\min_{w,b} \frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}(\\xi_i + \\xi_i^*)$$\n\nSubject to:\n- $y_i - (w^Tx_i + b) \\leq \\epsilon + \\xi_i$\n- $(w^Tx_i + b) - y_i \\leq \\epsilon + \\xi_i^*$\n- $\\xi_i, \\xi_i^* \\geq 0$\n\n### Hyperparameter Tuning\n\n**Key Parameters:**\n1. **C (Regularization):** Controls trade-off between margin and errors\n2. **Kernel type:** Choice of kernel function\n3. **Gamma (γ):** For RBF kernel, defines influence radius\n4. **Degree (d):** For polynomial kernel\n5. **Epsilon (ε):** For SVR, width of tube\n\nUse Grid Search or Randomized Search with cross-validation.\n\n### Advantages of SVM\n\n1. Effective in high-dimensional spaces\n2. Memory efficient (uses subset of training points)\n3. Versatile through different kernel functions\n4. Works well with clear margin of separation\n5. Robust to outliers (soft margin)\n\n### Disadvantages\n\n1. Computationally expensive for large datasets (O($n^2$) to O($n^3$))\n2. Sensitive to feature scaling\n3. Difficult to interpret (especially with kernels)\n4. No probabilistic interpretation (requires additional calibration)\n5. Choosing right kernel and parameters is challenging\n\n### Practical Considerations\n\n- **Feature Scaling:** Always standardize features\n- **Class Imbalance:** Adjust class weights\n- **Large Datasets:** Use SGD-based linear SVM\n- **Probability Estimates:** Use Platt scaling or isotonic regression\n\n### Applications\n\n- Text classification\n- Image recognition\n- Bioinformatics (protein classification)\n- Handwriting recognition\n- Face detection\n- Time series prediction\n\n---\n\n\n**Các khái niệm quan trọng:**\n- Directed Acyclic Graph SVM (DAGSVM) là một phương pháp tiếp cận hiệu quả cho bài toán phân loại đa lớp sử dụng chiến lược One-vs-One (OvO). Thay vì huấn luyện N(N-1)/2 bộ phân loại nhị phân độc lập, DAGSVM tổ chức các bộ phân loại thành một đồ thị không chu trình có hướng để đưa ra quyết định phân loại nhanh hơn."
        },
        "output_without_context": {
            "question": "Trong phân loại đa lớp với SVM, phương pháp nào so sánh từng lớp với tất cả các lớp còn lại?",
            "answer": "One-vs-Rest"
        }
    }
]