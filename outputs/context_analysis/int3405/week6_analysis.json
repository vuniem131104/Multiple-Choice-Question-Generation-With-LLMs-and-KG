[
    {
        "topic_description": "Kiểm tra sự hiểu biết của sinh viên về định nghĩa và mục đích của phân tích cụm trong máy học. Nội dung này sẽ yêu cầu sinh viên mô tả quá trình gộp nhóm các đối tượng và các ứng dụng thực tiễn của nó như phân đoạn khách hàng hay phát hiện bất thường.",
        "output_with_context": {
            "question": "Phân tích cụm trong máy học có mục đích chính là gì?",
            "answer": "Nhóm các đối tượng tương tự lại với nhau để tìm ra cấu trúc trong dữ liệu.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Lựa Chọn Đặc Trưng & Tối Ưu Hóa Mô Hình\n- Precision, Recall, F1, AUC\n- Business metrics\n\n**6. Avoid Data Leakage:**\n- **Proper CV:** Fit preprocessors trên train folds only\n- **Time-based splits:** cho time series\n- **No target leakage:** Features không chứa info về target\n- **Test set untouched:** Cho đến cuối\n\n**7. Document Everything:**\n- Experiments log\n- Model versions\n- Hyperparameters\n- Results và insights\n\n**8. Reproducibility:**\n- Set random seeds\n- Version control code\n- Save data versions\n- Document environment\n- Use containers (Docker)\n\n**9. Model Versioning:**\n- MLflow, DVC\n- Track models\n- Compare versions\n- Rollback nếu cần\n\n**10. Validation Strategy:**\n- Robust CV\n- Hold-out test set\n- Temporal validation cho time series\n\n**11. Feature Engineering First:**\n- \"Data > Algorithms\"\n- Good features > Complex models\n- Domain knowledge valuable\n\n**12. Monitor Training:**\n- Training vs validation\n- Learning curves\n- Early signs of overfitting\n\n**13. Consider Production:**\n- Inference time\n- Model size\n- Dependencies\n- Maintenance\n- Explainability\n\n**14. Test on Real Data:**\n- Not just metrics\n- Qualitative analysis\n- Edge cases\n- Failure modes\n\n---\n\n---\n\n## Học Không Giám Sát (Unsupervised Learning)\n\n### Giới Thiệu Về Học Không Giám Sát\n\nHọc không giám sát khám phá các mẫu ẩn trong dữ liệu không có nhãn mà không cần biến mục tiêu tường minh. Nó được sử dụng cho phân tích dữ liệu khám phá, nhận dạng mẫu và nén dữ liệu.\n\n**Đặc điểm chính:**\n- Không có labels (y)\n- Chỉ có features (X)\n- Tìm structure trong data\n- Exploratory analysis\n\n**So với Supervised Learning:**\n| Tiêu chí | Supervised | Unsupervised |\n|----------|-----------|--------------|\n| Labels | Có | Không |\n| Mục tiêu | Dự đoán | Khám phá |\n| Feedback | Có (accuracy) | Không rõ ràng |\n| Ứng dụng | Classification, Regression | Clustering, Dimensionality Reduction |\n\n**Các tác vụ chính:**\n1. **Clustering:** Nhóm dữ liệu tương tự\n2. **Dimensionality Reduction:** Giảm số chiều\n3. **Anomaly Detection:** Phát hiện bất thường\n4. **Association Rule Learning:** Tìm mối quan hệ\n\n**Thách thức:**\n- Không có ground truth để đánh giá\n- Khó xác định số clusters/components\n- Kết quả có thể subjective\n- Cần domain knowledge để interpret\n\n### Clustering (Phân Cụm)\n\nNhóm các điểm dữ liệu tương tự lại với nhau.\n\n**Mục tiêu:**\n- High intra-cluster similarity (trong cùng cluster)\n- Low inter-cluster similarity (giữa các clusters)\n\n**Ứng dụng:**\n- Customer segmentation\n- Document clustering\n- Image segmentation\n- Anomaly detection\n- Data compression\n\n### K-Means Clustering\n\nThuật toán phân cụm phổ biến nhất, chia dữ liệu thành K clusters.\n\n**Thuật toán:**\n\n**Bước 1: Initialization**\n- Chọn K centroids ngẫu nhiên\n- Có thể từ data points hoặc random positions\n\n**Bước 2: Assignment**\n- Gán mỗi điểm đến centroid gần nhất\n- Sử dụng Euclidean distance:\n$$d(x, \\mu_k) = ||x - \\mu_k|| = \\sqrt{\\sum_{j=1}^{n}(x_j - \\mu_{kj})^2}$$\n\n**Bước 3: Update**\n- Cập nhật centroids = mean của các điểm assigned\n$$\\mu_k = \frac{1}{|C_k|}\\sum_{x \\in C_k}x$$\n\n\n**Các khái niệm quan trọng:**\n- Document clustering là ứng dụng của clustering để nhóm các tài liệu văn bản tương tự lại với nhau dựa trên nội dung của chúng, thường được sử dụng để tổ chức thông tin, tìm kiếm và tóm tắt tài liệu.\n- Customer segmentation là ứng dụng của clustering để phân chia khách hàng thành các nhóm dựa trên hành vi, nhân khẩu học hoặc các đặc điểm khác, giúp doanh nghiệp cá nhân hóa chiến lược marketing và dịch vụ.\n- Document clustering là một ứng dụng của học không giám sát, nhóm các tài liệu tương tự nhau vào cùng một cụm dựa trên nội dung của chúng, giúp tổ chức và tìm kiếm thông tin hiệu quả hơn.\n- Clustering (Phân cụm) là một tác vụ trong học không giám sát nhằm nhóm các điểm dữ liệu tương tự lại với nhau. Mục tiêu là đạt được sự tương đồng cao trong cùng một cụm (high intra-cluster similarity) và sự khác biệt thấp giữa các cụm (low inter-cluster similarity).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n$$\\gamma_{ik} = \frac{\\pi_k\\mathcal{N}(x_i|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(x_i|\\mu_j, \\Sigma_j)}$$\n\n- $\\gamma_{ik}$: Probability điểm $i$ belongs to cluster $k$\n- $\\sum_{k=1}^{K}\\gamma_{ik} = 1$ for each $i$\n\n**M-Step (Maximization):**\nUpdate parameters dựa trên responsibilities:\n\n**Mixing coefficients:**\n$$\\pi_k = \frac{1}{N}\\sum_{i=1}^{N}\\gamma_{ik}$$\n\n**Means:**\n$$\\mu_k = \frac{\\sum_{i=1}^{N}\\gamma_{ik}x_i}{\\sum_{i=1}^{N}\\gamma_{ik}}$$\n\n**Covariances:**\n$$\\Sigma_k = \frac{\\sum_{i=1}^{N}\\gamma_{ik}(x_i-\\mu_k)(x_i-\\mu_k)^T}{\\sum_{i=1}^{N}\\gamma_{ik}}$$\n\n**Repeat E-Step và M-Step** cho đến convergence.\n\n**Convergence:**\n- Log-likelihood không thay đổi nhiều\n- Parameters stable\n- Đạt max iterations\n\n**Ưu Điểm:**\n\n**1. Soft clustering:**\n- Probability of membership cho mỗi cluster\n- Captures uncertainty\n- More nuanced than hard assignment\n\n**2. Flexible cluster shapes:**\n- Elliptical clusters\n- Different sizes\n- Different orientations\n\n**3. Probabilistic framework:**\n- Sound mathematical foundation\n- Can compute likelihoods\n- Model selection với BIC/AIC\n\n**4. Generative model:**\n- Có thể generate new samples\n- Understand data distribution\n\n**Nhược Điểm:**\n\n**1. Cần specify K:**\n- Không tự động determine\n- Use BIC/AIC để chọn\n\n**2. Sensitive to initialization:**\n- EM có thể converge to local optima\n- Run multiple times\n\n**3. Assumes Gaussian:**\n- Không phù hợp nếu data không Gaussian\n- Limited to elliptical shapes\n\n**4. Computationally expensive:**\n- Covariance matrix inversion\n- Slower than K-Means\n\n**5. Singular covariance matrices:**\n- Có thể xảy ra với small clusters\n- Need regularization\n\n**So Sánh K-Means vs GMM:**\n\n| Tiêu chí | K-Means | GMM |\n|----------|---------|-----|\n| Assignment | Hard | Soft (probabilistic) |\n| Cluster shape | Spherical | Elliptical |\n| Parameters | Centroids | Means + Covariances |\n| Speed | Nhanh | Chậm hơn |\n| Flexibility | Ít | Nhiều |\n| Probabilistic | Không | Có |\n\n**Chọn K:**\n- Bayesian Information Criterion (BIC)\n- Akaike Information Criterion (AIC)\n- Cross-validation\n- Silhouette score\n\n### Các Thuật Toán Clustering Khác\n\n**1. Mean Shift:**\n\n**Nguyên lý:**\n- Density-based, no need chỉ định số clusters\n- Shift points toward mode (density maxima)\n\n**Thuật toán:**\n1. Khởi tạo window around each point\n2. Tính mean của points trong window\n3. Shift center đến mean\n4. Repeat until convergence\n5. Points converging to same mode = same cluster\n\n**Ưu điểm:**\n- Không cần specify K\n- Arbitrary shapes\n- Automatic K\n\n**Nhược điểm:**\n- Slow (O(N²))\n- Bandwidth parameter critical\n\n**2. Spectral Clustering:**\n\n**Nguyên lý:**\n- Uses graph theory\n- Eigenvalues của similarity matrix\n- Good cho non-convex clusters\n\n**Steps:**\n1. Construct similarity graph\n2. Compute Laplacian matrix\n3. Eigenvalue decomposition\n4. K-Means trên eigenvectors\n\n**Ưu điểm:**\n- Handle complex shapes\n- Thường tốt hơn K-Means\n- Based on graph cuts\n\n**Nhược điểm:**\n- Computationally expensive\n- Cần tune similarity function\n- Sensitive to parameters\n\n\n**Các khái niệm quan trọng:**\n- Học Không Giám Sát là một nhánh của Học Máy, nơi các mô hình hoặc thuật toán học từ dữ liệu không được gán nhãn. Mục tiêu chính là tìm kiếm các cấu trúc, mẫu hoặc nhóm ẩn trong dữ liệu. Các kỹ thuật phổ biến bao gồm phân cụm, giảm chiều dữ liệu và phân tích liên kết.\n\n**Mối quan hệ:**\n- Học Không Giám Sát chứa Mean Shift như một thuật toán phân cụm dựa trên mật độ.\n- Học Không Giám Sát được áp dụng trong Customer Segmentation để nhóm khách hàng dựa trên hành vi của họ.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n   - Merge chúng thành 1 cluster\n3. Until: Chỉ còn 1 cluster\n\n**Steps chi tiết:**\n- Initialize: N clusters\n- Iteration 1: N-1 clusters\n- Iteration 2: N-2 clusters\n- ...\n- Final: 1 cluster\n\n**2. Divisive (Top-Down - Phân Chia):**\n\n**Thuật toán:**\n1. Start: Tất cả điểm trong 1 cluster\n2. Repeat:\n   - Chọn cluster để split\n   - Chia thành 2 sub-clusters\n3. Until: Mỗi điểm là 1 cluster\n\n**Ít phổ biến:** Computationally expensive hơn\n\n**Linkage Methods (Cách Đo Khoảng Cách Giữa Clusters):**\n\n**1. Single Linkage (Minimum):**\n$$d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoảng cách giữa 2 điểm gần nhất\n- Tạo long, chain-like clusters\n- Sensitive to noise và outliers\n\n**2. Complete Linkage (Maximum):**\n$$d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoảng cách giữa 2 điểm xa nhất\n- Tạo compact, spherical clusters\n- Ít sensitive to outliers\n\n**3. Average Linkage:**\n$$d(C_i, C_j) = \frac{1}{|C_i||C_j|}\\sum_{x \\in C_i}\\sum_{y \\in C_j}d(x,y)$$\n- Trung bình tất cả pairwise distances\n- Balance giữa single và complete\n- Phổ biến choice\n\n**4. Ward's Method:**\n- Minimize within-cluster variance sau khi merge\n- Maximize between-cluster variance\n- Tạo balanced, compact clusters\n- Thường cho kết quả tốt nhất\n- Phổ biến nhất trong thực tế\n\n**Dendrogram (Biểu Đồ Cây):**\n\nTree diagram showing cluster hierarchy.\n\n**Đọc Dendrogram:**\n- Vertical axis: Distance/dissimilarity\n- Horizontal axis: Samples\n- Height của merge: Distance giữa clusters\n- Càng cao merge càng dissimilar\n\n**Cutting Dendrogram:**\n- Vẽ horizontal line\n- Number of intersections = Number of clusters\n- Height của cut = dissimilarity threshold\n\n**Ưu Điểm:**\n- Không cần specify K trước\n- Dendrogram provides insights\n- Flexible - có thể chọn K sau\n- Deterministic (no randomness)\n\n**Nhược Điểm:**\n- Computationally expensive: O(N²log N) or O(N³)\n- Không scale với large datasets\n- Một khi merge không thể undo\n- Memory intensive\n\n**Khi Nào Dùng:**\n- Small-medium datasets (< 10,000)\n- Cần understand hierarchy\n- Không biết K optimal\n- Exploratory analysis\n\n### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nNhóm các điểm có mật độ cao, robust to outliers và arbitrary shapes.\n\n**Tham Số:**\n\n**1. ε (epsilon):**\n- Maximum distance giữa 2 điểm để được coi là neighbors\n- Định nghĩa neighborhood radius\n- Quá nhỏ: Nhiều noise points\n- Quá lớn: Merge nhiều clusters\n\n**2. MinPts (Minimum Points):**\n- Minimum số điểm trong ε-neighborhood để là core point\n- Thường: 4, 5, hoặc 2×dim\n- Larger MinPts: Ít core points, stricter\n\n**Các Loại Điểm:**\n\n**1. Core Point:**\n- Có ≥ MinPts điểm khác trong ε-neighborhood (bao gồm cả chính nó)\n- Trung tâm của clusters\n- Can form clusters\n\n**2. Border Point:**\n- Nằm trong ε-neighborhood của core point\n- Có < MinPts neighbors\n- Thuộc cluster nhưng không core\n- Ở biên của cluster\n\n\n**Các khái niệm quan trọng:**\n- Học Không Giám Sát là một nhánh của Học Máy, nơi các mô hình hoặc thuật toán học từ dữ liệu không được gán nhãn. Mục tiêu chính là tìm kiếm các cấu trúc, mẫu hoặc nhóm ẩn trong dữ liệu. Các kỹ thuật phổ biến bao gồm phân cụm, giảm chiều dữ liệu và phân tích liên kết.\n\n**Mối quan hệ:**\n- Học Không Giám Sát chứa Mean Shift như một thuật toán phân cụm dựa trên mật độ.\n- Học Không Giám Sát được áp dụng trong Customer Segmentation để nhóm khách hàng dựa trên hành vi của họ.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n5. Repeat until no more frequent itemsets\n6. Generate rules từ frequent itemsets\n\n**Ưu điểm:**\n- Guaranteed to find all frequent itemsets\n- Pruning reduces candidates\n\n**Nhược điểm:**\n- Multiple database scans\n- Expensive với large datasets\n- Chậm với low support threshold\n\n**2. FP-Growth (Frequent Pattern Growth):**\n\n**Nguyên lý:**\n- Compress database into FP-tree\n- Mine directly từ tree\n- Chỉ 2 database scans\n\n**Steps:**\n1. Scan database, find frequent items\n2. Build FP-tree (compressed representation)\n3. Mine frequent patterns từ FP-tree\n\n**Ưu điểm:**\n- Nhanh hơn Apriori nhiều\n- Không generate candidates\n- Memory efficient (tree compression)\n\n**Nhược điểm:**\n- Complex implementation\n- FP-tree construction overhead\n\n**Thresholds:**\n- **min_support:** 0.01 - 0.1 (1% - 10%)\n- **min_confidence:** 0.5 - 0.8 (50% - 80%)\n- **min_lift:** > 1\n\n**Ứng dụng:**\n\n**1. Retail:**\n- Product recommendations\n- Store layout optimization\n- Cross-selling strategies\n\n**2. Web Mining:**\n- Clickstream analysis\n- Page recommendation\n- User behavior patterns\n\n**3. Healthcare:**\n- Symptom-disease associations\n- Drug interactions\n- Treatment effectiveness\n\n**4. Telecommunications:**\n- Calling patterns\n- Service bundles\n- Churn prediction\n\n### Đánh Giá Clustering\n\nLàm thế nào đánh giá quality của clustering khi không có ground truth?\n\n**Internal Metrics (Không Cần Ground Truth):**\n\n**1. Silhouette Coefficient:**\n$$s = \frac{b - a}{\\max(a,b)}$$\n\n- Range: [-1, 1]\n- 1: Perfect\n- 0: Overlapping clusters\n- -1: Wrong assignment\n\n**2. Davies-Bouldin Index:**\n$$DB = \frac{1}{K}\\sum_{i=1}^{K}\\max_{j \neq i}\\left(\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)}\right)$$\n\n- Lower is better\n- Ratio of within-cluster to between-cluster distances\n\n**3. Calinski-Harabasz Index (Variance Ratio Criterion):**\n$$CH = \frac{SS_B/(K-1)}{SS_W/(N-K)}$$\n\n- Higher is better\n- Ratio of between-cluster to within-cluster variance\n\n**4. Within-Cluster Sum of Squares (WCSS):**\n- Used trong elbow method\n- Lower is better\n- Decreases với more clusters\n\n**External Metrics (Có Ground Truth):**\n\n**1. Adjusted Rand Index (ARI):**\n- Measure agreement between two clusterings\n- Adjusted for chance\n- Range: [-1, 1], 1 = perfect match\n\n**2. Normalized Mutual Information (NMI):**\n- Information theoretic measure\n- Range: [0, 1], 1 = perfect match\n- Không bị ảnh hưởng bởi số clusters\n\n**3. Fowlkes-Mallows Index:**\n- Geometric mean của precision và recall\n- Range: [0, 1]\n\n**4. Purity:**\n$$Purity = \frac{1}{N}\\sum_{k=1}^{K}\\max_j|C_k \\cap T_j|$$\n\n- Simple, intuitive\n- Increases với more clusters (biased)\n\n**Chọn Metrics:**\n- **No ground truth:** Silhouette, Davies-Bouldin\n- **Have ground truth:** ARI, NMI\n- **Exploratory:** Multiple metrics\n- **Business context:** Domain-specific metrics\n\n### Ứng Dụng Của Unsupervised Learning\n\n**1. Customer Segmentation:**\n- Nhóm customers theo behavior\n- Targeted marketing\n- Personalization\n- Churn prediction\n\n**2. Image Compression:**\n- K-Means cho color quantization\n- Reduce number of colors\n- Smaller file size\n\n**3. Anomaly Detection:**\n- Fraud detection (credit cards, insurance)\n- Network intrusion detection\n- Manufacturing quality control\n- Medical diagnosis (rare diseases)\n\n**4. Recommender Systems:**\n- Collaborative filtering\n- User clustering\n- Item clustering\n\n**Các khái niệm quan trọng:**\n- Học Không Giám Sát là một nhánh của Học Máy, nơi các mô hình hoặc thuật toán học từ dữ liệu không được gán nhãn. Mục tiêu chính là tìm kiếm các cấu trúc, mẫu hoặc nhóm ẩn trong dữ liệu. Các kỹ thuật phổ biến bao gồm phân cụm, giảm chiều dữ liệu và phân tích liên kết.\n- Customer Segmentation là một kỹ thuật trong Học Không Giám Sát, sử dụng các thuật toán phân cụm để nhóm khách hàng dựa trên hành vi, đặc điểm hoặc sở thích của họ. Kỹ thuật này giúp các doanh nghiệp thực hiện marketing mục tiêu, cá nhân hóa trải nghiệm và dự đoán churn (khách hàng rời bỏ).\n\n**Mối quan hệ:**\n- Học Không Giám Sát được áp dụng trong Customer Segmentation để nhóm khách hàng dựa trên hành vi của họ."
        },
        "output_without_context": {
            "question": "Phân tích cụm trong máy học được định nghĩa là gì?",
            "answer": "Quá trình gộp nhóm các đối tượng dựa trên sự tương đồng của chúng."
        }
    },
    {
        "topic_description": "Đánh giá khả năng của sinh viên trong việc phân tích các yếu tố có thể ảnh hưởng đến kết quả của phân tích cụm. Sinh viên sẽ cần hiểu các thuộc tính của dữ liệu, hình dạng cụm và vai trò của thuật toán trong phân cụm.",
        "output_with_context": {
            "question": "Trong phân tích cụm, yếu tố nào không cần thiết phải chỉ định trước khi thực hiện thuật toán Mean Shift?",
            "answer": "Số lượng cụm K.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**3. Affinity Propagation:**\n\n**Nguyên lý:**\n- Message passing between data points\n- Points \"vote\" on their exemplars\n- Automatically determines K\n\n**Messages:**\n- **Responsibility:** Điểm $i$ chọn điểm $k$ làm exemplar\n- **Availability:** Điểm $k$ available làm exemplar cho $i$\n\n**Ưu điểm:**\n- Không cần specify K\n- Flexible\n- Finds exemplars (representative points)\n\n**Nhược điểm:**\n- Slow (O(N²T), T = iterations)\n- Memory intensive\n- Sensitive to preferences\n\n**4. OPTICS (Ordering Points To Identify Clustering Structure):**\n\n**Nguyên lý:**\n- Extension của DBSCAN\n- Tạo ordering của points\n- Handle varying densities better\n\n**Output:**\n- Reachability plot\n- Extract clusters ở different density levels\n\n**Ưu điểm:**\n- Varying densities\n- Không cần ε cụ thể\n- Hierarchical view\n\n**Nhược điểm:**\n- Complex interpretation\n- Still need MinPts\n\n**So Sánh Các Thuật Toán:**\n\n| Algorithm | K needed | Shape | Outliers | Speed | Best for |\n|-----------|----------|-------|----------|-------|----------|\n| K-Means | Yes | Spherical | Sensitive | Fast | Large, simple |\n| Hierarchical | No | Any | Sensitive | Slow | Small, hierarchy |\n| DBSCAN | No | Arbitrary | Robust | Medium | Spatial, noise |\n| GMM | Yes | Elliptical | Sensitive | Slow | Probabilistic |\n| Mean Shift | No | Arbitrary | Robust | Slow | Non-uniform |\n| Spectral | Yes | Complex | Sensitive | Slow | Graph-like |\n\n### Giảm Số Chiều (Dimensionality Reduction)\n\nĐã được cover chi tiết trong phần Feature Selection & Model Optimization, đây là summary.\n\n### Principal Component Analysis (PCA)\n\n**Nguyên lý:**\n- Linear transformation sang orthogonal components\n- Components ordered by variance\n- Maximize variance retained\n\n**Công thức:**\n$$Z = XW_k$$\n\nTrong đó $W_k$ là matrix của k eigenvectors.\n\n**Steps:**\n1. Standardize data: $X' = \frac{X - \\mu}{\\sigma}$\n2. Covariance matrix: $\\Sigma = \frac{1}{n}X'^TX'$\n3. Eigendecomposition: $\\Sigma = V\\Lambda V^T$\n4. Chọn top k eigenvectors\n5. Transform: $Z = X'W_k$\n\n**Variance Explained:**\n$$\frac{\\lambda_k}{\\sum_{i=1}^{n}\\lambda_i} \times 100\\%$$\n\n**Chọn số components:**\n- Cumulative variance ≥ 95% hoặc 99%\n- Scree plot (elbow)\n- Kaiser criterion (eigenvalue > 1)\n\n**Ứng dụng:**\n- Visualization (2D/3D)\n- Noise reduction\n- Feature extraction\n- Speed up learning\n- Preprocessing\n\n### Singular Value Decomposition (SVD)\n\n**Matrix factorization:**\n$$X = U\\Sigma V^T$$\n\n- $U$: Left singular vectors (m × m)\n- $\\Sigma$: Singular values (m × n, diagonal)\n- $V^T$: Right singular vectors (n × n)\n\n**Quan hệ với PCA:**\n- PCA eigenvectors = right singular vectors\n- PCA eigenvalues = squared singular values\n\n**Ứng dụng:**\n- PCA computation\n- Latent Semantic Analysis (LSA)\n- Recommender systems (matrix completion)\n- Image compression\n- Data compression\n\n**Truncated SVD:**\n- Giữ top k components\n- Approximation: $X \\approx U_k\\Sigma_kV_k^T$\n\n### Independent Component Analysis (ICA)\n\n**Nguyên lý:**\n- Tách signal thành independent components\n- Maximize statistical independence\n- Non-Gaussian components\n\n**Model:**\n$$X = AS$$\n\nTrong đó:\n- $X$: Observed signals (mixed)\n- $A$: Mixing matrix (unknown)\n- $S$: Source signals (independent, unknown)\n\n**Mục tiêu:** Estimate $A$ và $S$ from $X$\n\n**Cocktail Party Problem:**\n- Multiple people talking simultaneously\n\n**Các khái niệm quan trọng:**\n- Mean Shift là một thuật toán phân cụm học không giám sát, dựa trên mật độ, không yêu cầu chỉ định số lượng cụm K trước. Thuật toán này hoạt động bằng cách dịch chuyển các điểm dữ liệu về phía các điểm cực đại mật độ (mode) trong không gian dữ liệu; các điểm hội tụ về cùng một mode được coi là thuộc cùng một cụm. Mean Shift có khả năng tìm các cụm có hình dạng tùy ý, tự động xác định số lượng cụm và mạnh mẽ với nhiễu (outliers). Tuy nhiên, nó có tốc độ chậm và phù hợp cho các phân bố dữ liệu không đồng nhất.\n- Spectral Clustering là một thuật toán phân cụm học không giám sát sử dụng lý thuyết đồ thị để phân cụm dữ liệu. Thuật toán này hoạt động bằng cách xây dựng một đồ thị tương tự (similarity graph) từ dữ liệu, sau đó sử dụng các giá trị riêng (eigenvalues) và vector riêng (eigenvectors) của ma trận Laplacian của đồ thị để giảm chiều dữ liệu. Sau khi giảm chiều, thuật toán thực hiện K-Means trên không gian giảm chiều đó. Spectral Clustering yêu cầu chỉ định số lượng cụm K trước và rất hiệu quả trong việc tìm các cụm có hình dạng phức tạp, phi lồi (non-convex) hoặc có cấu trúc đồ thị. Tuy nhiên, nó nhạy cảm với các ngoại lệ và có tốc độ chậm.\n\n**Mối quan hệ:**\n- Học Không Giám Sát chứa Spectral Clustering như một thuật toán phân cụm sử dụng lý thuyết đồ thị.\n- Mean Shift là một thuật toán phân cụm không cần chỉ định số lượng cụm K trước, mà tự động xác định K dựa trên mật độ dữ liệu.\n- Học Không Giám Sát chứa Mean Shift như một thuật toán phân cụm dựa trên mật độ.\n- Spectral Clustering tính toán Laplacian matrix từ Similarity graph để phân tích cấu trúc đồ thị và tìm các vector riêng.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n$$\\gamma_{ik} = \frac{\\pi_k\\mathcal{N}(x_i|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(x_i|\\mu_j, \\Sigma_j)}$$\n\n- $\\gamma_{ik}$: Probability điểm $i$ belongs to cluster $k$\n- $\\sum_{k=1}^{K}\\gamma_{ik} = 1$ for each $i$\n\n**M-Step (Maximization):**\nUpdate parameters dựa trên responsibilities:\n\n**Mixing coefficients:**\n$$\\pi_k = \frac{1}{N}\\sum_{i=1}^{N}\\gamma_{ik}$$\n\n**Means:**\n$$\\mu_k = \frac{\\sum_{i=1}^{N}\\gamma_{ik}x_i}{\\sum_{i=1}^{N}\\gamma_{ik}}$$\n\n**Covariances:**\n$$\\Sigma_k = \frac{\\sum_{i=1}^{N}\\gamma_{ik}(x_i-\\mu_k)(x_i-\\mu_k)^T}{\\sum_{i=1}^{N}\\gamma_{ik}}$$\n\n**Repeat E-Step và M-Step** cho đến convergence.\n\n**Convergence:**\n- Log-likelihood không thay đổi nhiều\n- Parameters stable\n- Đạt max iterations\n\n**Ưu Điểm:**\n\n**1. Soft clustering:**\n- Probability of membership cho mỗi cluster\n- Captures uncertainty\n- More nuanced than hard assignment\n\n**2. Flexible cluster shapes:**\n- Elliptical clusters\n- Different sizes\n- Different orientations\n\n**3. Probabilistic framework:**\n- Sound mathematical foundation\n- Can compute likelihoods\n- Model selection với BIC/AIC\n\n**4. Generative model:**\n- Có thể generate new samples\n- Understand data distribution\n\n**Nhược Điểm:**\n\n**1. Cần specify K:**\n- Không tự động determine\n- Use BIC/AIC để chọn\n\n**2. Sensitive to initialization:**\n- EM có thể converge to local optima\n- Run multiple times\n\n**3. Assumes Gaussian:**\n- Không phù hợp nếu data không Gaussian\n- Limited to elliptical shapes\n\n**4. Computationally expensive:**\n- Covariance matrix inversion\n- Slower than K-Means\n\n**5. Singular covariance matrices:**\n- Có thể xảy ra với small clusters\n- Need regularization\n\n**So Sánh K-Means vs GMM:**\n\n| Tiêu chí | K-Means | GMM |\n|----------|---------|-----|\n| Assignment | Hard | Soft (probabilistic) |\n| Cluster shape | Spherical | Elliptical |\n| Parameters | Centroids | Means + Covariances |\n| Speed | Nhanh | Chậm hơn |\n| Flexibility | Ít | Nhiều |\n| Probabilistic | Không | Có |\n\n**Chọn K:**\n- Bayesian Information Criterion (BIC)\n- Akaike Information Criterion (AIC)\n- Cross-validation\n- Silhouette score\n\n### Các Thuật Toán Clustering Khác\n\n**1. Mean Shift:**\n\n**Nguyên lý:**\n- Density-based, no need chỉ định số clusters\n- Shift points toward mode (density maxima)\n\n**Thuật toán:**\n1. Khởi tạo window around each point\n2. Tính mean của points trong window\n3. Shift center đến mean\n4. Repeat until convergence\n5. Points converging to same mode = same cluster\n\n**Ưu điểm:**\n- Không cần specify K\n- Arbitrary shapes\n- Automatic K\n\n**Nhược điểm:**\n- Slow (O(N²))\n- Bandwidth parameter critical\n\n**2. Spectral Clustering:**\n\n**Nguyên lý:**\n- Uses graph theory\n- Eigenvalues của similarity matrix\n- Good cho non-convex clusters\n\n**Steps:**\n1. Construct similarity graph\n2. Compute Laplacian matrix\n3. Eigenvalue decomposition\n4. K-Means trên eigenvectors\n\n**Ưu điểm:**\n- Handle complex shapes\n- Thường tốt hơn K-Means\n- Based on graph cuts\n\n**Nhược điểm:**\n- Computationally expensive\n- Cần tune similarity function\n- Sensitive to parameters\n\n\n**Các khái niệm quan trọng:**\n- Mean Shift là một thuật toán phân cụm học không giám sát, dựa trên mật độ, không yêu cầu chỉ định số lượng cụm K trước. Thuật toán này hoạt động bằng cách dịch chuyển các điểm dữ liệu về phía các điểm cực đại mật độ (mode) trong không gian dữ liệu; các điểm hội tụ về cùng một mode được coi là thuộc cùng một cụm. Mean Shift có khả năng tìm các cụm có hình dạng tùy ý, tự động xác định số lượng cụm và mạnh mẽ với nhiễu (outliers). Tuy nhiên, nó có tốc độ chậm và phù hợp cho các phân bố dữ liệu không đồng nhất.\n- Spectral Clustering là một thuật toán phân cụm học không giám sát sử dụng lý thuyết đồ thị để phân cụm dữ liệu. Thuật toán này hoạt động bằng cách xây dựng một đồ thị tương tự (similarity graph) từ dữ liệu, sau đó sử dụng các giá trị riêng (eigenvalues) và vector riêng (eigenvectors) của ma trận Laplacian của đồ thị để giảm chiều dữ liệu. Sau khi giảm chiều, thuật toán thực hiện K-Means trên không gian giảm chiều đó. Spectral Clustering yêu cầu chỉ định số lượng cụm K trước và rất hiệu quả trong việc tìm các cụm có hình dạng phức tạp, phi lồi (non-convex) hoặc có cấu trúc đồ thị. Tuy nhiên, nó nhạy cảm với các ngoại lệ và có tốc độ chậm.\n\n**Mối quan hệ:**\n- Mean Shift là một thuật toán phân cụm không cần chỉ định số lượng cụm K trước, mà tự động xác định K dựa trên mật độ dữ liệu.\n- Spectral Clustering tính toán Laplacian matrix từ Similarity graph để phân tích cấu trúc đồ thị và tìm các vector riêng.\n- Học Không Giám Sát chứa Spectral Clustering như một thuật toán phân cụm sử dụng lý thuyết đồ thị.\n- Học Không Giám Sát chứa Mean Shift như một thuật toán phân cụm dựa trên mật độ.\n- Bandwidth parameter kiểm soát hành vi của thuật toán Mean Shift, ảnh hưởng đến kích thước cửa sổ và do đó là số lượng và hình dạng của các cụm được tìm thấy.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**Bước 4: Repeat**\n- Lặp lại Steps 2-3 cho đến khi convergence\n\n**Convergence khi:**\n- Centroids không đổi\n- Assignments không đổi\n- Đạt max iterations\n\n**Objective Function (WCSS - Within-Cluster Sum of Squares):**\n$$J = \\sum_{k=1}^{K}\\sum_{x \\in C_k}||x - \\mu_k||^2$$\n\nMục tiêu: Minimize J\n\n**Chọn K (Số Clusters):**\n\n**1. Elbow Method:**\n- Vẽ WCSS vs K\n- Tìm \"khuỷu tay\" (elbow) - điểm mà WCSS giảm chậm lại\n- Tradeoff giữa số clusters và fit\n\n**Ví dụ:**\n```\nWCSS\n  |  \\\n  |    \\\n  |      \\___\n  |          ----___\n  +----------------->\n  1  2  3  4  5  6  K\n       ↑ Elbow ~ K=3\n```\n\n**2. Silhouette Score:**\n$$s(i) = \frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n\nTrong đó:\n- $a(i)$: Average distance đến các điểm trong cùng cluster\n- $b(i)$: Average distance đến các điểm trong nearest cluster\n- Score: -1 đến 1\n  - ~1: Tốt, điểm xa cluster khác\n  - ~0: Gần boundary\n  - Negative: Có thể assign sai\n\n**Average Silhouette Score:**\n- Trung bình trên tất cả điểm\n- Chọn K có score cao nhất\n\n**3. Gap Statistic:**\n- So sánh WCSS với expected WCSS dưới null distribution\n- Chọn K where gap lớn nhất\n\n**4. Domain Knowledge:**\n- Business requirements\n- Interpretability\n- Practical constraints\n\n**Ưu Điểm:**\n- Đơn giản, dễ implement\n- Nhanh, scalable\n- Hoạt động tốt với spherical clusters\n- Dễ interpret\n\n**Nhược Điểm:**\n\n**1. Phải chỉ định K trước:**\n- Không biết K optimal\n- Cần thử nhiều giá trị\n\n**2. Nhạy cảm với initialization:**\n- Different initializations → different results\n- Có thể stuck ở local minima\n\n**3. Giả định spherical clusters:**\n- Không tốt với elongated/irregular shapes\n- Equal-sized clusters\n\n**4. Nhạy cảm với outliers:**\n- Outliers ảnh hưởng đến centroids\n- Có thể tạo clusters cho outliers\n\n**5. Phụ thuộc vào scale:**\n- Features có scale lớn dominate\n- Cần scaling trước\n\n**Cải Tiến:**\n\n**K-Means++:**\n- Better initialization strategy\n- Chọn centroids xa nhau\n- Giảm chance của bad initialization\n- Convergence nhanh hơn\n\n**Mini-batch K-Means:**\n- Sử dụng random mini-batches\n- Nhanh hơn nhiều với large datasets\n- Trade-off: Hơi kém chính xác\n- Good cho online learning\n\n**Practical Tips:**\n- Luôn standardize features\n- Run multiple times với different initializations\n- Use K-Means++ initialization\n- Try different K values\n- Visualize results nếu có thể\n\n### Hierarchical Clustering (Phân Cụm Phân Cấp)\n\nXây dựng hierarchy của clusters mà không cần chỉ định K trước.\n\n**Đặc điểm:**\n- Tạo tree structure (dendrogram)\n- Có thể chọn số clusters sau\n- Two approaches: Agglomerative và Divisive\n\n**1. Agglomerative (Bottom-Up - Tích Tụ):**\n\n**Thuật toán:**\n1. Start: Mỗi điểm là một cluster (N clusters)\n2. Repeat:\n   - Tìm 2 clusters gần nhất\n\n**Các khái niệm quan trọng:**\n- Spherical clusters (cụm hình cầu) là một giả định của thuật toán K-Means, cho rằng các cụm dữ liệu có hình dạng gần giống hình cầu và có kích thước tương đương. K-Means hoạt động kém hiệu quả với các cụm có hình dạng kéo dài (elongated) hoặc không đều, vì nó cố gắng tìm các tâm cụm dựa trên khoảng cách Euclidean, phù hợp nhất với các cụm hình cầu.\n\n**Mối quan hệ:**\n- K-Means giả định rằng các cụm dữ liệu có hình dạng spherical clusters, điều này có thể hạn chế hiệu quả của nó với các hình dạng cụm phức tạp hơn."
        },
        "output_without_context": {
            "question": "Các yếu tố nào có thể ảnh hưởng đến kết quả của phân tích cụm?",
            "answer": "Thuộc tính của dữ liệu, hình dạng cụm và thuật toán phân cụm."
        }
    },
    {
        "topic_description": "Học sinh sẽ được yêu cầu nhận diện các phương pháp khác nhau để đo lường độ tương tự giữa các điểm dữ liệu như khoảng cách Euclidean, Manhattan và Cosine. Chủ đề này giúp củng cố kiến thức về các công thức và cách sử dụng chúng trong phân tích dữ liệu.",
        "output_with_context": {
            "question": "Trong các phương pháp đo lường độ tương tự, công thức nào được sử dụng để tính khoảng cách Euclidean giữa hai điểm dữ liệu x và y?",
            "answer": "d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Phân Loại (Classification)\n$$P(x_i|C_k) = \frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}\right)$$\n- Ước lượng $\\mu_k$ (mean) và $\\sigma_k^2$ (variance) từ dữ liệu\n- Ứng dụng: Phân loại văn bản, nhận dạng mẫu\n\n**2. Multinomial Naive Bayes:**\n- Cho đếm rời rạc (word counts, frequencies)\n- Phân phối đa thức\n$$P(x_i|C_k) = \frac{N_{ki} + \\alpha}{N_k + \\alpha n}$$\n  - $N_{ki}$: Số lần đặc trưng $i$ xuất hiện trong lớp $k$\n  - $N_k$: Tổng số đếm trong lớp $k$\n  - $\\alpha$: Laplace smoothing (thường = 1)\n- Ứng dụng: Phân loại văn bản, phân tích cảm xúc, lọc spam\n\n**3. Bernoulli Naive Bayes:**\n- Cho đặc trưng nhị phân (có/không)\n- Phân phối Bernoulli\n$$P(x_i|C_k) = P(i|C_k)x_i + (1-P(i|C_k))(1-x_i)$$\n- Tính cả việc đặc trưng xuất hiện và không xuất hiện\n- Ứng dụng: Phân loại văn bản với binary features\n\n**Ưu Điểm:**\n- Nhanh, hiệu quả\n- Hoạt động tốt với dữ liệu nhỏ\n- Dễ triển khai và diễn giải\n- Hoạt động tốt với nhiều đặc trưng\n- Không nhạy cảm với đặc trưng không liên quan\n\n**Nhược Điểm:**\n- Giả định độc lập hiếm khi đúng trong thực tế\n- \"Zero frequency problem\" cần smoothing\n- Ước lượng xác suất có thể không chính xác\n- Không tốt khi đặc trưng tương quan\n\n**Laplace Smoothing:**\nXử lý vấn đề xác suất = 0:\n$$P(x_i|C_k) = \frac{count(x_i, C_k) + \\alpha}{count(C_k) + \\alpha \times |V|}$$\n\n### k-Nearest Neighbors (k-NN) - K Láng Giềng Gần Nhất\n\nPhương pháp non-parametric phân loại dựa trên đa số vote của k láng giềng gần nhất.\n\n**Thuật Toán:**\n1. Tính khoảng cách từ điểm cần phân loại đến tất cả điểm huấn luyện\n2. Chọn k điểm gần nhất\n3. Vote: Lớp xuất hiện nhiều nhất trong k láng giềng\n4. Gán nhãn lớp đó cho điểm mới\n\n**Các Độ Đo Khoảng Cách:**\n\n**1. Euclidean Distance (Khoảng cách Euclid):**\n$$d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}$$\n- Phổ biến nhất\n- Khoảng cách đường thẳng\n- Nhạy cảm với scale của đặc trưng\n\n**2. Manhattan Distance (Khoảng cách Manhattan):**\n$$d(x,y) = \\sum_{i=1}^{n}|x_i-y_i|$$\n- Khoảng cách theo lưới đô thị\n- Ít nhạy cảm với outliers\n- Tốt cho dữ liệu high-dimensional\n\n\n**Các khái niệm quan trọng:**\n- Euclidean Distance là một độ đo khoảng cách phổ biến, đo khoảng cách đường thẳng giữa hai điểm trong không gian Euclidean. Nó là một trường hợp đặc biệt của Minkowski Distance khi p=2. Công thức của nó là $$d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}$$. Euclidean Distance nhạy cảm với sự khác biệt về scale của các đặc trưng.\n\n**Mối quan hệ:**\n- k-Nearest Neighbors (k-NN) sử dụng Euclidean Distance để tính khoảng cách giữa các điểm dữ liệu, xác định các láng giềng gần nhất.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Phân Loại (Classification)\n**3. Minkowski Distance:**\n$$d(x,y) = \\left(\\sum_{i=1}^{n}|x_i-y_i|^p\right)^{1/p}$$\n- Tổng quát hóa Euclidean (p=2) và Manhattan (p=1)\n- p=∞: Chebyshev distance\n\n**4. Cosine Similarity:**\n$$similarity = \frac{x \\cdot y}{||x|| \times ||y||}$$\n- Đo góc giữa vectors\n- Tốt cho văn bản, high-dimensional sparse data\n- Không phụ thuộc vào độ lớn\n\n**5. Hamming Distance:**\n- Số vị trí khác nhau giữa hai chuỗi\n- Cho dữ liệu categorical hoặc binary\n\n**Chọn Giá Trị k:**\n\n**k quá nhỏ (k=1):**\n- Nhạy cảm với noise\n- Overfitting\n- Decision boundary phức tạp, không mượt\n\n**k quá lớn:**\n- Underfitting\n- Decision boundary quá mượt\n- Tính toán chậm\n- Có thể bị ảnh hưởng bởi lớp đa số\n\n**Lựa chọn k tối ưu:**\n- Cross-validation\n- Thử các giá trị k khác nhau (1, 3, 5, 7, 11, ...)\n- Chọn k lẻ để tránh tie trong binary classification\n- Thường k = √n (n là số mẫu training)\n- Elbow method: Vẽ error vs k\n\n**Weighted k-NN:**\n- Gán trọng số cho láng giềng dựa trên khoảng cách\n- Láng giềng gần hơn có ảnh hưởng lớn hơn\n$$weight = \frac{1}{distance}$$ hoặc $$weight = e^{-distance}$$\n\n**Ưu Điểm:**\n- Đơn giản, trực quan\n- Không có giai đoạn training (lazy learning)\n- Tự nhiên xử lý đa lớp\n- Hiệu quả với decision boundary phức tạp\n- Không giả định về phân phối dữ liệu\n\n**Nhược Điểm:**\n- Dự đoán chậm (O(n) cho mỗi dự đoán)\n- Yêu cầu lưu trữ toàn bộ training data\n- Nhạy cảm với scale của đặc trưng (cần scaling)\n- Hiệu suất giảm với high-dimensional data (curse of dimensionality)\n- Không xử lý tốt imbalanced data\n- Khó xử lý missing values\n\n**Tối Ưu Hóa k-NN:**\n- **KD-Tree, Ball Tree:** Cấu trúc dữ liệu để tìm kiếm nhanh\n- **Approximate Nearest Neighbors:** Hy sinh chút độ chính xác để tăng tốc\n- **Feature Selection:** Giảm dimensionality\n- **Dimensionality Reduction:** PCA, t-SNE\n\n### Các Chỉ Số Đánh Giá Phân Loại\n\n**Ma Trận Nhầm Lẫn (Confusion Matrix):**\n\n```\n                    Dự Đoán\n                Positive  Negative\nThực Tế  Pos      TP        FN\n         Neg      FP        TN\n```\n\n- **TP (True Positive):** Dự đoán đúng lớp dương\n- **TN (True Negative):** Dự đoán đúng lớp âm\n- **FP (False Positive):** Dự đoán sai là dương (Type I error)\n- **FN (False Negative):** Dự đoán sai là âm (Type II error)\n\n**Các Chỉ Số Chính:**\n\n**1. Accuracy (Độ Chính Xác):**\n$$Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$\n\n- Tỷ lệ dự đoán đúng tổng thể\n\n**Các khái niệm quan trọng:**\n- Hamming Distance là một thước đo sự khác biệt giữa hai chuỗi có độ dài bằng nhau, được định nghĩa là số vị trí mà các ký hiệu tương ứng khác nhau. Nó thường được sử dụng cho dữ liệu categorical hoặc binary để đánh giá sự khác biệt.\n- Euclidean Distance là một độ đo khoảng cách phổ biến, đo khoảng cách đường thẳng giữa hai điểm trong không gian Euclidean. Nó là một trường hợp đặc biệt của Minkowski Distance khi p=2. Công thức của nó là $$d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}$$. Euclidean Distance nhạy cảm với sự khác biệt về scale của các đặc trưng.\n\n**Mối quan hệ:**\n- k-Nearest Neighbors (k-NN) sử dụng Euclidean Distance để tính khoảng cách giữa các điểm dữ liệu, xác định các láng giềng gần nhất.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n- Multiple microphones recording\n- ICA separates individual voices\n\n**So với PCA:**\n- PCA: Decorrelation, orthogonal, Gaussian assumption\n- ICA: Independence, not orthogonal, non-Gaussian\n\n**Ứng dụng:**\n- Blind source separation\n- Signal processing (audio, EEG, fMRI)\n- Feature extraction\n- Artifact removal\n\n### Non-negative Matrix Factorization (NMF)\n\n**Nguyên lý:**\n- Factorize matrix into non-negative factors\n- Parts-based representation\n\n**Model:**\n$$X \\approx WH$$\n\n**Constraints:**\n- $X \\geq 0$: Input non-negative\n- $W \\geq 0$: Basis matrix non-negative\n- $H \\geq 0$: Coefficient matrix non-negative\n\n**Interpretation:**\n- $W$: Basis vectors (features, topics)\n- $H$: Coefficients (weights, memberships)\n- Each column của $X$ = linear combination của columns của $W$\n\n**Optimization:**\nMinimize: $||X - WH||^2$ with constraints\n\n**Ứng dụng:**\n\n**1. Topic Modeling:**\n- $X$: Document-term matrix\n- $W$: Term-topic matrix\n- $H$: Topic-document matrix\n\n**2. Image Processing:**\n- Learn parts of faces\n- $W$: Facial features\n- $H$: How to combine them\n\n**3. Recommender Systems:**\n- $X$: User-item matrix\n- $W$: User factors\n- $H$: Item factors\n\n**Ưu điểm:**\n- Interpretable (non-negativity)\n- Parts-based representation\n- Sparse solutions\n\n**Nhược điểm:**\n- Non-convex (local minima)\n- Slower than PCA\n- Requires non-negative data\n\n### Manifold Learning\n\nKhám phá non-linear structure trong high-dimensional data.\n\n**Manifold:**\n- Low-dimensional surface embedded trong high-dimensional space\n- Ví dụ: Swiss roll (2D manifold trong 3D)\n\n**1. Isomap (Isometric Feature Mapping):**\n\n**Nguyên lý:**\n- Preserve geodesic distances (shortest path trên manifold)\n- Global structure preservation\n\n**Steps:**\n1. Construct neighborhood graph\n2. Compute shortest path distances (Dijkstra)\n3. Apply MDS (Multi-Dimensional Scaling)\n\n**Ưu điểm:**\n- Global structure\n- Theoretical foundation\n\n**Nhược điểm:**\n- Expensive (shortest paths)\n- Sensitive to noise\n- Cần connected graph\n\n**2. Locally Linear Embedding (LLE):**\n\n**Nguyên lý:**\n- Preserve local relationships\n- Each point reconstructed từ neighbors\n\n**Steps:**\n1. Find k nearest neighbors\n2. Compute reconstruction weights\n3. Embed với same weights trong low-dim\n\n**Ưu điểm:**\n- Fast\n- Non-iterative\n- Good local preservation\n\n**Nhược điểm:**\n- Sensitive to k\n- Can produce distorted results\n\n**3. t-SNE:**\nĐã cover chi tiết trước đó.\n\n**4. UMAP:**\nĐã cover chi tiết trước đó.\n\n**So sánh:**\n- **Isomap:** Global, geodesic distances\n- **LLE:** Local, linear reconstruction\n- **t-SNE:** Local, visualization, stochastic\n- **UMAP:** Both local & global, faster than t-SNE\n\n### Phát Hiện Bất Thường (Anomaly Detection)\n\nXác định các items, events, hoặc observations hiếm.\n\n**Anomaly types:**\n- **Point anomalies:** Single data point\n- **Contextual anomalies:** Trong specific context\n- **Collective anomalies:** Collection của points\n\n**1. Statistical Methods:**\n\n**Z-score:**\n$$z = \frac{x - \\mu}{\\sigma}$$\n\n- |z| > 3: Anomaly (99.7% rule)\n- Giả định Gaussian distribution\n\n**Modified Z-score (Robust):**\n$$M = \frac{0.6745(x - median)}{MAD}$$\n\n- MAD = Median Absolute Deviation\n- Robust to outliers\n\n**Interquartile Range (IQR):**\n- IQR = Q3 - Q1\n- Outliers: < Q1 - 1.5×IQR hoặc > Q3 + 1.5×IQR\n\n**2. Isolation Forest:**\n\n\n**Các khái niệm quan trọng:**\n- Geodesic distances (khoảng cách trắc địa) là khoảng cách đường đi ngắn nhất giữa hai điểm trên một bề mặt cong (manifold), khác với khoảng cách Euclidean trong không gian thẳng. Isomap sử dụng geodesic distances để bảo toàn cấu trúc toàn cục của dữ liệu khi giảm chiều, đảm bảo rằng các điểm gần nhau trên manifold vẫn gần nhau trong không gian chiều thấp.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**Bước 4: Repeat**\n- Lặp lại Steps 2-3 cho đến khi convergence\n\n**Convergence khi:**\n- Centroids không đổi\n- Assignments không đổi\n- Đạt max iterations\n\n**Objective Function (WCSS - Within-Cluster Sum of Squares):**\n$$J = \\sum_{k=1}^{K}\\sum_{x \\in C_k}||x - \\mu_k||^2$$\n\nMục tiêu: Minimize J\n\n**Chọn K (Số Clusters):**\n\n**1. Elbow Method:**\n- Vẽ WCSS vs K\n- Tìm \"khuỷu tay\" (elbow) - điểm mà WCSS giảm chậm lại\n- Tradeoff giữa số clusters và fit\n\n**Ví dụ:**\n```\nWCSS\n  |  \\\n  |    \\\n  |      \\___\n  |          ----___\n  +----------------->\n  1  2  3  4  5  6  K\n       ↑ Elbow ~ K=3\n```\n\n**2. Silhouette Score:**\n$$s(i) = \frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n\nTrong đó:\n- $a(i)$: Average distance đến các điểm trong cùng cluster\n- $b(i)$: Average distance đến các điểm trong nearest cluster\n- Score: -1 đến 1\n  - ~1: Tốt, điểm xa cluster khác\n  - ~0: Gần boundary\n  - Negative: Có thể assign sai\n\n**Average Silhouette Score:**\n- Trung bình trên tất cả điểm\n- Chọn K có score cao nhất\n\n**3. Gap Statistic:**\n- So sánh WCSS với expected WCSS dưới null distribution\n- Chọn K where gap lớn nhất\n\n**4. Domain Knowledge:**\n- Business requirements\n- Interpretability\n- Practical constraints\n\n**Ưu Điểm:**\n- Đơn giản, dễ implement\n- Nhanh, scalable\n- Hoạt động tốt với spherical clusters\n- Dễ interpret\n\n**Nhược Điểm:**\n\n**1. Phải chỉ định K trước:**\n- Không biết K optimal\n- Cần thử nhiều giá trị\n\n**2. Nhạy cảm với initialization:**\n- Different initializations → different results\n- Có thể stuck ở local minima\n\n**3. Giả định spherical clusters:**\n- Không tốt với elongated/irregular shapes\n- Equal-sized clusters\n\n**4. Nhạy cảm với outliers:**\n- Outliers ảnh hưởng đến centroids\n- Có thể tạo clusters cho outliers\n\n**5. Phụ thuộc vào scale:**\n- Features có scale lớn dominate\n- Cần scaling trước\n\n**Cải Tiến:**\n\n**K-Means++:**\n- Better initialization strategy\n- Chọn centroids xa nhau\n- Giảm chance của bad initialization\n- Convergence nhanh hơn\n\n**Mini-batch K-Means:**\n- Sử dụng random mini-batches\n- Nhanh hơn nhiều với large datasets\n- Trade-off: Hơi kém chính xác\n- Good cho online learning\n\n**Practical Tips:**\n- Luôn standardize features\n- Run multiple times với different initializations\n- Use K-Means++ initialization\n- Try different K values\n- Visualize results nếu có thể\n\n### Hierarchical Clustering (Phân Cụm Phân Cấp)\n\nXây dựng hierarchy của clusters mà không cần chỉ định K trước.\n\n**Đặc điểm:**\n- Tạo tree structure (dendrogram)\n- Có thể chọn số clusters sau\n- Two approaches: Agglomerative và Divisive\n\n**1. Agglomerative (Bottom-Up - Tích Tụ):**\n\n**Thuật toán:**\n1. Start: Mỗi điểm là một cluster (N clusters)\n2. Repeat:\n   - Tìm 2 clusters gần nhất\n\n**Các khái niệm quan trọng:**\n- Công thức s(i) = (b(i) - a(i)) / max(a(i), b(i)) là công thức tính Silhouette Score cho một điểm dữ liệu i. Trong đó, a(i) là khoảng cách trung bình từ điểm i đến tất cả các điểm khác trong cùng cụm, và b(i) là khoảng cách trung bình từ điểm i đến tất cả các điểm trong cụm gần nhất (cụm láng giềng). Silhouette Score đo lường mức độ tương đồng của một điểm với cụm của nó so với các cụm khác, với giá trị từ -1 đến 1. Giá trị cao hơn cho thấy cụm tốt hơn.\n- Standardize features (chuẩn hóa đặc trưng) là một kỹ thuật tiền xử lý dữ liệu quan trọng, đặc biệt đối với các thuật toán dựa trên khoảng cách như K-Means. Nó biến đổi dữ liệu sao cho các đặc trưng có giá trị trung bình bằng 0 và độ lệch chuẩn bằng 1. Việc chuẩn hóa giúp loại bỏ sự phụ thuộc vào thang đo của các đặc trưng, đảm bảo rằng không có đặc trưng nào chi phối quá trình tính toán khoảng cách chỉ vì nó có phạm vi giá trị lớn hơn, từ đó cải thiện hiệu suất của thuật toán phân cụm.\n\n**Mối quan hệ:**\n- Silhouette Score được tính toán bằng công thức s(i) = (b(i) - a(i)) / max(a(i), b(i)), sử dụng khoảng cách trung bình trong cùng cụm và đến cụm gần nhất.\n- Standardize features cải thiện hiệu suất của K-Means bằng cách đảm bảo tất cả các đặc trưng đóng góp công bằng vào quá trình phân cụm, ngăn chặn các đặc trưng có thang đo lớn chi phối.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n   - Merge chúng thành 1 cluster\n3. Until: Chỉ còn 1 cluster\n\n**Steps chi tiết:**\n- Initialize: N clusters\n- Iteration 1: N-1 clusters\n- Iteration 2: N-2 clusters\n- ...\n- Final: 1 cluster\n\n**2. Divisive (Top-Down - Phân Chia):**\n\n**Thuật toán:**\n1. Start: Tất cả điểm trong 1 cluster\n2. Repeat:\n   - Chọn cluster để split\n   - Chia thành 2 sub-clusters\n3. Until: Mỗi điểm là 1 cluster\n\n**Ít phổ biến:** Computationally expensive hơn\n\n**Linkage Methods (Cách Đo Khoảng Cách Giữa Clusters):**\n\n**1. Single Linkage (Minimum):**\n$$d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoảng cách giữa 2 điểm gần nhất\n- Tạo long, chain-like clusters\n- Sensitive to noise và outliers\n\n**2. Complete Linkage (Maximum):**\n$$d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoảng cách giữa 2 điểm xa nhất\n- Tạo compact, spherical clusters\n- Ít sensitive to outliers\n\n**3. Average Linkage:**\n$$d(C_i, C_j) = \frac{1}{|C_i||C_j|}\\sum_{x \\in C_i}\\sum_{y \\in C_j}d(x,y)$$\n- Trung bình tất cả pairwise distances\n- Balance giữa single và complete\n- Phổ biến choice\n\n**4. Ward's Method:**\n- Minimize within-cluster variance sau khi merge\n- Maximize between-cluster variance\n- Tạo balanced, compact clusters\n- Thường cho kết quả tốt nhất\n- Phổ biến nhất trong thực tế\n\n**Dendrogram (Biểu Đồ Cây):**\n\nTree diagram showing cluster hierarchy.\n\n**Đọc Dendrogram:**\n- Vertical axis: Distance/dissimilarity\n- Horizontal axis: Samples\n- Height của merge: Distance giữa clusters\n- Càng cao merge càng dissimilar\n\n**Cutting Dendrogram:**\n- Vẽ horizontal line\n- Number of intersections = Number of clusters\n- Height của cut = dissimilarity threshold\n\n**Ưu Điểm:**\n- Không cần specify K trước\n- Dendrogram provides insights\n- Flexible - có thể chọn K sau\n- Deterministic (no randomness)\n\n**Nhược Điểm:**\n- Computationally expensive: O(N²log N) or O(N³)\n- Không scale với large datasets\n- Một khi merge không thể undo\n- Memory intensive\n\n**Khi Nào Dùng:**\n- Small-medium datasets (< 10,000)\n- Cần understand hierarchy\n- Không biết K optimal\n- Exploratory analysis\n\n### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nNhóm các điểm có mật độ cao, robust to outliers và arbitrary shapes.\n\n**Tham Số:**\n\n**1. ε (epsilon):**\n- Maximum distance giữa 2 điểm để được coi là neighbors\n- Định nghĩa neighborhood radius\n- Quá nhỏ: Nhiều noise points\n- Quá lớn: Merge nhiều clusters\n\n**2. MinPts (Minimum Points):**\n- Minimum số điểm trong ε-neighborhood để là core point\n- Thường: 4, 5, hoặc 2×dim\n- Larger MinPts: Ít core points, stricter\n\n**Các Loại Điểm:**\n\n**1. Core Point:**\n- Có ≥ MinPts điểm khác trong ε-neighborhood (bao gồm cả chính nó)\n- Trung tâm của clusters\n- Can form clusters\n\n**2. Border Point:**\n- Nằm trong ε-neighborhood của core point\n- Có < MinPts neighbors\n- Thuộc cluster nhưng không core\n- Ở biên của cluster\n\n\n**Các khái niệm quan trọng:**\n- Công thức này tính khoảng cách giữa hai cụm Cᵢ và Cⱼ. Trong phân cụm phân cấp, có hai phương pháp chính: Single Linkage, định nghĩa khoảng cách là khoảng cách nhỏ nhất giữa bất kỳ cặp điểm nào (một điểm thuộc Cᵢ và một điểm thuộc Cⱼ); và Complete Linkage, định nghĩa khoảng cách là khoảng cách lớn nhất giữa bất kỳ cặp điểm nào (một điểm thuộc Cᵢ và một điểm thuộc Cⱼ).\n\n**Mối quan hệ:**\n- Complete Linkage tính toán khoảng cách giữa các cụm bằng công thức d(Cᵢ, Cⱼ) = max(x ∈ Cᵢ, y ∈ Cⱼ)d(x,y)."
        },
        "output_without_context": {
            "question": "Trong các phương pháp đo lường độ tương tự giữa các điểm dữ liệu, công thức nào được sử dụng để tính khoảng cách giữa hai điểm trong không gian hai chiều với tọa độ (x1, y1) và (x2, y2)?",
            "answer": "Khoảng cách Euclidean: √((x2 - x1)² + (y2 - y1)²)"
        }
    },
    {
        "topic_description": "Kiểm tra khả năng của sinh viên trong việc áp dụng thuật toán K-Means vào phân tích dữ liệu. Học sinh sẽ giải thích cách lựa chọn K, cách thuật toán này hoạt động và các chỉ số đánh giá như WCSS.",
        "output_with_context": {
            "question": "Trong thuật toán K-Means, phương pháp nào được sử dụng để xác định số lượng cụm K tối ưu bằng cách tìm điểm khuỷu tay trên đồ thị WCSS so với K?",
            "answer": "Elbow Method",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**Bước 4: Repeat**\n- Lặp lại Steps 2-3 cho đến khi convergence\n\n**Convergence khi:**\n- Centroids không đổi\n- Assignments không đổi\n- Đạt max iterations\n\n**Objective Function (WCSS - Within-Cluster Sum of Squares):**\n$$J = \\sum_{k=1}^{K}\\sum_{x \\in C_k}||x - \\mu_k||^2$$\n\nMục tiêu: Minimize J\n\n**Chọn K (Số Clusters):**\n\n**1. Elbow Method:**\n- Vẽ WCSS vs K\n- Tìm \"khuỷu tay\" (elbow) - điểm mà WCSS giảm chậm lại\n- Tradeoff giữa số clusters và fit\n\n**Ví dụ:**\n```\nWCSS\n  |  \\\n  |    \\\n  |      \\___\n  |          ----___\n  +----------------->\n  1  2  3  4  5  6  K\n       ↑ Elbow ~ K=3\n```\n\n**2. Silhouette Score:**\n$$s(i) = \frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n\nTrong đó:\n- $a(i)$: Average distance đến các điểm trong cùng cluster\n- $b(i)$: Average distance đến các điểm trong nearest cluster\n- Score: -1 đến 1\n  - ~1: Tốt, điểm xa cluster khác\n  - ~0: Gần boundary\n  - Negative: Có thể assign sai\n\n**Average Silhouette Score:**\n- Trung bình trên tất cả điểm\n- Chọn K có score cao nhất\n\n**3. Gap Statistic:**\n- So sánh WCSS với expected WCSS dưới null distribution\n- Chọn K where gap lớn nhất\n\n**4. Domain Knowledge:**\n- Business requirements\n- Interpretability\n- Practical constraints\n\n**Ưu Điểm:**\n- Đơn giản, dễ implement\n- Nhanh, scalable\n- Hoạt động tốt với spherical clusters\n- Dễ interpret\n\n**Nhược Điểm:**\n\n**1. Phải chỉ định K trước:**\n- Không biết K optimal\n- Cần thử nhiều giá trị\n\n**2. Nhạy cảm với initialization:**\n- Different initializations → different results\n- Có thể stuck ở local minima\n\n**3. Giả định spherical clusters:**\n- Không tốt với elongated/irregular shapes\n- Equal-sized clusters\n\n**4. Nhạy cảm với outliers:**\n- Outliers ảnh hưởng đến centroids\n- Có thể tạo clusters cho outliers\n\n**5. Phụ thuộc vào scale:**\n- Features có scale lớn dominate\n- Cần scaling trước\n\n**Cải Tiến:**\n\n**K-Means++:**\n- Better initialization strategy\n- Chọn centroids xa nhau\n- Giảm chance của bad initialization\n- Convergence nhanh hơn\n\n**Mini-batch K-Means:**\n- Sử dụng random mini-batches\n- Nhanh hơn nhiều với large datasets\n- Trade-off: Hơi kém chính xác\n- Good cho online learning\n\n**Practical Tips:**\n- Luôn standardize features\n- Run multiple times với different initializations\n- Use K-Means++ initialization\n- Try different K values\n- Visualize results nếu có thể\n\n### Hierarchical Clustering (Phân Cụm Phân Cấp)\n\nXây dựng hierarchy của clusters mà không cần chỉ định K trước.\n\n**Đặc điểm:**\n- Tạo tree structure (dendrogram)\n- Có thể chọn số clusters sau\n- Two approaches: Agglomerative và Divisive\n\n**1. Agglomerative (Bottom-Up - Tích Tụ):**\n\n**Thuật toán:**\n1. Start: Mỗi điểm là một cluster (N clusters)\n2. Repeat:\n   - Tìm 2 clusters gần nhất\n\n**Các khái niệm quan trọng:**\n- K là tham số trong thuật toán K-Means (K-Means Clustering), đại diện cho số lượng cụm (clusters) mà dữ liệu sẽ được phân chia thành. Việc chọn giá trị K tối ưu hoặc phù hợp là một thách thức trong học không giám sát và có thể được thực hiện bằng các phương pháp như Elbow Method, Silhouette Score, Gap Statistic hoặc dựa trên kiến thức miền (domain knowledge).\n- Convergence trong K-Means là trạng thái mà thuật toán dừng lại khi các tâm cụm (centroids) không còn thay đổi đáng kể, hoặc các điểm dữ liệu không còn thay đổi cụm được gán, hoặc khi đạt đến số lần lặp tối đa (max iterations). Đây là dấu hiệu cho thấy thuật toán đã tìm thấy một cấu hình cụm ổn định.\n- Initialization trong K-Means là quá trình chọn các vị trí ban đầu cho K tâm cụm (centroids). Việc khởi tạo ngẫu nhiên có thể dẫn đến các kết quả phân cụm khác nhau và có thể bị mắc kẹt ở các cực tiểu cục bộ (local minima). Các phương pháp cải tiến như K-Means++ được thiết kế để chọn các tâm cụm ban đầu tốt hơn, giúp cải thiện chất lượng và tốc độ hội tụ của thuật toán.\n- Mini-batch K-Means là một biến thể của thuật toán K-Means được thiết kế để xử lý các tập dữ liệu lớn hiệu quả hơn. Thay vì sử dụng toàn bộ tập dữ liệu để cập nhật tâm cụm trong mỗi lần lặp, Mini-batch K-Means sử dụng các \"mini-batches\" (tập con ngẫu nhiên) của dữ liệu. Điều này giúp tăng tốc độ tính toán đáng kể, đặc biệt với dữ liệu lớn, mặc dù có thể có một chút đánh đổi về độ chính xác so với K-Means truyền thống. Nó cũng phù hợp cho các tình huống học trực tuyến (online learning).\n\n**Mối quan hệ:**\n- Mean Shift là một thuật toán phân cụm không cần chỉ định số lượng cụm K trước, mà tự động xác định K dựa trên mật độ dữ liệu.\n- K-Means++ ngăn chặn vấn đề khởi tạo kém trong K-Means bằng cách chọn các tâm cụm ban đầu một cách chiến lược, giúp giảm khả năng mắc kẹt ở cực tiểu cục bộ.\n- Gap Statistic được sử dụng để kiểm soát và chọn giá trị K tối ưu bằng cách so sánh WCSS thực tế với WCSS dự kiến từ phân phối tham chiếu.\n- Domain Knowledge được sử dụng để kiểm soát và chọn giá trị K (số lượng cụm) dựa trên các yêu cầu nghiệp vụ và khả năng diễn giải thực tế.\n- Elbow Method được sử dụng để kiểm soát và xác định giá trị tối ưu cho tham số K (số lượng cụm) trong thuật toán K-Means bằng cách phân tích đồ thị WCSS.\n- Mini-batch K-Means cải thiện hiệu suất của K-Means truyền thống bằng cách sử dụng các mini-batches dữ liệu, giúp tăng tốc độ tính toán đáng kể với các tập dữ liệu lớn.\n- Silhouette Score được sử dụng để kiểm soát và chọn giá trị K tối ưu bằng cách tính điểm trung bình cho các giá trị K khác nhau và chọn K có điểm cao nhất, cho thấy chất lượng cụm tốt nhất.\n- Silhouette score được sử dụng để đánh giá chất lượng của các cụm và có thể giúp chọn số lượng cụm K tối ưu bằng cách tìm K cho ra điểm số cao nhất.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Phân Loại (Classification)\n$$P(x_i|C_k) = \frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}\right)$$\n- Ước lượng $\\mu_k$ (mean) và $\\sigma_k^2$ (variance) từ dữ liệu\n- Ứng dụng: Phân loại văn bản, nhận dạng mẫu\n\n**2. Multinomial Naive Bayes:**\n- Cho đếm rời rạc (word counts, frequencies)\n- Phân phối đa thức\n$$P(x_i|C_k) = \frac{N_{ki} + \\alpha}{N_k + \\alpha n}$$\n  - $N_{ki}$: Số lần đặc trưng $i$ xuất hiện trong lớp $k$\n  - $N_k$: Tổng số đếm trong lớp $k$\n  - $\\alpha$: Laplace smoothing (thường = 1)\n- Ứng dụng: Phân loại văn bản, phân tích cảm xúc, lọc spam\n\n**3. Bernoulli Naive Bayes:**\n- Cho đặc trưng nhị phân (có/không)\n- Phân phối Bernoulli\n$$P(x_i|C_k) = P(i|C_k)x_i + (1-P(i|C_k))(1-x_i)$$\n- Tính cả việc đặc trưng xuất hiện và không xuất hiện\n- Ứng dụng: Phân loại văn bản với binary features\n\n**Ưu Điểm:**\n- Nhanh, hiệu quả\n- Hoạt động tốt với dữ liệu nhỏ\n- Dễ triển khai và diễn giải\n- Hoạt động tốt với nhiều đặc trưng\n- Không nhạy cảm với đặc trưng không liên quan\n\n**Nhược Điểm:**\n- Giả định độc lập hiếm khi đúng trong thực tế\n- \"Zero frequency problem\" cần smoothing\n- Ước lượng xác suất có thể không chính xác\n- Không tốt khi đặc trưng tương quan\n\n**Laplace Smoothing:**\nXử lý vấn đề xác suất = 0:\n$$P(x_i|C_k) = \frac{count(x_i, C_k) + \\alpha}{count(C_k) + \\alpha \times |V|}$$\n\n### k-Nearest Neighbors (k-NN) - K Láng Giềng Gần Nhất\n\nPhương pháp non-parametric phân loại dựa trên đa số vote của k láng giềng gần nhất.\n\n**Thuật Toán:**\n1. Tính khoảng cách từ điểm cần phân loại đến tất cả điểm huấn luyện\n2. Chọn k điểm gần nhất\n3. Vote: Lớp xuất hiện nhiều nhất trong k láng giềng\n4. Gán nhãn lớp đó cho điểm mới\n\n**Các Độ Đo Khoảng Cách:**\n\n**1. Euclidean Distance (Khoảng cách Euclid):**\n$$d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}$$\n- Phổ biến nhất\n- Khoảng cách đường thẳng\n- Nhạy cảm với scale của đặc trưng\n\n**2. Manhattan Distance (Khoảng cách Manhattan):**\n$$d(x,y) = \\sum_{i=1}^{n}|x_i-y_i|$$\n- Khoảng cách theo lưới đô thị\n- Ít nhạy cảm với outliers\n- Tốt cho dữ liệu high-dimensional\n\n\n**Các khái niệm quan trọng:**\n- k-Nearest Neighbors (k-NN) là một thuật toán học máy non-parametric được sử dụng cho cả phân loại và hồi quy. Nó hoạt động dựa trên nguyên tắc \"lazy learning\" vì không có giai đoạn huấn luyện rõ ràng; thay vào đó, nó lưu trữ toàn bộ dữ liệu huấn luyện. Để phân loại một điểm dữ liệu mới, k-NN tìm k điểm dữ liệu gần nhất trong tập huấn luyện (thường sử dụng các độ đo khoảng cách như Minkowski Distance) và gán nhãn cho điểm mới dựa trên đa số phiếu của k láng giềng đó. k-NN có khả năng xử lý các bài toán đa lớp và tạo ra các decision boundary phức tạp, nhưng nó nhạy cảm với scale của các đặc trưng và có thể bị ảnh hưởng bởi \"curse of dimensionality\".\n\n**Mối quan hệ:**\n- Tham số k kiểm soát số lượng láng giềng được xem xét trong thuật toán k-Nearest Neighbors (k-NN) để đưa ra quyết định phân loại hoặc hồi quy."
        },
        "output_without_context": {
            "question": "Trong thuật toán K-Means, chỉ số nào được sử dụng để đánh giá độ tốt của phân cụm và thường được tối thiểu hóa?",
            "answer": "WCSS (Within-Cluster Sum of Squares)"
        }
    },
    {
        "topic_description": "Giúp sinh viên phân tích và đánh giá ưu điểm và nhược điểm của thuật toán K-Means trong các tình huống phân cụm khác nhau. Học sinh sẽ cần áp dụng kiến thức từ tuần hiện tại và tuần trước để đưa ra các lập luận.",
        "output_with_context": {
            "question": "Trong thuật toán K-Means, điều gì là một trong những nhược điểm chính khi xác định số lượng cụm K?",
            "answer": "Cần chỉ định K trước mà không biết giá trị tối ưu.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Lựa Chọn Đặc Trưng & Tối Ưu Hóa Mô Hình\n- Precision, Recall, F1, AUC\n- Business metrics\n\n**6. Avoid Data Leakage:**\n- **Proper CV:** Fit preprocessors trên train folds only\n- **Time-based splits:** cho time series\n- **No target leakage:** Features không chứa info về target\n- **Test set untouched:** Cho đến cuối\n\n**7. Document Everything:**\n- Experiments log\n- Model versions\n- Hyperparameters\n- Results và insights\n\n**8. Reproducibility:**\n- Set random seeds\n- Version control code\n- Save data versions\n- Document environment\n- Use containers (Docker)\n\n**9. Model Versioning:**\n- MLflow, DVC\n- Track models\n- Compare versions\n- Rollback nếu cần\n\n**10. Validation Strategy:**\n- Robust CV\n- Hold-out test set\n- Temporal validation cho time series\n\n**11. Feature Engineering First:**\n- \"Data > Algorithms\"\n- Good features > Complex models\n- Domain knowledge valuable\n\n**12. Monitor Training:**\n- Training vs validation\n- Learning curves\n- Early signs of overfitting\n\n**13. Consider Production:**\n- Inference time\n- Model size\n- Dependencies\n- Maintenance\n- Explainability\n\n**14. Test on Real Data:**\n- Not just metrics\n- Qualitative analysis\n- Edge cases\n- Failure modes\n\n---\n\n---\n\n## Học Không Giám Sát (Unsupervised Learning)\n\n### Giới Thiệu Về Học Không Giám Sát\n\nHọc không giám sát khám phá các mẫu ẩn trong dữ liệu không có nhãn mà không cần biến mục tiêu tường minh. Nó được sử dụng cho phân tích dữ liệu khám phá, nhận dạng mẫu và nén dữ liệu.\n\n**Đặc điểm chính:**\n- Không có labels (y)\n- Chỉ có features (X)\n- Tìm structure trong data\n- Exploratory analysis\n\n**So với Supervised Learning:**\n| Tiêu chí | Supervised | Unsupervised |\n|----------|-----------|--------------|\n| Labels | Có | Không |\n| Mục tiêu | Dự đoán | Khám phá |\n| Feedback | Có (accuracy) | Không rõ ràng |\n| Ứng dụng | Classification, Regression | Clustering, Dimensionality Reduction |\n\n**Các tác vụ chính:**\n1. **Clustering:** Nhóm dữ liệu tương tự\n2. **Dimensionality Reduction:** Giảm số chiều\n3. **Anomaly Detection:** Phát hiện bất thường\n4. **Association Rule Learning:** Tìm mối quan hệ\n\n**Thách thức:**\n- Không có ground truth để đánh giá\n- Khó xác định số clusters/components\n- Kết quả có thể subjective\n- Cần domain knowledge để interpret\n\n### Clustering (Phân Cụm)\n\nNhóm các điểm dữ liệu tương tự lại với nhau.\n\n**Mục tiêu:**\n- High intra-cluster similarity (trong cùng cluster)\n- Low inter-cluster similarity (giữa các clusters)\n\n**Ứng dụng:**\n- Customer segmentation\n- Document clustering\n- Image segmentation\n- Anomaly detection\n- Data compression\n\n### K-Means Clustering\n\nThuật toán phân cụm phổ biến nhất, chia dữ liệu thành K clusters.\n\n**Thuật toán:**\n\n**Bước 1: Initialization**\n- Chọn K centroids ngẫu nhiên\n- Có thể từ data points hoặc random positions\n\n**Bước 2: Assignment**\n- Gán mỗi điểm đến centroid gần nhất\n- Sử dụng Euclidean distance:\n$$d(x, \\mu_k) = ||x - \\mu_k|| = \\sqrt{\\sum_{j=1}^{n}(x_j - \\mu_{kj})^2}$$\n\n**Bước 3: Update**\n- Cập nhật centroids = mean của các điểm assigned\n$$\\mu_k = \frac{1}{|C_k|}\\sum_{x \\in C_k}x$$\n\n\n**Các khái niệm quan trọng:**\n- K là tham số trong thuật toán K-Means (K-Means Clustering), đại diện cho số lượng cụm (clusters) mà dữ liệu sẽ được phân chia thành. Việc chọn giá trị K tối ưu hoặc phù hợp là một thách thức trong học không giám sát và có thể được thực hiện bằng các phương pháp như Elbow Method, Silhouette Score, Gap Statistic hoặc dựa trên kiến thức miền (domain knowledge).\n- Centroids là các điểm trung tâm của mỗi cụm trong thuật toán K-Means Clustering. Ban đầu chúng được chọn ngẫu nhiên và sau đó được cập nhật lặp đi lặp lại để đại diện cho trung bình của các điểm dữ liệu trong cụm tương ứng.\n- K-Means Clustering là thuật toán phân cụm (clustering algorithm) phổ biến nhất trong học không giám sát, được sử dụng để phân chia n điểm dữ liệu thành K cụm, trong đó mỗi điểm dữ liệu thuộc về cụm có tâm (centroid) gần nhất. Thuật toán hoạt động bằng cách lặp lại hai bước chính: gán mỗi điểm dữ liệu vào centroid gần nhất và cập nhật centroid bằng cách tính trung bình của tất cả các điểm trong cụm đó, cho đến khi đạt được sự hội tụ. K-Means là một thuật toán phân cụm cứng (hard clustering), yêu cầu chỉ định số lượng cụm K trước và thường sử dụng Within-Cluster Sum of Squares (WCSS) làm hàm mục tiêu để cực tiểu hóa. Thuật toán giả định các cụm có hình dạng cầu và kích thước tương tự, hoạt động tốt nhất với các cụm có hình dạng cầu, và nhạy cảm với việc khởi tạo tâm cụm cũng như các ngoại lệ. K-Means nhanh, đơn giản, hiệu quả, phù hợp cho các tập dữ liệu lớn, nhưng kém linh hoạt hơn GMM trong việc xử lý các hình dạng cụm phức tạp hoặc các cụm có mật độ khác nhau.\n- Clustering (Phân cụm) là một tác vụ trong học không giám sát nhằm nhóm các điểm dữ liệu tương tự lại với nhau. Mục tiêu là đạt được sự tương đồng cao trong cùng một cụm (high intra-cluster similarity) và sự khác biệt thấp giữa các cụm (low inter-cluster similarity).\n\n**Mối quan hệ:**\n- Mean Shift là một thuật toán phân cụm không cần chỉ định số lượng cụm K trước, mà tự động xác định K dựa trên mật độ dữ liệu.\n- Elbow Method được sử dụng để kiểm soát và xác định giá trị tối ưu cho tham số K (số lượng cụm) trong thuật toán K-Means bằng cách phân tích đồ thị WCSS.\n- K-Means sử dụng Centroids làm tham số chính để đại diện cho tâm của mỗi cụm.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**Bước 4: Repeat**\n- Lặp lại Steps 2-3 cho đến khi convergence\n\n**Convergence khi:**\n- Centroids không đổi\n- Assignments không đổi\n- Đạt max iterations\n\n**Objective Function (WCSS - Within-Cluster Sum of Squares):**\n$$J = \\sum_{k=1}^{K}\\sum_{x \\in C_k}||x - \\mu_k||^2$$\n\nMục tiêu: Minimize J\n\n**Chọn K (Số Clusters):**\n\n**1. Elbow Method:**\n- Vẽ WCSS vs K\n- Tìm \"khuỷu tay\" (elbow) - điểm mà WCSS giảm chậm lại\n- Tradeoff giữa số clusters và fit\n\n**Ví dụ:**\n```\nWCSS\n  |  \\\n  |    \\\n  |      \\___\n  |          ----___\n  +----------------->\n  1  2  3  4  5  6  K\n       ↑ Elbow ~ K=3\n```\n\n**2. Silhouette Score:**\n$$s(i) = \frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n\nTrong đó:\n- $a(i)$: Average distance đến các điểm trong cùng cluster\n- $b(i)$: Average distance đến các điểm trong nearest cluster\n- Score: -1 đến 1\n  - ~1: Tốt, điểm xa cluster khác\n  - ~0: Gần boundary\n  - Negative: Có thể assign sai\n\n**Average Silhouette Score:**\n- Trung bình trên tất cả điểm\n- Chọn K có score cao nhất\n\n**3. Gap Statistic:**\n- So sánh WCSS với expected WCSS dưới null distribution\n- Chọn K where gap lớn nhất\n\n**4. Domain Knowledge:**\n- Business requirements\n- Interpretability\n- Practical constraints\n\n**Ưu Điểm:**\n- Đơn giản, dễ implement\n- Nhanh, scalable\n- Hoạt động tốt với spherical clusters\n- Dễ interpret\n\n**Nhược Điểm:**\n\n**1. Phải chỉ định K trước:**\n- Không biết K optimal\n- Cần thử nhiều giá trị\n\n**2. Nhạy cảm với initialization:**\n- Different initializations → different results\n- Có thể stuck ở local minima\n\n**3. Giả định spherical clusters:**\n- Không tốt với elongated/irregular shapes\n- Equal-sized clusters\n\n**4. Nhạy cảm với outliers:**\n- Outliers ảnh hưởng đến centroids\n- Có thể tạo clusters cho outliers\n\n**5. Phụ thuộc vào scale:**\n- Features có scale lớn dominate\n- Cần scaling trước\n\n**Cải Tiến:**\n\n**K-Means++:**\n- Better initialization strategy\n- Chọn centroids xa nhau\n- Giảm chance của bad initialization\n- Convergence nhanh hơn\n\n**Mini-batch K-Means:**\n- Sử dụng random mini-batches\n- Nhanh hơn nhiều với large datasets\n- Trade-off: Hơi kém chính xác\n- Good cho online learning\n\n**Practical Tips:**\n- Luôn standardize features\n- Run multiple times với different initializations\n- Use K-Means++ initialization\n- Try different K values\n- Visualize results nếu có thể\n\n### Hierarchical Clustering (Phân Cụm Phân Cấp)\n\nXây dựng hierarchy của clusters mà không cần chỉ định K trước.\n\n**Đặc điểm:**\n- Tạo tree structure (dendrogram)\n- Có thể chọn số clusters sau\n- Two approaches: Agglomerative và Divisive\n\n**1. Agglomerative (Bottom-Up - Tích Tụ):**\n\n**Thuật toán:**\n1. Start: Mỗi điểm là một cluster (N clusters)\n2. Repeat:\n   - Tìm 2 clusters gần nhất\n\n**Các khái niệm quan trọng:**\n- K là tham số trong thuật toán K-Means (K-Means Clustering), đại diện cho số lượng cụm (clusters) mà dữ liệu sẽ được phân chia thành. Việc chọn giá trị K tối ưu hoặc phù hợp là một thách thức trong học không giám sát và có thể được thực hiện bằng các phương pháp như Elbow Method, Silhouette Score, Gap Statistic hoặc dựa trên kiến thức miền (domain knowledge).\n- Initialization trong K-Means là quá trình chọn các vị trí ban đầu cho K tâm cụm (centroids). Việc khởi tạo ngẫu nhiên có thể dẫn đến các kết quả phân cụm khác nhau và có thể bị mắc kẹt ở các cực tiểu cục bộ (local minima). Các phương pháp cải tiến như K-Means++ được thiết kế để chọn các tâm cụm ban đầu tốt hơn, giúp cải thiện chất lượng và tốc độ hội tụ của thuật toán.\n- Mini-batch K-Means là một biến thể của thuật toán K-Means được thiết kế để xử lý các tập dữ liệu lớn hiệu quả hơn. Thay vì sử dụng toàn bộ tập dữ liệu để cập nhật tâm cụm trong mỗi lần lặp, Mini-batch K-Means sử dụng các \"mini-batches\" (tập con ngẫu nhiên) của dữ liệu. Điều này giúp tăng tốc độ tính toán đáng kể, đặc biệt với dữ liệu lớn, mặc dù có thể có một chút đánh đổi về độ chính xác so với K-Means truyền thống. Nó cũng phù hợp cho các tình huống học trực tuyến (online learning).\n\n**Mối quan hệ:**\n- Mean Shift là một thuật toán phân cụm không cần chỉ định số lượng cụm K trước, mà tự động xác định K dựa trên mật độ dữ liệu.\n- Elbow Method được sử dụng để kiểm soát và xác định giá trị tối ưu cho tham số K (số lượng cụm) trong thuật toán K-Means bằng cách phân tích đồ thị WCSS.\n- Mini-batch K-Means cải thiện hiệu suất của K-Means truyền thống bằng cách sử dụng các mini-batches dữ liệu, giúp tăng tốc độ tính toán đáng kể với các tập dữ liệu lớn.\n- K-Means++ ngăn chặn vấn đề khởi tạo kém trong K-Means bằng cách chọn các tâm cụm ban đầu một cách chiến lược, giúp giảm khả năng mắc kẹt ở cực tiểu cục bộ.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n$$\\gamma_{ik} = \frac{\\pi_k\\mathcal{N}(x_i|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(x_i|\\mu_j, \\Sigma_j)}$$\n\n- $\\gamma_{ik}$: Probability điểm $i$ belongs to cluster $k$\n- $\\sum_{k=1}^{K}\\gamma_{ik} = 1$ for each $i$\n\n**M-Step (Maximization):**\nUpdate parameters dựa trên responsibilities:\n\n**Mixing coefficients:**\n$$\\pi_k = \frac{1}{N}\\sum_{i=1}^{N}\\gamma_{ik}$$\n\n**Means:**\n$$\\mu_k = \frac{\\sum_{i=1}^{N}\\gamma_{ik}x_i}{\\sum_{i=1}^{N}\\gamma_{ik}}$$\n\n**Covariances:**\n$$\\Sigma_k = \frac{\\sum_{i=1}^{N}\\gamma_{ik}(x_i-\\mu_k)(x_i-\\mu_k)^T}{\\sum_{i=1}^{N}\\gamma_{ik}}$$\n\n**Repeat E-Step và M-Step** cho đến convergence.\n\n**Convergence:**\n- Log-likelihood không thay đổi nhiều\n- Parameters stable\n- Đạt max iterations\n\n**Ưu Điểm:**\n\n**1. Soft clustering:**\n- Probability of membership cho mỗi cluster\n- Captures uncertainty\n- More nuanced than hard assignment\n\n**2. Flexible cluster shapes:**\n- Elliptical clusters\n- Different sizes\n- Different orientations\n\n**3. Probabilistic framework:**\n- Sound mathematical foundation\n- Can compute likelihoods\n- Model selection với BIC/AIC\n\n**4. Generative model:**\n- Có thể generate new samples\n- Understand data distribution\n\n**Nhược Điểm:**\n\n**1. Cần specify K:**\n- Không tự động determine\n- Use BIC/AIC để chọn\n\n**2. Sensitive to initialization:**\n- EM có thể converge to local optima\n- Run multiple times\n\n**3. Assumes Gaussian:**\n- Không phù hợp nếu data không Gaussian\n- Limited to elliptical shapes\n\n**4. Computationally expensive:**\n- Covariance matrix inversion\n- Slower than K-Means\n\n**5. Singular covariance matrices:**\n- Có thể xảy ra với small clusters\n- Need regularization\n\n**So Sánh K-Means vs GMM:**\n\n| Tiêu chí | K-Means | GMM |\n|----------|---------|-----|\n| Assignment | Hard | Soft (probabilistic) |\n| Cluster shape | Spherical | Elliptical |\n| Parameters | Centroids | Means + Covariances |\n| Speed | Nhanh | Chậm hơn |\n| Flexibility | Ít | Nhiều |\n| Probabilistic | Không | Có |\n\n**Chọn K:**\n- Bayesian Information Criterion (BIC)\n- Akaike Information Criterion (AIC)\n- Cross-validation\n- Silhouette score\n\n### Các Thuật Toán Clustering Khác\n\n**1. Mean Shift:**\n\n**Nguyên lý:**\n- Density-based, no need chỉ định số clusters\n- Shift points toward mode (density maxima)\n\n**Thuật toán:**\n1. Khởi tạo window around each point\n2. Tính mean của points trong window\n3. Shift center đến mean\n4. Repeat until convergence\n5. Points converging to same mode = same cluster\n\n**Ưu điểm:**\n- Không cần specify K\n- Arbitrary shapes\n- Automatic K\n\n**Nhược điểm:**\n- Slow (O(N²))\n- Bandwidth parameter critical\n\n**2. Spectral Clustering:**\n\n**Nguyên lý:**\n- Uses graph theory\n- Eigenvalues của similarity matrix\n- Good cho non-convex clusters\n\n**Steps:**\n1. Construct similarity graph\n2. Compute Laplacian matrix\n3. Eigenvalue decomposition\n4. K-Means trên eigenvectors\n\n**Ưu điểm:**\n- Handle complex shapes\n- Thường tốt hơn K-Means\n- Based on graph cuts\n\n**Nhược điểm:**\n- Computationally expensive\n- Cần tune similarity function\n- Sensitive to parameters\n\n\n**Các khái niệm quan trọng:**\n- Mean Shift là một thuật toán phân cụm học không giám sát, dựa trên mật độ, không yêu cầu chỉ định số lượng cụm K trước. Thuật toán này hoạt động bằng cách dịch chuyển các điểm dữ liệu về phía các điểm cực đại mật độ (mode) trong không gian dữ liệu; các điểm hội tụ về cùng một mode được coi là thuộc cùng một cụm. Mean Shift có khả năng tìm các cụm có hình dạng tùy ý, tự động xác định số lượng cụm và mạnh mẽ với nhiễu (outliers). Tuy nhiên, nó có tốc độ chậm và phù hợp cho các phân bố dữ liệu không đồng nhất.\n\n**Mối quan hệ:**\n- Mean Shift là một thuật toán phân cụm không cần chỉ định số lượng cụm K trước, mà tự động xác định K dựa trên mật độ dữ liệu."
        },
        "output_without_context": {
            "question": "Trong các tình huống phân cụm, ưu điểm nào của thuật toán K-Means có thể được coi là quan trọng nhất khi đánh giá hiệu quả của nó?",
            "answer": "Tính đơn giản và khả năng xử lý nhanh với dữ liệu lớn."
        }
    },
    {
        "topic_description": "Sinh viên sẽ được yêu cầu sử dụng và giải thích các chỉ số đánh giá như Davies-Bouldin, Dunn và hệ số Silhouette. Chủ đề này kiểm tra khả năng tổng hợp và phân tích hiệu suất của phân cụm trong các điều kiện khác nhau, kết nối với kiến thức từ những tuần trước.",
        "output_with_context": {
            "question": "Chỉ số nào dưới đây được sử dụng để đánh giá chất lượng phân cụm mà không cần ground truth và có giá trị nằm trong khoảng từ -1 đến 1?",
            "answer": "Silhouette Coefficient",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n5. Repeat until no more frequent itemsets\n6. Generate rules từ frequent itemsets\n\n**Ưu điểm:**\n- Guaranteed to find all frequent itemsets\n- Pruning reduces candidates\n\n**Nhược điểm:**\n- Multiple database scans\n- Expensive với large datasets\n- Chậm với low support threshold\n\n**2. FP-Growth (Frequent Pattern Growth):**\n\n**Nguyên lý:**\n- Compress database into FP-tree\n- Mine directly từ tree\n- Chỉ 2 database scans\n\n**Steps:**\n1. Scan database, find frequent items\n2. Build FP-tree (compressed representation)\n3. Mine frequent patterns từ FP-tree\n\n**Ưu điểm:**\n- Nhanh hơn Apriori nhiều\n- Không generate candidates\n- Memory efficient (tree compression)\n\n**Nhược điểm:**\n- Complex implementation\n- FP-tree construction overhead\n\n**Thresholds:**\n- **min_support:** 0.01 - 0.1 (1% - 10%)\n- **min_confidence:** 0.5 - 0.8 (50% - 80%)\n- **min_lift:** > 1\n\n**Ứng dụng:**\n\n**1. Retail:**\n- Product recommendations\n- Store layout optimization\n- Cross-selling strategies\n\n**2. Web Mining:**\n- Clickstream analysis\n- Page recommendation\n- User behavior patterns\n\n**3. Healthcare:**\n- Symptom-disease associations\n- Drug interactions\n- Treatment effectiveness\n\n**4. Telecommunications:**\n- Calling patterns\n- Service bundles\n- Churn prediction\n\n### Đánh Giá Clustering\n\nLàm thế nào đánh giá quality của clustering khi không có ground truth?\n\n**Internal Metrics (Không Cần Ground Truth):**\n\n**1. Silhouette Coefficient:**\n$$s = \frac{b - a}{\\max(a,b)}$$\n\n- Range: [-1, 1]\n- 1: Perfect\n- 0: Overlapping clusters\n- -1: Wrong assignment\n\n**2. Davies-Bouldin Index:**\n$$DB = \frac{1}{K}\\sum_{i=1}^{K}\\max_{j \neq i}\\left(\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)}\right)$$\n\n- Lower is better\n- Ratio of within-cluster to between-cluster distances\n\n**3. Calinski-Harabasz Index (Variance Ratio Criterion):**\n$$CH = \frac{SS_B/(K-1)}{SS_W/(N-K)}$$\n\n- Higher is better\n- Ratio of between-cluster to within-cluster variance\n\n**4. Within-Cluster Sum of Squares (WCSS):**\n- Used trong elbow method\n- Lower is better\n- Decreases với more clusters\n\n**External Metrics (Có Ground Truth):**\n\n**1. Adjusted Rand Index (ARI):**\n- Measure agreement between two clusterings\n- Adjusted for chance\n- Range: [-1, 1], 1 = perfect match\n\n**2. Normalized Mutual Information (NMI):**\n- Information theoretic measure\n- Range: [0, 1], 1 = perfect match\n- Không bị ảnh hưởng bởi số clusters\n\n**3. Fowlkes-Mallows Index:**\n- Geometric mean của precision và recall\n- Range: [0, 1]\n\n**4. Purity:**\n$$Purity = \frac{1}{N}\\sum_{k=1}^{K}\\max_j|C_k \\cap T_j|$$\n\n- Simple, intuitive\n- Increases với more clusters (biased)\n\n**Chọn Metrics:**\n- **No ground truth:** Silhouette, Davies-Bouldin\n- **Have ground truth:** ARI, NMI\n- **Exploratory:** Multiple metrics\n- **Business context:** Domain-specific metrics\n\n### Ứng Dụng Của Unsupervised Learning\n\n**1. Customer Segmentation:**\n- Nhóm customers theo behavior\n- Targeted marketing\n- Personalization\n- Churn prediction\n\n**2. Image Compression:**\n- K-Means cho color quantization\n- Reduce number of colors\n- Smaller file size\n\n**3. Anomaly Detection:**\n- Fraud detection (credit cards, insurance)\n- Network intrusion detection\n- Manufacturing quality control\n- Medical diagnosis (rare diseases)\n\n**4. Recommender Systems:**\n- Collaborative filtering\n- User clustering\n- Item clustering\n\n**Các khái niệm quan trọng:**\n- Davies-Bouldin Index là một thước đo nội bộ (internal metric) để đánh giá chất lượng của phân cụm. Nó được tính bằng $$DB = \frac{1}{K}\\sum_{i=1}^{K}\\max_{j \neq i}\\left(\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)}\right)$$, trong đó K là số cụm, σᵢ là độ phân tán trung bình trong cụm i, và d(cᵢ, cⱼ) là khoảng cách giữa tâm cụm i và j. Giá trị Davies-Bouldin Index càng thấp càng tốt, cho thấy các cụm được phân tách tốt và có độ chặt chẽ cao.\n- Calinski-Harabasz Index (còn gọi là Variance Ratio Criterion) là một thước đo nội bộ (internal metric) để đánh giá chất lượng của phân cụm. Nó được tính bằng $$CH = \frac{SS_B/(K-1)}{SS_W/(N-K)}$$, trong đó SS_B là tổng bình phương khoảng cách giữa các cụm (between-cluster variance), SS_W là tổng bình phương khoảng cách trong các cụm (within-cluster variance), K là số cụm, và N là tổng số điểm dữ liệu. Giá trị Calinski-Harabasz Index càng cao càng tốt, cho thấy các cụm được phân tách rõ ràng và có độ chặt chẽ cao.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**Bước 4: Repeat**\n- Lặp lại Steps 2-3 cho đến khi convergence\n\n**Convergence khi:**\n- Centroids không đổi\n- Assignments không đổi\n- Đạt max iterations\n\n**Objective Function (WCSS - Within-Cluster Sum of Squares):**\n$$J = \\sum_{k=1}^{K}\\sum_{x \\in C_k}||x - \\mu_k||^2$$\n\nMục tiêu: Minimize J\n\n**Chọn K (Số Clusters):**\n\n**1. Elbow Method:**\n- Vẽ WCSS vs K\n- Tìm \"khuỷu tay\" (elbow) - điểm mà WCSS giảm chậm lại\n- Tradeoff giữa số clusters và fit\n\n**Ví dụ:**\n```\nWCSS\n  |  \\\n  |    \\\n  |      \\___\n  |          ----___\n  +----------------->\n  1  2  3  4  5  6  K\n       ↑ Elbow ~ K=3\n```\n\n**2. Silhouette Score:**\n$$s(i) = \frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n\nTrong đó:\n- $a(i)$: Average distance đến các điểm trong cùng cluster\n- $b(i)$: Average distance đến các điểm trong nearest cluster\n- Score: -1 đến 1\n  - ~1: Tốt, điểm xa cluster khác\n  - ~0: Gần boundary\n  - Negative: Có thể assign sai\n\n**Average Silhouette Score:**\n- Trung bình trên tất cả điểm\n- Chọn K có score cao nhất\n\n**3. Gap Statistic:**\n- So sánh WCSS với expected WCSS dưới null distribution\n- Chọn K where gap lớn nhất\n\n**4. Domain Knowledge:**\n- Business requirements\n- Interpretability\n- Practical constraints\n\n**Ưu Điểm:**\n- Đơn giản, dễ implement\n- Nhanh, scalable\n- Hoạt động tốt với spherical clusters\n- Dễ interpret\n\n**Nhược Điểm:**\n\n**1. Phải chỉ định K trước:**\n- Không biết K optimal\n- Cần thử nhiều giá trị\n\n**2. Nhạy cảm với initialization:**\n- Different initializations → different results\n- Có thể stuck ở local minima\n\n**3. Giả định spherical clusters:**\n- Không tốt với elongated/irregular shapes\n- Equal-sized clusters\n\n**4. Nhạy cảm với outliers:**\n- Outliers ảnh hưởng đến centroids\n- Có thể tạo clusters cho outliers\n\n**5. Phụ thuộc vào scale:**\n- Features có scale lớn dominate\n- Cần scaling trước\n\n**Cải Tiến:**\n\n**K-Means++:**\n- Better initialization strategy\n- Chọn centroids xa nhau\n- Giảm chance của bad initialization\n- Convergence nhanh hơn\n\n**Mini-batch K-Means:**\n- Sử dụng random mini-batches\n- Nhanh hơn nhiều với large datasets\n- Trade-off: Hơi kém chính xác\n- Good cho online learning\n\n**Practical Tips:**\n- Luôn standardize features\n- Run multiple times với different initializations\n- Use K-Means++ initialization\n- Try different K values\n- Visualize results nếu có thể\n\n### Hierarchical Clustering (Phân Cụm Phân Cấp)\n\nXây dựng hierarchy của clusters mà không cần chỉ định K trước.\n\n**Đặc điểm:**\n- Tạo tree structure (dendrogram)\n- Có thể chọn số clusters sau\n- Two approaches: Agglomerative và Divisive\n\n**1. Agglomerative (Bottom-Up - Tích Tụ):**\n\n**Thuật toán:**\n1. Start: Mỗi điểm là một cluster (N clusters)\n2. Repeat:\n   - Tìm 2 clusters gần nhất\n\n**Các khái niệm quan trọng:**\n- Silhouette Score (hay Silhouette Coefficient) là một thước đo nội bộ được sử dụng để đánh giá chất lượng của các cụm được tạo ra bởi thuật toán phân cụm, đặc biệt là K-Means, khi không có ground truth. Nó đo lường mức độ tương đồng của một đối tượng với cụm của chính nó so với các cụm khác. Silhouette Score được tính cho mỗi điểm dữ liệu i bằng công thức s(i) = (b(i) - a(i)) / max(a(i), b(i)), trong đó a(i) là khoảng cách trung bình đến các điểm trong cùng cụm và b(i) là khoảng cách trung bình đến các điểm trong cụm gần nhất (cụm gần nhất). Giá trị của Silhouette Score nằm trong khoảng từ -1 đến 1: giá trị gần 1 cho thấy cụm tốt và được phân tách hoàn hảo, giá trị gần 0 cho thấy điểm gần ranh giới cụm hoặc các cụm chồng chéo, và giá trị âm (gần -1) cho thấy điểm có thể đã được gán sai cụm. Average Silhouette Score là giá trị trung bình của tất cả các điểm, và giá trị K có score cao nhất thường được chọn để xác định số lượng cụm tối ưu.\n- b(i) trong Silhouette Score là khoảng cách trung bình từ điểm dữ liệu i đến tất cả các điểm trong cụm láng giềng gần nhất (nearest cluster). Nó đo lường mức độ không gắn kết của điểm i với cụm của nó so với cụm láng giềng. Giá trị b(i) càng lớn thì điểm i càng được phân tách tốt khỏi các cụm khác.\n- Hierarchical Clustering (Phân cụm phân cấp) là một loại thuật toán phân cụm xây dựng một hệ thống phân cấp các cụm, thường được biểu diễn dưới dạng cây (dendrogram). Không giống như K-Means, nó không yêu cầu chỉ định số lượng cụm K trước, mà số lượng cụm có thể được chọn sau khi cây phân cấp đã được xây dựng. Có hai phương pháp chính: Agglomerative (tích tụ, từ dưới lên) và Divisive (phân chia, từ trên xuống). Agglomerative Clustering là một thuật toán phân cụm phân cấp theo phương pháp Bottom-Up (từ dưới lên), bắt đầu với mỗi điểm dữ liệu là một cụm riêng biệt. Sau đó, nó lặp lại việc tìm và hợp nhất hai cụm gần nhất thành một cụm lớn hơn cho đến khi chỉ còn một cụm duy nhất hoặc đạt được một tiêu chí dừng nào đó, tạo ra một cấu trúc cây (dendrogram) thể hiện mối quan hệ phân cấp giữa các cụm.\n- Công thức s(i) = (b(i) - a(i)) / max(a(i), b(i)) là công thức tính Silhouette Score cho một điểm dữ liệu i. Trong đó, a(i) là khoảng cách trung bình từ điểm i đến tất cả các điểm khác trong cùng cụm, và b(i) là khoảng cách trung bình từ điểm i đến tất cả các điểm trong cụm gần nhất (cụm láng giềng). Silhouette Score đo lường mức độ tương đồng của một điểm với cụm của nó so với các cụm khác, với giá trị từ -1 đến 1. Giá trị cao hơn cho thấy cụm tốt hơn.\n\n**Mối quan hệ:**\n- Silhouette Score được tính toán bằng công thức s(i) = (b(i) - a(i)) / max(a(i), b(i)), sử dụng khoảng cách trung bình trong cùng cụm và đến cụm gần nhất.\n- Agglomerative Clustering có thể sử dụng Ward's Method để đo khoảng cách giữa các cụm khi quyết định hợp nhất.\n- Agglomerative Clustering tạo ra một Dendrogram để trực quan hóa cấu trúc phân cấp của các cụm.\n- Silhouette Score được sử dụng để kiểm soát và chọn giá trị K tối ưu bằng cách tính điểm trung bình cho các giá trị K khác nhau và chọn K có điểm cao nhất, cho thấy chất lượng cụm tốt nhất.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Lựa Chọn Đặc Trưng & Tối Ưu Hóa Mô Hình\n- Precision, Recall, F1, AUC\n- Business metrics\n\n**6. Avoid Data Leakage:**\n- **Proper CV:** Fit preprocessors trên train folds only\n- **Time-based splits:** cho time series\n- **No target leakage:** Features không chứa info về target\n- **Test set untouched:** Cho đến cuối\n\n**7. Document Everything:**\n- Experiments log\n- Model versions\n- Hyperparameters\n- Results và insights\n\n**8. Reproducibility:**\n- Set random seeds\n- Version control code\n- Save data versions\n- Document environment\n- Use containers (Docker)\n\n**9. Model Versioning:**\n- MLflow, DVC\n- Track models\n- Compare versions\n- Rollback nếu cần\n\n**10. Validation Strategy:**\n- Robust CV\n- Hold-out test set\n- Temporal validation cho time series\n\n**11. Feature Engineering First:**\n- \"Data > Algorithms\"\n- Good features > Complex models\n- Domain knowledge valuable\n\n**12. Monitor Training:**\n- Training vs validation\n- Learning curves\n- Early signs of overfitting\n\n**13. Consider Production:**\n- Inference time\n- Model size\n- Dependencies\n- Maintenance\n- Explainability\n\n**14. Test on Real Data:**\n- Not just metrics\n- Qualitative analysis\n- Edge cases\n- Failure modes\n\n---\n\n---\n\n## Học Không Giám Sát (Unsupervised Learning)\n\n### Giới Thiệu Về Học Không Giám Sát\n\nHọc không giám sát khám phá các mẫu ẩn trong dữ liệu không có nhãn mà không cần biến mục tiêu tường minh. Nó được sử dụng cho phân tích dữ liệu khám phá, nhận dạng mẫu và nén dữ liệu.\n\n**Đặc điểm chính:**\n- Không có labels (y)\n- Chỉ có features (X)\n- Tìm structure trong data\n- Exploratory analysis\n\n**So với Supervised Learning:**\n| Tiêu chí | Supervised | Unsupervised |\n|----------|-----------|--------------|\n| Labels | Có | Không |\n| Mục tiêu | Dự đoán | Khám phá |\n| Feedback | Có (accuracy) | Không rõ ràng |\n| Ứng dụng | Classification, Regression | Clustering, Dimensionality Reduction |\n\n**Các tác vụ chính:**\n1. **Clustering:** Nhóm dữ liệu tương tự\n2. **Dimensionality Reduction:** Giảm số chiều\n3. **Anomaly Detection:** Phát hiện bất thường\n4. **Association Rule Learning:** Tìm mối quan hệ\n\n**Thách thức:**\n- Không có ground truth để đánh giá\n- Khó xác định số clusters/components\n- Kết quả có thể subjective\n- Cần domain knowledge để interpret\n\n### Clustering (Phân Cụm)\n\nNhóm các điểm dữ liệu tương tự lại với nhau.\n\n**Mục tiêu:**\n- High intra-cluster similarity (trong cùng cluster)\n- Low inter-cluster similarity (giữa các clusters)\n\n**Ứng dụng:**\n- Customer segmentation\n- Document clustering\n- Image segmentation\n- Anomaly detection\n- Data compression\n\n### K-Means Clustering\n\nThuật toán phân cụm phổ biến nhất, chia dữ liệu thành K clusters.\n\n**Thuật toán:**\n\n**Bước 1: Initialization**\n- Chọn K centroids ngẫu nhiên\n- Có thể từ data points hoặc random positions\n\n**Bước 2: Assignment**\n- Gán mỗi điểm đến centroid gần nhất\n- Sử dụng Euclidean distance:\n$$d(x, \\mu_k) = ||x - \\mu_k|| = \\sqrt{\\sum_{j=1}^{n}(x_j - \\mu_{kj})^2}$$\n\n**Bước 3: Update**\n- Cập nhật centroids = mean của các điểm assigned\n$$\\mu_k = \frac{1}{|C_k|}\\sum_{x \\in C_k}x$$\n\n\n**Các khái niệm quan trọng:**\n- Clustering (Phân cụm) là một tác vụ trong học không giám sát nhằm nhóm các điểm dữ liệu tương tự lại với nhau. Mục tiêu là đạt được sự tương đồng cao trong cùng một cụm (high intra-cluster similarity) và sự khác biệt thấp giữa các cụm (low inter-cluster similarity).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n   - Merge chúng thành 1 cluster\n3. Until: Chỉ còn 1 cluster\n\n**Steps chi tiết:**\n- Initialize: N clusters\n- Iteration 1: N-1 clusters\n- Iteration 2: N-2 clusters\n- ...\n- Final: 1 cluster\n\n**2. Divisive (Top-Down - Phân Chia):**\n\n**Thuật toán:**\n1. Start: Tất cả điểm trong 1 cluster\n2. Repeat:\n   - Chọn cluster để split\n   - Chia thành 2 sub-clusters\n3. Until: Mỗi điểm là 1 cluster\n\n**Ít phổ biến:** Computationally expensive hơn\n\n**Linkage Methods (Cách Đo Khoảng Cách Giữa Clusters):**\n\n**1. Single Linkage (Minimum):**\n$$d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoảng cách giữa 2 điểm gần nhất\n- Tạo long, chain-like clusters\n- Sensitive to noise và outliers\n\n**2. Complete Linkage (Maximum):**\n$$d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoảng cách giữa 2 điểm xa nhất\n- Tạo compact, spherical clusters\n- Ít sensitive to outliers\n\n**3. Average Linkage:**\n$$d(C_i, C_j) = \frac{1}{|C_i||C_j|}\\sum_{x \\in C_i}\\sum_{y \\in C_j}d(x,y)$$\n- Trung bình tất cả pairwise distances\n- Balance giữa single và complete\n- Phổ biến choice\n\n**4. Ward's Method:**\n- Minimize within-cluster variance sau khi merge\n- Maximize between-cluster variance\n- Tạo balanced, compact clusters\n- Thường cho kết quả tốt nhất\n- Phổ biến nhất trong thực tế\n\n**Dendrogram (Biểu Đồ Cây):**\n\nTree diagram showing cluster hierarchy.\n\n**Đọc Dendrogram:**\n- Vertical axis: Distance/dissimilarity\n- Horizontal axis: Samples\n- Height của merge: Distance giữa clusters\n- Càng cao merge càng dissimilar\n\n**Cutting Dendrogram:**\n- Vẽ horizontal line\n- Number of intersections = Number of clusters\n- Height của cut = dissimilarity threshold\n\n**Ưu Điểm:**\n- Không cần specify K trước\n- Dendrogram provides insights\n- Flexible - có thể chọn K sau\n- Deterministic (no randomness)\n\n**Nhược Điểm:**\n- Computationally expensive: O(N²log N) or O(N³)\n- Không scale với large datasets\n- Một khi merge không thể undo\n- Memory intensive\n\n**Khi Nào Dùng:**\n- Small-medium datasets (< 10,000)\n- Cần understand hierarchy\n- Không biết K optimal\n- Exploratory analysis\n\n### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nNhóm các điểm có mật độ cao, robust to outliers và arbitrary shapes.\n\n**Tham Số:**\n\n**1. ε (epsilon):**\n- Maximum distance giữa 2 điểm để được coi là neighbors\n- Định nghĩa neighborhood radius\n- Quá nhỏ: Nhiều noise points\n- Quá lớn: Merge nhiều clusters\n\n**2. MinPts (Minimum Points):**\n- Minimum số điểm trong ε-neighborhood để là core point\n- Thường: 4, 5, hoặc 2×dim\n- Larger MinPts: Ít core points, stricter\n\n**Các Loại Điểm:**\n\n**1. Core Point:**\n- Có ≥ MinPts điểm khác trong ε-neighborhood (bao gồm cả chính nó)\n- Trung tâm của clusters\n- Can form clusters\n\n**2. Border Point:**\n- Nằm trong ε-neighborhood của core point\n- Có < MinPts neighbors\n- Thuộc cluster nhưng không core\n- Ở biên của cluster\n\n\n**Các khái niệm quan trọng:**\n- Hierarchical Clustering (Phân cụm phân cấp) là một loại thuật toán phân cụm xây dựng một hệ thống phân cấp các cụm, thường được biểu diễn dưới dạng cây (dendrogram). Không giống như K-Means, nó không yêu cầu chỉ định số lượng cụm K trước, mà số lượng cụm có thể được chọn sau khi cây phân cấp đã được xây dựng. Có hai phương pháp chính: Agglomerative (tích tụ, từ dưới lên) và Divisive (phân chia, từ trên xuống). Agglomerative Clustering là một thuật toán phân cụm phân cấp theo phương pháp Bottom-Up (từ dưới lên), bắt đầu với mỗi điểm dữ liệu là một cụm riêng biệt. Sau đó, nó lặp lại việc tìm và hợp nhất hai cụm gần nhất thành một cụm lớn hơn cho đến khi chỉ còn một cụm duy nhất hoặc đạt được một tiêu chí dừng nào đó, tạo ra một cấu trúc cây (dendrogram) thể hiện mối quan hệ phân cấp giữa các cụm.\n\n**Mối quan hệ:**\n- Agglomerative Clustering có thể sử dụng Ward's Method để đo khoảng cách giữa các cụm khi quyết định hợp nhất."
        },
        "output_without_context": {
            "question": "Trong các chỉ số đánh giá phân cụm không giám sát, chỉ số nào được sử dụng để đo lường độ tương đồng giữa các cụm và độ phân tán trong cùng một cụm?",
            "answer": "Hệ số Silhouette"
        }
    },
    {
        "topic_description": "Học sinh cần mô tả và giải thích thuật toán phân cụm phân cấp, cách hoạt động của nó và các biến thể của nó. Chủ đề này kết hợp kiến thức từ tuần hiện tại và trước đó về hình dạng và cấu trúc của dữ liệu.",
        "output_with_context": {
            "question": "Phân cụm phân cấp là thuật toán phân cụm nào không yêu cầu chỉ định số lượng cụm K trước và có thể xử lý các cụm có hình dạng bất kỳ?",
            "answer": "Phân cụm phân cấp (Hierarchical Clustering)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**3. Affinity Propagation:**\n\n**Nguyên lý:**\n- Message passing between data points\n- Points \"vote\" on their exemplars\n- Automatically determines K\n\n**Messages:**\n- **Responsibility:** Điểm $i$ chọn điểm $k$ làm exemplar\n- **Availability:** Điểm $k$ available làm exemplar cho $i$\n\n**Ưu điểm:**\n- Không cần specify K\n- Flexible\n- Finds exemplars (representative points)\n\n**Nhược điểm:**\n- Slow (O(N²T), T = iterations)\n- Memory intensive\n- Sensitive to preferences\n\n**4. OPTICS (Ordering Points To Identify Clustering Structure):**\n\n**Nguyên lý:**\n- Extension của DBSCAN\n- Tạo ordering của points\n- Handle varying densities better\n\n**Output:**\n- Reachability plot\n- Extract clusters ở different density levels\n\n**Ưu điểm:**\n- Varying densities\n- Không cần ε cụ thể\n- Hierarchical view\n\n**Nhược điểm:**\n- Complex interpretation\n- Still need MinPts\n\n**So Sánh Các Thuật Toán:**\n\n| Algorithm | K needed | Shape | Outliers | Speed | Best for |\n|-----------|----------|-------|----------|-------|----------|\n| K-Means | Yes | Spherical | Sensitive | Fast | Large, simple |\n| Hierarchical | No | Any | Sensitive | Slow | Small, hierarchy |\n| DBSCAN | No | Arbitrary | Robust | Medium | Spatial, noise |\n| GMM | Yes | Elliptical | Sensitive | Slow | Probabilistic |\n| Mean Shift | No | Arbitrary | Robust | Slow | Non-uniform |\n| Spectral | Yes | Complex | Sensitive | Slow | Graph-like |\n\n### Giảm Số Chiều (Dimensionality Reduction)\n\nĐã được cover chi tiết trong phần Feature Selection & Model Optimization, đây là summary.\n\n### Principal Component Analysis (PCA)\n\n**Nguyên lý:**\n- Linear transformation sang orthogonal components\n- Components ordered by variance\n- Maximize variance retained\n\n**Công thức:**\n$$Z = XW_k$$\n\nTrong đó $W_k$ là matrix của k eigenvectors.\n\n**Steps:**\n1. Standardize data: $X' = \frac{X - \\mu}{\\sigma}$\n2. Covariance matrix: $\\Sigma = \frac{1}{n}X'^TX'$\n3. Eigendecomposition: $\\Sigma = V\\Lambda V^T$\n4. Chọn top k eigenvectors\n5. Transform: $Z = X'W_k$\n\n**Variance Explained:**\n$$\frac{\\lambda_k}{\\sum_{i=1}^{n}\\lambda_i} \times 100\\%$$\n\n**Chọn số components:**\n- Cumulative variance ≥ 95% hoặc 99%\n- Scree plot (elbow)\n- Kaiser criterion (eigenvalue > 1)\n\n**Ứng dụng:**\n- Visualization (2D/3D)\n- Noise reduction\n- Feature extraction\n- Speed up learning\n- Preprocessing\n\n### Singular Value Decomposition (SVD)\n\n**Matrix factorization:**\n$$X = U\\Sigma V^T$$\n\n- $U$: Left singular vectors (m × m)\n- $\\Sigma$: Singular values (m × n, diagonal)\n- $V^T$: Right singular vectors (n × n)\n\n**Quan hệ với PCA:**\n- PCA eigenvectors = right singular vectors\n- PCA eigenvalues = squared singular values\n\n**Ứng dụng:**\n- PCA computation\n- Latent Semantic Analysis (LSA)\n- Recommender systems (matrix completion)\n- Image compression\n- Data compression\n\n**Truncated SVD:**\n- Giữ top k components\n- Approximation: $X \\approx U_k\\Sigma_kV_k^T$\n\n### Independent Component Analysis (ICA)\n\n**Nguyên lý:**\n- Tách signal thành independent components\n- Maximize statistical independence\n- Non-Gaussian components\n\n**Model:**\n$$X = AS$$\n\nTrong đó:\n- $X$: Observed signals (mixed)\n- $A$: Mixing matrix (unknown)\n- $S$: Source signals (independent, unknown)\n\n**Mục tiêu:** Estimate $A$ và $S$ from $X$\n\n**Cocktail Party Problem:**\n- Multiple people talking simultaneously\n\n**Các khái niệm quan trọng:**\n- Hierarchical Clustering là một thuật toán phân cụm học không giám sát không yêu cầu chỉ định số lượng cụm K trước. Nó có thể xử lý các cụm có hình dạng bất kỳ nhưng nhạy cảm với các ngoại lệ và có tốc độ chậm. Thuật toán này phù hợp cho các tập dữ liệu nhỏ và khi cần một cái nhìn phân cấp về cấu trúc dữ liệu.\n- Hierarchical Clustering (Phân cụm phân cấp) là một loại thuật toán phân cụm xây dựng một hệ thống phân cấp các cụm, thường được biểu diễn dưới dạng cây (dendrogram). Không giống như K-Means, nó không yêu cầu chỉ định số lượng cụm K trước, mà số lượng cụm có thể được chọn sau khi cây phân cấp đã được xây dựng. Có hai phương pháp chính: Agglomerative (tích tụ, từ dưới lên) và Divisive (phân chia, từ trên xuống). Agglomerative Clustering là một thuật toán phân cụm phân cấp theo phương pháp Bottom-Up (từ dưới lên), bắt đầu với mỗi điểm dữ liệu là một cụm riêng biệt. Sau đó, nó lặp lại việc tìm và hợp nhất hai cụm gần nhất thành một cụm lớn hơn cho đến khi chỉ còn một cụm duy nhất hoặc đạt được một tiêu chí dừng nào đó, tạo ra một cấu trúc cây (dendrogram) thể hiện mối quan hệ phân cấp giữa các cụm.\n- Spectral Clustering là một thuật toán phân cụm học không giám sát sử dụng lý thuyết đồ thị để phân cụm dữ liệu. Thuật toán này hoạt động bằng cách xây dựng một đồ thị tương tự (similarity graph) từ dữ liệu, sau đó sử dụng các giá trị riêng (eigenvalues) và vector riêng (eigenvectors) của ma trận Laplacian của đồ thị để giảm chiều dữ liệu. Sau khi giảm chiều, thuật toán thực hiện K-Means trên không gian giảm chiều đó. Spectral Clustering yêu cầu chỉ định số lượng cụm K trước và rất hiệu quả trong việc tìm các cụm có hình dạng phức tạp, phi lồi (non-convex) hoặc có cấu trúc đồ thị. Tuy nhiên, nó nhạy cảm với các ngoại lệ và có tốc độ chậm.\n\n**Mối quan hệ:**\n- Học Không Giám Sát chứa Spectral Clustering như một thuật toán phân cụm sử dụng lý thuyết đồ thị.\n- Hierarchical Clustering tạo ra một dendrogram, một biểu đồ dạng cây minh họa cấu trúc phân cấp của các cụm, cho phép người dùng chọn số lượng cụm sau khi quá trình phân cụm hoàn tất.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n$$\\gamma_{ik} = \frac{\\pi_k\\mathcal{N}(x_i|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(x_i|\\mu_j, \\Sigma_j)}$$\n\n- $\\gamma_{ik}$: Probability điểm $i$ belongs to cluster $k$\n- $\\sum_{k=1}^{K}\\gamma_{ik} = 1$ for each $i$\n\n**M-Step (Maximization):**\nUpdate parameters dựa trên responsibilities:\n\n**Mixing coefficients:**\n$$\\pi_k = \frac{1}{N}\\sum_{i=1}^{N}\\gamma_{ik}$$\n\n**Means:**\n$$\\mu_k = \frac{\\sum_{i=1}^{N}\\gamma_{ik}x_i}{\\sum_{i=1}^{N}\\gamma_{ik}}$$\n\n**Covariances:**\n$$\\Sigma_k = \frac{\\sum_{i=1}^{N}\\gamma_{ik}(x_i-\\mu_k)(x_i-\\mu_k)^T}{\\sum_{i=1}^{N}\\gamma_{ik}}$$\n\n**Repeat E-Step và M-Step** cho đến convergence.\n\n**Convergence:**\n- Log-likelihood không thay đổi nhiều\n- Parameters stable\n- Đạt max iterations\n\n**Ưu Điểm:**\n\n**1. Soft clustering:**\n- Probability of membership cho mỗi cluster\n- Captures uncertainty\n- More nuanced than hard assignment\n\n**2. Flexible cluster shapes:**\n- Elliptical clusters\n- Different sizes\n- Different orientations\n\n**3. Probabilistic framework:**\n- Sound mathematical foundation\n- Can compute likelihoods\n- Model selection với BIC/AIC\n\n**4. Generative model:**\n- Có thể generate new samples\n- Understand data distribution\n\n**Nhược Điểm:**\n\n**1. Cần specify K:**\n- Không tự động determine\n- Use BIC/AIC để chọn\n\n**2. Sensitive to initialization:**\n- EM có thể converge to local optima\n- Run multiple times\n\n**3. Assumes Gaussian:**\n- Không phù hợp nếu data không Gaussian\n- Limited to elliptical shapes\n\n**4. Computationally expensive:**\n- Covariance matrix inversion\n- Slower than K-Means\n\n**5. Singular covariance matrices:**\n- Có thể xảy ra với small clusters\n- Need regularization\n\n**So Sánh K-Means vs GMM:**\n\n| Tiêu chí | K-Means | GMM |\n|----------|---------|-----|\n| Assignment | Hard | Soft (probabilistic) |\n| Cluster shape | Spherical | Elliptical |\n| Parameters | Centroids | Means + Covariances |\n| Speed | Nhanh | Chậm hơn |\n| Flexibility | Ít | Nhiều |\n| Probabilistic | Không | Có |\n\n**Chọn K:**\n- Bayesian Information Criterion (BIC)\n- Akaike Information Criterion (AIC)\n- Cross-validation\n- Silhouette score\n\n### Các Thuật Toán Clustering Khác\n\n**1. Mean Shift:**\n\n**Nguyên lý:**\n- Density-based, no need chỉ định số clusters\n- Shift points toward mode (density maxima)\n\n**Thuật toán:**\n1. Khởi tạo window around each point\n2. Tính mean của points trong window\n3. Shift center đến mean\n4. Repeat until convergence\n5. Points converging to same mode = same cluster\n\n**Ưu điểm:**\n- Không cần specify K\n- Arbitrary shapes\n- Automatic K\n\n**Nhược điểm:**\n- Slow (O(N²))\n- Bandwidth parameter critical\n\n**2. Spectral Clustering:**\n\n**Nguyên lý:**\n- Uses graph theory\n- Eigenvalues của similarity matrix\n- Good cho non-convex clusters\n\n**Steps:**\n1. Construct similarity graph\n2. Compute Laplacian matrix\n3. Eigenvalue decomposition\n4. K-Means trên eigenvectors\n\n**Ưu điểm:**\n- Handle complex shapes\n- Thường tốt hơn K-Means\n- Based on graph cuts\n\n**Nhược điểm:**\n- Computationally expensive\n- Cần tune similarity function\n- Sensitive to parameters\n\n\n**Các khái niệm quan trọng:**\n- Spectral Clustering là một thuật toán phân cụm học không giám sát sử dụng lý thuyết đồ thị để phân cụm dữ liệu. Thuật toán này hoạt động bằng cách xây dựng một đồ thị tương tự (similarity graph) từ dữ liệu, sau đó sử dụng các giá trị riêng (eigenvalues) và vector riêng (eigenvectors) của ma trận Laplacian của đồ thị để giảm chiều dữ liệu. Sau khi giảm chiều, thuật toán thực hiện K-Means trên không gian giảm chiều đó. Spectral Clustering yêu cầu chỉ định số lượng cụm K trước và rất hiệu quả trong việc tìm các cụm có hình dạng phức tạp, phi lồi (non-convex) hoặc có cấu trúc đồ thị. Tuy nhiên, nó nhạy cảm với các ngoại lệ và có tốc độ chậm.\n\n**Mối quan hệ:**\n- Học Không Giám Sát chứa Spectral Clustering như một thuật toán phân cụm sử dụng lý thuyết đồ thị.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n   - Merge chúng thành 1 cluster\n3. Until: Chỉ còn 1 cluster\n\n**Steps chi tiết:**\n- Initialize: N clusters\n- Iteration 1: N-1 clusters\n- Iteration 2: N-2 clusters\n- ...\n- Final: 1 cluster\n\n**2. Divisive (Top-Down - Phân Chia):**\n\n**Thuật toán:**\n1. Start: Tất cả điểm trong 1 cluster\n2. Repeat:\n   - Chọn cluster để split\n   - Chia thành 2 sub-clusters\n3. Until: Mỗi điểm là 1 cluster\n\n**Ít phổ biến:** Computationally expensive hơn\n\n**Linkage Methods (Cách Đo Khoảng Cách Giữa Clusters):**\n\n**1. Single Linkage (Minimum):**\n$$d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoảng cách giữa 2 điểm gần nhất\n- Tạo long, chain-like clusters\n- Sensitive to noise và outliers\n\n**2. Complete Linkage (Maximum):**\n$$d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoảng cách giữa 2 điểm xa nhất\n- Tạo compact, spherical clusters\n- Ít sensitive to outliers\n\n**3. Average Linkage:**\n$$d(C_i, C_j) = \frac{1}{|C_i||C_j|}\\sum_{x \\in C_i}\\sum_{y \\in C_j}d(x,y)$$\n- Trung bình tất cả pairwise distances\n- Balance giữa single và complete\n- Phổ biến choice\n\n**4. Ward's Method:**\n- Minimize within-cluster variance sau khi merge\n- Maximize between-cluster variance\n- Tạo balanced, compact clusters\n- Thường cho kết quả tốt nhất\n- Phổ biến nhất trong thực tế\n\n**Dendrogram (Biểu Đồ Cây):**\n\nTree diagram showing cluster hierarchy.\n\n**Đọc Dendrogram:**\n- Vertical axis: Distance/dissimilarity\n- Horizontal axis: Samples\n- Height của merge: Distance giữa clusters\n- Càng cao merge càng dissimilar\n\n**Cutting Dendrogram:**\n- Vẽ horizontal line\n- Number of intersections = Number of clusters\n- Height của cut = dissimilarity threshold\n\n**Ưu Điểm:**\n- Không cần specify K trước\n- Dendrogram provides insights\n- Flexible - có thể chọn K sau\n- Deterministic (no randomness)\n\n**Nhược Điểm:**\n- Computationally expensive: O(N²log N) or O(N³)\n- Không scale với large datasets\n- Một khi merge không thể undo\n- Memory intensive\n\n**Khi Nào Dùng:**\n- Small-medium datasets (< 10,000)\n- Cần understand hierarchy\n- Không biết K optimal\n- Exploratory analysis\n\n### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nNhóm các điểm có mật độ cao, robust to outliers và arbitrary shapes.\n\n**Tham Số:**\n\n**1. ε (epsilon):**\n- Maximum distance giữa 2 điểm để được coi là neighbors\n- Định nghĩa neighborhood radius\n- Quá nhỏ: Nhiều noise points\n- Quá lớn: Merge nhiều clusters\n\n**2. MinPts (Minimum Points):**\n- Minimum số điểm trong ε-neighborhood để là core point\n- Thường: 4, 5, hoặc 2×dim\n- Larger MinPts: Ít core points, stricter\n\n**Các Loại Điểm:**\n\n**1. Core Point:**\n- Có ≥ MinPts điểm khác trong ε-neighborhood (bao gồm cả chính nó)\n- Trung tâm của clusters\n- Can form clusters\n\n**2. Border Point:**\n- Nằm trong ε-neighborhood của core point\n- Có < MinPts neighbors\n- Thuộc cluster nhưng không core\n- Ở biên của cluster\n\n\n**Các khái niệm quan trọng:**\n- Divisive Clustering là một thuật toán phân cụm phân cấp (hierarchical clustering) theo phương pháp Top-Down (từ trên xuống). Nó bắt đầu với tất cả các điểm dữ liệu trong một cụm duy nhất và sau đó lặp lại việc chia các cụm thành hai cụm con cho đến khi mỗi điểm là một cụm riêng biệt hoặc đạt được một tiêu chí dừng nào đó. Thuật toán này ít phổ biến hơn do chi phí tính toán cao hơn.\n- Hierarchical Clustering (Phân cụm phân cấp) là một loại thuật toán phân cụm xây dựng một hệ thống phân cấp các cụm, thường được biểu diễn dưới dạng cây (dendrogram). Không giống như K-Means, nó không yêu cầu chỉ định số lượng cụm K trước, mà số lượng cụm có thể được chọn sau khi cây phân cấp đã được xây dựng. Có hai phương pháp chính: Agglomerative (tích tụ, từ dưới lên) và Divisive (phân chia, từ trên xuống). Agglomerative Clustering là một thuật toán phân cụm phân cấp theo phương pháp Bottom-Up (từ dưới lên), bắt đầu với mỗi điểm dữ liệu là một cụm riêng biệt. Sau đó, nó lặp lại việc tìm và hợp nhất hai cụm gần nhất thành một cụm lớn hơn cho đến khi chỉ còn một cụm duy nhất hoặc đạt được một tiêu chí dừng nào đó, tạo ra một cấu trúc cây (dendrogram) thể hiện mối quan hệ phân cấp giữa các cụm.\n\n**Mối quan hệ:**\n- Divisive Clustering là một thuật toán thuộc lĩnh vực Học Không Giám Sát, được sử dụng để phân cụm dữ liệu không nhãn.\n- Agglomerative Clustering là một thuật toán thuộc lĩnh vực Học Không Giám Sát, được sử dụng để phân cụm dữ liệu không nhãn.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Lựa Chọn Đặc Trưng & Tối Ưu Hóa Mô Hình\n- Precision, Recall, F1, AUC\n- Business metrics\n\n**6. Avoid Data Leakage:**\n- **Proper CV:** Fit preprocessors trên train folds only\n- **Time-based splits:** cho time series\n- **No target leakage:** Features không chứa info về target\n- **Test set untouched:** Cho đến cuối\n\n**7. Document Everything:**\n- Experiments log\n- Model versions\n- Hyperparameters\n- Results và insights\n\n**8. Reproducibility:**\n- Set random seeds\n- Version control code\n- Save data versions\n- Document environment\n- Use containers (Docker)\n\n**9. Model Versioning:**\n- MLflow, DVC\n- Track models\n- Compare versions\n- Rollback nếu cần\n\n**10. Validation Strategy:**\n- Robust CV\n- Hold-out test set\n- Temporal validation cho time series\n\n**11. Feature Engineering First:**\n- \"Data > Algorithms\"\n- Good features > Complex models\n- Domain knowledge valuable\n\n**12. Monitor Training:**\n- Training vs validation\n- Learning curves\n- Early signs of overfitting\n\n**13. Consider Production:**\n- Inference time\n- Model size\n- Dependencies\n- Maintenance\n- Explainability\n\n**14. Test on Real Data:**\n- Not just metrics\n- Qualitative analysis\n- Edge cases\n- Failure modes\n\n---\n\n---\n\n## Học Không Giám Sát (Unsupervised Learning)\n\n### Giới Thiệu Về Học Không Giám Sát\n\nHọc không giám sát khám phá các mẫu ẩn trong dữ liệu không có nhãn mà không cần biến mục tiêu tường minh. Nó được sử dụng cho phân tích dữ liệu khám phá, nhận dạng mẫu và nén dữ liệu.\n\n**Đặc điểm chính:**\n- Không có labels (y)\n- Chỉ có features (X)\n- Tìm structure trong data\n- Exploratory analysis\n\n**So với Supervised Learning:**\n| Tiêu chí | Supervised | Unsupervised |\n|----------|-----------|--------------|\n| Labels | Có | Không |\n| Mục tiêu | Dự đoán | Khám phá |\n| Feedback | Có (accuracy) | Không rõ ràng |\n| Ứng dụng | Classification, Regression | Clustering, Dimensionality Reduction |\n\n**Các tác vụ chính:**\n1. **Clustering:** Nhóm dữ liệu tương tự\n2. **Dimensionality Reduction:** Giảm số chiều\n3. **Anomaly Detection:** Phát hiện bất thường\n4. **Association Rule Learning:** Tìm mối quan hệ\n\n**Thách thức:**\n- Không có ground truth để đánh giá\n- Khó xác định số clusters/components\n- Kết quả có thể subjective\n- Cần domain knowledge để interpret\n\n### Clustering (Phân Cụm)\n\nNhóm các điểm dữ liệu tương tự lại với nhau.\n\n**Mục tiêu:**\n- High intra-cluster similarity (trong cùng cluster)\n- Low inter-cluster similarity (giữa các clusters)\n\n**Ứng dụng:**\n- Customer segmentation\n- Document clustering\n- Image segmentation\n- Anomaly detection\n- Data compression\n\n### K-Means Clustering\n\nThuật toán phân cụm phổ biến nhất, chia dữ liệu thành K clusters.\n\n**Thuật toán:**\n\n**Bước 1: Initialization**\n- Chọn K centroids ngẫu nhiên\n- Có thể từ data points hoặc random positions\n\n**Bước 2: Assignment**\n- Gán mỗi điểm đến centroid gần nhất\n- Sử dụng Euclidean distance:\n$$d(x, \\mu_k) = ||x - \\mu_k|| = \\sqrt{\\sum_{j=1}^{n}(x_j - \\mu_{kj})^2}$$\n\n**Bước 3: Update**\n- Cập nhật centroids = mean của các điểm assigned\n$$\\mu_k = \frac{1}{|C_k|}\\sum_{x \\in C_k}x$$\n\n\n**Các khái niệm quan trọng:**\n- Clustering (Phân cụm) là một tác vụ trong học không giám sát nhằm nhóm các điểm dữ liệu tương tự lại với nhau. Mục tiêu là đạt được sự tương đồng cao trong cùng một cụm (high intra-cluster similarity) và sự khác biệt thấp giữa các cụm (low inter-cluster similarity).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**Bước 4: Repeat**\n- Lặp lại Steps 2-3 cho đến khi convergence\n\n**Convergence khi:**\n- Centroids không đổi\n- Assignments không đổi\n- Đạt max iterations\n\n**Objective Function (WCSS - Within-Cluster Sum of Squares):**\n$$J = \\sum_{k=1}^{K}\\sum_{x \\in C_k}||x - \\mu_k||^2$$\n\nMục tiêu: Minimize J\n\n**Chọn K (Số Clusters):**\n\n**1. Elbow Method:**\n- Vẽ WCSS vs K\n- Tìm \"khuỷu tay\" (elbow) - điểm mà WCSS giảm chậm lại\n- Tradeoff giữa số clusters và fit\n\n**Ví dụ:**\n```\nWCSS\n  |  \\\n  |    \\\n  |      \\___\n  |          ----___\n  +----------------->\n  1  2  3  4  5  6  K\n       ↑ Elbow ~ K=3\n```\n\n**2. Silhouette Score:**\n$$s(i) = \frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n\nTrong đó:\n- $a(i)$: Average distance đến các điểm trong cùng cluster\n- $b(i)$: Average distance đến các điểm trong nearest cluster\n- Score: -1 đến 1\n  - ~1: Tốt, điểm xa cluster khác\n  - ~0: Gần boundary\n  - Negative: Có thể assign sai\n\n**Average Silhouette Score:**\n- Trung bình trên tất cả điểm\n- Chọn K có score cao nhất\n\n**3. Gap Statistic:**\n- So sánh WCSS với expected WCSS dưới null distribution\n- Chọn K where gap lớn nhất\n\n**4. Domain Knowledge:**\n- Business requirements\n- Interpretability\n- Practical constraints\n\n**Ưu Điểm:**\n- Đơn giản, dễ implement\n- Nhanh, scalable\n- Hoạt động tốt với spherical clusters\n- Dễ interpret\n\n**Nhược Điểm:**\n\n**1. Phải chỉ định K trước:**\n- Không biết K optimal\n- Cần thử nhiều giá trị\n\n**2. Nhạy cảm với initialization:**\n- Different initializations → different results\n- Có thể stuck ở local minima\n\n**3. Giả định spherical clusters:**\n- Không tốt với elongated/irregular shapes\n- Equal-sized clusters\n\n**4. Nhạy cảm với outliers:**\n- Outliers ảnh hưởng đến centroids\n- Có thể tạo clusters cho outliers\n\n**5. Phụ thuộc vào scale:**\n- Features có scale lớn dominate\n- Cần scaling trước\n\n**Cải Tiến:**\n\n**K-Means++:**\n- Better initialization strategy\n- Chọn centroids xa nhau\n- Giảm chance của bad initialization\n- Convergence nhanh hơn\n\n**Mini-batch K-Means:**\n- Sử dụng random mini-batches\n- Nhanh hơn nhiều với large datasets\n- Trade-off: Hơi kém chính xác\n- Good cho online learning\n\n**Practical Tips:**\n- Luôn standardize features\n- Run multiple times với different initializations\n- Use K-Means++ initialization\n- Try different K values\n- Visualize results nếu có thể\n\n### Hierarchical Clustering (Phân Cụm Phân Cấp)\n\nXây dựng hierarchy của clusters mà không cần chỉ định K trước.\n\n**Đặc điểm:**\n- Tạo tree structure (dendrogram)\n- Có thể chọn số clusters sau\n- Two approaches: Agglomerative và Divisive\n\n**1. Agglomerative (Bottom-Up - Tích Tụ):**\n\n**Thuật toán:**\n1. Start: Mỗi điểm là một cluster (N clusters)\n2. Repeat:\n   - Tìm 2 clusters gần nhất\n\n**Các khái niệm quan trọng:**\n- Hierarchical Clustering là một thuật toán phân cụm học không giám sát không yêu cầu chỉ định số lượng cụm K trước. Nó có thể xử lý các cụm có hình dạng bất kỳ nhưng nhạy cảm với các ngoại lệ và có tốc độ chậm. Thuật toán này phù hợp cho các tập dữ liệu nhỏ và khi cần một cái nhìn phân cấp về cấu trúc dữ liệu.\n- Hierarchical Clustering (Phân cụm phân cấp) là một loại thuật toán phân cụm xây dựng một hệ thống phân cấp các cụm, thường được biểu diễn dưới dạng cây (dendrogram). Không giống như K-Means, nó không yêu cầu chỉ định số lượng cụm K trước, mà số lượng cụm có thể được chọn sau khi cây phân cấp đã được xây dựng. Có hai phương pháp chính: Agglomerative (tích tụ, từ dưới lên) và Divisive (phân chia, từ trên xuống). Agglomerative Clustering là một thuật toán phân cụm phân cấp theo phương pháp Bottom-Up (từ dưới lên), bắt đầu với mỗi điểm dữ liệu là một cụm riêng biệt. Sau đó, nó lặp lại việc tìm và hợp nhất hai cụm gần nhất thành một cụm lớn hơn cho đến khi chỉ còn một cụm duy nhất hoặc đạt được một tiêu chí dừng nào đó, tạo ra một cấu trúc cây (dendrogram) thể hiện mối quan hệ phân cấp giữa các cụm.\n\n**Mối quan hệ:**\n- Hierarchical Clustering tạo ra một dendrogram, một biểu đồ dạng cây minh họa cấu trúc phân cấp của các cụm, cho phép người dùng chọn số lượng cụm sau khi quá trình phân cụm hoàn tất.\n- Agglomerative Clustering là một thuật toán thuộc lĩnh vực Học Không Giám Sát, được sử dụng để phân cụm dữ liệu không nhãn."
        },
        "output_without_context": {
            "question": "Phân cụm phân cấp là thuật toán gì trong phân tích dữ liệu?",
            "answer": "Thuật toán phân cụm phân cấp là một phương pháp nhóm các đối tượng thành các cụm theo cấu trúc phân cấp."
        }
    },
    {
        "topic_description": "Khám phá ý tưởng chính của thuật toán phân cụm DBSCAN, bao gồm cách phát hiện các cụm dựa trên mật độ. Học sinh sẽ cần áp dụng kiến thức đã học về phân tách dữ liệu và kết hợp các khái niệm từ các tuần trước.",
        "output_with_context": {
            "question": "Trong thuật toán DBSCAN, tham số nào xác định khoảng cách tối đa giữa hai điểm để chúng được coi là hàng xóm?",
            "answer": "ε (epsilon)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n   - Merge chúng thành 1 cluster\n3. Until: Chỉ còn 1 cluster\n\n**Steps chi tiết:**\n- Initialize: N clusters\n- Iteration 1: N-1 clusters\n- Iteration 2: N-2 clusters\n- ...\n- Final: 1 cluster\n\n**2. Divisive (Top-Down - Phân Chia):**\n\n**Thuật toán:**\n1. Start: Tất cả điểm trong 1 cluster\n2. Repeat:\n   - Chọn cluster để split\n   - Chia thành 2 sub-clusters\n3. Until: Mỗi điểm là 1 cluster\n\n**Ít phổ biến:** Computationally expensive hơn\n\n**Linkage Methods (Cách Đo Khoảng Cách Giữa Clusters):**\n\n**1. Single Linkage (Minimum):**\n$$d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoảng cách giữa 2 điểm gần nhất\n- Tạo long, chain-like clusters\n- Sensitive to noise và outliers\n\n**2. Complete Linkage (Maximum):**\n$$d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoảng cách giữa 2 điểm xa nhất\n- Tạo compact, spherical clusters\n- Ít sensitive to outliers\n\n**3. Average Linkage:**\n$$d(C_i, C_j) = \frac{1}{|C_i||C_j|}\\sum_{x \\in C_i}\\sum_{y \\in C_j}d(x,y)$$\n- Trung bình tất cả pairwise distances\n- Balance giữa single và complete\n- Phổ biến choice\n\n**4. Ward's Method:**\n- Minimize within-cluster variance sau khi merge\n- Maximize between-cluster variance\n- Tạo balanced, compact clusters\n- Thường cho kết quả tốt nhất\n- Phổ biến nhất trong thực tế\n\n**Dendrogram (Biểu Đồ Cây):**\n\nTree diagram showing cluster hierarchy.\n\n**Đọc Dendrogram:**\n- Vertical axis: Distance/dissimilarity\n- Horizontal axis: Samples\n- Height của merge: Distance giữa clusters\n- Càng cao merge càng dissimilar\n\n**Cutting Dendrogram:**\n- Vẽ horizontal line\n- Number of intersections = Number of clusters\n- Height của cut = dissimilarity threshold\n\n**Ưu Điểm:**\n- Không cần specify K trước\n- Dendrogram provides insights\n- Flexible - có thể chọn K sau\n- Deterministic (no randomness)\n\n**Nhược Điểm:**\n- Computationally expensive: O(N²log N) or O(N³)\n- Không scale với large datasets\n- Một khi merge không thể undo\n- Memory intensive\n\n**Khi Nào Dùng:**\n- Small-medium datasets (< 10,000)\n- Cần understand hierarchy\n- Không biết K optimal\n- Exploratory analysis\n\n### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nNhóm các điểm có mật độ cao, robust to outliers và arbitrary shapes.\n\n**Tham Số:**\n\n**1. ε (epsilon):**\n- Maximum distance giữa 2 điểm để được coi là neighbors\n- Định nghĩa neighborhood radius\n- Quá nhỏ: Nhiều noise points\n- Quá lớn: Merge nhiều clusters\n\n**2. MinPts (Minimum Points):**\n- Minimum số điểm trong ε-neighborhood để là core point\n- Thường: 4, 5, hoặc 2×dim\n- Larger MinPts: Ít core points, stricter\n\n**Các Loại Điểm:**\n\n**1. Core Point:**\n- Có ≥ MinPts điểm khác trong ε-neighborhood (bao gồm cả chính nó)\n- Trung tâm của clusters\n- Can form clusters\n\n**2. Border Point:**\n- Nằm trong ε-neighborhood của core point\n- Có < MinPts neighbors\n- Thuộc cluster nhưng không core\n- Ở biên của cluster\n\n\n**Các khái niệm quan trọng:**\n- DBSCAN (Density-Based Spatial Clustering of Applications with Noise) là một thuật toán phân cụm học không giám sát dựa trên mật độ, không yêu cầu chỉ định số lượng cụm K trước. Thuật toán này có khả năng tìm các cụm có hình dạng tùy ý, nhóm các điểm có mật độ cao và mạnh mẽ với nhiễu (outliers). DBSCAN hoạt động bằng cách mở rộng các cụm từ các \"core points\" dựa trên hai tham số ε (bán kính) và MinPts (số điểm tối thiểu), có tốc độ trung bình và phù hợp cho dữ liệu không gian có nhiễu.\n\n**Mối quan hệ:**\n- DBSCAN là một thuật toán thuộc lĩnh vực Học Không Giám Sát, được sử dụng để phân cụm dữ liệu không nhãn.\n- DBSCAN cải thiện khả năng phân cụm bằng cách mạnh mẽ với các điểm ngoại lai, tự động coi chúng là nhiễu và không gộp vào các cụm chính.\n- DBSCAN cải thiện khả năng phân cụm bằng cách xử lý các cụm có hình dạng tùy ý, không giới hạn bởi giả định hình cầu.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**3. Affinity Propagation:**\n\n**Nguyên lý:**\n- Message passing between data points\n- Points \"vote\" on their exemplars\n- Automatically determines K\n\n**Messages:**\n- **Responsibility:** Điểm $i$ chọn điểm $k$ làm exemplar\n- **Availability:** Điểm $k$ available làm exemplar cho $i$\n\n**Ưu điểm:**\n- Không cần specify K\n- Flexible\n- Finds exemplars (representative points)\n\n**Nhược điểm:**\n- Slow (O(N²T), T = iterations)\n- Memory intensive\n- Sensitive to preferences\n\n**4. OPTICS (Ordering Points To Identify Clustering Structure):**\n\n**Nguyên lý:**\n- Extension của DBSCAN\n- Tạo ordering của points\n- Handle varying densities better\n\n**Output:**\n- Reachability plot\n- Extract clusters ở different density levels\n\n**Ưu điểm:**\n- Varying densities\n- Không cần ε cụ thể\n- Hierarchical view\n\n**Nhược điểm:**\n- Complex interpretation\n- Still need MinPts\n\n**So Sánh Các Thuật Toán:**\n\n| Algorithm | K needed | Shape | Outliers | Speed | Best for |\n|-----------|----------|-------|----------|-------|----------|\n| K-Means | Yes | Spherical | Sensitive | Fast | Large, simple |\n| Hierarchical | No | Any | Sensitive | Slow | Small, hierarchy |\n| DBSCAN | No | Arbitrary | Robust | Medium | Spatial, noise |\n| GMM | Yes | Elliptical | Sensitive | Slow | Probabilistic |\n| Mean Shift | No | Arbitrary | Robust | Slow | Non-uniform |\n| Spectral | Yes | Complex | Sensitive | Slow | Graph-like |\n\n### Giảm Số Chiều (Dimensionality Reduction)\n\nĐã được cover chi tiết trong phần Feature Selection & Model Optimization, đây là summary.\n\n### Principal Component Analysis (PCA)\n\n**Nguyên lý:**\n- Linear transformation sang orthogonal components\n- Components ordered by variance\n- Maximize variance retained\n\n**Công thức:**\n$$Z = XW_k$$\n\nTrong đó $W_k$ là matrix của k eigenvectors.\n\n**Steps:**\n1. Standardize data: $X' = \frac{X - \\mu}{\\sigma}$\n2. Covariance matrix: $\\Sigma = \frac{1}{n}X'^TX'$\n3. Eigendecomposition: $\\Sigma = V\\Lambda V^T$\n4. Chọn top k eigenvectors\n5. Transform: $Z = X'W_k$\n\n**Variance Explained:**\n$$\frac{\\lambda_k}{\\sum_{i=1}^{n}\\lambda_i} \times 100\\%$$\n\n**Chọn số components:**\n- Cumulative variance ≥ 95% hoặc 99%\n- Scree plot (elbow)\n- Kaiser criterion (eigenvalue > 1)\n\n**Ứng dụng:**\n- Visualization (2D/3D)\n- Noise reduction\n- Feature extraction\n- Speed up learning\n- Preprocessing\n\n### Singular Value Decomposition (SVD)\n\n**Matrix factorization:**\n$$X = U\\Sigma V^T$$\n\n- $U$: Left singular vectors (m × m)\n- $\\Sigma$: Singular values (m × n, diagonal)\n- $V^T$: Right singular vectors (n × n)\n\n**Quan hệ với PCA:**\n- PCA eigenvectors = right singular vectors\n- PCA eigenvalues = squared singular values\n\n**Ứng dụng:**\n- PCA computation\n- Latent Semantic Analysis (LSA)\n- Recommender systems (matrix completion)\n- Image compression\n- Data compression\n\n**Truncated SVD:**\n- Giữ top k components\n- Approximation: $X \\approx U_k\\Sigma_kV_k^T$\n\n### Independent Component Analysis (ICA)\n\n**Nguyên lý:**\n- Tách signal thành independent components\n- Maximize statistical independence\n- Non-Gaussian components\n\n**Model:**\n$$X = AS$$\n\nTrong đó:\n- $X$: Observed signals (mixed)\n- $A$: Mixing matrix (unknown)\n- $S$: Source signals (independent, unknown)\n\n**Mục tiêu:** Estimate $A$ và $S$ from $X$\n\n**Cocktail Party Problem:**\n- Multiple people talking simultaneously\n\n**Các khái niệm quan trọng:**\n- DBSCAN (Density-Based Spatial Clustering of Applications with Noise) là một thuật toán phân cụm học không giám sát dựa trên mật độ, không yêu cầu chỉ định số lượng cụm K trước. Thuật toán này có khả năng tìm các cụm có hình dạng tùy ý, nhóm các điểm có mật độ cao và mạnh mẽ với nhiễu (outliers). DBSCAN hoạt động bằng cách mở rộng các cụm từ các \"core points\" dựa trên hai tham số ε (bán kính) và MinPts (số điểm tối thiểu), có tốc độ trung bình và phù hợp cho dữ liệu không gian có nhiễu.\n\n**Mối quan hệ:**\n- DBSCAN là một thuật toán thuộc lĩnh vực Học Không Giám Sát, được sử dụng để phân cụm dữ liệu không nhãn.\n- DBSCAN cải thiện khả năng phân cụm bằng cách mạnh mẽ với các điểm ngoại lai, tự động coi chúng là nhiễu và không gộp vào các cụm chính.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Không Giám Sát (Unsupervised Learning)\n**3. Noise Point (Outlier):**\n- Không phải core point\n- Không nằm trong ε của bất kỳ core point nào\n- Isolated points\n\n**Thuật Toán:**\n\n1. For each unvisited point:\n   - Mark as visited\n   - Tìm ε-neighbors\n   - If neighbors < MinPts: Mark as noise\n   - Else: Start new cluster\n     - Add point và neighbors to cluster\n     - For each neighbor:\n       - If unvisited: Visit và expand cluster\n       - If noise: Add to cluster\n\n**Ưu Điểm:**\n\n**1. Arbitrary shapes:**\n- Không giả định spherical\n- Handle complex geometries\n- Non-convex clusters\n\n**2. Robust to outliers:**\n- Outliers = noise points\n- Không ảnh hưởng clusters\n\n**3. Không cần specify K:**\n- Tự động determine số clusters\n- Dựa trên density\n\n**4. Deterministic:**\n- Same parameters → same results (mostly)\n\n**Nhược Điểm:**\n\n**1. Sensitive to parameters:**\n- ε và MinPts khó chọn\n- Cần domain knowledge hoặc tuning\n\n**2. Varying densities:**\n- Một cặp (ε, MinPts) không phù hợp cho tất cả\n- Clusters với different densities problematic\n\n**3. High-dimensional data:**\n- Distance metrics less meaningful\n- Curse of dimensionality\n\n**4. Memory và computation:**\n- O(N²) worst case\n- Index structures help (KD-tree, Ball-tree)\n\n**Chọn Parameters:**\n\n**ε (epsilon):**\n- **K-distance graph:**\n  - Vẽ sorted k-distances (k=MinPts-1)\n  - Tìm \"knee\" - nơi tăng đột ngột\n  - ε = distance tại knee\n\n**MinPts:**\n- Rule of thumb: MinPts ≥ dim + 1\n- Thường: 4 hoặc 5\n- Larger for noisy data\n- Smaller for cleaner data\n\n**Variants:**\n\n**HDBSCAN (Hierarchical DBSCAN):**\n- Hierarchical approach\n- Không cần specify ε\n- Better với varying densities\n- Extract clusters ở different density levels\n\n**OPTICS:**\n- Ordering points to identify clustering structure\n- Tạo reachability plot\n- Flexible extraction\n\n**Khi Nào Dùng:**\n- Non-spherical clusters\n- Outliers present\n- Không biết K\n- Arbitrary shaped regions\n\n### Gaussian Mixture Models (GMM) - Mô Hình Hỗn Hợp Gaussian\n\nMô hình xác suất giả định data đến từ hỗn hợp các phân phối Gaussian.\n\n**Khái Niệm:**\n- Data generated từ K Gaussian distributions\n- Mỗi Gaussian = một cluster\n- Soft assignment (probabilities)\n\n**Model:**\n$$P(x) = \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(x|\\mu_k, \\Sigma_k)$$\n\nTrong đó:\n- $\\pi_k$: Mixing coefficient (weight) của component k\n  - $\\sum_{k=1}^{K}\\pi_k = 1$\n  - $0 \\leq \\pi_k \\leq 1$\n- $\\mu_k$: Mean vector của Gaussian k\n- $\\Sigma_k$: Covariance matrix của Gaussian k\n- $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$: Gaussian distribution\n\n**Gaussian Distribution:**\n$$\\mathcal{N}(x|\\mu, \\Sigma) = \frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}\\exp\\left(-\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\right)$$\n\n**Expectation-Maximization (EM) Algorithm:**\n\nIterative algorithm để estimate parameters.\n\n**E-Step (Expectation):**\nTính responsibility (xác suất mềm) của mỗi component cho mỗi điểm:\n\n\n**Các khái niệm quan trọng:**\n- DBSCAN (Density-Based Spatial Clustering of Applications with Noise) là một thuật toán phân cụm học không giám sát dựa trên mật độ, không yêu cầu chỉ định số lượng cụm K trước. Thuật toán này có khả năng tìm các cụm có hình dạng tùy ý, nhóm các điểm có mật độ cao và mạnh mẽ với nhiễu (outliers). DBSCAN hoạt động bằng cách mở rộng các cụm từ các \"core points\" dựa trên hai tham số ε (bán kính) và MinPts (số điểm tối thiểu), có tốc độ trung bình và phù hợp cho dữ liệu không gian có nhiễu.\n- Robust to outliers là một ưu điểm của thuật toán DBSCAN, nơi các điểm ngoại lai được tự động xác định là \"noise points\" và không ảnh hưởng đến việc hình thành các cụm chính, giúp mô hình ít bị nhiễu bởi dữ liệu bất thường.\n- Arbitrary shapes là một ưu điểm của thuật toán DBSCAN, cho phép nó xử lý các cụm có hình dạng phức tạp, không giả định hình cầu (non-convex clusters), khác với các thuật toán như K-Means.\n\n**Mối quan hệ:**\n- DBSCAN là một thuật toán thuộc lĩnh vực Học Không Giám Sát, được sử dụng để phân cụm dữ liệu không nhãn.\n- DBSCAN cải thiện khả năng phân cụm bằng cách mạnh mẽ với các điểm ngoại lai, tự động coi chúng là nhiễu và không gộp vào các cụm chính.\n- DBSCAN cải thiện khả năng phân cụm bằng cách xử lý các cụm có hình dạng tùy ý, không giới hạn bởi giả định hình cầu."
        },
        "output_without_context": {
            "question": "Trong thuật toán DBSCAN, điều gì được sử dụng để xác định các cụm dựa trên mật độ?",
            "answer": "Mật độ điểm dữ liệu."
        }
    }
]