[
    {
        "topic_description": "KhÃ¡m phÃ¡ cÃ¡c váº¥n Ä‘á» chÃ­nh trong há»c mÃ¡y lÃ  ráº¥t quan trá»ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c thuáº­t toÃ¡n. Há»c sinh sáº½ Ä‘Æ°á»£c kiá»ƒm tra hiá»ƒu biáº¿t vá» khÃ´ng gian giáº£ thuyáº¿t, tá»‘i Æ°u hÃ³a Ä‘á»™ chÃ­nh xÃ¡c, vÃ  Ä‘Ã¡nh giÃ¡ Ä‘á»™ tin cáº­y cá»§a káº¿t quáº£. Cá»¥ thá»ƒ lÃ  cÃ¡c khÃ¡i niá»‡m vá» overfitting vÃ  yÃªu cáº§u dá»¯ liá»‡u huáº¥n luyá»‡n.",
        "output_with_context": {
            "question": "Overfitting trong há»c mÃ¡y Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  gÃ¬?",
            "answer": "Overfitting lÃ  khi mÃ´ hÃ¬nh há»c quÃ¡ sÃ¡t vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n, dáº«n Ä‘áº¿n hiá»‡u suáº¥t tá»‘t trÃªn táº­p huáº¥n luyá»‡n nhÆ°ng kÃ©m trÃªn dá»¯ liá»‡u má»›i.",
            "rag_context": "**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Lá»±a Chá»n Äáº·c TrÆ°ng & Tá»‘i Æ¯u HÃ³a MÃ´ HÃ¬nh\n- CV cho stable estimate\n\n**1. K-Fold Cross-Validation:**\n\n**Thuáº­t toÃ¡n:**\n1. Chia data thÃ nh k folds\n2. For i = 1 to k:\n   - Use fold i lÃ m validation\n   - Use k-1 folds cÃ²n láº¡i lÃ m training\n   - Train vÃ  evaluate\n3. Average metrics across k folds\n\n**Chá»n k:**\n- k=5: Standard, good balance\n- k=10: More stable, more computational\n- Larger k: Less bias, more variance, more expensive\n\n**Æ¯u Ä‘iá»ƒm:**\n- Sá»­ dá»¥ng toÃ n bá»™ data\n- Stable estimate\n- Reduce variance\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- k láº§n training (expensive)\n- CÃ³ thá»ƒ cháº­m\n\n**2. Stratified K-Fold:**\n\n**NguyÃªn lÃ½:**\n- Maintain class distribution trong má»—i fold\n- Each fold representative\n\n**Khi nÃ o dÃ¹ng:**\n- Imbalanced datasets\n- Classification tasks\n- Äáº£m báº£o má»—i fold cÃ³ Ä‘á»§ samples má»—i class\n\n**Æ¯u Ä‘iá»ƒm:**\n- Fair evaluation vá»›i imbalanced data\n- Consistent class proportions\n\n**3. Leave-One-Out (LOO):**\n\n**NguyÃªn lÃ½:**\n- k = n (n = sá»‘ samples)\n- Má»—i sample lÃ  má»™t fold\n\n**Æ¯u Ä‘iá»ƒm:**\n- Maximum data cho training\n- No randomness\n- Deterministic\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Ráº¥t cháº­m (n iterations)\n- High variance\n- Chá»‰ kháº£ thi vá»›i small datasets (< 1000)\n\n**Khi nÃ o dÃ¹ng:**\n- Very small datasets\n- Need maximum training data\n- Computational resources available\n\n**4. Time Series Cross-Validation:**\n\n**NguyÃªn lÃ½:**\n- Respect temporal order\n- Train on past, validate on future\n- No data leakage from future\n\n**Expanding Window:**\n```\nFold 1: Train [1:100] â†’ Test [101:120]\nFold 2: Train [1:120] â†’ Test [121:140]\nFold 3: Train [1:140] â†’ Test [141:160]\n```\n\n**Rolling Window:**\n```\nFold 1: Train [1:100] â†’ Test [101:120]\nFold 2: Train [21:120] â†’ Test [121:140]\nFold 3: Train [41:140] â†’ Test [141:160]\n```\n\n**Quan trá»ng:**\n- **KHÃ”NG shuffle data**\n- Maintain temporal order\n- Avoid look-ahead bias\n\n**5. Nested Cross-Validation:**\n\n**NguyÃªn lÃ½:**\n- Outer loop: Model evaluation\n- Inner loop: Hyperparameter tuning\n- Prevents overfitting in parameter selection\n\n**Structure:**\n```\nOuter CV (5-fold):\n  For each outer fold:\n    Inner CV (5-fold):\n      Hyperparameter tuning\n    Train with best params\n    Evaluate on outer fold\n```\n\n**Æ¯u Ä‘iá»ƒm:**\n- Unbiased performance estimate\n- Proper hyperparameter tuning\n- Gold standard\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Very expensive (k_outer Ã— k_inner trainings)\n- Overkill cho simple problems\n\n**Khi nÃ o dÃ¹ng:**\n- Need unbiased estimate\n- Publishing results\n- Critical applications\n- Have computational resources\n\n### Learning Curves (ÄÆ°á»ng Cong Há»c)\n\nPhÃ¢n tÃ­ch hiá»‡u suáº¥t mÃ´ hÃ¬nh vs kÃ­ch thÆ°á»›c training set.\n\n**Váº½ gÃ¬:**\n- X-axis: Training set size\n- Y-axis: Error (hoáº·c Score)\n- Two curves: Training error & Validation error\n\n**Cháº©n ÄoÃ¡n:**\n\n**1. High Bias (Underfitting):**\n```\nTraining error: Cao\nValidation error: Cao\nGap: Nhá»\nBoth plateau at high error\n```\n**Dáº¥u hiá»‡u:**\n- Cáº£ hai curves plateau\n- Performance kÃ©m ngay cáº£ vá»›i nhiá»u data\n- ThÃªm data khÃ´ng giÃºp\n\n**Giáº£i phÃ¡p:**\n- Increase model complexity\n- Add features\n- Reduce regularization\n- Try complex model\n\n**2. High Variance (Overfitting):**\n```\nTraining error: Tháº¥p\nValidation error: Cao\nGap: Lá»›n\nGap doesn't close with more data\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Overfitting (há»c quÃ¡ má»©c/quÃ¡ khá»›p) lÃ  má»™t váº¥n Ä‘á» phá»• biáº¿n trong há»c mÃ¡y khi mÃ´ hÃ¬nh há»c quÃ¡ sÃ¡t vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n, bao gá»“m cáº£ nhiá»…u, cÃ¡c máº«u ngáº«u nhiÃªn, vÃ  cÃ¡c chi tiáº¿t khÃ´ng tá»•ng quÃ¡t. Äiá»u nÃ y dáº«n Ä‘áº¿n hiá»‡u suáº¥t ráº¥t tá»‘t (Ä‘á»™ lá»—i tháº¥p, Ä‘á»™ chÃ­nh xÃ¡c cao) trÃªn táº­p huáº¥n luyá»‡n nhÆ°ng láº¡i kÃ©m (Ä‘á»™ lá»—i cao, Ä‘á»™ chÃ­nh xÃ¡c tháº¥p) trÃªn dá»¯ liá»‡u má»›i, chÆ°a tá»«ng tháº¥y (táº­p kiá»ƒm tra/validation).\n\nCÃ¡c dáº¥u hiá»‡u cá»§a overfitting bao gá»“m:\n- Äá»™ lá»—i tháº¥p trÃªn táº­p huáº¥n luyá»‡n nhÆ°ng Ä‘á»™ lá»—i cao trÃªn táº­p kiá»ƒm tra.\n- Khoáº£ng cÃ¡ch lá»›n giá»¯a learning curves cá»§a training vÃ  validation.\n- Training error tiáº¿p tá»¥c giáº£m nhÆ°ng validation error khÃ´ng cáº£i thiá»‡n hoáº·c tháº­m chÃ­ tÄƒng.\n- MÃ´ hÃ¬nh cÃ³ high variance vÃ  generalization kÃ©m.\n\nCÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p, Ä‘áº·c biá»‡t lÃ  cÃ¡c mÃ´ hÃ¬nh cÃ³ nhiá»u tham sá»‘ nhÆ° Deep Learning, Transformers, hoáº·c CÃ¢y Quyáº¿t Ä‘á»‹nh quÃ¡ sÃ¢u/phá»©c táº¡p, cÃ³ nguy cÆ¡ cao bá»‹ overfitting náº¿u khÃ´ng cÃ³ Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n tá»‘t.\n\nÄá»ƒ ngÄƒn cháº·n overfitting, cÃ¡c ká»¹ thuáº­t sau thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng:\n- **Regularization**: Bao gá»“m L1 Regularization (Lasso), L2 Regularization, vÃ  Dropout, giÃºp Ä‘Æ¡n giáº£n hÃ³a mÃ´ hÃ¬nh vÃ  khuyáº¿n khÃ­ch tÃ­nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n.\n- **Early Stopping**: Dá»«ng quÃ¡ trÃ¬nh huáº¥n luyá»‡n khi hiá»‡u suáº¥t trÃªn táº­p validation báº¯t Ä‘áº§u giáº£m hoáº·c khÃ´ng cáº£i thiá»‡n.\n- **Data Augmentation**: TÄƒng cÆ°á»ng dá»¯ liá»‡u huáº¥n luyá»‡n báº±ng cÃ¡ch táº¡o ra cÃ¡c biáº¿n thá»ƒ cá»§a dá»¯ liá»‡u hiá»‡n cÃ³.\n- **Giáº£m Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh**: VÃ­ dá»¥, giáº£m sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng, hoáº·c cáº¯t tá»‰a (pruning) cÃ¢y quyáº¿t Ä‘á»‹nh.\n- **ThÃªm dá»¯ liá»‡u huáº¥n luyá»‡n**: Cung cáº¥p nhiá»u dá»¯ liá»‡u hÆ¡n Ä‘á»ƒ mÃ´ hÃ¬nh há»c cÃ¡c máº«u tá»•ng quÃ¡t thay vÃ¬ ghi nhá»› nhiá»…u.\n- **CÃ¡c ká»¹ thuáº­t khÃ¡c**: Pooling vÃ  Global Average Pooling cÅ©ng cÃ³ thá»ƒ giÃºp giáº£m overfitting.\n\nNgÆ°á»£c láº¡i, Underfitting (há»c dÆ°á»›i má»©c) lÃ  váº¥n Ä‘á» khi mÃ´ hÃ¬nh quÃ¡ Ä‘Æ¡n giáº£n Ä‘á»ƒ náº¯m báº¯t cÃ¡c máº«u cÆ¡ báº£n trong dá»¯ liá»‡u, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m trÃªn cáº£ dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  dá»¯ liá»‡u má»›i. Underfitting liÃªn quan Ä‘áº¿n High Bias, vá»›i dáº¥u hiá»‡u lÃ  training error cao vÃ  validation error cao, cÃ¹ng vá»›i má»™t khoáº£ng cÃ¡ch nhá» giá»¯a chÃºng trÃªn Learning Curves.\n- High Bias (thiÃªn vá»‹ cao) lÃ  má»™t váº¥n Ä‘á» trong há»c mÃ¡y, cÃ²n Ä‘Æ°á»£c gá»i lÃ  Underfitting (há»c dÆ°á»›i má»©c). NÃ³ xáº£y ra khi mÃ´ hÃ¬nh quÃ¡ Ä‘Æ¡n giáº£n Ä‘á»ƒ náº¯m báº¯t Ä‘Æ°á»£c má»‘i quan há»‡ phá»©c táº¡p trong dá»¯ liá»‡u, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m trÃªn cáº£ táº­p huáº¥n luyá»‡n vÃ  táº­p validation. TrÃªn Learning Curves, cáº£ Training error vÃ  Validation error Ä‘á»u cao vÃ  cÃ³ khoáº£ng cÃ¡ch nhá», cáº£ hai Ä‘Æ°á»ng Ä‘á»u Ä‘áº¡t Ä‘áº¿n má»™t má»©c cao vÃ  khÃ´ng cáº£i thiá»‡n khi thÃªm dá»¯ liá»‡u.\n- Data leakage (rÃ² rá»‰ dá»¯ liá»‡u) lÃ  má»™t váº¥n Ä‘á» nghiÃªm trá»ng trong há»c mÃ¡y, xáº£y ra khi thÃ´ng tin tá»« táº­p dá»¯ liá»‡u kiá»ƒm tra (hoáº·c dá»¯ liá»‡u mÃ  mÃ´ hÃ¬nh sáº½ gáº·p trong thá»±c táº¿/dá»¯ liá»‡u tÆ°Æ¡ng lai/dá»¯ liá»‡u má»¥c tiÃªu) vÃ´ tÃ¬nh bá»‹ rÃ² rá»‰ vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh. Äiá»u nÃ y dáº«n Ä‘áº¿n viá»‡c Æ°á»›c lÆ°á»£ng hiá»‡u suáº¥t mÃ´ hÃ¬nh quÃ¡ láº¡c quan trÃªn táº­p kiá»ƒm tra, khÃ´ng pháº£n Ã¡nh kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a thá»±c sá»± cá»§a mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u má»›i, vÃ  khiáº¿n mÃ´ hÃ¬nh cÃ³ váº» hoáº¡t Ä‘á»™ng tá»‘t trÃªn táº­p kiá»ƒm tra nhÆ°ng láº¡i kÃ©m hiá»‡u quáº£ trong thá»±c táº¿.\n\n**Má»‘i quan há»‡:**\n- Learning Curves cháº©n Ä‘oÃ¡n váº¥n Ä‘á» High Bias (Underfitting) khi cáº£ training error vÃ  validation error Ä‘á»u cao vÃ  cÃ³ khoáº£ng cÃ¡ch nhá», cáº£ hai Ä‘Æ°á»ng Ä‘á»u Ä‘áº¡t Ä‘áº¿n má»™t má»©c cao vÃ  khÃ´ng cáº£i thiá»‡n khi thÃªm dá»¯ liá»‡u.\n- Time Series Cross-Validation ngÄƒn cháº·n rÃ² rá»‰ dá»¯ liá»‡u (data leakage) báº±ng cÃ¡ch Ä‘áº£m báº£o ráº±ng dá»¯ liá»‡u tá»« tÆ°Æ¡ng lai khÃ´ng bao giá» Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n hoáº·c Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh, Ä‘áº·c biá»‡t lÃ  trÃ¡nh look-ahead bias.\n- Äá»ƒ giáº£i quyáº¿t High Bias, cáº§n giáº£m regularization (Reduce regularization) Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c Ä‘Æ°á»£c nhiá»u hÆ¡n tá»« dá»¯ liá»‡u huáº¥n luyá»‡n.\n- Äá»ƒ giáº£i quyáº¿t High Bias, cáº§n tÄƒng Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh (Increase model complexity) Ä‘á»ƒ nÃ³ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ phá»©c táº¡p hÆ¡n trong dá»¯ liá»‡u.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Lá»±a Chá»n Äáº·c TrÆ°ng & Tá»‘i Æ¯u HÃ³a MÃ´ HÃ¬nh\n```\n**Dáº¥u hiá»‡u:**\n- Large gap giá»¯a curves\n- Training error tiáº¿p tá»¥c giáº£m\n- Validation error khÃ´ng cáº£i thiá»‡n\n\n**Giáº£i phÃ¡p:**\n- Get more training data\n- Reduce model complexity\n- Increase regularization\n- Feature selection\n- Dropout, early stopping\n\n**3. Good Fit:**\n```\nTraining error: Tháº¥p\nValidation error: Tháº¥p\nGap: Nhá»\nBoth converge\n```\n**Dáº¥u hiá»‡u:**\n- Small gap\n- Both errors low\n- Converged performance\n\n**4. More Data Helps:**\n```\nValidation error giáº£m khi tÄƒng data\nGap Ä‘ang Ä‘Ã³ng láº¡i\nChÆ°a plateau\n```\n**HÃ nh Ä‘á»™ng:** Get more data!\n\n**5. More Data Doesn't Help:**\n```\nBoth curves plateau\nAdding data khÃ´ng cáº£i thiá»‡n\n```\n**HÃ nh Ä‘á»™ng:** Improve features hoáº·c model\n\n### Bias-Variance Tradeoff (Sá»± ÄÃ¡nh Äá»•i Bias-Variance)\n\n**CÃ´ng thá»©c:**\n$$Expected\\ Error = Bias^2 + Variance + Irreducible\\ Error$$\n\n**Bias (ThiÃªn Lá»‡ch):**\n- Error tá»« giáº£ Ä‘á»‹nh Ä‘Æ¡n giáº£n hÃ³a\n- Underfitting\n- Model khÃ´ng capture Ä‘Æ°á»£c patterns\n- High bias â†’ Systematic errors\n\n**Variance (PhÆ°Æ¡ng Sai):**\n- Error tá»« sensitivity to training data\n- Overfitting\n- Model learns noise\n- High variance â†’ Different results vá»›i different data\n\n**Irreducible Error:**\n- Noise trong data\n- KhÃ´ng thá»ƒ giáº£m\n- Comes from data collection\n\n**Tradeoff:**\n- Decrease bias â†’ Increase variance\n- Decrease variance â†’ Increase bias\n- Cáº§n balance\n\n**Strategies:**\n\n**Giáº£m High Bias:**\n1. Increase model complexity\n2. Add more features/polynomial features\n3. Decrease regularization\n4. Train longer\n5. Use ensemble methods\n\n**Giáº£m High Variance:**\n1. Get more training data\n2. Reduce model complexity\n3. Increase regularization (L1, L2, dropout)\n4. Feature selection\n5. Early stopping\n6. Ensemble methods (bagging)\n\n**Sweet Spot:**\n- Minimize total error\n- Balance bias vÃ  variance\n- Depends on problem vÃ  data\n\n**Visualize:**\n```\nTotal Error\n    |     \\\n    |      \\___BiasÂ²\n    |___________\\\n    |            \\___\n    |Variance_____\\___Total\n    |________________\\___\n    |___________________\\___\n    +----------------------->\n    Simple          Complex\n            Model Complexity\n```\n\n### PhÆ°Æ¡ng PhÃ¡p Ensemble\n\nKáº¿t há»£p nhiá»u models Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t.\n\n**\"Wisdom of crowds\"**\n\n**Táº¡i sao hoáº¡t Ä‘á»™ng:**\n- Errors cá»§a individual models cancel out\n- Diverse models capture different patterns\n- Reduce variance\n- More robust\n\n**1. Bagging (Bootstrap Aggregating):**\n\n**NguyÃªn lÃ½:**\n- Train multiple models trÃªn bootstrap samples\n- Average predictions (regression) hoáº·c vote (classification)\n\n**Bootstrap Sampling:**\n- Sample with replacement\n- Same size as original\n- ~63% unique samples má»—i bootstrap\n\n**Thuáº­t toÃ¡n:**\n1. For i = 1 to M:\n   - Create bootstrap sample $D_i$\n   - Train model $M_i$ on $D_i$\n2. Combine:\n   - Regression: $\\hat{y} = \frac{1}{M}\\sum_{i=1}^{M}M_i(x)$\n   - Classification: Majority vote\n\n**Æ¯u Ä‘iá»ƒm:**\n- Reduce variance\n- Parallel training\n- Works vá»›i high-variance models\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- KhÃ´ng giáº£m bias\n- CÃ³ thá»ƒ cháº­m (many models)\n\n**VÃ­ dá»¥:** Random Forest\n\n**2. Boosting:**\n\n**NguyÃªn lÃ½:**\n- Sequential training\n- Each model corrects errors cá»§a previous models\n- Weighted combination\n\n**Thuáº­t toÃ¡n (general):**\n1. Initialize equal weights\n2. For i = 1 to M:\n   - Train model $M_i$ on weighted data\n   - TÃ­nh error\n   - Update weights (increase for misclassified)\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Overfitting (há»c quÃ¡ má»©c/quÃ¡ khá»›p) lÃ  má»™t váº¥n Ä‘á» phá»• biáº¿n trong há»c mÃ¡y khi mÃ´ hÃ¬nh há»c quÃ¡ sÃ¡t vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n, bao gá»“m cáº£ nhiá»…u, cÃ¡c máº«u ngáº«u nhiÃªn, vÃ  cÃ¡c chi tiáº¿t khÃ´ng tá»•ng quÃ¡t. Äiá»u nÃ y dáº«n Ä‘áº¿n hiá»‡u suáº¥t ráº¥t tá»‘t (Ä‘á»™ lá»—i tháº¥p, Ä‘á»™ chÃ­nh xÃ¡c cao) trÃªn táº­p huáº¥n luyá»‡n nhÆ°ng láº¡i kÃ©m (Ä‘á»™ lá»—i cao, Ä‘á»™ chÃ­nh xÃ¡c tháº¥p) trÃªn dá»¯ liá»‡u má»›i, chÆ°a tá»«ng tháº¥y (táº­p kiá»ƒm tra/validation).\n\nCÃ¡c dáº¥u hiá»‡u cá»§a overfitting bao gá»“m:\n- Äá»™ lá»—i tháº¥p trÃªn táº­p huáº¥n luyá»‡n nhÆ°ng Ä‘á»™ lá»—i cao trÃªn táº­p kiá»ƒm tra.\n- Khoáº£ng cÃ¡ch lá»›n giá»¯a learning curves cá»§a training vÃ  validation.\n- Training error tiáº¿p tá»¥c giáº£m nhÆ°ng validation error khÃ´ng cáº£i thiá»‡n hoáº·c tháº­m chÃ­ tÄƒng.\n- MÃ´ hÃ¬nh cÃ³ high variance vÃ  generalization kÃ©m.\n\nCÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p, Ä‘áº·c biá»‡t lÃ  cÃ¡c mÃ´ hÃ¬nh cÃ³ nhiá»u tham sá»‘ nhÆ° Deep Learning, Transformers, hoáº·c CÃ¢y Quyáº¿t Ä‘á»‹nh quÃ¡ sÃ¢u/phá»©c táº¡p, cÃ³ nguy cÆ¡ cao bá»‹ overfitting náº¿u khÃ´ng cÃ³ Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n tá»‘t.\n\nÄá»ƒ ngÄƒn cháº·n overfitting, cÃ¡c ká»¹ thuáº­t sau thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng:\n- **Regularization**: Bao gá»“m L1 Regularization (Lasso), L2 Regularization, vÃ  Dropout, giÃºp Ä‘Æ¡n giáº£n hÃ³a mÃ´ hÃ¬nh vÃ  khuyáº¿n khÃ­ch tÃ­nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n.\n- **Early Stopping**: Dá»«ng quÃ¡ trÃ¬nh huáº¥n luyá»‡n khi hiá»‡u suáº¥t trÃªn táº­p validation báº¯t Ä‘áº§u giáº£m hoáº·c khÃ´ng cáº£i thiá»‡n.\n- **Data Augmentation**: TÄƒng cÆ°á»ng dá»¯ liá»‡u huáº¥n luyá»‡n báº±ng cÃ¡ch táº¡o ra cÃ¡c biáº¿n thá»ƒ cá»§a dá»¯ liá»‡u hiá»‡n cÃ³.\n- **Giáº£m Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh**: VÃ­ dá»¥, giáº£m sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng, hoáº·c cáº¯t tá»‰a (pruning) cÃ¢y quyáº¿t Ä‘á»‹nh.\n- **ThÃªm dá»¯ liá»‡u huáº¥n luyá»‡n**: Cung cáº¥p nhiá»u dá»¯ liá»‡u hÆ¡n Ä‘á»ƒ mÃ´ hÃ¬nh há»c cÃ¡c máº«u tá»•ng quÃ¡t thay vÃ¬ ghi nhá»› nhiá»…u.\n- **CÃ¡c ká»¹ thuáº­t khÃ¡c**: Pooling vÃ  Global Average Pooling cÅ©ng cÃ³ thá»ƒ giÃºp giáº£m overfitting.\n\nNgÆ°á»£c láº¡i, Underfitting (há»c dÆ°á»›i má»©c) lÃ  váº¥n Ä‘á» khi mÃ´ hÃ¬nh quÃ¡ Ä‘Æ¡n giáº£n Ä‘á»ƒ náº¯m báº¯t cÃ¡c máº«u cÆ¡ báº£n trong dá»¯ liá»‡u, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m trÃªn cáº£ dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  dá»¯ liá»‡u má»›i. Underfitting liÃªn quan Ä‘áº¿n High Bias, vá»›i dáº¥u hiá»‡u lÃ  training error cao vÃ  validation error cao, cÃ¹ng vá»›i má»™t khoáº£ng cÃ¡ch nhá» giá»¯a chÃºng trÃªn Learning Curves."
        },
        "output_without_context": {
            "question": "Overfitting trong há»c mÃ¡y Ä‘Æ°á»£c hiá»ƒu lÃ  gÃ¬?",
            "answer": "Hiá»‡n tÆ°á»£ng mÃ´ hÃ¬nh há»c quÃ¡ ká»¹ tá»« dá»¯ liá»‡u huáº¥n luyá»‡n, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m trÃªn dá»¯ liá»‡u má»›i."
        }
    },
    {
        "topic_description": "Chá»§ Ä‘á» nÃ y sáº½ kiá»ƒm tra hiá»ƒu biáº¿t cá»§a há»c sinh vá» Äá»‹nh lÃ½ Bayes, mÃ  xÃ¡c Ä‘á»‹nh cÃ¡ch xÃ¡c suáº¥t háº­u nghiá»‡m liÃªn quan Ä‘áº¿n xÃ¡c suáº¥t kháº£ dÄ© vÃ  tiÃªn nghiá»‡m. Há»c sinh sáº½ Ä‘Æ°á»£c yÃªu cáº§u Ã¡p dá»¥ng cÃ´ng thá»©c cá»§a Äá»‹nh lÃ½ Bayes trong cÃ¡c vÃ­ dá»¥ cá»¥ thá»ƒ hoáº·c cÃ¢u há»i tÃ­nh toÃ¡n.",
        "output_with_context": {
            "question": "Trong Äá»‹nh lÃ½ Bayes, xÃ¡c suáº¥t háº­u nghiá»‡m Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c nÃ o?",
            "answer": "P(C_k|x) = \\frac{P(x|C_k)P(C_k)}{P(x)}",
            "rag_context": "**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## PhÃ¢n Loáº¡i (Classification)\n- Æ¯u Ä‘iá»ƒm: Má»—i mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n hÆ¡n, cÃ¢n báº±ng hÆ¡n\n- NhÆ°á»£c Ä‘iá»ƒm: Nhiá»u mÃ´ hÃ¬nh (phá»©c táº¡p khi K lá»›n)\n\n**VÃ­ dá»¥:** 3 lá»›p (A, B, C)\n- MÃ´ hÃ¬nh 1: A vs B\n- MÃ´ hÃ¬nh 2: A vs C\n- MÃ´ hÃ¬nh 3: B vs C\n\n**3. Softmax Regression (Multinomial Logistic Regression):**\n\nMá»Ÿ rá»™ng trá»±c tiáº¿p cá»§a logistic regression cho Ä‘a lá»›p.\n\n**CÃ´ng thá»©c:**\n$$P(y=k|x) = \frac{e^{z_k}}{\\sum_{j=1}^{K}e^{z_j}}$$\n\nTrong Ä‘Ã³: $z_k = \beta_k^Tx$ vá»›i $\beta_k$ lÃ  vector há»‡ sá»‘ cho lá»›p $k$\n\n**Äáº·c Ä‘iá»ƒm:**\n- Tá»•ng cÃ¡c xÃ¡c suáº¥t = 1: $\\sum_{k=1}^{K}P(y=k|x) = 1$\n- Output lÃ  phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn táº¥t cáº£ lá»›p\n- Huáº¥n luyá»‡n Ä‘á»“ng thá»i táº¥t cáº£ lá»›p\n\n**HÃ m chi phÃ­ (Categorical Cross-Entropy):**\n$$J(\beta) = -\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}\\log(P(y=k|x^{(i)}))$$\n\nTrong Ä‘Ã³ $y_k^{(i)}$ lÃ  one-hot encoding cá»§a nhÃ£n.\n\n**Lá»±a chá»n giá»¯a OvR, OvO, vÃ  Softmax:**\n- **Softmax:** Tá»‘t nháº¥t khi cáº§n xÃ¡c suáº¥t, K khÃ´ng quÃ¡ lá»›n\n- **OvR:** ÄÆ¡n giáº£n, hiá»‡u quáº£ vá»›i K lá»›n\n- **OvO:** Tá»‘t vá»›i SVM, K nhá»/trung bÃ¬nh\n\n### Naive Bayes Classifier (Bá»™ PhÃ¢n Loáº¡i Naive Bayes)\n\nDá»±a trÃªn Ä‘á»‹nh lÃ½ Bayes vá»›i giáº£ Ä‘á»‹nh \"ngÃ¢y thÆ¡\" (naive) vá» tÃ­nh Ä‘á»™c láº­p Ä‘áº·c trÆ°ng.\n\n**Äá»‹nh LÃ½ Bayes:**\n$$P(C_k|x) = \frac{P(x|C_k)P(C_k)}{P(x)}$$\n\nTrong Ä‘Ã³:\n- $P(C_k|x)$: XÃ¡c suáº¥t háº­u nghiá»‡m (posterior) - xÃ¡c suáº¥t lá»›p $C_k$ cho trÆ°á»›c $x$\n- $P(x|C_k)$: Likelihood - xÃ¡c suáº¥t cá»§a $x$ trong lá»›p $C_k$\n- $P(C_k)$: XÃ¡c suáº¥t tiÃªn nghiá»‡m (prior) cá»§a lá»›p $C_k$\n- $P(x)$: Evidence - xÃ¡c suáº¥t cá»§a $x$\n\n**Giáº£ Äá»‹nh Naive (Äá»™c Láº­p Äiá»u Kiá»‡n):**\n$$P(x_1, x_2, ..., x_n|C_k) = \\prod_{i=1}^{n}P(x_i|C_k)$$\n\nCÃ¡c Ä‘áº·c trÆ°ng Ä‘á»™c láº­p vá»›i nhau khi biáº¿t lá»›p.\n\n**CÃ´ng Thá»©c Äáº§y Äá»§:**\n$$P(C_k|x_1,...,x_n) = \frac{P(C_k)\\prod_{i=1}^{n}P(x_i|C_k)}{P(x_1,...,x_n)}$$\n\n**Quyáº¿t Äá»‹nh:**\n$$\\hat{y} = \\arg\\max_{k} P(C_k)\\prod_{i=1}^{n}P(x_i|C_k)$$\n\nKhÃ´ng cáº§n tÃ­nh $P(x)$ vÃ¬ nÃ³ giá»‘ng nhau cho táº¥t cáº£ lá»›p.\n\n**CÃ¡c Biáº¿n Thá»ƒ:**\n\n**1. Gaussian Naive Bayes:**\n- Cho Ä‘áº·c trÆ°ng liÃªn tá»¥c\n- Giáº£ Ä‘á»‹nh phÃ¢n phá»‘i Gaussian (chuáº©n)\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- P(x) lÃ  evidence, Ä‘áº¡i diá»‡n cho xÃ¡c suáº¥t cá»§a cÃ¡c Ä‘áº·c trÆ°ng x. Trong Naive Bayes Classifier, P(x) thÆ°á»ng khÃ´ng cáº§n tÃ­nh toÃ¡n trá»±c tiáº¿p vÃ¬ nÃ³ lÃ  má»™t háº±ng sá»‘ cho táº¥t cáº£ cÃ¡c lá»›p khi so sÃ¡nh xÃ¡c suáº¥t háº­u nghiá»‡m, vÃ  do Ä‘Ã³ khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n quyáº¿t Ä‘á»‹nh phÃ¢n loáº¡i cuá»‘i cÃ¹ng.\n- Naive Bayes Classifier lÃ  má»™t bá»™ phÃ¢n loáº¡i dá»±a trÃªn Äá»‹nh lÃ½ Bayes vá»›i giáº£ Ä‘á»‹nh \"ngÃ¢y thÆ¡\" (naive) vá» tÃ­nh Ä‘á»™c láº­p cÃ³ Ä‘iá»u kiá»‡n cá»§a cÃ¡c Ä‘áº·c trÆ°ng. MÃ´ hÃ¬nh nÃ y tÃ­nh toÃ¡n xÃ¡c suáº¥t háº­u nghiá»‡m cá»§a má»™t lá»›p cho trÆ°á»›c cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o vÃ  chá»n lá»›p cÃ³ xÃ¡c suáº¥t cao nháº¥t. Naive Bayes Ä‘Æ¡n giáº£n, hiá»‡u quáº£ vÃ  thÆ°á»ng hoáº¡t Ä‘á»™ng tá»‘t ngay cáº£ vá»›i lÆ°á»£ng dá»¯ liá»‡u nhá».\n- P(C_k) lÃ  xÃ¡c suáº¥t tiÃªn nghiá»‡m (prior probability) cá»§a lá»›p C_k, Ä‘áº¡i diá»‡n cho xÃ¡c suáº¥t má»™t máº«u thuá»™c vá» lá»›p C_k mÃ  khÃ´ng cáº§n biáº¿t báº¥t ká»³ Ä‘áº·c trÆ°ng nÃ o. ÄÃ¢y lÃ  xÃ¡c suáº¥t cÆ¡ báº£n cá»§a lá»›p trÆ°á»›c khi xem xÃ©t dá»¯ liá»‡u.\n\n**Má»‘i quan há»‡:**\n- Naive Bayes Classifier lÃ  má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n PhÃ¢n loáº¡i, Ä‘áº·c biá»‡t hiá»‡u quáº£ vá»›i giáº£ Ä‘á»‹nh Ä‘á»™c láº­p Ä‘áº·c trÆ°ng.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## PhÃ¢n Loáº¡i (Classification)\n- KhÃ´ng phÃ¹ há»£p vá»›i imbalanced data\n- VÃ­ dá»¥: 95% accuracy nghe tá»‘t nhÆ°ng vÃ´ nghÄ©a náº¿u 95% data thuá»™c 1 lá»›p\n\n**2. Precision (Äá»™ ChÃ­nh XÃ¡c DÆ°Æ¡ng):**\n$$Precision = \frac{TP}{TP+FP}$$\n\n- Trong cÃ¡c dá»± Ä‘oÃ¡n dÆ°Æ¡ng, bao nhiÃªu thá»±c sá»± dÆ°Æ¡ng?\n- Quan trá»ng khi cost cá»§a FP cao\n- VÃ­ dá»¥: PhÃ¡t hiá»‡n spam (khÃ´ng muá»‘n email quan trá»ng bá»‹ Ä‘Ã¡nh dáº¥u spam)\n\n**3. Recall/Sensitivity/True Positive Rate (Äá»™ Bao Phá»§):**\n$$Recall = \frac{TP}{TP+FN}$$\n\n- Trong cÃ¡c máº«u dÆ°Æ¡ng thá»±c táº¿, bao nhiÃªu Ä‘Æ°á»£c phÃ¡t hiá»‡n?\n- Quan trá»ng khi cost cá»§a FN cao\n- VÃ­ dá»¥: Cháº©n Ä‘oÃ¡n bá»‡nh (khÃ´ng muá»‘n bá» sÃ³t bá»‡nh nhÃ¢n)\n\n**4. Specificity/True Negative Rate (Äá»™ Äáº·c Hiá»‡u):**\n$$Specificity = \frac{TN}{TN+FP}$$\n\n- Trong cÃ¡c máº«u Ã¢m thá»±c táº¿, bao nhiÃªu Ä‘Æ°á»£c phÃ¡t hiá»‡n Ä‘Ãºng?\n- Bá»• sung cho recall\n\n**5. F1-Score:**\n$$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$\n\n- Trung bÃ¬nh Ä‘iá»u hÃ²a cá»§a Precision vÃ  Recall\n- CÃ¢n báº±ng giá»¯a Precision vÃ  Recall\n- Tá»‘t cho imbalanced data\n- F1 = 1: HoÃ n háº£o, F1 = 0: Tá»‡ nháº¥t\n\n**6. F-Beta Score:**\n$$F_\beta = (1 + \beta^2) \times \frac{Precision \times Recall}{\beta^2 \times Precision + Recall}$$\n\n- $\beta < 1$: Æ¯u tiÃªn Precision\n- $\beta > 1$: Æ¯u tiÃªn Recall\n- $\beta = 1$: F1-Score\n\n**7. Matthews Correlation Coefficient (MCC):**\n$$MCC = \frac{TP \times TN - FP \times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n\n- GiÃ¡ trá»‹: -1 Ä‘áº¿n +1\n- +1: Dá»± Ä‘oÃ¡n hoÃ n háº£o\n- 0: Ngáº«u nhiÃªn\n- -1: KhÃ´ng Ä‘á»“ng Ã½ hoÃ n toÃ n\n- Tá»‘t cho imbalanced data\n\n**ÄÆ°á»ng Cong ROC vÃ  AUC:**\n\n**ROC Curve (Receiver Operating Characteristic):**\n- Trá»¥c X: False Positive Rate = $\frac{FP}{FP+TN}$ = 1 - Specificity\n- Trá»¥c Y: True Positive Rate = Recall = $\frac{TP}{TP+FN}$\n- Váº½ vá»›i cÃ¡c ngÆ°á»¡ng threshold khÃ¡c nhau\n- Cho tháº¥y trade-off giá»¯a TPR vÃ  FPR\n\n**AUC (Area Under Curve):**\n- Diá»‡n tÃ­ch dÆ°á»›i Ä‘Æ°á»ng cong ROC\n- GiÃ¡ trá»‹: 0 Ä‘áº¿n 1\n- AUC = 1: PhÃ¢n loáº¡i hoÃ n háº£o\n- AUC = 0.5: PhÃ¢n loáº¡i ngáº«u nhiÃªn\n- AUC < 0.5: Tá»‡ hÆ¡n ngáº«u nhiÃªn\n- KhÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi threshold\n- Äo kháº£ nÄƒng phÃ¢n biá»‡t giá»¯a cÃ¡c lá»›p\n\n**Diá»…n giáº£i AUC:**\n- 0.9-1.0: Xuáº¥t sáº¯c\n- 0.8-0.9: Tá»‘t\n- 0.7-0.8: Cháº¥p nháº­n Ä‘Æ°á»£c\n- 0.6-0.7: KÃ©m\n- 0.5-0.6: Tháº¥t báº¡i\n\n**ÄÆ°á»ng Cong Precision-Recall:**\n- Trá»¥c X: Recall\n- Trá»¥c Y: Precision\n- Tá»‘t cho imbalanced data\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- CÃ´ng thá»©c tÃ­nh F-Beta Score lÃ  F_Î² = (1 + Î²Â²) * (Precision * Recall) / (Î²Â² * Precision + Recall). CÃ´ng thá»©c nÃ y cho phÃ©p Æ°u tiÃªn Precision hoáº·c Recall báº±ng cÃ¡ch Ä‘iá»u chá»‰nh tham sá»‘ Î².\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## PhÃ¢n Loáº¡i (Classification)\n$$P(x_i|C_k) = \frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}\right)$$\n- Æ¯á»›c lÆ°á»£ng $\\mu_k$ (mean) vÃ  $\\sigma_k^2$ (variance) tá»« dá»¯ liá»‡u\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n, nháº­n dáº¡ng máº«u\n\n**2. Multinomial Naive Bayes:**\n- Cho Ä‘áº¿m rá»i ráº¡c (word counts, frequencies)\n- PhÃ¢n phá»‘i Ä‘a thá»©c\n$$P(x_i|C_k) = \frac{N_{ki} + \\alpha}{N_k + \\alpha n}$$\n  - $N_{ki}$: Sá»‘ láº§n Ä‘áº·c trÆ°ng $i$ xuáº¥t hiá»‡n trong lá»›p $k$\n  - $N_k$: Tá»•ng sá»‘ Ä‘áº¿m trong lá»›p $k$\n  - $\\alpha$: Laplace smoothing (thÆ°á»ng = 1)\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n, phÃ¢n tÃ­ch cáº£m xÃºc, lá»c spam\n\n**3. Bernoulli Naive Bayes:**\n- Cho Ä‘áº·c trÆ°ng nhá»‹ phÃ¢n (cÃ³/khÃ´ng)\n- PhÃ¢n phá»‘i Bernoulli\n$$P(x_i|C_k) = P(i|C_k)x_i + (1-P(i|C_k))(1-x_i)$$\n- TÃ­nh cáº£ viá»‡c Ä‘áº·c trÆ°ng xuáº¥t hiá»‡n vÃ  khÃ´ng xuáº¥t hiá»‡n\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n vá»›i binary features\n\n**Æ¯u Äiá»ƒm:**\n- Nhanh, hiá»‡u quáº£\n- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i dá»¯ liá»‡u nhá»\n- Dá»… triá»ƒn khai vÃ  diá»…n giáº£i\n- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i nhiá»u Ä‘áº·c trÆ°ng\n- KhÃ´ng nháº¡y cáº£m vá»›i Ä‘áº·c trÆ°ng khÃ´ng liÃªn quan\n\n**NhÆ°á»£c Äiá»ƒm:**\n- Giáº£ Ä‘á»‹nh Ä‘á»™c láº­p hiáº¿m khi Ä‘Ãºng trong thá»±c táº¿\n- \"Zero frequency problem\" cáº§n smoothing\n- Æ¯á»›c lÆ°á»£ng xÃ¡c suáº¥t cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c\n- KhÃ´ng tá»‘t khi Ä‘áº·c trÆ°ng tÆ°Æ¡ng quan\n\n**Laplace Smoothing:**\nXá»­ lÃ½ váº¥n Ä‘á» xÃ¡c suáº¥t = 0:\n$$P(x_i|C_k) = \frac{count(x_i, C_k) + \\alpha}{count(C_k) + \\alpha \times |V|}$$\n\n### k-Nearest Neighbors (k-NN) - K LÃ¡ng Giá»ng Gáº§n Nháº¥t\n\nPhÆ°Æ¡ng phÃ¡p non-parametric phÃ¢n loáº¡i dá»±a trÃªn Ä‘a sá»‘ vote cá»§a k lÃ¡ng giá»ng gáº§n nháº¥t.\n\n**Thuáº­t ToÃ¡n:**\n1. TÃ­nh khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm cáº§n phÃ¢n loáº¡i Ä‘áº¿n táº¥t cáº£ Ä‘iá»ƒm huáº¥n luyá»‡n\n2. Chá»n k Ä‘iá»ƒm gáº§n nháº¥t\n3. Vote: Lá»›p xuáº¥t hiá»‡n nhiá»u nháº¥t trong k lÃ¡ng giá»ng\n4. GÃ¡n nhÃ£n lá»›p Ä‘Ã³ cho Ä‘iá»ƒm má»›i\n\n**CÃ¡c Äá»™ Äo Khoáº£ng CÃ¡ch:**\n\n**1. Euclidean Distance (Khoáº£ng cÃ¡ch Euclid):**\n$$d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}$$\n- Phá»• biáº¿n nháº¥t\n- Khoáº£ng cÃ¡ch Ä‘Æ°á»ng tháº³ng\n- Nháº¡y cáº£m vá»›i scale cá»§a Ä‘áº·c trÆ°ng\n\n**2. Manhattan Distance (Khoáº£ng cÃ¡ch Manhattan):**\n$$d(x,y) = \\sum_{i=1}^{n}|x_i-y_i|$$\n- Khoáº£ng cÃ¡ch theo lÆ°á»›i Ä‘Ã´ thá»‹\n- Ãt nháº¡y cáº£m vá»›i outliers\n- Tá»‘t cho dá»¯ liá»‡u high-dimensional\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Bernoulli Naive Bayes lÃ  má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i dá»±a trÃªn Ä‘á»‹nh lÃ½ Bayes, Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c Ä‘áº·c trÆ°ng nhá»‹ phÃ¢n (binary features), tá»©c lÃ  cÃ¡c Ä‘áº·c trÆ°ng chá»‰ cÃ³ thá»ƒ cÃ³ hai giÃ¡ trá»‹ (vÃ­ dá»¥: cÃ³ hoáº·c khÃ´ng xuáº¥t hiá»‡n). NÃ³ sá»­ dá»¥ng phÃ¢n phá»‘i Bernoulli Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t cá»§a cÃ¡c Ä‘áº·c trÆ°ng.\n\n**Má»‘i quan há»‡:**\n- Bernoulli Naive Bayes Ä‘Æ°á»£c á»©ng dá»¥ng trong phÃ¢n loáº¡i vÄƒn báº£n khi cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng nhá»‹ phÃ¢n (vÃ­ dá»¥: tá»« cÃ³ xuáº¥t hiá»‡n hay khÃ´ng)."
        },
        "output_without_context": {
            "question": "Äá»‹nh lÃ½ Bayes Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh xÃ¡c suáº¥t háº­u nghiá»‡m dá»±a trÃªn xÃ¡c suáº¥t tiÃªn nghiá»‡m vÃ  xÃ¡c suáº¥t kháº£ dÄ©. CÃ´ng thá»©c nÃ o sau Ä‘Ã¢y lÃ  Ä‘Ãºng cho Äá»‹nh lÃ½ Bayes?",
            "answer": "P(A|B) = (P(B|A) * P(A)) / P(B)"
        }
    },
    {
        "topic_description": "Chá»§ Ä‘á» nÃ y liÃªn quan Ä‘áº¿n khÃ¡i niá»‡m Há»c táº­p Háº­u nghiá»‡m tá»‘i Ä‘a, giÃºp há»c sinh hiá»ƒu cÃ¡ch tÃ¬m giáº£ thuyáº¿t tá»‘i Æ°u dá»±a trÃªn dá»¯ liá»‡u. Báº±ng cÃ¡ch phÃ¢n tÃ­ch cÃ´ng thá»©c vÃ  á»©ng dá»¥ng thá»±c táº¿, há»c sinh sáº½ Ä‘Æ°á»£c kiá»ƒm tra vá» kháº£ nÄƒng Ã¡p dá»¥ng MAP trong cÃ¡c tÃ¬nh huá»‘ng há»c mÃ¡y.",
        "output_with_context": {
            "question": "Trong Há»c táº­p Háº­u nghiá»‡m tá»‘i Ä‘a (MAP), cÃ´ng thá»©c nÃ o Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ¬m giáº£ thuyáº¿t tá»‘i Æ°u dá»±a trÃªn dá»¯ liá»‡u?",
            "answer": "MAP lÃ  tá»‘i Ä‘a hÃ³a xÃ¡c suáº¥t háº­u nghiá»‡m P(H|D) thÃ´ng qua cÃ´ng thá»©c Bayes.",
            "rag_context": "**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»c KhÃ´ng GiÃ¡m SÃ¡t (Unsupervised Learning)\n- Matrix factorization (NMF, SVD)\n\n**5. Topic Modeling:**\n- Document clustering\n- Automatic tagging\n- Content organization\n- Trend detection\n\n**6. Gene Expression Analysis:**\n- Group similar genes\n- Identify cancer subtypes\n- Drug discovery\n- Understanding diseases\n\n**7. Social Network Analysis:**\n- Community detection\n- Influencer identification\n- Link prediction\n- Recommendation\n\n**8. Data Preprocessing:**\n- Feature extraction (PCA, ICA)\n- Noise reduction (autoencoders)\n- Data compression\n- Dimensionality reduction\n\n**9. Market Basket Analysis:**\n- Product recommendations\n- Store layout\n- Promotions\n- Cross-selling\n\n**10. Image Segmentation:**\n- Medical imaging\n- Object detection preparation\n- Video processing\n- Computer vision preprocessing\n\n---\n\n## Há»c SÃ¢u (Deep Learning)\n\n### Giá»›i Thiá»‡u vá» Há»c SÃ¢u\n\nHá»c sÃ¢u (Deep Learning) lÃ  má»™t nhÃ¡nh con cá»§a há»c mÃ¡y sá»­ dá»¥ng máº¡ng nÆ¡-ron nhÃ¢n táº¡o vá»›i nhiá»u lá»›p áº©n Ä‘á»ƒ há»c cÃ¡c biá»ƒu diá»…n phÃ¢n cáº¥p cá»§a dá»¯ liá»‡u. KhÃ¡c vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p há»c mÃ¡y truyá»n thá»‘ng, há»c sÃ¢u cÃ³ kháº£ nÄƒng tá»± Ä‘á»™ng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u thÃ´ mÃ  khÃ´ng cáº§n ká»¹ thuáº­t Ä‘áº·c trÆ°ng thá»§ cÃ´ng.\n\n**Äáº·c Ä‘iá»ƒm chÃ­nh:**\n- **Há»c biá»ƒu diá»…n phÃ¢n cáº¥p:** CÃ¡c lá»›p Ä‘áº§u há»c cÃ¡c Ä‘áº·c trÆ°ng cáº¥p tháº¥p (cáº¡nh, gÃ³c), cÃ¡c lá»›p sau há»c Ä‘áº·c trÆ°ng cáº¥p cao hÆ¡n (hÃ¬nh dáº¡ng, Ä‘á»‘i tÆ°á»£ng)\n- **Kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u lá»›n:** Hiá»‡u suáº¥t tÄƒng theo lÆ°á»£ng dá»¯ liá»‡u\n- **End-to-end learning:** Há»c trá»±c tiáº¿p tá»« Ä‘áº§u vÃ o thÃ´ Ä‘áº¿n Ä‘áº§u ra mong muá»‘n\n- **Tá»± Ä‘á»™ng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng:** KhÃ´ng cáº§n thiáº¿t káº¿ Ä‘áº·c trÆ°ng thá»§ cÃ´ng\n\n**á»¨ng dá»¥ng Ä‘Ã£ cÃ¡ch máº¡ng hÃ³a:**\n- Thá»‹ giÃ¡c mÃ¡y tÃ­nh (nháº­n dáº¡ng áº£nh, phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng)\n- Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (dá»‹ch mÃ¡y, chatbot, sinh vÄƒn báº£n)\n- Nháº­n dáº¡ng giá»ng nÃ³i (trá»£ lÃ½ áº£o, chuyá»ƒn Ä‘á»•i giá»ng nÃ³i thÃ nh vÄƒn báº£n)\n- Y táº¿ (cháº©n Ä‘oÃ¡n hÃ¬nh áº£nh, phÃ¡t triá»ƒn thuá»‘c)\n- Tá»± Ä‘á»™ng hÃ³a (xe tá»± lÃ¡i, robot)\n\n### Máº¡ng NÆ¡-ron NhÃ¢n Táº¡o (Artificial Neural Networks - ANN)\n\nMáº¡ng nÆ¡-ron nhÃ¢n táº¡o Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a nÃ£o ngÆ°á»i, trong Ä‘Ã³ cÃ¡c nÆ¡-ron sinh há»c truyá»n tÃ­n hiá»‡u cho nhau thÃ´ng qua cÃ¡c synapse.\n\n### Perceptron - ÄÆ¡n Vá»‹ CÆ¡ Báº£n\n\nPerceptron lÃ  Ä‘Æ¡n vá»‹ máº¡ng nÆ¡-ron Ä‘Æ¡n giáº£n nháº¥t, Ä‘Æ°á»£c phÃ¡t minh bá»Ÿi Frank Rosenblatt nÄƒm 1958.\n\n**CÃ´ng thá»©c:**\n$$y = \\sigma(w^Tx + b)$$\n\nTrong Ä‘Ã³:\n- $x = [x_1, x_2, ..., x_n]^T$: Vector Ä‘áº§u vÃ o (cÃ¡c Ä‘áº·c trÆ°ng)\n- $w = [w_1, w_2, ..., w_n]^T$: Vector trá»ng sá»‘ (weights)\n- $b$: Há»‡ sá»‘ Ä‘iá»u chá»‰nh (bias) - cho phÃ©p dá»‹ch chuyá»ƒn hÃ m quyáº¿t Ä‘á»‹nh\n- $\\sigma$: HÃ m kÃ­ch hoáº¡t (activation function)\n- $y$: Äáº§u ra dá»± Ä‘oÃ¡n\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t lÃ  má»™t nhÃ¡nh cá»§a Há»c MÃ¡y, nÆ¡i cÃ¡c mÃ´ hÃ¬nh hoáº·c thuáº­t toÃ¡n há»c tá»« dá»¯ liá»‡u khÃ´ng Ä‘Æ°á»£c gÃ¡n nhÃ£n. Má»¥c tiÃªu chÃ­nh lÃ  tÃ¬m kiáº¿m cÃ¡c cáº¥u trÃºc, máº«u hoáº·c nhÃ³m áº©n trong dá»¯ liá»‡u. CÃ¡c ká»¹ thuáº­t phá»• biáº¿n bao gá»“m phÃ¢n cá»¥m, giáº£m chiá»u dá»¯ liá»‡u vÃ  phÃ¢n tÃ­ch liÃªn káº¿t.\n- Há»c MÃ¡y lÃ  má»™t lÄ©nh vá»±c cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o cho phÃ©p há»‡ thá»‘ng há»c tá»« dá»¯ liá»‡u, xÃ¡c Ä‘á»‹nh cÃ¡c máº«u vÃ  Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh vá»›i sá»± can thiá»‡p tá»‘i thiá»ƒu cá»§a con ngÆ°á»i, mÃ  khÃ´ng cáº§n Ä‘Æ°á»£c láº­p trÃ¬nh rÃµ rÃ ng. NÃ³ bao gá»“m nhiá»u phÆ°Æ¡ng phÃ¡p vÃ  thuáº­t toÃ¡n Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n hoáº·c phÃ¢n loáº¡i, vá»›i cÃ¡c nhÃ¡nh nhÆ° Há»c KhÃ´ng GiÃ¡m SÃ¡t vÃ  Há»c SÃ¢u.\n\n**Má»‘i quan há»‡:**\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t sá»­ dá»¥ng Market Basket Analysis Ä‘á»ƒ phÃ¢n tÃ­ch giá» hÃ ng.\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t sá»­ dá»¥ng Topic Modeling Ä‘á»ƒ khÃ¡m phÃ¡ cÃ¡c chá»§ Ä‘á» trong tÃ i liá»‡u.\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t sá»­ dá»¥ng Matrix factorization nhÆ° má»™t ká»¹ thuáº­t Ä‘á»ƒ tÃ¬m kiáº¿m cáº¥u trÃºc áº©n trong dá»¯ liá»‡u khÃ´ng nhÃ£n.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»c KhÃ´ng GiÃ¡m SÃ¡t (Unsupervised Learning)\n   - Merge chÃºng thÃ nh 1 cluster\n3. Until: Chá»‰ cÃ²n 1 cluster\n\n**Steps chi tiáº¿t:**\n- Initialize: N clusters\n- Iteration 1: N-1 clusters\n- Iteration 2: N-2 clusters\n- ...\n- Final: 1 cluster\n\n**2. Divisive (Top-Down - PhÃ¢n Chia):**\n\n**Thuáº­t toÃ¡n:**\n1. Start: Táº¥t cáº£ Ä‘iá»ƒm trong 1 cluster\n2. Repeat:\n   - Chá»n cluster Ä‘á»ƒ split\n   - Chia thÃ nh 2 sub-clusters\n3. Until: Má»—i Ä‘iá»ƒm lÃ  1 cluster\n\n**Ãt phá»• biáº¿n:** Computationally expensive hÆ¡n\n\n**Linkage Methods (CÃ¡ch Äo Khoáº£ng CÃ¡ch Giá»¯a Clusters):**\n\n**1. Single Linkage (Minimum):**\n$$d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoáº£ng cÃ¡ch giá»¯a 2 Ä‘iá»ƒm gáº§n nháº¥t\n- Táº¡o long, chain-like clusters\n- Sensitive to noise vÃ  outliers\n\n**2. Complete Linkage (Maximum):**\n$$d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoáº£ng cÃ¡ch giá»¯a 2 Ä‘iá»ƒm xa nháº¥t\n- Táº¡o compact, spherical clusters\n- Ãt sensitive to outliers\n\n**3. Average Linkage:**\n$$d(C_i, C_j) = \frac{1}{|C_i||C_j|}\\sum_{x \\in C_i}\\sum_{y \\in C_j}d(x,y)$$\n- Trung bÃ¬nh táº¥t cáº£ pairwise distances\n- Balance giá»¯a single vÃ  complete\n- Phá»• biáº¿n choice\n\n**4. Ward's Method:**\n- Minimize within-cluster variance sau khi merge\n- Maximize between-cluster variance\n- Táº¡o balanced, compact clusters\n- ThÆ°á»ng cho káº¿t quáº£ tá»‘t nháº¥t\n- Phá»• biáº¿n nháº¥t trong thá»±c táº¿\n\n**Dendrogram (Biá»ƒu Äá»“ CÃ¢y):**\n\nTree diagram showing cluster hierarchy.\n\n**Äá»c Dendrogram:**\n- Vertical axis: Distance/dissimilarity\n- Horizontal axis: Samples\n- Height cá»§a merge: Distance giá»¯a clusters\n- CÃ ng cao merge cÃ ng dissimilar\n\n**Cutting Dendrogram:**\n- Váº½ horizontal line\n- Number of intersections = Number of clusters\n- Height cá»§a cut = dissimilarity threshold\n\n**Æ¯u Äiá»ƒm:**\n- KhÃ´ng cáº§n specify K trÆ°á»›c\n- Dendrogram provides insights\n- Flexible - cÃ³ thá»ƒ chá»n K sau\n- Deterministic (no randomness)\n\n**NhÆ°á»£c Äiá»ƒm:**\n- Computationally expensive: O(NÂ²log N) or O(NÂ³)\n- KhÃ´ng scale vá»›i large datasets\n- Má»™t khi merge khÃ´ng thá»ƒ undo\n- Memory intensive\n\n**Khi NÃ o DÃ¹ng:**\n- Small-medium datasets (< 10,000)\n- Cáº§n understand hierarchy\n- KhÃ´ng biáº¿t K optimal\n- Exploratory analysis\n\n### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nNhÃ³m cÃ¡c Ä‘iá»ƒm cÃ³ máº­t Ä‘á»™ cao, robust to outliers vÃ  arbitrary shapes.\n\n**Tham Sá»‘:**\n\n**1. Îµ (epsilon):**\n- Maximum distance giá»¯a 2 Ä‘iá»ƒm Ä‘á»ƒ Ä‘Æ°á»£c coi lÃ  neighbors\n- Äá»‹nh nghÄ©a neighborhood radius\n- QuÃ¡ nhá»: Nhiá»u noise points\n- QuÃ¡ lá»›n: Merge nhiá»u clusters\n\n**2. MinPts (Minimum Points):**\n- Minimum sá»‘ Ä‘iá»ƒm trong Îµ-neighborhood Ä‘á»ƒ lÃ  core point\n- ThÆ°á»ng: 4, 5, hoáº·c 2Ã—dim\n- Larger MinPts: Ãt core points, stricter\n\n**CÃ¡c Loáº¡i Äiá»ƒm:**\n\n**1. Core Point:**\n- CÃ³ â‰¥ MinPts Ä‘iá»ƒm khÃ¡c trong Îµ-neighborhood (bao gá»“m cáº£ chÃ­nh nÃ³)\n- Trung tÃ¢m cá»§a clusters\n- Can form clusters\n\n**2. Border Point:**\n- Náº±m trong Îµ-neighborhood cá»§a core point\n- CÃ³ < MinPts neighbors\n- Thuá»™c cluster nhÆ°ng khÃ´ng core\n- á» biÃªn cá»§a cluster\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t lÃ  má»™t nhÃ¡nh cá»§a Há»c MÃ¡y, nÆ¡i cÃ¡c mÃ´ hÃ¬nh hoáº·c thuáº­t toÃ¡n há»c tá»« dá»¯ liá»‡u khÃ´ng Ä‘Æ°á»£c gÃ¡n nhÃ£n. Má»¥c tiÃªu chÃ­nh lÃ  tÃ¬m kiáº¿m cÃ¡c cáº¥u trÃºc, máº«u hoáº·c nhÃ³m áº©n trong dá»¯ liá»‡u. CÃ¡c ká»¹ thuáº­t phá»• biáº¿n bao gá»“m phÃ¢n cá»¥m, giáº£m chiá»u dá»¯ liá»‡u vÃ  phÃ¢n tÃ­ch liÃªn káº¿t.\n- Há»c MÃ¡y lÃ  má»™t lÄ©nh vá»±c cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o cho phÃ©p há»‡ thá»‘ng há»c tá»« dá»¯ liá»‡u, xÃ¡c Ä‘á»‹nh cÃ¡c máº«u vÃ  Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh vá»›i sá»± can thiá»‡p tá»‘i thiá»ƒu cá»§a con ngÆ°á»i, mÃ  khÃ´ng cáº§n Ä‘Æ°á»£c láº­p trÃ¬nh rÃµ rÃ ng. NÃ³ bao gá»“m nhiá»u phÆ°Æ¡ng phÃ¡p vÃ  thuáº­t toÃ¡n Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n hoáº·c phÃ¢n loáº¡i, vá»›i cÃ¡c nhÃ¡nh nhÆ° Há»c KhÃ´ng GiÃ¡m SÃ¡t vÃ  Há»c SÃ¢u.\n\n**Má»‘i quan há»‡:**\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t sá»­ dá»¥ng Market Basket Analysis Ä‘á»ƒ phÃ¢n tÃ­ch giá» hÃ ng.\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t sá»­ dá»¥ng Topic Modeling Ä‘á»ƒ khÃ¡m phÃ¡ cÃ¡c chá»§ Ä‘á» trong tÃ i liá»‡u.\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t sá»­ dá»¥ng Matrix factorization nhÆ° má»™t ká»¹ thuáº­t Ä‘á»ƒ tÃ¬m kiáº¿m cáº¥u trÃºc áº©n trong dá»¯ liá»‡u khÃ´ng nhÃ£n.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## CÃ¢y Quyáº¿t Äá»‹nh (Decision Tree)\n\n### Giá»›i Thiá»‡u Vá» CÃ¢y Quyáº¿t Äá»‹nh\n\nCÃ¢y quyáº¿t Ä‘á»‹nh lÃ  thuáº­t toÃ¡n há»c cÃ³ giÃ¡m sÃ¡t Ä‘a nÄƒng cÃ³ thá»ƒ thá»±c hiá»‡n cáº£ tÃ¡c vá»¥ phÃ¢n loáº¡i vÃ  há»“i quy. ChÃºng há»c cÃ¡c quy táº¯c quyáº¿t Ä‘á»‹nh tá»« cÃ¡c Ä‘áº·c trÆ°ng Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ má»¥c tiÃªu thÃ´ng qua cáº¥u trÃºc dáº¡ng cÃ¢y.\n\n**á»¨ng dá»¥ng thá»±c táº¿:**\n- Cháº©n Ä‘oÃ¡n y táº¿ (chuá»—i quyáº¿t Ä‘á»‹nh dá»±a trÃªn triá»‡u chá»©ng)\n- ÄÃ¡nh giÃ¡ rá»§i ro tÃ­n dá»¥ng\n- Dá»± Ä‘oÃ¡n churn khÃ¡ch hÃ ng\n- PhÃ¡t hiá»‡n gian láº­n\n- Há»‡ thá»‘ng chuyÃªn gia\n- PhÃ¢n loáº¡i email spam\n\n**Táº¡i sao gá»i lÃ  \"cÃ¢y\":**\n- Cáº¥u trÃºc phÃ¢n cáº¥p giá»‘ng cÃ¢y ngÆ°á»£c\n- Gá»‘c á»Ÿ trÃªn, lÃ¡ á»Ÿ dÆ°á»›i\n- Quyáº¿t Ä‘á»‹nh Ä‘Æ°á»£c Ä‘Æ°a ra táº¡i má»—i nÃºt ná»™i bá»™\n- Káº¿t quáº£ cuá»‘i cÃ¹ng á»Ÿ nÃºt lÃ¡\n\n### Cáº¥u TrÃºc CÃ¢y\n\n**1. NÃºt Gá»‘c (Root Node):**\n- NÃºt trÃªn cÃ¹ng Ä‘áº¡i diá»‡n cho toÃ n bá»™ táº­p dá»¯ liá»‡u\n- Chá»©a táº¥t cáº£ máº«u training\n- Äiá»ƒm báº¯t Ä‘áº§u cá»§a quÃ¡ trÃ¬nh quyáº¿t Ä‘á»‹nh\n- CÃ³ phÃ¢n chia Ä‘áº§u tiÃªn dá»±a trÃªn Ä‘áº·c trÆ°ng quan trá»ng nháº¥t\n\n**2. NÃºt Ná»™i Bá»™ (Internal Nodes):**\n- CÃ¡c nÃºt quyáº¿t Ä‘á»‹nh dá»±a trÃªn kiá»ƒm tra Ä‘áº·c trÆ°ng\n- Má»—i nÃºt thá»±c hiá»‡n má»™t cÃ¢u há»i yes/no vá» Ä‘áº·c trÆ°ng\n- VÃ­ dá»¥: \"Tuá»•i > 30?\", \"Thu nháº­p < 50,000?\"\n- Chia dá»¯ liá»‡u thÃ nh cÃ¡c táº­p con\n\n**3. NhÃ¡nh (Branches):**\n- Káº¿t quáº£ cá»§a cÃ¡c quyáº¿t Ä‘á»‹nh\n- Káº¿t ná»‘i nÃºt cha vá»›i nÃºt con\n- Äáº¡i diá»‡n cho giÃ¡ trá»‹ hoáº·c pháº¡m vi giÃ¡ trá»‹ cá»§a Ä‘áº·c trÆ°ng\n\n**4. NÃºt LÃ¡ (Leaf Nodes):**\n- NÃºt cuá»‘i cÃ¹ng khÃ´ng cÃ³ nhÃ¡nh con\n- Chá»©a dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng\n- PhÃ¢n loáº¡i: NhÃ£n lá»›p\n- Há»“i quy: GiÃ¡ trá»‹ sá»‘\n\n**VÃ­ dá»¥ minh há»a - Quyáº¿t Ä‘á»‹nh mua nhÃ :**\n```\n                 [Thu nháº­p > 50K?]\n                /                 \\\n            YES                    NO\n           /                         \\\n   [Tuá»•i > 30?]                [KhÃ´ng mua]\n    /        \\\n  YES        NO\n  /            \\\n[Mua]      [ThuÃª]\n```\n\n### XÃ¢y Dá»±ng CÃ¢y Quyáº¿t Äá»‹nh\n\n**TiÃªu ChÃ­ PhÃ¢n Chia (Splitting Criteria):**\n\nMá»¥c tiÃªu: TÃ¬m phÃ¢n chia tá»‘t nháº¥t lÃ m tÄƒng \"Ä‘á»™ thuáº§n khiáº¿t\" (purity) cá»§a cÃ¡c táº­p con.\n\n**Cho PhÃ¢n Loáº¡i:**\n\n**1. Gini Impurity (Chá»‰ Sá»‘ Gini):**\n$$Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$$\n\nTrong Ä‘Ã³:\n- $p_i$ lÃ  tá»· lá»‡ máº«u thuá»™c lá»›p $i$ táº¡i nÃºt $t$\n- $C$ lÃ  sá»‘ lá»›p\n- Gini = 0: NÃºt hoÃ n toÃ n thuáº§n khiáº¿t (táº¥t cáº£ máº«u cÃ¹ng lá»›p)\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Há»“i quy lÃ  má»™t tÃ¡c vá»¥ trong há»c mÃ¡y mÃ  CÃ¢y Quyáº¿t Äá»‹nh cÃ³ thá»ƒ thá»±c hiá»‡n, nháº±m dá»± Ä‘oÃ¡n má»™t giÃ¡ trá»‹ sá»‘ liÃªn tá»¥c cho cÃ¡c máº«u dá»¯ liá»‡u dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o. Káº¿t quáº£ dá»± Ä‘oÃ¡n á»Ÿ nÃºt lÃ¡ lÃ  má»™t giÃ¡ trá»‹ sá»‘.\n\n**Má»‘i quan há»‡:**\n- CÃ¢y Quyáº¿t Äá»‹nh lÃ  má»™t thuáº­t toÃ¡n há»c cÃ³ giÃ¡m sÃ¡t cÃ³ thá»ƒ thá»±c hiá»‡n tÃ¡c vá»¥ Há»“i quy, dá»± Ä‘oÃ¡n giÃ¡ trá»‹ sá»‘ liÃªn tá»¥c.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»“i Quy Tuyáº¿n TÃ­nh (Linear Regression)\nHá»‡ sá»‘ gÃ³c: $\beta_1 = \frac{\\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\\sum_{i=1}^{n}(x_i - \bar{x})^2}$\n\nHá»‡ sá»‘ cháº·n: $\beta_0 = \bar{y} - \beta_1\bar{x}$\n\nTrong Ä‘Ã³ $\bar{x}$ vÃ  $\bar{y}$ lÃ  giÃ¡ trá»‹ trung bÃ¬nh cá»§a $x$ vÃ  $y$.\n\n**VÃ­ Dá»¥ Minh Há»a:**\nGiáº£ sá»­ chÃºng ta muá»‘n dá»± Ä‘oÃ¡n giÃ¡ nhÃ  (triá»‡u Ä‘á»“ng) dá»±a trÃªn diá»‡n tÃ­ch (mÂ²):\n- Dá»¯ liá»‡u: Diá»‡n tÃ­ch [50, 60, 70, 80, 90], GiÃ¡ [1500, 1800, 2100, 2400, 2700]\n- Sau khi Ã¡p dá»¥ng OLS, ta cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c: $y = 300 + 30x$\n- Diá»…n giáº£i: GiÃ¡ cÆ¡ báº£n lÃ  300 triá»‡u, má»—i mÂ² tÄƒng thÃªm 30 triá»‡u\n\n### Há»“i Quy Tuyáº¿n TÃ­nh Bá»™i\n\nKhi xá»­ lÃ½ nhiá»u Ä‘áº·c trÆ°ng, phÆ°Æ¡ng trÃ¬nh má»Ÿ rá»™ng thÃ nh:\n\n$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \\epsilon$$\n\n**VÃ­ dá»¥:** Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  vá»›i nhiá»u yáº¿u tá»‘:\n$$GiÃ¡ = \beta_0 + \beta_1 \times Diá»‡n\\ tÃ­ch + \beta_2 \times Sá»‘\\ phÃ²ng + \beta_3 \times Khoáº£ng\\ cÃ¡ch\\ trung\\ tÃ¢m$$\n\n**Dáº¡ng Ma Tráº­n:**\n$$\\mathbf{y} = \\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\\epsilon}$$\n\nTrong Ä‘Ã³:\n- $\\mathbf{y}$ lÃ  vector cá»™t cá»§a cÃ¡c giÃ¡ trá»‹ má»¥c tiÃªu (kÃ­ch thÆ°á»›c $m \times 1$)\n- $\\mathbf{X}$ lÃ  ma tráº­n Ä‘áº·c trÆ°ng (kÃ­ch thÆ°á»›c $m \times (n+1)$), bao gá»“m cá»™t 1 cho há»‡ sá»‘ cháº·n\n- $\boldsymbol{\beta}$ lÃ  vector cÃ¡c há»‡ sá»‘ (kÃ­ch thÆ°á»›c $(n+1) \times 1$)\n- $\boldsymbol{\\epsilon}$ lÃ  vector sai sá»‘\n\n**Nghiá»‡m Dáº¡ng ÄÃ³ng (Closed-form Solution):**\n$$\boldsymbol{\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n\n**Æ¯u Ä‘iá»ƒm cá»§a nghiá»‡m dáº¡ng Ä‘Ã³ng:**\n- TÃ­nh toÃ¡n trá»±c tiáº¿p, khÃ´ng cáº§n láº·p\n- Cho káº¿t quáº£ chÃ­nh xÃ¡c (khÃ´ng phá»¥ thuá»™c tá»‘c Ä‘á»™ há»c)\n- PhÃ¹ há»£p khi sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng nhá» (< 10,000)\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Phá»©c táº¡p tÃ­nh toÃ¡n: $O(n^3)$ vá»›i $n$ lÃ  sá»‘ Ä‘áº·c trÆ°ng\n- YÃªu cáº§u $\\mathbf{X}^T\\mathbf{X}$ kháº£ nghá»‹ch\n- KhÃ´ng hiá»‡u quáº£ vá»›i dá»¯ liá»‡u lá»›n\n\n### CÃ¡c Giáº£ Äá»‹nh Cá»§a Há»“i Quy Tuyáº¿n TÃ­nh\n\nÄá»ƒ há»“i quy tuyáº¿n tÃ­nh hoáº¡t Ä‘á»™ng tá»‘t, cáº§n thá»a mÃ£n cÃ¡c giáº£ Ä‘á»‹nh sau:\n\n**1. TÃ­nh Tuyáº¿n TÃ­nh (Linearity):**\n- Má»‘i quan há»‡ giá»¯a cÃ¡c Ä‘áº·c trÆ°ng vÃ  má»¥c tiÃªu lÃ  tuyáº¿n tÃ­nh\n- Kiá»ƒm tra: Váº½ biá»ƒu Ä‘á»“ phÃ¢n tÃ¡n giá»¯a $x$ vÃ  $y$\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Há»“i Quy Tuyáº¿n TÃ­nh lÃ  má»™t mÃ´ hÃ¬nh há»c mÃ¡y cÆ¡ báº£n dÃ¹ng Ä‘á»ƒ dá»± Ä‘oÃ¡n má»™t biáº¿n má»¥c tiÃªu liÃªn tá»¥c (y) dá»±a trÃªn má»™t hoáº·c nhiá»u biáº¿n Ä‘á»™c láº­p (x). MÃ´ hÃ¬nh nÃ y giáº£ Ä‘á»‹nh má»‘i quan há»‡ tuyáº¿n tÃ­nh giá»¯a cÃ¡c biáº¿n, Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng phÆ°Æ¡ng trÃ¬nh y = Î²â‚€ + Î²â‚x + Îµ cho trÆ°á»ng há»£p Ä‘Æ¡n giáº£n, hoáº·c y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²nxn + Îµ cho há»“i quy tuyáº¿n tÃ­nh bá»™i, trong Ä‘Ã³ xáµ¢ lÃ  cÃ¡c Ä‘áº·c trÆ°ng vÃ  Î²áµ¢ lÃ  cÃ¡c há»‡ sá»‘ tÆ°Æ¡ng á»©ng. Má»¥c tiÃªu lÃ  tÃ¬m cÃ¡c há»‡ sá»‘ Î² sao cho sai sá»‘ giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c táº¿ lÃ  nhá» nháº¥t. Há»“i Quy Tuyáº¿n TÃ­nh Bá»™i lÃ  má»™t má»Ÿ rá»™ng cá»§a Há»“i Quy Tuyáº¿n TÃ­nh, Ä‘Æ°á»£c sá»­ dá»¥ng khi cÃ³ nhiá»u Ä‘áº·c trÆ°ng (biáº¿n Ä‘á»™c láº­p) Ä‘á»ƒ dá»± Ä‘oÃ¡n má»™t biáº¿n má»¥c tiÃªu liÃªn tá»¥c. Dáº¡ng ma tráº­n cá»§a mÃ´ hÃ¬nh lÃ  ğ² = ğ—ğ›½ + ğœ–, vá»›i ğ² lÃ  vector giÃ¡ trá»‹ má»¥c tiÃªu, ğ— lÃ  ma tráº­n Ä‘áº·c trÆ°ng, ğ›½ lÃ  vector há»‡ sá»‘ vÃ  ğœ– lÃ  vector sai sá»‘. MÃ´ hÃ¬nh nÃ y cÃ³ nghiá»‡m dáº¡ng Ä‘Ã³ng, cá»¥ thá»ƒ lÃ  ğ›½ = (ğ—áµ€ğ—)â»Â¹ğ—áµ€ğ², vÃ  cÃ¡c giáº£ Ä‘á»‹nh cá»¥ thá»ƒ cáº§n Ä‘Æ°á»£c thá»a mÃ£n Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh hiá»‡u quáº£ vÃ  Ä‘á»™ tin cáº­y cá»§a káº¿t quáº£.\n\n**Má»‘i quan há»‡:**\n- Há»“i Quy Tuyáº¿n TÃ­nh Bá»™i Ã¡p dá»¥ng Nghiá»‡m Dáº¡ng ÄÃ³ng Ä‘á»ƒ tÃ¬m cÃ¡c há»‡ sá»‘ ğ›½ má»™t cÃ¡ch trá»±c tiáº¿p, khÃ´ng cáº§n láº·p."
        },
        "output_without_context": {
            "question": "Trong Há»c táº­p Háº­u nghiá»‡m tá»‘i Ä‘a (MAP), giáº£ thuyáº¿t tá»‘i Æ°u Ä‘Æ°á»£c tÃ¬m kiáº¿m dá»±a trÃªn yáº¿u tá»‘ nÃ o trong dá»¯ liá»‡u?",
            "answer": "XÃ¡c suáº¥t háº­u nghiá»‡m cá»§a giáº£ thuyáº¿t."
        }
    },
    {
        "topic_description": "Kiá»ƒm tra sá»± hiá»ƒu biáº¿t vá» Æ°á»›c lÆ°á»£ng kháº£ dÄ© tá»‘i Ä‘a (MLE) vÃ  cÃ¡c giáº£ Ä‘á»‹nh liÃªn quan Ä‘áº¿n nÃ³. Há»c sinh sáº½ hiá»ƒu cÃ¡ch tá»‘i Ä‘a hÃ³a kháº£ dÄ© cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘á»ƒ tÃ¬m ra mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c.",
        "output_with_context": {
            "question": "Trong Æ°á»›c lÆ°á»£ng kháº£ dÄ© tá»‘i Ä‘a (MLE), giáº£ Ä‘á»‹nh nÃ o lÃ  quan trá»ng nháº¥t Ä‘á»ƒ tá»‘i Ä‘a hÃ³a kháº£ dÄ© cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n?",
            "answer": "Giáº£ Ä‘á»‹nh ráº±ng cÃ¡c quan sÃ¡t lÃ  Ä‘á»™c láº­p vÃ  phÃ¢n phá»‘i theo cÃ¹ng má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t.",
            "rag_context": "**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## PhÃ¢n Loáº¡i (Classification)\n$$P(x_i|C_k) = \frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}\right)$$\n- Æ¯á»›c lÆ°á»£ng $\\mu_k$ (mean) vÃ  $\\sigma_k^2$ (variance) tá»« dá»¯ liá»‡u\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n, nháº­n dáº¡ng máº«u\n\n**2. Multinomial Naive Bayes:**\n- Cho Ä‘áº¿m rá»i ráº¡c (word counts, frequencies)\n- PhÃ¢n phá»‘i Ä‘a thá»©c\n$$P(x_i|C_k) = \frac{N_{ki} + \\alpha}{N_k + \\alpha n}$$\n  - $N_{ki}$: Sá»‘ láº§n Ä‘áº·c trÆ°ng $i$ xuáº¥t hiá»‡n trong lá»›p $k$\n  - $N_k$: Tá»•ng sá»‘ Ä‘áº¿m trong lá»›p $k$\n  - $\\alpha$: Laplace smoothing (thÆ°á»ng = 1)\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n, phÃ¢n tÃ­ch cáº£m xÃºc, lá»c spam\n\n**3. Bernoulli Naive Bayes:**\n- Cho Ä‘áº·c trÆ°ng nhá»‹ phÃ¢n (cÃ³/khÃ´ng)\n- PhÃ¢n phá»‘i Bernoulli\n$$P(x_i|C_k) = P(i|C_k)x_i + (1-P(i|C_k))(1-x_i)$$\n- TÃ­nh cáº£ viá»‡c Ä‘áº·c trÆ°ng xuáº¥t hiá»‡n vÃ  khÃ´ng xuáº¥t hiá»‡n\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n vá»›i binary features\n\n**Æ¯u Äiá»ƒm:**\n- Nhanh, hiá»‡u quáº£\n- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i dá»¯ liá»‡u nhá»\n- Dá»… triá»ƒn khai vÃ  diá»…n giáº£i\n- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i nhiá»u Ä‘áº·c trÆ°ng\n- KhÃ´ng nháº¡y cáº£m vá»›i Ä‘áº·c trÆ°ng khÃ´ng liÃªn quan\n\n**NhÆ°á»£c Äiá»ƒm:**\n- Giáº£ Ä‘á»‹nh Ä‘á»™c láº­p hiáº¿m khi Ä‘Ãºng trong thá»±c táº¿\n- \"Zero frequency problem\" cáº§n smoothing\n- Æ¯á»›c lÆ°á»£ng xÃ¡c suáº¥t cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c\n- KhÃ´ng tá»‘t khi Ä‘áº·c trÆ°ng tÆ°Æ¡ng quan\n\n**Laplace Smoothing:**\nXá»­ lÃ½ váº¥n Ä‘á» xÃ¡c suáº¥t = 0:\n$$P(x_i|C_k) = \frac{count(x_i, C_k) + \\alpha}{count(C_k) + \\alpha \times |V|}$$\n\n### k-Nearest Neighbors (k-NN) - K LÃ¡ng Giá»ng Gáº§n Nháº¥t\n\nPhÆ°Æ¡ng phÃ¡p non-parametric phÃ¢n loáº¡i dá»±a trÃªn Ä‘a sá»‘ vote cá»§a k lÃ¡ng giá»ng gáº§n nháº¥t.\n\n**Thuáº­t ToÃ¡n:**\n1. TÃ­nh khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm cáº§n phÃ¢n loáº¡i Ä‘áº¿n táº¥t cáº£ Ä‘iá»ƒm huáº¥n luyá»‡n\n2. Chá»n k Ä‘iá»ƒm gáº§n nháº¥t\n3. Vote: Lá»›p xuáº¥t hiá»‡n nhiá»u nháº¥t trong k lÃ¡ng giá»ng\n4. GÃ¡n nhÃ£n lá»›p Ä‘Ã³ cho Ä‘iá»ƒm má»›i\n\n**CÃ¡c Äá»™ Äo Khoáº£ng CÃ¡ch:**\n\n**1. Euclidean Distance (Khoáº£ng cÃ¡ch Euclid):**\n$$d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}$$\n- Phá»• biáº¿n nháº¥t\n- Khoáº£ng cÃ¡ch Ä‘Æ°á»ng tháº³ng\n- Nháº¡y cáº£m vá»›i scale cá»§a Ä‘áº·c trÆ°ng\n\n**2. Manhattan Distance (Khoáº£ng cÃ¡ch Manhattan):**\n$$d(x,y) = \\sum_{i=1}^{n}|x_i-y_i|$$\n- Khoáº£ng cÃ¡ch theo lÆ°á»›i Ä‘Ã´ thá»‹\n- Ãt nháº¡y cáº£m vá»›i outliers\n- Tá»‘t cho dá»¯ liá»‡u high-dimensional\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Æ¯á»›c lÆ°á»£ng lÃ  quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh (vÃ­ dá»¥: mean Î¼_k vÃ  variance Ïƒ_k^2 trong Gaussian Naive Bayes) tá»« dá»¯ liá»‡u huáº¥n luyá»‡n.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## CÃ¢y Quyáº¿t Äá»‹nh (Decision Tree)\n\n### Giá»›i Thiá»‡u Vá» CÃ¢y Quyáº¿t Äá»‹nh\n\nCÃ¢y quyáº¿t Ä‘á»‹nh lÃ  thuáº­t toÃ¡n há»c cÃ³ giÃ¡m sÃ¡t Ä‘a nÄƒng cÃ³ thá»ƒ thá»±c hiá»‡n cáº£ tÃ¡c vá»¥ phÃ¢n loáº¡i vÃ  há»“i quy. ChÃºng há»c cÃ¡c quy táº¯c quyáº¿t Ä‘á»‹nh tá»« cÃ¡c Ä‘áº·c trÆ°ng Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ má»¥c tiÃªu thÃ´ng qua cáº¥u trÃºc dáº¡ng cÃ¢y.\n\n**á»¨ng dá»¥ng thá»±c táº¿:**\n- Cháº©n Ä‘oÃ¡n y táº¿ (chuá»—i quyáº¿t Ä‘á»‹nh dá»±a trÃªn triá»‡u chá»©ng)\n- ÄÃ¡nh giÃ¡ rá»§i ro tÃ­n dá»¥ng\n- Dá»± Ä‘oÃ¡n churn khÃ¡ch hÃ ng\n- PhÃ¡t hiá»‡n gian láº­n\n- Há»‡ thá»‘ng chuyÃªn gia\n- PhÃ¢n loáº¡i email spam\n\n**Táº¡i sao gá»i lÃ  \"cÃ¢y\":**\n- Cáº¥u trÃºc phÃ¢n cáº¥p giá»‘ng cÃ¢y ngÆ°á»£c\n- Gá»‘c á»Ÿ trÃªn, lÃ¡ á»Ÿ dÆ°á»›i\n- Quyáº¿t Ä‘á»‹nh Ä‘Æ°á»£c Ä‘Æ°a ra táº¡i má»—i nÃºt ná»™i bá»™\n- Káº¿t quáº£ cuá»‘i cÃ¹ng á»Ÿ nÃºt lÃ¡\n\n### Cáº¥u TrÃºc CÃ¢y\n\n**1. NÃºt Gá»‘c (Root Node):**\n- NÃºt trÃªn cÃ¹ng Ä‘áº¡i diá»‡n cho toÃ n bá»™ táº­p dá»¯ liá»‡u\n- Chá»©a táº¥t cáº£ máº«u training\n- Äiá»ƒm báº¯t Ä‘áº§u cá»§a quÃ¡ trÃ¬nh quyáº¿t Ä‘á»‹nh\n- CÃ³ phÃ¢n chia Ä‘áº§u tiÃªn dá»±a trÃªn Ä‘áº·c trÆ°ng quan trá»ng nháº¥t\n\n**2. NÃºt Ná»™i Bá»™ (Internal Nodes):**\n- CÃ¡c nÃºt quyáº¿t Ä‘á»‹nh dá»±a trÃªn kiá»ƒm tra Ä‘áº·c trÆ°ng\n- Má»—i nÃºt thá»±c hiá»‡n má»™t cÃ¢u há»i yes/no vá» Ä‘áº·c trÆ°ng\n- VÃ­ dá»¥: \"Tuá»•i > 30?\", \"Thu nháº­p < 50,000?\"\n- Chia dá»¯ liá»‡u thÃ nh cÃ¡c táº­p con\n\n**3. NhÃ¡nh (Branches):**\n- Káº¿t quáº£ cá»§a cÃ¡c quyáº¿t Ä‘á»‹nh\n- Káº¿t ná»‘i nÃºt cha vá»›i nÃºt con\n- Äáº¡i diá»‡n cho giÃ¡ trá»‹ hoáº·c pháº¡m vi giÃ¡ trá»‹ cá»§a Ä‘áº·c trÆ°ng\n\n**4. NÃºt LÃ¡ (Leaf Nodes):**\n- NÃºt cuá»‘i cÃ¹ng khÃ´ng cÃ³ nhÃ¡nh con\n- Chá»©a dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng\n- PhÃ¢n loáº¡i: NhÃ£n lá»›p\n- Há»“i quy: GiÃ¡ trá»‹ sá»‘\n\n**VÃ­ dá»¥ minh há»a - Quyáº¿t Ä‘á»‹nh mua nhÃ :**\n```\n                 [Thu nháº­p > 50K?]\n                /                 \\\n            YES                    NO\n           /                         \\\n   [Tuá»•i > 30?]                [KhÃ´ng mua]\n    /        \\\n  YES        NO\n  /            \\\n[Mua]      [ThuÃª]\n```\n\n### XÃ¢y Dá»±ng CÃ¢y Quyáº¿t Äá»‹nh\n\n**TiÃªu ChÃ­ PhÃ¢n Chia (Splitting Criteria):**\n\nMá»¥c tiÃªu: TÃ¬m phÃ¢n chia tá»‘t nháº¥t lÃ m tÄƒng \"Ä‘á»™ thuáº§n khiáº¿t\" (purity) cá»§a cÃ¡c táº­p con.\n\n**Cho PhÃ¢n Loáº¡i:**\n\n**1. Gini Impurity (Chá»‰ Sá»‘ Gini):**\n$$Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$$\n\nTrong Ä‘Ã³:\n- $p_i$ lÃ  tá»· lá»‡ máº«u thuá»™c lá»›p $i$ táº¡i nÃºt $t$\n- $C$ lÃ  sá»‘ lá»›p\n- Gini = 0: NÃºt hoÃ n toÃ n thuáº§n khiáº¿t (táº¥t cáº£ máº«u cÃ¹ng lá»›p)\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Há»“i quy lÃ  má»™t tÃ¡c vá»¥ trong há»c mÃ¡y mÃ  CÃ¢y Quyáº¿t Äá»‹nh cÃ³ thá»ƒ thá»±c hiá»‡n, nháº±m dá»± Ä‘oÃ¡n má»™t giÃ¡ trá»‹ sá»‘ liÃªn tá»¥c cho cÃ¡c máº«u dá»¯ liá»‡u dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o. Káº¿t quáº£ dá»± Ä‘oÃ¡n á»Ÿ nÃºt lÃ¡ lÃ  má»™t giÃ¡ trá»‹ sá»‘.\n\n**Má»‘i quan há»‡:**\n- CÃ¢y Quyáº¿t Äá»‹nh lÃ  má»™t thuáº­t toÃ¡n há»c cÃ³ giÃ¡m sÃ¡t cÃ³ thá»ƒ thá»±c hiá»‡n tÃ¡c vá»¥ Há»“i quy, dá»± Ä‘oÃ¡n giÃ¡ trá»‹ sá»‘ liÃªn tá»¥c.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»“i Quy Tuyáº¿n TÃ­nh (Linear Regression)\nHá»‡ sá»‘ gÃ³c: $\beta_1 = \frac{\\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\\sum_{i=1}^{n}(x_i - \bar{x})^2}$\n\nHá»‡ sá»‘ cháº·n: $\beta_0 = \bar{y} - \beta_1\bar{x}$\n\nTrong Ä‘Ã³ $\bar{x}$ vÃ  $\bar{y}$ lÃ  giÃ¡ trá»‹ trung bÃ¬nh cá»§a $x$ vÃ  $y$.\n\n**VÃ­ Dá»¥ Minh Há»a:**\nGiáº£ sá»­ chÃºng ta muá»‘n dá»± Ä‘oÃ¡n giÃ¡ nhÃ  (triá»‡u Ä‘á»“ng) dá»±a trÃªn diá»‡n tÃ­ch (mÂ²):\n- Dá»¯ liá»‡u: Diá»‡n tÃ­ch [50, 60, 70, 80, 90], GiÃ¡ [1500, 1800, 2100, 2400, 2700]\n- Sau khi Ã¡p dá»¥ng OLS, ta cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c: $y = 300 + 30x$\n- Diá»…n giáº£i: GiÃ¡ cÆ¡ báº£n lÃ  300 triá»‡u, má»—i mÂ² tÄƒng thÃªm 30 triá»‡u\n\n### Há»“i Quy Tuyáº¿n TÃ­nh Bá»™i\n\nKhi xá»­ lÃ½ nhiá»u Ä‘áº·c trÆ°ng, phÆ°Æ¡ng trÃ¬nh má»Ÿ rá»™ng thÃ nh:\n\n$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \\epsilon$$\n\n**VÃ­ dá»¥:** Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  vá»›i nhiá»u yáº¿u tá»‘:\n$$GiÃ¡ = \beta_0 + \beta_1 \times Diá»‡n\\ tÃ­ch + \beta_2 \times Sá»‘\\ phÃ²ng + \beta_3 \times Khoáº£ng\\ cÃ¡ch\\ trung\\ tÃ¢m$$\n\n**Dáº¡ng Ma Tráº­n:**\n$$\\mathbf{y} = \\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\\epsilon}$$\n\nTrong Ä‘Ã³:\n- $\\mathbf{y}$ lÃ  vector cá»™t cá»§a cÃ¡c giÃ¡ trá»‹ má»¥c tiÃªu (kÃ­ch thÆ°á»›c $m \times 1$)\n- $\\mathbf{X}$ lÃ  ma tráº­n Ä‘áº·c trÆ°ng (kÃ­ch thÆ°á»›c $m \times (n+1)$), bao gá»“m cá»™t 1 cho há»‡ sá»‘ cháº·n\n- $\boldsymbol{\beta}$ lÃ  vector cÃ¡c há»‡ sá»‘ (kÃ­ch thÆ°á»›c $(n+1) \times 1$)\n- $\boldsymbol{\\epsilon}$ lÃ  vector sai sá»‘\n\n**Nghiá»‡m Dáº¡ng ÄÃ³ng (Closed-form Solution):**\n$$\boldsymbol{\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n\n**Æ¯u Ä‘iá»ƒm cá»§a nghiá»‡m dáº¡ng Ä‘Ã³ng:**\n- TÃ­nh toÃ¡n trá»±c tiáº¿p, khÃ´ng cáº§n láº·p\n- Cho káº¿t quáº£ chÃ­nh xÃ¡c (khÃ´ng phá»¥ thuá»™c tá»‘c Ä‘á»™ há»c)\n- PhÃ¹ há»£p khi sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng nhá» (< 10,000)\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Phá»©c táº¡p tÃ­nh toÃ¡n: $O(n^3)$ vá»›i $n$ lÃ  sá»‘ Ä‘áº·c trÆ°ng\n- YÃªu cáº§u $\\mathbf{X}^T\\mathbf{X}$ kháº£ nghá»‹ch\n- KhÃ´ng hiá»‡u quáº£ vá»›i dá»¯ liá»‡u lá»›n\n\n### CÃ¡c Giáº£ Äá»‹nh Cá»§a Há»“i Quy Tuyáº¿n TÃ­nh\n\nÄá»ƒ há»“i quy tuyáº¿n tÃ­nh hoáº¡t Ä‘á»™ng tá»‘t, cáº§n thá»a mÃ£n cÃ¡c giáº£ Ä‘á»‹nh sau:\n\n**1. TÃ­nh Tuyáº¿n TÃ­nh (Linearity):**\n- Má»‘i quan há»‡ giá»¯a cÃ¡c Ä‘áº·c trÆ°ng vÃ  má»¥c tiÃªu lÃ  tuyáº¿n tÃ­nh\n- Kiá»ƒm tra: Váº½ biá»ƒu Ä‘á»“ phÃ¢n tÃ¡n giá»¯a $x$ vÃ  $y$\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Training lÃ  quÃ¡ trÃ¬nh mÃ´ hÃ¬nh há»c cÃ¡c máº«u tá»« dá»¯ liá»‡u huáº¥n luyá»‡n báº±ng cÃ¡ch Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ ná»™i bá»™ cá»§a nÃ³ (vÃ­ dá»¥: weights, biases) Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a hÃ m loss, nháº±m má»¥c Ä‘Ã­ch thá»±c hiá»‡n má»™t tÃ¡c vá»¥ cá»¥ thá»ƒ nhÆ° phÃ¢n loáº¡i hoáº·c há»“i quy.\n- Training lÃ  quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c mÃ¡y Ä‘á»ƒ há»c cÃ¡c má»‘i quan há»‡ tá»« dá»¯ liá»‡u. Trong Há»“i Quy Tuyáº¿n TÃ­nh, quÃ¡ trÃ¬nh training bao gá»“m viá»‡c tÃ¬m cÃ¡c há»‡ sá»‘ Î² tá»‘i Æ°u Ä‘á»ƒ cá»±c tiá»ƒu hÃ³a sai sá»‘ dá»± Ä‘oÃ¡n, cÃ³ thá»ƒ thÃ´ng qua nghiá»‡m dáº¡ng Ä‘Ã³ng hoáº·c cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u láº·p.\n\n**Má»‘i quan há»‡:**\n- QuÃ¡ trÃ¬nh Training cá»§a Transformer sá»­ dá»¥ng ká»¹ thuáº­t Teacher forcing Ä‘á»ƒ cung cáº¥p ground truth output lÃ m input cho decoder.\n- QuÃ¡ trÃ¬nh Training cá»§a Transformer sá»­ dá»¥ng ká»¹ thuáº­t Masking trong decoder self-attention Ä‘á»ƒ ngÄƒn nhÃ¬n vÃ o future tokens.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Lá»±a Chá»n Äáº·c TrÆ°ng & Tá»‘i Æ¯u HÃ³a MÃ´ HÃ¬nh\n- Precision, Recall, F1, AUC\n- Business metrics\n\n**6. Avoid Data Leakage:**\n- **Proper CV:** Fit preprocessors trÃªn train folds only\n- **Time-based splits:** cho time series\n- **No target leakage:** Features khÃ´ng chá»©a info vá» target\n- **Test set untouched:** Cho Ä‘áº¿n cuá»‘i\n\n**7. Document Everything:**\n- Experiments log\n- Model versions\n- Hyperparameters\n- Results vÃ  insights\n\n**8. Reproducibility:**\n- Set random seeds\n- Version control code\n- Save data versions\n- Document environment\n- Use containers (Docker)\n\n**9. Model Versioning:**\n- MLflow, DVC\n- Track models\n- Compare versions\n- Rollback náº¿u cáº§n\n\n**10. Validation Strategy:**\n- Robust CV\n- Hold-out test set\n- Temporal validation cho time series\n\n**11. Feature Engineering First:**\n- \"Data > Algorithms\"\n- Good features > Complex models\n- Domain knowledge valuable\n\n**12. Monitor Training:**\n- Training vs validation\n- Learning curves\n- Early signs of overfitting\n\n**13. Consider Production:**\n- Inference time\n- Model size\n- Dependencies\n- Maintenance\n- Explainability\n\n**14. Test on Real Data:**\n- Not just metrics\n- Qualitative analysis\n- Edge cases\n- Failure modes\n\n---\n\n---\n\n## Há»c KhÃ´ng GiÃ¡m SÃ¡t (Unsupervised Learning)\n\n### Giá»›i Thiá»‡u Vá» Há»c KhÃ´ng GiÃ¡m SÃ¡t\n\nHá»c khÃ´ng giÃ¡m sÃ¡t khÃ¡m phÃ¡ cÃ¡c máº«u áº©n trong dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n mÃ  khÃ´ng cáº§n biáº¿n má»¥c tiÃªu tÆ°á»ng minh. NÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng cho phÃ¢n tÃ­ch dá»¯ liá»‡u khÃ¡m phÃ¡, nháº­n dáº¡ng máº«u vÃ  nÃ©n dá»¯ liá»‡u.\n\n**Äáº·c Ä‘iá»ƒm chÃ­nh:**\n- KhÃ´ng cÃ³ labels (y)\n- Chá»‰ cÃ³ features (X)\n- TÃ¬m structure trong data\n- Exploratory analysis\n\n**So vá»›i Supervised Learning:**\n| TiÃªu chÃ­ | Supervised | Unsupervised |\n|----------|-----------|--------------|\n| Labels | CÃ³ | KhÃ´ng |\n| Má»¥c tiÃªu | Dá»± Ä‘oÃ¡n | KhÃ¡m phÃ¡ |\n| Feedback | CÃ³ (accuracy) | KhÃ´ng rÃµ rÃ ng |\n| á»¨ng dá»¥ng | Classification, Regression | Clustering, Dimensionality Reduction |\n\n**CÃ¡c tÃ¡c vá»¥ chÃ­nh:**\n1. **Clustering:** NhÃ³m dá»¯ liá»‡u tÆ°Æ¡ng tá»±\n2. **Dimensionality Reduction:** Giáº£m sá»‘ chiá»u\n3. **Anomaly Detection:** PhÃ¡t hiá»‡n báº¥t thÆ°á»ng\n4. **Association Rule Learning:** TÃ¬m má»‘i quan há»‡\n\n**ThÃ¡ch thá»©c:**\n- KhÃ´ng cÃ³ ground truth Ä‘á»ƒ Ä‘Ã¡nh giÃ¡\n- KhÃ³ xÃ¡c Ä‘á»‹nh sá»‘ clusters/components\n- Káº¿t quáº£ cÃ³ thá»ƒ subjective\n- Cáº§n domain knowledge Ä‘á»ƒ interpret\n\n### Clustering (PhÃ¢n Cá»¥m)\n\nNhÃ³m cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u tÆ°Æ¡ng tá»± láº¡i vá»›i nhau.\n\n**Má»¥c tiÃªu:**\n- High intra-cluster similarity (trong cÃ¹ng cluster)\n- Low inter-cluster similarity (giá»¯a cÃ¡c clusters)\n\n**á»¨ng dá»¥ng:**\n- Customer segmentation\n- Document clustering\n- Image segmentation\n- Anomaly detection\n- Data compression\n\n### K-Means Clustering\n\nThuáº­t toÃ¡n phÃ¢n cá»¥m phá»• biáº¿n nháº¥t, chia dá»¯ liá»‡u thÃ nh K clusters.\n\n**Thuáº­t toÃ¡n:**\n\n**BÆ°á»›c 1: Initialization**\n- Chá»n K centroids ngáº«u nhiÃªn\n- CÃ³ thá»ƒ tá»« data points hoáº·c random positions\n\n**BÆ°á»›c 2: Assignment**\n- GÃ¡n má»—i Ä‘iá»ƒm Ä‘áº¿n centroid gáº§n nháº¥t\n- Sá»­ dá»¥ng Euclidean distance:\n$$d(x, \\mu_k) = ||x - \\mu_k|| = \\sqrt{\\sum_{j=1}^{n}(x_j - \\mu_{kj})^2}$$\n\n**BÆ°á»›c 3: Update**\n- Cáº­p nháº­t centroids = mean cá»§a cÃ¡c Ä‘iá»ƒm assigned\n$$\\mu_k = \frac{1}{|C_k|}\\sum_{x \\in C_k}x$$\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Training lÃ  quÃ¡ trÃ¬nh mÃ´ hÃ¬nh há»c cÃ¡c máº«u tá»« dá»¯ liá»‡u huáº¥n luyá»‡n báº±ng cÃ¡ch Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ ná»™i bá»™ cá»§a nÃ³ (vÃ­ dá»¥: weights, biases) Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a hÃ m loss, nháº±m má»¥c Ä‘Ã­ch thá»±c hiá»‡n má»™t tÃ¡c vá»¥ cá»¥ thá»ƒ nhÆ° phÃ¢n loáº¡i hoáº·c há»“i quy.\n- Training lÃ  quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c mÃ¡y Ä‘á»ƒ há»c cÃ¡c má»‘i quan há»‡ tá»« dá»¯ liá»‡u. Trong Há»“i Quy Tuyáº¿n TÃ­nh, quÃ¡ trÃ¬nh training bao gá»“m viá»‡c tÃ¬m cÃ¡c há»‡ sá»‘ Î² tá»‘i Æ°u Ä‘á»ƒ cá»±c tiá»ƒu hÃ³a sai sá»‘ dá»± Ä‘oÃ¡n, cÃ³ thá»ƒ thÃ´ng qua nghiá»‡m dáº¡ng Ä‘Ã³ng hoáº·c cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u láº·p.\n\n**Má»‘i quan há»‡:**\n- QuÃ¡ trÃ¬nh Training cá»§a Transformer sá»­ dá»¥ng ká»¹ thuáº­t Teacher forcing Ä‘á»ƒ cung cáº¥p ground truth output lÃ m input cho decoder.\n- QuÃ¡ trÃ¬nh Training cá»§a Transformer sá»­ dá»¥ng ká»¹ thuáº­t Masking trong decoder self-attention Ä‘á»ƒ ngÄƒn nhÃ¬n vÃ o future tokens."
        },
        "output_without_context": {
            "question": "Trong Æ°á»›c lÆ°á»£ng kháº£ dÄ© tá»‘i Ä‘a (MLE), Ä‘iá»u gÃ¬ lÃ  cáº§n thiáº¿t Ä‘á»ƒ tá»‘i Ä‘a hÃ³a kháº£ dÄ© cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n?",
            "answer": "Tá»‘i Ä‘a hÃ³a hÃ m kháº£ dÄ© cá»§a dá»¯ liá»‡u."
        }
    },
    {
        "topic_description": "Ná»™i dung chá»§ yáº¿u táº­p trung vÃ o mÃ´ hÃ¬nh sinh xÃ¡c suáº¥t vÃ  cÃ¡ch phÃ¢n loáº¡i cÃ¡c thá»ƒ hiá»‡n vÃ o cÃ¡c lá»›p khÃ¡c nhau. Há»c sinh sáº½ Ä‘Æ°á»£c kiá»ƒm tra kháº£ nÄƒng xÃ¡c Ä‘á»‹nh cÃ¡c tham sá»‘ cáº§n thiáº¿t vÃ  hiá»ƒu rÃµ vá» mÃ´ hÃ¬nh nÃ y trong má»™t sá»‘ á»©ng dá»¥ng thá»±c táº¿.",
        "output_with_context": {
            "question": "Trong mÃ´ hÃ¬nh Softmax Regression, cÃ´ng thá»©c nÃ o Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh xÃ¡c suáº¥t cá»§a lá»›p k cho má»™t Ä‘áº§u vÃ o x?",
            "answer": "P(y=k|x) = \\frac{e^{z_k}}{\\sum_{j=1}^{K}e^{z_j}}",
            "rag_context": "**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## PhÃ¢n Loáº¡i (Classification)\n- Æ¯u Ä‘iá»ƒm: Má»—i mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n hÆ¡n, cÃ¢n báº±ng hÆ¡n\n- NhÆ°á»£c Ä‘iá»ƒm: Nhiá»u mÃ´ hÃ¬nh (phá»©c táº¡p khi K lá»›n)\n\n**VÃ­ dá»¥:** 3 lá»›p (A, B, C)\n- MÃ´ hÃ¬nh 1: A vs B\n- MÃ´ hÃ¬nh 2: A vs C\n- MÃ´ hÃ¬nh 3: B vs C\n\n**3. Softmax Regression (Multinomial Logistic Regression):**\n\nMá»Ÿ rá»™ng trá»±c tiáº¿p cá»§a logistic regression cho Ä‘a lá»›p.\n\n**CÃ´ng thá»©c:**\n$$P(y=k|x) = \frac{e^{z_k}}{\\sum_{j=1}^{K}e^{z_j}}$$\n\nTrong Ä‘Ã³: $z_k = \beta_k^Tx$ vá»›i $\beta_k$ lÃ  vector há»‡ sá»‘ cho lá»›p $k$\n\n**Äáº·c Ä‘iá»ƒm:**\n- Tá»•ng cÃ¡c xÃ¡c suáº¥t = 1: $\\sum_{k=1}^{K}P(y=k|x) = 1$\n- Output lÃ  phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn táº¥t cáº£ lá»›p\n- Huáº¥n luyá»‡n Ä‘á»“ng thá»i táº¥t cáº£ lá»›p\n\n**HÃ m chi phÃ­ (Categorical Cross-Entropy):**\n$$J(\beta) = -\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}\\log(P(y=k|x^{(i)}))$$\n\nTrong Ä‘Ã³ $y_k^{(i)}$ lÃ  one-hot encoding cá»§a nhÃ£n.\n\n**Lá»±a chá»n giá»¯a OvR, OvO, vÃ  Softmax:**\n- **Softmax:** Tá»‘t nháº¥t khi cáº§n xÃ¡c suáº¥t, K khÃ´ng quÃ¡ lá»›n\n- **OvR:** ÄÆ¡n giáº£n, hiá»‡u quáº£ vá»›i K lá»›n\n- **OvO:** Tá»‘t vá»›i SVM, K nhá»/trung bÃ¬nh\n\n### Naive Bayes Classifier (Bá»™ PhÃ¢n Loáº¡i Naive Bayes)\n\nDá»±a trÃªn Ä‘á»‹nh lÃ½ Bayes vá»›i giáº£ Ä‘á»‹nh \"ngÃ¢y thÆ¡\" (naive) vá» tÃ­nh Ä‘á»™c láº­p Ä‘áº·c trÆ°ng.\n\n**Äá»‹nh LÃ½ Bayes:**\n$$P(C_k|x) = \frac{P(x|C_k)P(C_k)}{P(x)}$$\n\nTrong Ä‘Ã³:\n- $P(C_k|x)$: XÃ¡c suáº¥t háº­u nghiá»‡m (posterior) - xÃ¡c suáº¥t lá»›p $C_k$ cho trÆ°á»›c $x$\n- $P(x|C_k)$: Likelihood - xÃ¡c suáº¥t cá»§a $x$ trong lá»›p $C_k$\n- $P(C_k)$: XÃ¡c suáº¥t tiÃªn nghiá»‡m (prior) cá»§a lá»›p $C_k$\n- $P(x)$: Evidence - xÃ¡c suáº¥t cá»§a $x$\n\n**Giáº£ Äá»‹nh Naive (Äá»™c Láº­p Äiá»u Kiá»‡n):**\n$$P(x_1, x_2, ..., x_n|C_k) = \\prod_{i=1}^{n}P(x_i|C_k)$$\n\nCÃ¡c Ä‘áº·c trÆ°ng Ä‘á»™c láº­p vá»›i nhau khi biáº¿t lá»›p.\n\n**CÃ´ng Thá»©c Äáº§y Äá»§:**\n$$P(C_k|x_1,...,x_n) = \frac{P(C_k)\\prod_{i=1}^{n}P(x_i|C_k)}{P(x_1,...,x_n)}$$\n\n**Quyáº¿t Äá»‹nh:**\n$$\\hat{y} = \\arg\\max_{k} P(C_k)\\prod_{i=1}^{n}P(x_i|C_k)$$\n\nKhÃ´ng cáº§n tÃ­nh $P(x)$ vÃ¬ nÃ³ giá»‘ng nhau cho táº¥t cáº£ lá»›p.\n\n**CÃ¡c Biáº¿n Thá»ƒ:**\n\n**1. Gaussian Naive Bayes:**\n- Cho Ä‘áº·c trÆ°ng liÃªn tá»¥c\n- Giáº£ Ä‘á»‹nh phÃ¢n phá»‘i Gaussian (chuáº©n)\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- P(x) lÃ  evidence, Ä‘áº¡i diá»‡n cho xÃ¡c suáº¥t cá»§a cÃ¡c Ä‘áº·c trÆ°ng x. Trong Naive Bayes Classifier, P(x) thÆ°á»ng khÃ´ng cáº§n tÃ­nh toÃ¡n trá»±c tiáº¿p vÃ¬ nÃ³ lÃ  má»™t háº±ng sá»‘ cho táº¥t cáº£ cÃ¡c lá»›p khi so sÃ¡nh xÃ¡c suáº¥t háº­u nghiá»‡m, vÃ  do Ä‘Ã³ khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n quyáº¿t Ä‘á»‹nh phÃ¢n loáº¡i cuá»‘i cÃ¹ng.\n- PhÃ¢n loáº¡i lÃ  má»™t tÃ¡c vá»¥ trong há»c mÃ¡y nháº±m gÃ¡n má»™t nhÃ£n lá»›p cá»¥ thá»ƒ cho cÃ¡c máº«u dá»¯ liá»‡u Ä‘áº§u vÃ o dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng cá»§a chÃºng. Má»¥c tiÃªu lÃ  xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh, vÃ­ dá»¥ nhÆ° CÃ¢y Quyáº¿t Äá»‹nh, cÃ³ kháº£ nÄƒng dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nhÃ£n lá»›p cá»§a cÃ¡c máº«u má»›i, vá»›i káº¿t quáº£ dá»± Ä‘oÃ¡n á»Ÿ nÃºt lÃ¡ lÃ  má»™t nhÃ£n lá»›p cá»¥ thá»ƒ.\n\n**Má»‘i quan há»‡:**\n- Softmax Regression lÃ  má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n PhÃ¢n loáº¡i Ä‘a lá»›p, cung cáº¥p xÃ¡c suáº¥t cho má»—i lá»›p.\n- Naive Bayes Classifier lÃ  má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n PhÃ¢n loáº¡i, Ä‘áº·c biá»‡t hiá»‡u quáº£ vá»›i giáº£ Ä‘á»‹nh Ä‘á»™c láº­p Ä‘áº·c trÆ°ng.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Lá»±a Chá»n Äáº·c TrÆ°ng & Tá»‘i Æ¯u HÃ³a MÃ´ HÃ¬nh\n- Precision, Recall, F1, AUC\n- Business metrics\n\n**6. Avoid Data Leakage:**\n- **Proper CV:** Fit preprocessors trÃªn train folds only\n- **Time-based splits:** cho time series\n- **No target leakage:** Features khÃ´ng chá»©a info vá» target\n- **Test set untouched:** Cho Ä‘áº¿n cuá»‘i\n\n**7. Document Everything:**\n- Experiments log\n- Model versions\n- Hyperparameters\n- Results vÃ  insights\n\n**8. Reproducibility:**\n- Set random seeds\n- Version control code\n- Save data versions\n- Document environment\n- Use containers (Docker)\n\n**9. Model Versioning:**\n- MLflow, DVC\n- Track models\n- Compare versions\n- Rollback náº¿u cáº§n\n\n**10. Validation Strategy:**\n- Robust CV\n- Hold-out test set\n- Temporal validation cho time series\n\n**11. Feature Engineering First:**\n- \"Data > Algorithms\"\n- Good features > Complex models\n- Domain knowledge valuable\n\n**12. Monitor Training:**\n- Training vs validation\n- Learning curves\n- Early signs of overfitting\n\n**13. Consider Production:**\n- Inference time\n- Model size\n- Dependencies\n- Maintenance\n- Explainability\n\n**14. Test on Real Data:**\n- Not just metrics\n- Qualitative analysis\n- Edge cases\n- Failure modes\n\n---\n\n---\n\n## Há»c KhÃ´ng GiÃ¡m SÃ¡t (Unsupervised Learning)\n\n### Giá»›i Thiá»‡u Vá» Há»c KhÃ´ng GiÃ¡m SÃ¡t\n\nHá»c khÃ´ng giÃ¡m sÃ¡t khÃ¡m phÃ¡ cÃ¡c máº«u áº©n trong dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n mÃ  khÃ´ng cáº§n biáº¿n má»¥c tiÃªu tÆ°á»ng minh. NÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng cho phÃ¢n tÃ­ch dá»¯ liá»‡u khÃ¡m phÃ¡, nháº­n dáº¡ng máº«u vÃ  nÃ©n dá»¯ liá»‡u.\n\n**Äáº·c Ä‘iá»ƒm chÃ­nh:**\n- KhÃ´ng cÃ³ labels (y)\n- Chá»‰ cÃ³ features (X)\n- TÃ¬m structure trong data\n- Exploratory analysis\n\n**So vá»›i Supervised Learning:**\n| TiÃªu chÃ­ | Supervised | Unsupervised |\n|----------|-----------|--------------|\n| Labels | CÃ³ | KhÃ´ng |\n| Má»¥c tiÃªu | Dá»± Ä‘oÃ¡n | KhÃ¡m phÃ¡ |\n| Feedback | CÃ³ (accuracy) | KhÃ´ng rÃµ rÃ ng |\n| á»¨ng dá»¥ng | Classification, Regression | Clustering, Dimensionality Reduction |\n\n**CÃ¡c tÃ¡c vá»¥ chÃ­nh:**\n1. **Clustering:** NhÃ³m dá»¯ liá»‡u tÆ°Æ¡ng tá»±\n2. **Dimensionality Reduction:** Giáº£m sá»‘ chiá»u\n3. **Anomaly Detection:** PhÃ¡t hiá»‡n báº¥t thÆ°á»ng\n4. **Association Rule Learning:** TÃ¬m má»‘i quan há»‡\n\n**ThÃ¡ch thá»©c:**\n- KhÃ´ng cÃ³ ground truth Ä‘á»ƒ Ä‘Ã¡nh giÃ¡\n- KhÃ³ xÃ¡c Ä‘á»‹nh sá»‘ clusters/components\n- Káº¿t quáº£ cÃ³ thá»ƒ subjective\n- Cáº§n domain knowledge Ä‘á»ƒ interpret\n\n### Clustering (PhÃ¢n Cá»¥m)\n\nNhÃ³m cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u tÆ°Æ¡ng tá»± láº¡i vá»›i nhau.\n\n**Má»¥c tiÃªu:**\n- High intra-cluster similarity (trong cÃ¹ng cluster)\n- Low inter-cluster similarity (giá»¯a cÃ¡c clusters)\n\n**á»¨ng dá»¥ng:**\n- Customer segmentation\n- Document clustering\n- Image segmentation\n- Anomaly detection\n- Data compression\n\n### K-Means Clustering\n\nThuáº­t toÃ¡n phÃ¢n cá»¥m phá»• biáº¿n nháº¥t, chia dá»¯ liá»‡u thÃ nh K clusters.\n\n**Thuáº­t toÃ¡n:**\n\n**BÆ°á»›c 1: Initialization**\n- Chá»n K centroids ngáº«u nhiÃªn\n- CÃ³ thá»ƒ tá»« data points hoáº·c random positions\n\n**BÆ°á»›c 2: Assignment**\n- GÃ¡n má»—i Ä‘iá»ƒm Ä‘áº¿n centroid gáº§n nháº¥t\n- Sá»­ dá»¥ng Euclidean distance:\n$$d(x, \\mu_k) = ||x - \\mu_k|| = \\sqrt{\\sum_{j=1}^{n}(x_j - \\mu_{kj})^2}$$\n\n**BÆ°á»›c 3: Update**\n- Cáº­p nháº­t centroids = mean cá»§a cÃ¡c Ä‘iá»ƒm assigned\n$$\\mu_k = \frac{1}{|C_k|}\\sum_{x \\in C_k}x$$\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Training lÃ  quÃ¡ trÃ¬nh mÃ´ hÃ¬nh há»c cÃ¡c máº«u tá»« dá»¯ liá»‡u huáº¥n luyá»‡n báº±ng cÃ¡ch Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ ná»™i bá»™ cá»§a nÃ³ (vÃ­ dá»¥: weights, biases) Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a hÃ m loss, nháº±m má»¥c Ä‘Ã­ch thá»±c hiá»‡n má»™t tÃ¡c vá»¥ cá»¥ thá»ƒ nhÆ° phÃ¢n loáº¡i hoáº·c há»“i quy.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## PhÃ¢n Loáº¡i (Classification)\n$$P(x_i|C_k) = \frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}\right)$$\n- Æ¯á»›c lÆ°á»£ng $\\mu_k$ (mean) vÃ  $\\sigma_k^2$ (variance) tá»« dá»¯ liá»‡u\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n, nháº­n dáº¡ng máº«u\n\n**2. Multinomial Naive Bayes:**\n- Cho Ä‘áº¿m rá»i ráº¡c (word counts, frequencies)\n- PhÃ¢n phá»‘i Ä‘a thá»©c\n$$P(x_i|C_k) = \frac{N_{ki} + \\alpha}{N_k + \\alpha n}$$\n  - $N_{ki}$: Sá»‘ láº§n Ä‘áº·c trÆ°ng $i$ xuáº¥t hiá»‡n trong lá»›p $k$\n  - $N_k$: Tá»•ng sá»‘ Ä‘áº¿m trong lá»›p $k$\n  - $\\alpha$: Laplace smoothing (thÆ°á»ng = 1)\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n, phÃ¢n tÃ­ch cáº£m xÃºc, lá»c spam\n\n**3. Bernoulli Naive Bayes:**\n- Cho Ä‘áº·c trÆ°ng nhá»‹ phÃ¢n (cÃ³/khÃ´ng)\n- PhÃ¢n phá»‘i Bernoulli\n$$P(x_i|C_k) = P(i|C_k)x_i + (1-P(i|C_k))(1-x_i)$$\n- TÃ­nh cáº£ viá»‡c Ä‘áº·c trÆ°ng xuáº¥t hiá»‡n vÃ  khÃ´ng xuáº¥t hiá»‡n\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n vá»›i binary features\n\n**Æ¯u Äiá»ƒm:**\n- Nhanh, hiá»‡u quáº£\n- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i dá»¯ liá»‡u nhá»\n- Dá»… triá»ƒn khai vÃ  diá»…n giáº£i\n- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i nhiá»u Ä‘áº·c trÆ°ng\n- KhÃ´ng nháº¡y cáº£m vá»›i Ä‘áº·c trÆ°ng khÃ´ng liÃªn quan\n\n**NhÆ°á»£c Äiá»ƒm:**\n- Giáº£ Ä‘á»‹nh Ä‘á»™c láº­p hiáº¿m khi Ä‘Ãºng trong thá»±c táº¿\n- \"Zero frequency problem\" cáº§n smoothing\n- Æ¯á»›c lÆ°á»£ng xÃ¡c suáº¥t cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c\n- KhÃ´ng tá»‘t khi Ä‘áº·c trÆ°ng tÆ°Æ¡ng quan\n\n**Laplace Smoothing:**\nXá»­ lÃ½ váº¥n Ä‘á» xÃ¡c suáº¥t = 0:\n$$P(x_i|C_k) = \frac{count(x_i, C_k) + \\alpha}{count(C_k) + \\alpha \times |V|}$$\n\n### k-Nearest Neighbors (k-NN) - K LÃ¡ng Giá»ng Gáº§n Nháº¥t\n\nPhÆ°Æ¡ng phÃ¡p non-parametric phÃ¢n loáº¡i dá»±a trÃªn Ä‘a sá»‘ vote cá»§a k lÃ¡ng giá»ng gáº§n nháº¥t.\n\n**Thuáº­t ToÃ¡n:**\n1. TÃ­nh khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm cáº§n phÃ¢n loáº¡i Ä‘áº¿n táº¥t cáº£ Ä‘iá»ƒm huáº¥n luyá»‡n\n2. Chá»n k Ä‘iá»ƒm gáº§n nháº¥t\n3. Vote: Lá»›p xuáº¥t hiá»‡n nhiá»u nháº¥t trong k lÃ¡ng giá»ng\n4. GÃ¡n nhÃ£n lá»›p Ä‘Ã³ cho Ä‘iá»ƒm má»›i\n\n**CÃ¡c Äá»™ Äo Khoáº£ng CÃ¡ch:**\n\n**1. Euclidean Distance (Khoáº£ng cÃ¡ch Euclid):**\n$$d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}$$\n- Phá»• biáº¿n nháº¥t\n- Khoáº£ng cÃ¡ch Ä‘Æ°á»ng tháº³ng\n- Nháº¡y cáº£m vá»›i scale cá»§a Ä‘áº·c trÆ°ng\n\n**2. Manhattan Distance (Khoáº£ng cÃ¡ch Manhattan):**\n$$d(x,y) = \\sum_{i=1}^{n}|x_i-y_i|$$\n- Khoáº£ng cÃ¡ch theo lÆ°á»›i Ä‘Ã´ thá»‹\n- Ãt nháº¡y cáº£m vá»›i outliers\n- Tá»‘t cho dá»¯ liá»‡u high-dimensional\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Æ¯á»›c lÆ°á»£ng lÃ  quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh (vÃ­ dá»¥: mean Î¼_k vÃ  variance Ïƒ_k^2 trong Gaussian Naive Bayes) tá»« dá»¯ liá»‡u huáº¥n luyá»‡n.\n- Bernoulli Naive Bayes lÃ  má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i dá»±a trÃªn Ä‘á»‹nh lÃ½ Bayes, Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c Ä‘áº·c trÆ°ng nhá»‹ phÃ¢n (binary features), tá»©c lÃ  cÃ¡c Ä‘áº·c trÆ°ng chá»‰ cÃ³ thá»ƒ cÃ³ hai giÃ¡ trá»‹ (vÃ­ dá»¥: cÃ³ hoáº·c khÃ´ng xuáº¥t hiá»‡n). NÃ³ sá»­ dá»¥ng phÃ¢n phá»‘i Bernoulli Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t cá»§a cÃ¡c Ä‘áº·c trÆ°ng.\n\n**Má»‘i quan há»‡:**\n- Bernoulli Naive Bayes Ä‘Æ°á»£c á»©ng dá»¥ng trong phÃ¢n loáº¡i vÄƒn báº£n khi cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng nhá»‹ phÃ¢n (vÃ­ dá»¥: tá»« cÃ³ xuáº¥t hiá»‡n hay khÃ´ng).\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»“i Quy Tuyáº¿n TÃ­nh (Linear Regression)\n**Underfitting (High Bias):**\n- Training error cao\n- Validation error cao\n- MÃ´ hÃ¬nh quÃ¡ Ä‘Æ¡n giáº£n\n- Giáº£i phÃ¡p: ThÃªm Ä‘áº·c trÆ°ng, tÄƒng Ä‘á»™ phá»©c táº¡p, giáº£m regularization\n\n**Overfitting (High Variance):**\n- Training error tháº¥p\n- Validation error cao (chÃªnh lá»‡ch lá»›n)\n- MÃ´ hÃ¬nh quÃ¡ phá»©c táº¡p\n- Giáº£i phÃ¡p: ThÃªm dá»¯ liá»‡u, regularization, giáº£m Ä‘áº·c trÆ°ng, early stopping\n\n**Good fit:**\n- Training error tháº¥p\n- Validation error tháº¥p\n- ChÃªnh lá»‡ch nhá» giá»¯a hai errors\n\n\n---\n\n## PhÃ¢n Loáº¡i (Classification)\n\n### Giá»›i Thiá»‡u Vá» PhÃ¢n Loáº¡i\n\nPhÃ¢n loáº¡i lÃ  má»™t tÃ¡c vá»¥ há»c cÃ³ giÃ¡m sÃ¡t trong Ä‘Ã³ má»¥c tiÃªu lÃ  dá»± Ä‘oÃ¡n nhÃ£n lá»›p rá»i ráº¡c. KhÃ¡c vá»›i há»“i quy dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c, phÃ¢n loáº¡i gÃ¡n cÃ¡c Ä‘áº§u vÃ o vÃ o cÃ¡c danh má»¥c Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c.\n\n**á»¨ng dá»¥ng thá»±c táº¿:**\n- PhÃ¡t hiá»‡n thÆ° rÃ¡c (spam/khÃ´ng spam)\n- Cháº©n Ä‘oÃ¡n bá»‡nh (bá»‡nh/khÃ´ng bá»‡nh)\n- Nháº­n dáº¡ng chá»¯ viáº¿t tay\n- PhÃ¢n tÃ­ch cáº£m xÃºc (tÃ­ch cá»±c/tiÃªu cá»±c/trung láº­p)\n- PhÃ¡t hiá»‡n gian láº­n tháº» tÃ­n dá»¥ng\n- Nháº­n dáº¡ng khuÃ´n máº·t\n- PhÃ¢n loáº¡i vÄƒn báº£n, hÃ¬nh áº£nh\n\n### CÃ¡c Loáº¡i BÃ i ToÃ¡n PhÃ¢n Loáº¡i\n\n**1. PhÃ¢n Loáº¡i Nhá»‹ PhÃ¢n (Binary Classification):**\n- Hai lá»›p duy nháº¥t\n- VÃ­ dá»¥: Email spam/khÃ´ng spam, Bá»‡nh/khá»e máº¡nh\n- MÃ£ hÃ³a nhÃ£n: 0 vÃ  1, hoáº·c -1 vÃ  +1\n\n**2. PhÃ¢n Loáº¡i Äa Lá»›p (Multiclass Classification):**\n- Nhiá»u hÆ¡n hai lá»›p\n- Má»—i máº«u thuá»™c Ä‘Ãºng má»™t lá»›p\n- VÃ­ dá»¥: Nháº­n dáº¡ng chá»¯ sá»‘ (0-9), PhÃ¢n loáº¡i loáº¡i hoa\n- MÃ£ hÃ³a nhÃ£n: One-hot encoding\n\n**3. PhÃ¢n Loáº¡i Äa NhÃ£n (Multilabel Classification):**\n- Má»—i máº«u cÃ³ thá»ƒ thuá»™c nhiá»u lá»›p\n- VÃ­ dá»¥: Gáº¯n tháº» bÃ i viáº¿t (cÃ´ng nghá»‡, kinh táº¿, chÃ­nh trá»‹), PhÃ¢n loáº¡i thá»ƒ loáº¡i phim\n\n### Há»“i Quy Logistic (Logistic Regression)\n\nMáº·c dÃ¹ cÃ³ tÃªn lÃ  \"regression\", há»“i quy logistic lÃ  thuáº­t toÃ¡n phÃ¢n loáº¡i mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t cá»§a káº¿t quáº£ nhá»‹ phÃ¢n.\n\n**HÃ m Sigmoid (Logistic Function):**\n$$\\sigma(z) = \frac{1}{1 + e^{-z}}$$\n\nTrong Ä‘Ã³: $z = \beta_0 + \beta_1x_1 + ... + \beta_nx_n = \beta^Tx$\n\n**Äáº·c Ä‘iá»ƒm hÃ m Sigmoid:**\n- Miá»n giÃ¡ trá»‹: $(0, 1)$ - phÃ¹ há»£p Ä‘á»ƒ biá»ƒu diá»…n xÃ¡c suáº¥t\n- $\\sigma(0) = 0.5$\n- $\\sigma(z) \to 1$ khi $z \to \\infty$\n- $\\sigma(z) \to 0$ khi $z \to -\\infty$\n- Äáº¡o hÃ m: $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n\n**Diá»…n Giáº£i:**\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- PhÃ¢n loáº¡i lÃ  má»™t tÃ¡c vá»¥ há»c cÃ³ giÃ¡m sÃ¡t trong Ä‘Ã³ má»¥c tiÃªu lÃ  dá»± Ä‘oÃ¡n nhÃ£n lá»›p rá»i ráº¡c. KhÃ¡c vá»›i há»“i quy dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c, phÃ¢n loáº¡i gÃ¡n cÃ¡c Ä‘áº§u vÃ o vÃ o cÃ¡c danh má»¥c Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c. CÃ¡c á»©ng dá»¥ng bao gá»“m phÃ¡t hiá»‡n thÆ° rÃ¡c, cháº©n Ä‘oÃ¡n bá»‡nh, nháº­n dáº¡ng chá»¯ viáº¿t tay, phÃ¢n tÃ­ch cáº£m xÃºc, phÃ¡t hiá»‡n gian láº­n tháº» tÃ­n dá»¥ng, nháº­n dáº¡ng khuÃ´n máº·t vÃ  phÃ¢n loáº¡i vÄƒn báº£n, hÃ¬nh áº£nh.\n\n**Má»‘i quan há»‡:**\n- PhÃ¢n loáº¡i chá»©a bÃ i toÃ¡n PhÃ¢n loáº¡i Nhá»‹ phÃ¢n, nÆ¡i má»¥c tiÃªu lÃ  dá»± Ä‘oÃ¡n má»™t trong hai lá»›p.\n- PhÃ¢n loáº¡i chá»©a bÃ i toÃ¡n PhÃ¢n loáº¡i Äa Lá»›p, nÆ¡i má»¥c tiÃªu lÃ  dá»± Ä‘oÃ¡n má»™t trong nhiá»u lá»›p.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## CÃ¢y Quyáº¿t Äá»‹nh (Decision Tree)\n\n### Giá»›i Thiá»‡u Vá» CÃ¢y Quyáº¿t Äá»‹nh\n\nCÃ¢y quyáº¿t Ä‘á»‹nh lÃ  thuáº­t toÃ¡n há»c cÃ³ giÃ¡m sÃ¡t Ä‘a nÄƒng cÃ³ thá»ƒ thá»±c hiá»‡n cáº£ tÃ¡c vá»¥ phÃ¢n loáº¡i vÃ  há»“i quy. ChÃºng há»c cÃ¡c quy táº¯c quyáº¿t Ä‘á»‹nh tá»« cÃ¡c Ä‘áº·c trÆ°ng Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ má»¥c tiÃªu thÃ´ng qua cáº¥u trÃºc dáº¡ng cÃ¢y.\n\n**á»¨ng dá»¥ng thá»±c táº¿:**\n- Cháº©n Ä‘oÃ¡n y táº¿ (chuá»—i quyáº¿t Ä‘á»‹nh dá»±a trÃªn triá»‡u chá»©ng)\n- ÄÃ¡nh giÃ¡ rá»§i ro tÃ­n dá»¥ng\n- Dá»± Ä‘oÃ¡n churn khÃ¡ch hÃ ng\n- PhÃ¡t hiá»‡n gian láº­n\n- Há»‡ thá»‘ng chuyÃªn gia\n- PhÃ¢n loáº¡i email spam\n\n**Táº¡i sao gá»i lÃ  \"cÃ¢y\":**\n- Cáº¥u trÃºc phÃ¢n cáº¥p giá»‘ng cÃ¢y ngÆ°á»£c\n- Gá»‘c á»Ÿ trÃªn, lÃ¡ á»Ÿ dÆ°á»›i\n- Quyáº¿t Ä‘á»‹nh Ä‘Æ°á»£c Ä‘Æ°a ra táº¡i má»—i nÃºt ná»™i bá»™\n- Káº¿t quáº£ cuá»‘i cÃ¹ng á»Ÿ nÃºt lÃ¡\n\n### Cáº¥u TrÃºc CÃ¢y\n\n**1. NÃºt Gá»‘c (Root Node):**\n- NÃºt trÃªn cÃ¹ng Ä‘áº¡i diá»‡n cho toÃ n bá»™ táº­p dá»¯ liá»‡u\n- Chá»©a táº¥t cáº£ máº«u training\n- Äiá»ƒm báº¯t Ä‘áº§u cá»§a quÃ¡ trÃ¬nh quyáº¿t Ä‘á»‹nh\n- CÃ³ phÃ¢n chia Ä‘áº§u tiÃªn dá»±a trÃªn Ä‘áº·c trÆ°ng quan trá»ng nháº¥t\n\n**2. NÃºt Ná»™i Bá»™ (Internal Nodes):**\n- CÃ¡c nÃºt quyáº¿t Ä‘á»‹nh dá»±a trÃªn kiá»ƒm tra Ä‘áº·c trÆ°ng\n- Má»—i nÃºt thá»±c hiá»‡n má»™t cÃ¢u há»i yes/no vá» Ä‘áº·c trÆ°ng\n- VÃ­ dá»¥: \"Tuá»•i > 30?\", \"Thu nháº­p < 50,000?\"\n- Chia dá»¯ liá»‡u thÃ nh cÃ¡c táº­p con\n\n**3. NhÃ¡nh (Branches):**\n- Káº¿t quáº£ cá»§a cÃ¡c quyáº¿t Ä‘á»‹nh\n- Káº¿t ná»‘i nÃºt cha vá»›i nÃºt con\n- Äáº¡i diá»‡n cho giÃ¡ trá»‹ hoáº·c pháº¡m vi giÃ¡ trá»‹ cá»§a Ä‘áº·c trÆ°ng\n\n**4. NÃºt LÃ¡ (Leaf Nodes):**\n- NÃºt cuá»‘i cÃ¹ng khÃ´ng cÃ³ nhÃ¡nh con\n- Chá»©a dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng\n- PhÃ¢n loáº¡i: NhÃ£n lá»›p\n- Há»“i quy: GiÃ¡ trá»‹ sá»‘\n\n**VÃ­ dá»¥ minh há»a - Quyáº¿t Ä‘á»‹nh mua nhÃ :**\n```\n                 [Thu nháº­p > 50K?]\n                /                 \\\n            YES                    NO\n           /                         \\\n   [Tuá»•i > 30?]                [KhÃ´ng mua]\n    /        \\\n  YES        NO\n  /            \\\n[Mua]      [ThuÃª]\n```\n\n### XÃ¢y Dá»±ng CÃ¢y Quyáº¿t Äá»‹nh\n\n**TiÃªu ChÃ­ PhÃ¢n Chia (Splitting Criteria):**\n\nMá»¥c tiÃªu: TÃ¬m phÃ¢n chia tá»‘t nháº¥t lÃ m tÄƒng \"Ä‘á»™ thuáº§n khiáº¿t\" (purity) cá»§a cÃ¡c táº­p con.\n\n**Cho PhÃ¢n Loáº¡i:**\n\n**1. Gini Impurity (Chá»‰ Sá»‘ Gini):**\n$$Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$$\n\nTrong Ä‘Ã³:\n- $p_i$ lÃ  tá»· lá»‡ máº«u thuá»™c lá»›p $i$ táº¡i nÃºt $t$\n- $C$ lÃ  sá»‘ lá»›p\n- Gini = 0: NÃºt hoÃ n toÃ n thuáº§n khiáº¿t (táº¥t cáº£ máº«u cÃ¹ng lá»›p)\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- PhÃ¢n loáº¡i lÃ  má»™t tÃ¡c vá»¥ trong há»c mÃ¡y nháº±m gÃ¡n má»™t nhÃ£n lá»›p cá»¥ thá»ƒ cho cÃ¡c máº«u dá»¯ liá»‡u Ä‘áº§u vÃ o dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng cá»§a chÃºng. Má»¥c tiÃªu lÃ  xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh, vÃ­ dá»¥ nhÆ° CÃ¢y Quyáº¿t Äá»‹nh, cÃ³ kháº£ nÄƒng dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nhÃ£n lá»›p cá»§a cÃ¡c máº«u má»›i, vá»›i káº¿t quáº£ dá»± Ä‘oÃ¡n á»Ÿ nÃºt lÃ¡ lÃ  má»™t nhÃ£n lá»›p cá»¥ thá»ƒ.\n\n**Má»‘i quan há»‡:**\n- Softmax Regression lÃ  má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n PhÃ¢n loáº¡i Ä‘a lá»›p, cung cáº¥p xÃ¡c suáº¥t cho má»—i lá»›p.\n- Naive Bayes Classifier lÃ  má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n PhÃ¢n loáº¡i, Ä‘áº·c biá»‡t hiá»‡u quáº£ vá»›i giáº£ Ä‘á»‹nh Ä‘á»™c láº­p Ä‘áº·c trÆ°ng."
        },
        "output_without_context": {
            "question": "Trong mÃ´ hÃ¬nh sinh xÃ¡c suáº¥t, Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c thá»ƒ hiá»‡n vÃ o cÃ¡c lá»›p khÃ¡c nhau, tham sá»‘ nÃ o lÃ  cáº§n thiáº¿t Ä‘á»ƒ xÃ¡c Ä‘á»‹nh xÃ¡c suáº¥t cá»§a má»—i lá»›p?",
            "answer": "CÃ¡c tham sá»‘ xÃ¡c suáº¥t Ä‘iá»u kiá»‡n cho má»—i lá»›p."
        }
    },
    {
        "topic_description": "Kiá»ƒm tra kháº£ nÄƒng hiá»ƒu vÃ  phÃ¢n tÃ­ch váº¥n Ä‘á» 'lá»i nguyá»n cá»§a chiá»u', má»™t thÃ¡ch thá»©c trong viá»‡c há»c vá»›i dá»¯ liá»‡u cÃ³ chiá»u cao. Há»c sinh sáº½ Ä‘Æ°á»£c há»i vá» áº£nh hÆ°á»Ÿng cá»§a sá»‘ chiá»u Ä‘á»‘i vá»›i máº«u dá»¯ liá»‡u vÃ  cÃ¡ch giáº£i quyáº¿t váº¥n Ä‘á» nÃ y.",
        "output_with_context": {
            "question": "Lá»i nguyá»n cá»§a chiá»u trong há»c mÃ¡y Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» gÃ¬ khi lÃ m viá»‡c vá»›i dá»¯ liá»‡u cÃ³ sá»‘ chiá»u cao?",
            "answer": "Khoáº£ng cÃ¡ch trá»Ÿ nÃªn kÃ©m Ã½ nghÄ©a vÃ  dá»¯ liá»‡u trá»Ÿ nÃªn thÆ°a thá»›t.",
            "rag_context": "**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»c KhÃ´ng GiÃ¡m SÃ¡t (Unsupervised Learning)\n**3. Noise Point (Outlier):**\n- KhÃ´ng pháº£i core point\n- KhÃ´ng náº±m trong Îµ cá»§a báº¥t ká»³ core point nÃ o\n- Isolated points\n\n**Thuáº­t ToÃ¡n:**\n\n1. For each unvisited point:\n   - Mark as visited\n   - TÃ¬m Îµ-neighbors\n   - If neighbors < MinPts: Mark as noise\n   - Else: Start new cluster\n     - Add point vÃ  neighbors to cluster\n     - For each neighbor:\n       - If unvisited: Visit vÃ  expand cluster\n       - If noise: Add to cluster\n\n**Æ¯u Äiá»ƒm:**\n\n**1. Arbitrary shapes:**\n- KhÃ´ng giáº£ Ä‘á»‹nh spherical\n- Handle complex geometries\n- Non-convex clusters\n\n**2. Robust to outliers:**\n- Outliers = noise points\n- KhÃ´ng áº£nh hÆ°á»Ÿng clusters\n\n**3. KhÃ´ng cáº§n specify K:**\n- Tá»± Ä‘á»™ng determine sá»‘ clusters\n- Dá»±a trÃªn density\n\n**4. Deterministic:**\n- Same parameters â†’ same results (mostly)\n\n**NhÆ°á»£c Äiá»ƒm:**\n\n**1. Sensitive to parameters:**\n- Îµ vÃ  MinPts khÃ³ chá»n\n- Cáº§n domain knowledge hoáº·c tuning\n\n**2. Varying densities:**\n- Má»™t cáº·p (Îµ, MinPts) khÃ´ng phÃ¹ há»£p cho táº¥t cáº£\n- Clusters vá»›i different densities problematic\n\n**3. High-dimensional data:**\n- Distance metrics less meaningful\n- Curse of dimensionality\n\n**4. Memory vÃ  computation:**\n- O(NÂ²) worst case\n- Index structures help (KD-tree, Ball-tree)\n\n**Chá»n Parameters:**\n\n**Îµ (epsilon):**\n- **K-distance graph:**\n  - Váº½ sorted k-distances (k=MinPts-1)\n  - TÃ¬m \"knee\" - nÆ¡i tÄƒng Ä‘á»™t ngá»™t\n  - Îµ = distance táº¡i knee\n\n**MinPts:**\n- Rule of thumb: MinPts â‰¥ dim + 1\n- ThÆ°á»ng: 4 hoáº·c 5\n- Larger for noisy data\n- Smaller for cleaner data\n\n**Variants:**\n\n**HDBSCAN (Hierarchical DBSCAN):**\n- Hierarchical approach\n- KhÃ´ng cáº§n specify Îµ\n- Better vá»›i varying densities\n- Extract clusters á»Ÿ different density levels\n\n**OPTICS:**\n- Ordering points to identify clustering structure\n- Táº¡o reachability plot\n- Flexible extraction\n\n**Khi NÃ o DÃ¹ng:**\n- Non-spherical clusters\n- Outliers present\n- KhÃ´ng biáº¿t K\n- Arbitrary shaped regions\n\n### Gaussian Mixture Models (GMM) - MÃ´ HÃ¬nh Há»—n Há»£p Gaussian\n\nMÃ´ hÃ¬nh xÃ¡c suáº¥t giáº£ Ä‘á»‹nh data Ä‘áº¿n tá»« há»—n há»£p cÃ¡c phÃ¢n phá»‘i Gaussian.\n\n**KhÃ¡i Niá»‡m:**\n- Data generated tá»« K Gaussian distributions\n- Má»—i Gaussian = má»™t cluster\n- Soft assignment (probabilities)\n\n**Model:**\n$$P(x) = \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(x|\\mu_k, \\Sigma_k)$$\n\nTrong Ä‘Ã³:\n- $\\pi_k$: Mixing coefficient (weight) cá»§a component k\n  - $\\sum_{k=1}^{K}\\pi_k = 1$\n  - $0 \\leq \\pi_k \\leq 1$\n- $\\mu_k$: Mean vector cá»§a Gaussian k\n- $\\Sigma_k$: Covariance matrix cá»§a Gaussian k\n- $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$: Gaussian distribution\n\n**Gaussian Distribution:**\n$$\\mathcal{N}(x|\\mu, \\Sigma) = \frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}\\exp\\left(-\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\right)$$\n\n**Expectation-Maximization (EM) Algorithm:**\n\nIterative algorithm Ä‘á»ƒ estimate parameters.\n\n**E-Step (Expectation):**\nTÃ­nh responsibility (xÃ¡c suáº¥t má»m) cá»§a má»—i component cho má»—i Ä‘iá»ƒm:\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Curse of dimensionality lÃ  má»™t váº¥n Ä‘á» phÃ¡t sinh khi lÃ m viá»‡c vá»›i dá»¯ liá»‡u cÃ³ sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng (chiá»u) cao. Trong khÃ´ng gian chiá»u cao, dá»¯ liá»‡u trá»Ÿ nÃªn thÆ°a thá»›t, khiáº¿n cÃ¡c khÃ¡i niá»‡m vá» khoáº£ng cÃ¡ch trá»Ÿ nÃªn kÃ©m Ã½ nghÄ©a vÃ  viá»‡c tÃ¬m kiáº¿m lÃ¡ng giá»ng gáº§n nháº¥t trá»Ÿ nÃªn kÃ©m hiá»‡u quáº£. Äiá»u nÃ y lÃ m giáº£m hiá»‡u suáº¥t cá»§a cÃ¡c thuáº­t toÃ¡n dá»±a trÃªn khoáº£ng cÃ¡ch nhÆ° DBSCAN vÃ  k-NN.\n- Curse of dimensionality lÃ  má»™t háº¡n cháº¿ vÃ  thÃ¡ch thá»©c lá»›n trong Reinforcement Learning, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p Dynamic Programming. NÃ³ Ä‘á» cáº­p Ä‘áº¿n sá»± tÄƒng trÆ°á»Ÿng theo cáº¥p sá»‘ nhÃ¢n cá»§a sá»‘ lÆ°á»£ng tráº¡ng thÃ¡i (hoáº·c hÃ nh Ä‘á»™ng) khi sá»‘ chiá»u cá»§a khÃ´ng gian tráº¡ng thÃ¡i tÄƒng lÃªn, khiáº¿n viá»‡c tÃ­nh toÃ¡n trá»Ÿ nÃªn khÃ´ng kháº£ thi vÃ  lÃ m cho viá»‡c giáº£i quyáº¿t bÃ i toÃ¡n trá»Ÿ nÃªn khÃ³ khÄƒn vá»›i khÃ´ng gian tráº¡ng thÃ¡i lá»›n. VÃ­ dá»¥, bÃ n cá» vÃ¢y cÃ³ khoáº£ng 10^170 tráº¡ng thÃ¡i. Function approximation lÃ  má»™t giáº£i phÃ¡p cho váº¥n Ä‘á» nÃ y.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»c KhÃ´ng GiÃ¡m SÃ¡t (Unsupervised Learning)\n- Multiple microphones recording\n- ICA separates individual voices\n\n**So vá»›i PCA:**\n- PCA: Decorrelation, orthogonal, Gaussian assumption\n- ICA: Independence, not orthogonal, non-Gaussian\n\n**á»¨ng dá»¥ng:**\n- Blind source separation\n- Signal processing (audio, EEG, fMRI)\n- Feature extraction\n- Artifact removal\n\n### Non-negative Matrix Factorization (NMF)\n\n**NguyÃªn lÃ½:**\n- Factorize matrix into non-negative factors\n- Parts-based representation\n\n**Model:**\n$$X \\approx WH$$\n\n**Constraints:**\n- $X \\geq 0$: Input non-negative\n- $W \\geq 0$: Basis matrix non-negative\n- $H \\geq 0$: Coefficient matrix non-negative\n\n**Interpretation:**\n- $W$: Basis vectors (features, topics)\n- $H$: Coefficients (weights, memberships)\n- Each column cá»§a $X$ = linear combination cá»§a columns cá»§a $W$\n\n**Optimization:**\nMinimize: $||X - WH||^2$ with constraints\n\n**á»¨ng dá»¥ng:**\n\n**1. Topic Modeling:**\n- $X$: Document-term matrix\n- $W$: Term-topic matrix\n- $H$: Topic-document matrix\n\n**2. Image Processing:**\n- Learn parts of faces\n- $W$: Facial features\n- $H$: How to combine them\n\n**3. Recommender Systems:**\n- $X$: User-item matrix\n- $W$: User factors\n- $H$: Item factors\n\n**Æ¯u Ä‘iá»ƒm:**\n- Interpretable (non-negativity)\n- Parts-based representation\n- Sparse solutions\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Non-convex (local minima)\n- Slower than PCA\n- Requires non-negative data\n\n### Manifold Learning\n\nKhÃ¡m phÃ¡ non-linear structure trong high-dimensional data.\n\n**Manifold:**\n- Low-dimensional surface embedded trong high-dimensional space\n- VÃ­ dá»¥: Swiss roll (2D manifold trong 3D)\n\n**1. Isomap (Isometric Feature Mapping):**\n\n**NguyÃªn lÃ½:**\n- Preserve geodesic distances (shortest path trÃªn manifold)\n- Global structure preservation\n\n**Steps:**\n1. Construct neighborhood graph\n2. Compute shortest path distances (Dijkstra)\n3. Apply MDS (Multi-Dimensional Scaling)\n\n**Æ¯u Ä‘iá»ƒm:**\n- Global structure\n- Theoretical foundation\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Expensive (shortest paths)\n- Sensitive to noise\n- Cáº§n connected graph\n\n**2. Locally Linear Embedding (LLE):**\n\n**NguyÃªn lÃ½:**\n- Preserve local relationships\n- Each point reconstructed tá»« neighbors\n\n**Steps:**\n1. Find k nearest neighbors\n2. Compute reconstruction weights\n3. Embed vá»›i same weights trong low-dim\n\n**Æ¯u Ä‘iá»ƒm:**\n- Fast\n- Non-iterative\n- Good local preservation\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Sensitive to k\n- Can produce distorted results\n\n**3. t-SNE:**\nÄÃ£ cover chi tiáº¿t trÆ°á»›c Ä‘Ã³.\n\n**4. UMAP:**\nÄÃ£ cover chi tiáº¿t trÆ°á»›c Ä‘Ã³.\n\n**So sÃ¡nh:**\n- **Isomap:** Global, geodesic distances\n- **LLE:** Local, linear reconstruction\n- **t-SNE:** Local, visualization, stochastic\n- **UMAP:** Both local & global, faster than t-SNE\n\n### PhÃ¡t Hiá»‡n Báº¥t ThÆ°á»ng (Anomaly Detection)\n\nXÃ¡c Ä‘á»‹nh cÃ¡c items, events, hoáº·c observations hiáº¿m.\n\n**Anomaly types:**\n- **Point anomalies:** Single data point\n- **Contextual anomalies:** Trong specific context\n- **Collective anomalies:** Collection cá»§a points\n\n**1. Statistical Methods:**\n\n**Z-score:**\n$$z = \frac{x - \\mu}{\\sigma}$$\n\n- |z| > 3: Anomaly (99.7% rule)\n- Giáº£ Ä‘á»‹nh Gaussian distribution\n\n**Modified Z-score (Robust):**\n$$M = \frac{0.6745(x - median)}{MAD}$$\n\n- MAD = Median Absolute Deviation\n- Robust to outliers\n\n**Interquartile Range (IQR):**\n- IQR = Q3 - Q1\n- Outliers: < Q1 - 1.5Ã—IQR hoáº·c > Q3 + 1.5Ã—IQR\n\n**2. Isolation Forest:**\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Manifold Learning lÃ  má»™t ká»¹ thuáº­t há»c khÃ´ng giÃ¡m sÃ¡t nháº±m khÃ¡m phÃ¡ cáº¥u trÃºc phi tuyáº¿n tÃ­nh (non-linear structure) tiá»m áº©n trong dá»¯ liá»‡u chiá»u cao. NÃ³ giáº£ Ä‘á»‹nh ráº±ng dá»¯ liá»‡u thá»±c táº¿ náº±m trÃªn má»™t \"manifold\" (Ä‘a táº¡p) chiá»u tháº¥p Ä‘Æ°á»£c nhÃºng trong khÃ´ng gian chiá»u cao. Má»¥c tiÃªu lÃ  giáº£m chiá»u dá»¯ liá»‡u trong khi váº«n báº£o toÃ n cÃ¡c má»‘i quan há»‡ ná»™i táº¡i cá»§a dá»¯ liá»‡u.\n\n**Má»‘i quan há»‡:**\n- Manifold Learning khÃ¡m phÃ¡ cáº¥u trÃºc Manifold tiá»m áº©n trong dá»¯ liá»‡u chiá»u cao.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»c SÃ¢u (Deep Learning)\n**CÃ¡ch hoáº¡t Ä‘á»™ng:**\n1. Nháº­n Ä‘áº§u vÃ o tá»« cÃ¡c Ä‘áº·c trÆ°ng\n2. TÃ­nh tá»•ng cÃ³ trá»ng sá»‘: $z = w^Tx + b = \\sum_{i=1}^{n}w_i x_i + b$\n3. Ãp dá»¥ng hÃ m kÃ­ch hoáº¡t Ä‘á»ƒ táº¡o Ä‘áº§u ra\n\n**Háº¡n cháº¿ quan trá»ng:** \n- Chá»‰ cÃ³ thá»ƒ há»c cÃ¡c máº«u phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh (linearly separable)\n- KhÃ´ng thá»ƒ giáº£i quyáº¿t bÃ i toÃ¡n XOR\n- KhÃ´ng thá»ƒ mÃ´ hÃ¬nh hÃ³a cÃ¡c quan há»‡ phi tuyáº¿n phá»©c táº¡p\n\n**VÃ­ dá»¥:** Má»™t perceptron cÃ³ thá»ƒ phÃ¢n loáº¡i Ä‘iá»ƒm náº±m phÃ­a trÃªn hay dÆ°á»›i má»™t Ä‘Æ°á»ng tháº³ng, nhÆ°ng khÃ´ng thá»ƒ phÃ¢n loáº¡i cÃ¡c Ä‘iá»ƒm trong bÃ i toÃ¡n XOR (cáº§n Ä‘Æ°á»ng cong Ä‘á»ƒ phÃ¢n tÃ¡ch).\n\n### Perceptron Äa Lá»›p (Multi-Layer Perceptron - MLP)\n\nMLP kháº¯c phá»¥c háº¡n cháº¿ cá»§a perceptron Ä‘Æ¡n báº±ng cÃ¡ch xáº¿p chá»“ng nhiá»u lá»›p, cho phÃ©p há»c cÃ¡c hÃ m phi tuyáº¿n phá»©c táº¡p.\n\n**Kiáº¿n trÃºc:**\n\n**1. Lá»›p Ä‘áº§u vÃ o (Input Layer):**\n- Nháº­n dá»¯ liá»‡u thÃ´\n- Sá»‘ nÆ¡-ron = sá»‘ Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o\n- KhÃ´ng cÃ³ phÃ©p biáº¿n Ä‘á»•i, chá»‰ truyá»n dá»¯ liá»‡u\n\n**2. Lá»›p áº©n (Hidden Layers):**\n- Thá»±c hiá»‡n cÃ¡c phÃ©p biáº¿n Ä‘á»•i phi tuyáº¿n\n- Sá»‘ lÆ°á»£ng cÃ³ thá»ƒ tá»« 1 Ä‘áº¿n hÃ ng trÄƒm lá»›p\n- Má»—i lá»›p há»c biá»ƒu diá»…n trá»«u tÆ°á»£ng hÆ¡n\n- Sá»‘ nÆ¡-ron trong má»—i lá»›p lÃ  hyperparameter\n\n**3. Lá»›p Ä‘áº§u ra (Output Layer):**\n- Táº¡o dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng\n- Sá»‘ nÆ¡-ron phá»¥ thuá»™c vÃ o bÃ i toÃ¡n:\n  - Há»“i quy: 1 nÆ¡-ron\n  - PhÃ¢n loáº¡i nhá»‹ phÃ¢n: 1 nÆ¡-ron (vá»›i sigmoid) hoáº·c 2 (vá»›i softmax)\n  - PhÃ¢n loáº¡i Ä‘a lá»›p: K nÆ¡-ron (K = sá»‘ lá»›p)\n\n**Lan truyá»n xuÃ´i (Forward Propagation):**\n\nQuÃ¡ trÃ¬nh tÃ­nh toÃ¡n tá»« Ä‘áº§u vÃ o Ä‘áº¿n Ä‘áº§u ra qua cÃ¡c lá»›p:\n\nVá»›i lá»›p $l$:\n$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n$$a^{[l]} = \\sigma(z^{[l]})$$\n\nTrong Ä‘Ã³:\n- $W^{[l]}$: Ma tráº­n trá»ng sá»‘ cá»§a lá»›p $l$ (kÃ­ch thÆ°á»›c $n^{[l]} \times n^{[l-1]}$)\n- $b^{[l]}$: Vector bias cá»§a lá»›p $l$ (kÃ­ch thÆ°á»›c $n^{[l]} \times 1$)\n- $a^{[l-1]}$: Activation cá»§a lá»›p trÆ°á»›c (Ä‘áº§u vÃ o cho lá»›p $l$)\n- $z^{[l]}$: Pre-activation (trÆ°á»›c khi Ã¡p dá»¥ng hÃ m kÃ­ch hoáº¡t)\n- $a^{[l]}$: Activation (sau khi Ã¡p dá»¥ng hÃ m kÃ­ch hoáº¡t)\n- $\\sigma$: HÃ m kÃ­ch hoáº¡t\n\n**VÃ­ dá»¥ minh há»a:**\n- Lá»›p 1: 3 nÆ¡-ron, nháº­n input 784 chiá»u (áº£nh 28Ã—28) â†’ $W^{[1]}$: 3Ã—784\n- Lá»›p 2: 2 nÆ¡-ron, nháº­n tá»« lá»›p 1 â†’ $W^{[2]}$: 2Ã—3\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Linearly separable (phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh) lÃ  má»™t váº¥n Ä‘á» trong Ä‘Ã³ dá»¯ liá»‡u cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n chia thÃ nh cÃ¡c lá»›p báº±ng má»™t Ä‘Æ°á»ng tháº³ng (trong 2D), máº·t pháº³ng (trong 3D) hoáº·c siÃªu pháº³ng (trong khÃ´ng gian nhiá»u chiá»u). Perceptron chá»‰ cÃ³ thá»ƒ há»c cÃ¡c máº«u phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Lá»±a Chá»n Äáº·c TrÆ°ng & Tá»‘i Æ¯u HÃ³a MÃ´ HÃ¬nh\n- CV cho stable estimate\n\n**1. K-Fold Cross-Validation:**\n\n**Thuáº­t toÃ¡n:**\n1. Chia data thÃ nh k folds\n2. For i = 1 to k:\n   - Use fold i lÃ m validation\n   - Use k-1 folds cÃ²n láº¡i lÃ m training\n   - Train vÃ  evaluate\n3. Average metrics across k folds\n\n**Chá»n k:**\n- k=5: Standard, good balance\n- k=10: More stable, more computational\n- Larger k: Less bias, more variance, more expensive\n\n**Æ¯u Ä‘iá»ƒm:**\n- Sá»­ dá»¥ng toÃ n bá»™ data\n- Stable estimate\n- Reduce variance\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- k láº§n training (expensive)\n- CÃ³ thá»ƒ cháº­m\n\n**2. Stratified K-Fold:**\n\n**NguyÃªn lÃ½:**\n- Maintain class distribution trong má»—i fold\n- Each fold representative\n\n**Khi nÃ o dÃ¹ng:**\n- Imbalanced datasets\n- Classification tasks\n- Äáº£m báº£o má»—i fold cÃ³ Ä‘á»§ samples má»—i class\n\n**Æ¯u Ä‘iá»ƒm:**\n- Fair evaluation vá»›i imbalanced data\n- Consistent class proportions\n\n**3. Leave-One-Out (LOO):**\n\n**NguyÃªn lÃ½:**\n- k = n (n = sá»‘ samples)\n- Má»—i sample lÃ  má»™t fold\n\n**Æ¯u Ä‘iá»ƒm:**\n- Maximum data cho training\n- No randomness\n- Deterministic\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Ráº¥t cháº­m (n iterations)\n- High variance\n- Chá»‰ kháº£ thi vá»›i small datasets (< 1000)\n\n**Khi nÃ o dÃ¹ng:**\n- Very small datasets\n- Need maximum training data\n- Computational resources available\n\n**4. Time Series Cross-Validation:**\n\n**NguyÃªn lÃ½:**\n- Respect temporal order\n- Train on past, validate on future\n- No data leakage from future\n\n**Expanding Window:**\n```\nFold 1: Train [1:100] â†’ Test [101:120]\nFold 2: Train [1:120] â†’ Test [121:140]\nFold 3: Train [1:140] â†’ Test [141:160]\n```\n\n**Rolling Window:**\n```\nFold 1: Train [1:100] â†’ Test [101:120]\nFold 2: Train [21:120] â†’ Test [121:140]\nFold 3: Train [41:140] â†’ Test [141:160]\n```\n\n**Quan trá»ng:**\n- **KHÃ”NG shuffle data**\n- Maintain temporal order\n- Avoid look-ahead bias\n\n**5. Nested Cross-Validation:**\n\n**NguyÃªn lÃ½:**\n- Outer loop: Model evaluation\n- Inner loop: Hyperparameter tuning\n- Prevents overfitting in parameter selection\n\n**Structure:**\n```\nOuter CV (5-fold):\n  For each outer fold:\n    Inner CV (5-fold):\n      Hyperparameter tuning\n    Train with best params\n    Evaluate on outer fold\n```\n\n**Æ¯u Ä‘iá»ƒm:**\n- Unbiased performance estimate\n- Proper hyperparameter tuning\n- Gold standard\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Very expensive (k_outer Ã— k_inner trainings)\n- Overkill cho simple problems\n\n**Khi nÃ o dÃ¹ng:**\n- Need unbiased estimate\n- Publishing results\n- Critical applications\n- Have computational resources\n\n### Learning Curves (ÄÆ°á»ng Cong Há»c)\n\nPhÃ¢n tÃ­ch hiá»‡u suáº¥t mÃ´ hÃ¬nh vs kÃ­ch thÆ°á»›c training set.\n\n**Váº½ gÃ¬:**\n- X-axis: Training set size\n- Y-axis: Error (hoáº·c Score)\n- Two curves: Training error & Validation error\n\n**Cháº©n ÄoÃ¡n:**\n\n**1. High Bias (Underfitting):**\n```\nTraining error: Cao\nValidation error: Cao\nGap: Nhá»\nBoth plateau at high error\n```\n**Dáº¥u hiá»‡u:**\n- Cáº£ hai curves plateau\n- Performance kÃ©m ngay cáº£ vá»›i nhiá»u data\n- ThÃªm data khÃ´ng giÃºp\n\n**Giáº£i phÃ¡p:**\n- Increase model complexity\n- Add features\n- Reduce regularization\n- Try complex model\n\n**2. High Variance (Overfitting):**\n```\nTraining error: Tháº¥p\nValidation error: Cao\nGap: Lá»›n\nGap doesn't close with more data\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Data leakage (rÃ² rá»‰ dá»¯ liá»‡u) lÃ  má»™t váº¥n Ä‘á» nghiÃªm trá»ng trong há»c mÃ¡y, xáº£y ra khi thÃ´ng tin tá»« táº­p dá»¯ liá»‡u kiá»ƒm tra (hoáº·c dá»¯ liá»‡u mÃ  mÃ´ hÃ¬nh sáº½ gáº·p trong thá»±c táº¿/dá»¯ liá»‡u tÆ°Æ¡ng lai/dá»¯ liá»‡u má»¥c tiÃªu) vÃ´ tÃ¬nh bá»‹ rÃ² rá»‰ vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh. Äiá»u nÃ y dáº«n Ä‘áº¿n viá»‡c Æ°á»›c lÆ°á»£ng hiá»‡u suáº¥t mÃ´ hÃ¬nh quÃ¡ láº¡c quan trÃªn táº­p kiá»ƒm tra, khÃ´ng pháº£n Ã¡nh kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a thá»±c sá»± cá»§a mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u má»›i, vÃ  khiáº¿n mÃ´ hÃ¬nh cÃ³ váº» hoáº¡t Ä‘á»™ng tá»‘t trÃªn táº­p kiá»ƒm tra nhÆ°ng láº¡i kÃ©m hiá»‡u quáº£ trong thá»±c táº¿.\n- High Bias (thiÃªn vá»‹ cao) lÃ  má»™t váº¥n Ä‘á» trong há»c mÃ¡y, cÃ²n Ä‘Æ°á»£c gá»i lÃ  Underfitting (há»c dÆ°á»›i má»©c). NÃ³ xáº£y ra khi mÃ´ hÃ¬nh quÃ¡ Ä‘Æ¡n giáº£n Ä‘á»ƒ náº¯m báº¯t Ä‘Æ°á»£c má»‘i quan há»‡ phá»©c táº¡p trong dá»¯ liá»‡u, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m trÃªn cáº£ táº­p huáº¥n luyá»‡n vÃ  táº­p validation. TrÃªn Learning Curves, cáº£ Training error vÃ  Validation error Ä‘á»u cao vÃ  cÃ³ khoáº£ng cÃ¡ch nhá», cáº£ hai Ä‘Æ°á»ng Ä‘á»u Ä‘áº¡t Ä‘áº¿n má»™t má»©c cao vÃ  khÃ´ng cáº£i thiá»‡n khi thÃªm dá»¯ liá»‡u.\n- Learning Curves (ÄÆ°á»ng cong há»c) lÃ  biá»ƒu Ä‘á»“ phÃ¢n tÃ­ch hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh (thÆ°á»ng lÃ  lá»—i hoáº·c Ä‘iá»ƒm sá»‘) so vá»›i kÃ­ch thÆ°á»›c cá»§a táº­p huáº¥n luyá»‡n. Biá»ƒu Ä‘á»“ nÃ y thÆ°á»ng cÃ³ hai Ä‘Æ°á»ng: Ä‘Æ°á»ng lá»—i huáº¥n luyá»‡n (Training error) vÃ  Ä‘Æ°á»ng lá»—i validation (Validation error). Learning Curves giÃºp cháº©n Ä‘oÃ¡n cÃ¡c váº¥n Ä‘á» nhÆ° High Bias (Underfitting) vÃ  High Variance (Overfitting) cá»§a mÃ´ hÃ¬nh.\n\n**Má»‘i quan há»‡:**\n- Äá»ƒ giáº£i quyáº¿t High Bias, cáº§n giáº£m regularization (Reduce regularization) Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c Ä‘Æ°á»£c nhiá»u hÆ¡n tá»« dá»¯ liá»‡u huáº¥n luyá»‡n.\n- Learning Curves cháº©n Ä‘oÃ¡n váº¥n Ä‘á» High Bias (Underfitting) khi cáº£ training error vÃ  validation error Ä‘á»u cao vÃ  cÃ³ khoáº£ng cÃ¡ch nhá», cáº£ hai Ä‘Æ°á»ng Ä‘á»u Ä‘áº¡t Ä‘áº¿n má»™t má»©c cao vÃ  khÃ´ng cáº£i thiá»‡n khi thÃªm dá»¯ liá»‡u.\n- Time Series Cross-Validation ngÄƒn cháº·n rÃ² rá»‰ dá»¯ liá»‡u (data leakage) báº±ng cÃ¡ch Ä‘áº£m báº£o ráº±ng dá»¯ liá»‡u tá»« tÆ°Æ¡ng lai khÃ´ng bao giá» Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n hoáº·c Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh, Ä‘áº·c biá»‡t lÃ  trÃ¡nh look-ahead bias.\n- Learning Curves cháº©n Ä‘oÃ¡n váº¥n Ä‘á» High Variance (Overfitting) khi training error tháº¥p, validation error cao vÃ  cÃ³ má»™t khoáº£ng cÃ¡ch lá»›n giá»¯a hai Ä‘Æ°á»ng, khoáº£ng cÃ¡ch nÃ y khÃ´ng thu háº¹p khi thÃªm dá»¯ liá»‡u.\n- Äá»ƒ giáº£i quyáº¿t High Bias, cáº§n tÄƒng Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh (Increase model complexity) Ä‘á»ƒ nÃ³ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ phá»©c táº¡p hÆ¡n trong dá»¯ liá»‡u."
        },
        "output_without_context": {
            "question": "Lá»i nguyá»n cá»§a chiá»u trong há»c mÃ¡y chá»§ yáº¿u Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» gÃ¬ khi lÃ m viá»‡c vá»›i dá»¯ liá»‡u cÃ³ nhiá»u chiá»u?",
            "answer": "Sá»± gia tÄƒng Ä‘á»™ phá»©c táº¡p vÃ  giáº£m hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh do sá»‘ chiá»u cao."
        }
    },
    {
        "topic_description": "Há»c sinh sáº½ phÃ¢n tÃ­ch Bá»™ phÃ¢n loáº¡i NaÃ¯ve Bayes, tÃ¬m hiá»ƒu cÃ¡ch mÃ  giáº£ Ä‘á»‹nh Ä‘á»™c láº­p cÃ³ Ä‘iá»u kiá»‡n áº£nh hÆ°á»Ÿng Ä‘áº¿n kháº£ nÄƒng Æ°á»›c tÃ­nh xÃ¡c suáº¥t. CÅ©ng nhÆ° cÃ¡ch xá»­ lÃ½ váº¥n Ä‘á» kháº£ dÄ© báº±ng 0 trong Ä‘iá»u kiá»‡n thá»±c táº¿.",
        "output_with_context": {
            "question": "Trong Bá»™ phÃ¢n loáº¡i Naive Bayes, giáº£ Ä‘á»‹nh Ä‘á»™c láº­p cÃ³ Ä‘iá»u kiá»‡n Ä‘Æ°á»£c thá»ƒ hiá»‡n báº±ng cÃ´ng thá»©c nÃ o sau Ä‘Ã¢y?",
            "answer": "P(x_1, x_2, ..., x_n|C_k) = \\prod_{i=1}^{n}P(x_i|C_k)",
            "rag_context": "**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## PhÃ¢n Loáº¡i (Classification)\n- Æ¯u Ä‘iá»ƒm: Má»—i mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n hÆ¡n, cÃ¢n báº±ng hÆ¡n\n- NhÆ°á»£c Ä‘iá»ƒm: Nhiá»u mÃ´ hÃ¬nh (phá»©c táº¡p khi K lá»›n)\n\n**VÃ­ dá»¥:** 3 lá»›p (A, B, C)\n- MÃ´ hÃ¬nh 1: A vs B\n- MÃ´ hÃ¬nh 2: A vs C\n- MÃ´ hÃ¬nh 3: B vs C\n\n**3. Softmax Regression (Multinomial Logistic Regression):**\n\nMá»Ÿ rá»™ng trá»±c tiáº¿p cá»§a logistic regression cho Ä‘a lá»›p.\n\n**CÃ´ng thá»©c:**\n$$P(y=k|x) = \frac{e^{z_k}}{\\sum_{j=1}^{K}e^{z_j}}$$\n\nTrong Ä‘Ã³: $z_k = \beta_k^Tx$ vá»›i $\beta_k$ lÃ  vector há»‡ sá»‘ cho lá»›p $k$\n\n**Äáº·c Ä‘iá»ƒm:**\n- Tá»•ng cÃ¡c xÃ¡c suáº¥t = 1: $\\sum_{k=1}^{K}P(y=k|x) = 1$\n- Output lÃ  phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn táº¥t cáº£ lá»›p\n- Huáº¥n luyá»‡n Ä‘á»“ng thá»i táº¥t cáº£ lá»›p\n\n**HÃ m chi phÃ­ (Categorical Cross-Entropy):**\n$$J(\beta) = -\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}\\log(P(y=k|x^{(i)}))$$\n\nTrong Ä‘Ã³ $y_k^{(i)}$ lÃ  one-hot encoding cá»§a nhÃ£n.\n\n**Lá»±a chá»n giá»¯a OvR, OvO, vÃ  Softmax:**\n- **Softmax:** Tá»‘t nháº¥t khi cáº§n xÃ¡c suáº¥t, K khÃ´ng quÃ¡ lá»›n\n- **OvR:** ÄÆ¡n giáº£n, hiá»‡u quáº£ vá»›i K lá»›n\n- **OvO:** Tá»‘t vá»›i SVM, K nhá»/trung bÃ¬nh\n\n### Naive Bayes Classifier (Bá»™ PhÃ¢n Loáº¡i Naive Bayes)\n\nDá»±a trÃªn Ä‘á»‹nh lÃ½ Bayes vá»›i giáº£ Ä‘á»‹nh \"ngÃ¢y thÆ¡\" (naive) vá» tÃ­nh Ä‘á»™c láº­p Ä‘áº·c trÆ°ng.\n\n**Äá»‹nh LÃ½ Bayes:**\n$$P(C_k|x) = \frac{P(x|C_k)P(C_k)}{P(x)}$$\n\nTrong Ä‘Ã³:\n- $P(C_k|x)$: XÃ¡c suáº¥t háº­u nghiá»‡m (posterior) - xÃ¡c suáº¥t lá»›p $C_k$ cho trÆ°á»›c $x$\n- $P(x|C_k)$: Likelihood - xÃ¡c suáº¥t cá»§a $x$ trong lá»›p $C_k$\n- $P(C_k)$: XÃ¡c suáº¥t tiÃªn nghiá»‡m (prior) cá»§a lá»›p $C_k$\n- $P(x)$: Evidence - xÃ¡c suáº¥t cá»§a $x$\n\n**Giáº£ Äá»‹nh Naive (Äá»™c Láº­p Äiá»u Kiá»‡n):**\n$$P(x_1, x_2, ..., x_n|C_k) = \\prod_{i=1}^{n}P(x_i|C_k)$$\n\nCÃ¡c Ä‘áº·c trÆ°ng Ä‘á»™c láº­p vá»›i nhau khi biáº¿t lá»›p.\n\n**CÃ´ng Thá»©c Äáº§y Äá»§:**\n$$P(C_k|x_1,...,x_n) = \frac{P(C_k)\\prod_{i=1}^{n}P(x_i|C_k)}{P(x_1,...,x_n)}$$\n\n**Quyáº¿t Äá»‹nh:**\n$$\\hat{y} = \\arg\\max_{k} P(C_k)\\prod_{i=1}^{n}P(x_i|C_k)$$\n\nKhÃ´ng cáº§n tÃ­nh $P(x)$ vÃ¬ nÃ³ giá»‘ng nhau cho táº¥t cáº£ lá»›p.\n\n**CÃ¡c Biáº¿n Thá»ƒ:**\n\n**1. Gaussian Naive Bayes:**\n- Cho Ä‘áº·c trÆ°ng liÃªn tá»¥c\n- Giáº£ Ä‘á»‹nh phÃ¢n phá»‘i Gaussian (chuáº©n)\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- P(x) lÃ  evidence, Ä‘áº¡i diá»‡n cho xÃ¡c suáº¥t cá»§a cÃ¡c Ä‘áº·c trÆ°ng x. Trong Naive Bayes Classifier, P(x) thÆ°á»ng khÃ´ng cáº§n tÃ­nh toÃ¡n trá»±c tiáº¿p vÃ¬ nÃ³ lÃ  má»™t háº±ng sá»‘ cho táº¥t cáº£ cÃ¡c lá»›p khi so sÃ¡nh xÃ¡c suáº¥t háº­u nghiá»‡m, vÃ  do Ä‘Ã³ khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n quyáº¿t Ä‘á»‹nh phÃ¢n loáº¡i cuá»‘i cÃ¹ng.\n- Naive Bayes Classifier lÃ  má»™t bá»™ phÃ¢n loáº¡i dá»±a trÃªn Äá»‹nh lÃ½ Bayes vá»›i giáº£ Ä‘á»‹nh \"ngÃ¢y thÆ¡\" (naive) vá» tÃ­nh Ä‘á»™c láº­p cÃ³ Ä‘iá»u kiá»‡n cá»§a cÃ¡c Ä‘áº·c trÆ°ng. MÃ´ hÃ¬nh nÃ y tÃ­nh toÃ¡n xÃ¡c suáº¥t háº­u nghiá»‡m cá»§a má»™t lá»›p cho trÆ°á»›c cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o vÃ  chá»n lá»›p cÃ³ xÃ¡c suáº¥t cao nháº¥t. Naive Bayes Ä‘Æ¡n giáº£n, hiá»‡u quáº£ vÃ  thÆ°á»ng hoáº¡t Ä‘á»™ng tá»‘t ngay cáº£ vá»›i lÆ°á»£ng dá»¯ liá»‡u nhá».\n\n**Má»‘i quan há»‡:**\n- Naive Bayes Classifier lÃ  má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n PhÃ¢n loáº¡i, Ä‘áº·c biá»‡t hiá»‡u quáº£ vá»›i giáº£ Ä‘á»‹nh Ä‘á»™c láº­p Ä‘áº·c trÆ°ng.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## PhÃ¢n Loáº¡i (Classification)\n$$P(x_i|C_k) = \frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}\right)$$\n- Æ¯á»›c lÆ°á»£ng $\\mu_k$ (mean) vÃ  $\\sigma_k^2$ (variance) tá»« dá»¯ liá»‡u\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n, nháº­n dáº¡ng máº«u\n\n**2. Multinomial Naive Bayes:**\n- Cho Ä‘áº¿m rá»i ráº¡c (word counts, frequencies)\n- PhÃ¢n phá»‘i Ä‘a thá»©c\n$$P(x_i|C_k) = \frac{N_{ki} + \\alpha}{N_k + \\alpha n}$$\n  - $N_{ki}$: Sá»‘ láº§n Ä‘áº·c trÆ°ng $i$ xuáº¥t hiá»‡n trong lá»›p $k$\n  - $N_k$: Tá»•ng sá»‘ Ä‘áº¿m trong lá»›p $k$\n  - $\\alpha$: Laplace smoothing (thÆ°á»ng = 1)\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n, phÃ¢n tÃ­ch cáº£m xÃºc, lá»c spam\n\n**3. Bernoulli Naive Bayes:**\n- Cho Ä‘áº·c trÆ°ng nhá»‹ phÃ¢n (cÃ³/khÃ´ng)\n- PhÃ¢n phá»‘i Bernoulli\n$$P(x_i|C_k) = P(i|C_k)x_i + (1-P(i|C_k))(1-x_i)$$\n- TÃ­nh cáº£ viá»‡c Ä‘áº·c trÆ°ng xuáº¥t hiá»‡n vÃ  khÃ´ng xuáº¥t hiá»‡n\n- á»¨ng dá»¥ng: PhÃ¢n loáº¡i vÄƒn báº£n vá»›i binary features\n\n**Æ¯u Äiá»ƒm:**\n- Nhanh, hiá»‡u quáº£\n- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i dá»¯ liá»‡u nhá»\n- Dá»… triá»ƒn khai vÃ  diá»…n giáº£i\n- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i nhiá»u Ä‘áº·c trÆ°ng\n- KhÃ´ng nháº¡y cáº£m vá»›i Ä‘áº·c trÆ°ng khÃ´ng liÃªn quan\n\n**NhÆ°á»£c Äiá»ƒm:**\n- Giáº£ Ä‘á»‹nh Ä‘á»™c láº­p hiáº¿m khi Ä‘Ãºng trong thá»±c táº¿\n- \"Zero frequency problem\" cáº§n smoothing\n- Æ¯á»›c lÆ°á»£ng xÃ¡c suáº¥t cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c\n- KhÃ´ng tá»‘t khi Ä‘áº·c trÆ°ng tÆ°Æ¡ng quan\n\n**Laplace Smoothing:**\nXá»­ lÃ½ váº¥n Ä‘á» xÃ¡c suáº¥t = 0:\n$$P(x_i|C_k) = \frac{count(x_i, C_k) + \\alpha}{count(C_k) + \\alpha \times |V|}$$\n\n### k-Nearest Neighbors (k-NN) - K LÃ¡ng Giá»ng Gáº§n Nháº¥t\n\nPhÆ°Æ¡ng phÃ¡p non-parametric phÃ¢n loáº¡i dá»±a trÃªn Ä‘a sá»‘ vote cá»§a k lÃ¡ng giá»ng gáº§n nháº¥t.\n\n**Thuáº­t ToÃ¡n:**\n1. TÃ­nh khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm cáº§n phÃ¢n loáº¡i Ä‘áº¿n táº¥t cáº£ Ä‘iá»ƒm huáº¥n luyá»‡n\n2. Chá»n k Ä‘iá»ƒm gáº§n nháº¥t\n3. Vote: Lá»›p xuáº¥t hiá»‡n nhiá»u nháº¥t trong k lÃ¡ng giá»ng\n4. GÃ¡n nhÃ£n lá»›p Ä‘Ã³ cho Ä‘iá»ƒm má»›i\n\n**CÃ¡c Äá»™ Äo Khoáº£ng CÃ¡ch:**\n\n**1. Euclidean Distance (Khoáº£ng cÃ¡ch Euclid):**\n$$d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}$$\n- Phá»• biáº¿n nháº¥t\n- Khoáº£ng cÃ¡ch Ä‘Æ°á»ng tháº³ng\n- Nháº¡y cáº£m vá»›i scale cá»§a Ä‘áº·c trÆ°ng\n\n**2. Manhattan Distance (Khoáº£ng cÃ¡ch Manhattan):**\n$$d(x,y) = \\sum_{i=1}^{n}|x_i-y_i|$$\n- Khoáº£ng cÃ¡ch theo lÆ°á»›i Ä‘Ã´ thá»‹\n- Ãt nháº¡y cáº£m vá»›i outliers\n- Tá»‘t cho dá»¯ liá»‡u high-dimensional\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Laplace smoothing lÃ  má»™t ká»¹ thuáº­t Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xá»­ lÃ½ \"zero frequency problem\" trong Naive Bayes, nÆ¡i má»™t xÃ¡c suáº¥t cÃ³ thá»ƒ báº±ng 0 náº¿u má»™t Ä‘áº·c trÆ°ng khÃ´ng xuáº¥t hiá»‡n trong má»™t lá»›p cá»¥ thá»ƒ. Ká»¹ thuáº­t nÃ y thÃªm má»™t giÃ¡ trá»‹ nhá» (Î±, thÆ°á»ng lÃ  1) vÃ o tá»­ sá»‘ vÃ  máº«u sá»‘ Ä‘á»ƒ trÃ¡nh xÃ¡c suáº¥t báº±ng 0, giÃºp Æ°á»›c lÆ°á»£ng xÃ¡c suáº¥t á»•n Ä‘á»‹nh hÆ¡n.\n- Multinomial Naive Bayes lÃ  má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i dá»±a trÃªn Ä‘á»‹nh lÃ½ Bayes, phÃ¹ há»£p cho cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº¿m rá»i ráº¡c nhÆ° sá»‘ láº§n xuáº¥t hiá»‡n cá»§a tá»« trong vÄƒn báº£n. NÃ³ sá»­ dá»¥ng phÃ¢n phá»‘i Ä‘a thá»©c Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t cá»§a cÃ¡c Ä‘áº·c trÆ°ng.\n- Æ¯á»›c lÆ°á»£ng lÃ  quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh (vÃ­ dá»¥: mean Î¼_k vÃ  variance Ïƒ_k^2 trong Gaussian Naive Bayes) tá»« dá»¯ liá»‡u huáº¥n luyá»‡n.\n- Gaussian Naive Bayes lÃ  má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i dá»±a trÃªn Ä‘á»‹nh lÃ½ Bayes vá»›i giáº£ Ä‘á»‹nh máº¡nh máº½ vá» sá»± Ä‘á»™c láº­p cÃ³ Ä‘iá»u kiá»‡n giá»¯a cÃ¡c Ä‘áº·c trÆ°ng. NÃ³ giáº£ Ä‘á»‹nh ráº±ng cÃ¡c Ä‘áº·c trÆ°ng liÃªn tá»¥c tuÃ¢n theo phÃ¢n phá»‘i Gaussian (chuáº©n).\n- Bernoulli Naive Bayes lÃ  má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i dá»±a trÃªn Ä‘á»‹nh lÃ½ Bayes, Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c Ä‘áº·c trÆ°ng nhá»‹ phÃ¢n (binary features), tá»©c lÃ  cÃ¡c Ä‘áº·c trÆ°ng chá»‰ cÃ³ thá»ƒ cÃ³ hai giÃ¡ trá»‹ (vÃ­ dá»¥: cÃ³ hoáº·c khÃ´ng xuáº¥t hiá»‡n). NÃ³ sá»­ dá»¥ng phÃ¢n phá»‘i Bernoulli Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t cá»§a cÃ¡c Ä‘áº·c trÆ°ng.\n\n**Má»‘i quan há»‡:**\n- Multinomial Naive Bayes Ä‘Æ°á»£c á»©ng dá»¥ng rá»™ng rÃ£i trong phÃ¢n loáº¡i vÄƒn báº£n, Ä‘áº·c biá»‡t vá»›i cÃ¡c Ä‘áº·c trÆ°ng lÃ  sá»‘ láº§n xuáº¥t hiá»‡n cá»§a tá»« (word counts).\n- Multinomial Naive Bayes Ä‘Æ°á»£c á»©ng dá»¥ng trong lá»c spam Ä‘á»ƒ phÃ¢n loáº¡i email lÃ  spam hay khÃ´ng spam dá»±a trÃªn táº§n suáº¥t tá»«.\n- Multinomial Naive Bayes Ã¡p dá»¥ng Laplace smoothing Ä‘á»ƒ xá»­ lÃ½ váº¥n Ä‘á» xÃ¡c suáº¥t báº±ng 0 khi má»™t Ä‘áº·c trÆ°ng khÃ´ng xuáº¥t hiá»‡n trong má»™t lá»›p cá»¥ thá»ƒ.\n- Gaussian Naive Bayes sá»­ dá»¥ng variance Ïƒ_k^2 lÃ m tham sá»‘ Ä‘á»ƒ mÃ´ táº£ Ä‘á»™ phÃ¢n tÃ¡n cá»§a phÃ¢n phá»‘i Gaussian cho má»—i Ä‘áº·c trÆ°ng trong má»—i lá»›p.\n- Gaussian Naive Bayes Ä‘Æ°á»£c á»©ng dá»¥ng trong phÃ¢n loáº¡i vÄƒn báº£n khi cÃ¡c Ä‘áº·c trÆ°ng cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a báº±ng phÃ¢n phá»‘i Gaussian.\n- Bernoulli Naive Bayes Ä‘Æ°á»£c á»©ng dá»¥ng trong phÃ¢n loáº¡i vÄƒn báº£n khi cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng nhá»‹ phÃ¢n (vÃ­ dá»¥: tá»« cÃ³ xuáº¥t hiá»‡n hay khÃ´ng)."
        },
        "output_without_context": {
            "question": "Trong Bá»™ phÃ¢n loáº¡i NaÃ¯ve Bayes, giáº£ Ä‘á»‹nh Ä‘á»™c láº­p cÃ³ Ä‘iá»u kiá»‡n áº£nh hÆ°á»Ÿng nhÆ° tháº¿ nÃ o Ä‘áº¿n kháº£ nÄƒng Æ°á»›c tÃ­nh xÃ¡c suáº¥t cá»§a cÃ¡c biáº¿n?",
            "answer": "Giáº£ Ä‘á»‹nh Ä‘á»™c láº­p giÃºp Ä‘Æ¡n giáº£n hÃ³a tÃ­nh toÃ¡n xÃ¡c suáº¥t báº±ng cÃ¡ch cho phÃ©p Æ°á»›c tÃ­nh xÃ¡c suáº¥t cá»§a tá»«ng biáº¿n Ä‘á»™c láº­p vá»›i nhau."
        }
    },
    {
        "topic_description": "Chá»§ Ä‘á» cuá»‘i cÃ¹ng káº¿t há»£p kiáº¿n thá»©c tá»« tuáº§n 1 vÃ  tuáº§n 2. Há»c sinh sáº½ Ä‘Æ°á»£c yÃªu cáº§u so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh phÃ¢n biá»‡t vÃ  sinh vá» Æ°u nhÆ°á»£c Ä‘iá»ƒm cá»§a chÃºng trong cÃ¡c tÃ¬nh huá»‘ng khÃ¡c nhau, Ä‘á»“ng thá»i lÃ m quen vá»›i cÃ¡ch nhá»¯ng mÃ´ hÃ¬nh nÃ y áº£nh hÆ°á»Ÿng Ä‘áº¿n viá»‡c lá»±a chá»n thuáº­t toÃ¡n há»c mÃ¡y thÃ­ch há»£p.",
        "output_with_context": {
            "question": "So sÃ¡nh mÃ´ hÃ¬nh phÃ¢n biá»‡t vÃ  mÃ´ hÃ¬nh sinh, mÃ´ hÃ¬nh nÃ o thÆ°á»ng Ä‘Æ°á»£c Æ°u tiÃªn sá»­ dá»¥ng trong cÃ¡c tÃ¬nh huá»‘ng cÃ³ dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng vÃ  táº¡i sao?",
            "answer": "MÃ´ hÃ¬nh sinh thÆ°á»ng Ä‘Æ°á»£c Æ°u tiÃªn sá»­ dá»¥ng trong cÃ¡c tÃ¬nh huá»‘ng cÃ³ dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng.",
            "rag_context": "**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## CÃ¢y Quyáº¿t Äá»‹nh (Decision Tree)\n\n### NhÆ°á»£c Äiá»ƒm\n\n**1. Dá»… Overfitting:**\n- CÃ¢y sÃ¢u há»c cáº£ noise\n- MÃ´ hÃ¬nh phá»©c táº¡p khÃ´ng generalize tá»‘t\n- Cáº§n pruning hoáº·c ensemble\n\n**2. KhÃ´ng á»”n Äá»‹nh:**\n- Thay Ä‘á»•i nhá» trong dá»¯ liá»‡u â†’ cÃ¢y hoÃ n toÃ n khÃ¡c\n- High variance\n- Giáº£i phÃ¡p: Ensemble methods (Random Forest)\n\n**3. ThiÃªn Vá»‹ Vá» Äáº·c TrÆ°ng CÃ³ Nhiá»u Má»©c:**\n- Äáº·c trÆ°ng vá»›i nhiá»u giÃ¡ trá»‹ unique Ä‘Æ°á»£c Æ°u tiÃªn\n- Information Gain thiÃªn vá»‹\n- Giáº£i phÃ¡p: Gain Ratio (C4.5)\n\n**4. KhÃ´ng Tá»‘i Æ¯u Cho Extrapolation:**\n- Há»“i quy chá»‰ dá»± Ä‘oÃ¡n trong pháº¡m vi training data\n- KhÃ´ng thá»ƒ dá»± Ä‘oÃ¡n ngoÃ i min/max Ä‘Ã£ tháº¥y\n- Dá»± Ä‘oÃ¡n lÃ  háº±ng sá»‘ á»Ÿ nÃºt lÃ¡\n\n**5. Táº¡o CÃ¢y ThiÃªn Vá»‹ Vá»›i Imbalanced Data:**\n- Æ¯u tiÃªn lá»›p Ä‘a sá»‘\n- Cáº§n class_weight hoáº·c resampling\n\n**6. Greedy Algorithm:**\n- Chá»n phÃ¢n chia tá»‘t nháº¥t táº¡i thá»i Ä‘iá»ƒm hiá»‡n táº¡i\n- KhÃ´ng Ä‘áº£m báº£o cÃ¢y tá»‘i Æ°u toÃ n cá»¥c\n- CÃ³ thá»ƒ bá» lá»¡ cÃ¢y tá»‘t hÆ¡n\n\n**7. KhÃ³ Báº¯t Má»‘i Quan Há»‡ Tuyáº¿n TÃ­nh:**\n- Cáº§n nhiá»u phÃ¢n chia Ä‘á»ƒ xáº¥p xá»‰ Ä‘Æ°á»ng tháº³ng\n- Linear model Ä‘Æ¡n giáº£n hÆ¡n cho quan há»‡ tuyáº¿n tÃ­nh\n\n### PhÆ°Æ¡ng PhÃ¡p Ensemble Vá»›i CÃ¢y\n\n**1. Random Forest (Rá»«ng Ngáº«u NhiÃªn):**\n\n**NguyÃªn lÃ½:**\n- XÃ¢y dá»±ng nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh\n- Má»—i cÃ¢y trÃªn bootstrap sample khÃ¡c nhau\n- Random subset Ä‘áº·c trÆ°ng táº¡i má»—i split\n- Káº¿t há»£p dá»± Ä‘oÃ¡n: Voting (classification) hoáº·c averaging (regression)\n\n**Tham sá»‘ chÃ­nh:**\n- `n_estimators`: Sá»‘ cÃ¢y (50-500)\n- `max_features`: Sá»‘ Ä‘áº·c trÆ°ng xem xÃ©t (sqrt(n) cho classification, n/3 cho regression)\n- `max_depth`: Äá»™ sÃ¢u má»—i cÃ¢y\n- `min_samples_split`, `min_samples_leaf`\n\n**Æ¯u Ä‘iá»ƒm:**\n- Giáº£m variance, Ã­t overfitting\n- á»”n Ä‘á»‹nh hÆ¡n cÃ¢y Ä‘Æ¡n\n- Feature importance Ä‘Ã¡ng tin cáº­y hÆ¡n\n- Xá»­ lÃ½ tá»‘t high-dimensional data\n- Out-of-bag error estimation\n\n**2. Gradient Boosting:**\n\n**NguyÃªn lÃ½:**\n- XÃ¢y dá»±ng cÃ¢y tuáº§n tá»±\n- Má»—i cÃ¢y há»c sá»­a lá»—i cá»§a cÃ¢y trÆ°á»›c\n- Má»—i cÃ¢y nhá» (weak learner)\n- Káº¿t há»£p cÃ³ trá»ng sá»‘\n\n**CÃ´ng thá»©c:**\n$$F_m(x) = F_{m-1}(x) + \nu \\cdot h_m(x)$$\n\nTrong Ä‘Ã³:\n- $F_m$ lÃ  mÃ´ hÃ¬nh táº¡i iteration $m$\n- $h_m$ lÃ  cÃ¢y má»›i\n- $\nu$ lÃ  learning rate\n\n**Implementations phá»• biáº¿n:**\n- **XGBoost:** Nhanh, regularization tá»‘t, xá»­ lÃ½ missing values\n- **LightGBM:** Ráº¥t nhanh, hiá»‡u quáº£ bá»™ nhá»›, leaf-wise growth\n- **CatBoost:** Tá»‘t cho categorical features, Ã­t overfitting\n\n**Æ¯u Ä‘iá»ƒm:**\n- Hiá»‡u suáº¥t cao nháº¥t trong nhiá»u competition\n- CÃ³ thá»ƒ Ä‘áº¡t accuracy ráº¥t cao\n- Xá»­ lÃ½ tá»‘t heterogeneous features\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Weak learner lÃ  má»™t mÃ´ hÃ¬nh há»c mÃ¡y cÃ³ hiá»‡u suáº¥t chá»‰ tá»‘t hÆ¡n má»™t chÃºt so vá»›i dá»± Ä‘oÃ¡n ngáº«u nhiÃªn. Trong Gradient Boosting, má»—i CÃ¢y Quyáº¿t Ä‘á»‹nh Ä‘Æ°á»£c xÃ¢y dá»±ng tuáº§n tá»± thÆ°á»ng lÃ  má»™t weak learner (cÃ¢y nhá», cÃ³ Ä‘á»™ sÃ¢u giá»›i háº¡n). CÃ¡c weak learner nÃ y Ä‘Æ°á»£c káº¿t há»£p láº¡i Ä‘á»ƒ táº¡o thÃ nh má»™t mÃ´ hÃ¬nh máº¡nh máº½ hÆ¡n, vá»›i má»—i cÃ¢y má»›i táº­p trung vÃ o viá»‡c sá»­a lá»—i cá»§a cÃ¡c cÃ¢y trÆ°á»›c Ä‘Ã³.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»c SÃ¢u (Deep Learning)\n- **Human evaluation:** Manual assessment (gold standard)\n\n### Há»c Chuyá»ƒn Giao (Transfer Learning)\n\nTransfer Learning lÃ  ká»¹ thuáº­t táº­n dá»¥ng kiáº¿n thá»©c Ä‘Ã£ há»c tá»« má»™t task Ä‘á»ƒ giáº£i quyáº¿t task khÃ¡c, Ä‘áº·c biá»‡t há»¯u Ã­ch khi dá»¯ liá»‡u háº¡n cháº¿.\n\n**Äá»™ng lá»±c:**\n- Training deep networks tá»« Ä‘áº§u cáº§n **ráº¥t nhiá»u data vÃ  computation**\n- Pre-trained models Ä‘Ã£ há»c Ä‘Æ°á»£c **general features** trÃªn large datasets\n- Low-level features (edges, textures) thÆ°á»ng **transferable** giá»¯a cÃ¡c tasks\n- Tiáº¿t kiá»‡m thá»i gian vÃ  tÃ i nguyÃªn\n\n**Khi nÃ o sá»­ dá»¥ng:**\n- âœ… Data má»›i **Ã­t** (hundreds Ä‘áº¿n thousands samples)\n- âœ… Task má»›i **tÆ°Æ¡ng tá»±** task Ä‘Ã£ train\n- âœ… KhÃ´ng Ä‘á»§ computational resources\n- âŒ Data má»›i ráº¥t khÃ¡c biá»‡t vá»›i pre-trained data\n- âŒ CÃ³ ráº¥t nhiá»u data vÃ  resources (cÃ³ thá»ƒ train from scratch)\n\n**CÃ¡c PhÆ°Æ¡ng PhÃ¡p:**\n\n**1. Feature Extraction (TrÃ­ch Xuáº¥t Äáº·c TrÆ°ng):**\n\n**CÃ¡ch lÃ m:**\n- **Freeze toÃ n bá»™ pre-trained layers** (set requires_grad = False)\n- **Remove output layer** cá»§a pre-trained model\n- **Add new output layers** cho task má»›i\n- **Train chá»‰ new layers**\n\n**VÃ­ dá»¥ vá»›i CNN:**\n```\nPre-trained ResNet-50 (ImageNet):\nInput â†’ Conv layers (frozen) â†’ ... â†’ FC (frozen) â†’ 1000 classes\n\nModified for new task (10 classes):\nInput â†’ Conv layers (frozen) â†’ ... â†’ [Remove old FC] â†’ New FC â†’ 10 classes\n```\n\n**Khi nÃ o dÃ¹ng:**\n- Dataset má»›i **ráº¥t nhá»** (< 1000 samples)\n- New task **tÆ°Æ¡ng tá»±** original task\n- Limited computational resources\n\n**Æ¯u Ä‘iá»ƒm:**\n- **Ráº¥t nhanh** (chá»‰ train vÃ i layers)\n- **Ãt parameters** â†’ Ã­t overfitting\n- KhÃ´ng cáº§n GPU máº¡nh\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Ãt flexible hÆ¡n\n- Performance cÃ³ thá»ƒ khÃ´ng tá»‘i Æ°u náº¿u tasks khÃ¡c nhau nhiá»u\n\n**2. Fine-tuning (Tinh Chá»‰nh):**\n\n**CÃ¡ch lÃ m:**\n1. Load pre-trained model\n2. **Replace output layer** cho task má»›i\n3. **Unfreeze má»™t sá»‘ layers** (thÆ°á»ng top layers)\n4. Train vá»›i **learning rate nhá»** (0.0001 - 0.00001)\n\n**Strategies:**\n\n**a) Fine-tune toÃ n bá»™ network:**\n- Unfreeze táº¥t cáº£ layers\n- Train vá»›i learning rate ráº¥t nhá»\n- Cáº§n nhiá»u data hÆ¡n\n\n**b) Fine-tune top layers:**\n- **Freeze early layers** (low-level features)\n- **Unfreeze later layers** (high-level, task-specific features)\n- Phá»• biáº¿n nháº¥t\n\n**c) Progressive unfreezing:**\n- Ban Ä‘áº§u freeze táº¥t cáº£, train new layers\n- Dáº§n dáº§n unfreeze tá»« top xuá»‘ng bottom layers\n- Train tá»«ng nhÃ³m layers\n\n**Khi nÃ o dÃ¹ng:**\n- Dataset má»›i **medium-sized** (1K - 100K samples)\n- Task **hÆ¡i khÃ¡c** original task\n- CÃ³ computational resources\n\n**Æ¯u Ä‘iá»ƒm:**\n- **Better performance** hÆ¡n feature extraction\n- Adapt Ä‘Æ°á»£c features cho task cá»¥ thá»ƒ\n- Balance giá»¯a from-scratch vÃ  feature extraction\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Cháº­m hÆ¡n feature extraction\n- Risk overfitting vá»›i data Ã­t\n- Cáº§n tune learning rate cáº©n tháº­n\n\n**Learning Rate Strategy:**\n- **Discriminative learning rates:** Layers khÃ¡c nhau cÃ³ LR khÃ¡c nhau\n  - Early layers: LR ráº¥t nhá» (0.00001)\n  - Middle layers: LR nhá» (0.0001)\n  - New layers: LR lá»›n hÆ¡n (0.001)\n- TrÃ¡nh phÃ¡ há»§y learned features á»Ÿ early layers\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Computational resources (tÃ i nguyÃªn tÃ­nh toÃ¡n) lÃ  cÃ¡c yáº¿u tá»‘ nhÆ° CPU, GPU, vÃ  bá»™ nhá»› cáº§n thiáº¿t Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y. Transfer Learning, Ä‘áº·c biá»‡t lÃ  Feature Extraction, giÃºp tiáº¿t kiá»‡m tÃ i nguyÃªn tÃ­nh toÃ¡n vÃ¬ chá»‰ má»™t pháº§n nhá» cá»§a mÃ´ hÃ¬nh cáº§n Ä‘Æ°á»£c huáº¥n luyá»‡n. NgÆ°á»£c láº¡i, huáº¥n luyá»‡n deep networks tá»« Ä‘áº§u Ä‘Ã²i há»i ráº¥t nhiá»u tÃ i nguyÃªn.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»c KhÃ´ng GiÃ¡m SÃ¡t (Unsupervised Learning)\n- Matrix factorization (NMF, SVD)\n\n**5. Topic Modeling:**\n- Document clustering\n- Automatic tagging\n- Content organization\n- Trend detection\n\n**6. Gene Expression Analysis:**\n- Group similar genes\n- Identify cancer subtypes\n- Drug discovery\n- Understanding diseases\n\n**7. Social Network Analysis:**\n- Community detection\n- Influencer identification\n- Link prediction\n- Recommendation\n\n**8. Data Preprocessing:**\n- Feature extraction (PCA, ICA)\n- Noise reduction (autoencoders)\n- Data compression\n- Dimensionality reduction\n\n**9. Market Basket Analysis:**\n- Product recommendations\n- Store layout\n- Promotions\n- Cross-selling\n\n**10. Image Segmentation:**\n- Medical imaging\n- Object detection preparation\n- Video processing\n- Computer vision preprocessing\n\n---\n\n## Há»c SÃ¢u (Deep Learning)\n\n### Giá»›i Thiá»‡u vá» Há»c SÃ¢u\n\nHá»c sÃ¢u (Deep Learning) lÃ  má»™t nhÃ¡nh con cá»§a há»c mÃ¡y sá»­ dá»¥ng máº¡ng nÆ¡-ron nhÃ¢n táº¡o vá»›i nhiá»u lá»›p áº©n Ä‘á»ƒ há»c cÃ¡c biá»ƒu diá»…n phÃ¢n cáº¥p cá»§a dá»¯ liá»‡u. KhÃ¡c vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p há»c mÃ¡y truyá»n thá»‘ng, há»c sÃ¢u cÃ³ kháº£ nÄƒng tá»± Ä‘á»™ng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u thÃ´ mÃ  khÃ´ng cáº§n ká»¹ thuáº­t Ä‘áº·c trÆ°ng thá»§ cÃ´ng.\n\n**Äáº·c Ä‘iá»ƒm chÃ­nh:**\n- **Há»c biá»ƒu diá»…n phÃ¢n cáº¥p:** CÃ¡c lá»›p Ä‘áº§u há»c cÃ¡c Ä‘áº·c trÆ°ng cáº¥p tháº¥p (cáº¡nh, gÃ³c), cÃ¡c lá»›p sau há»c Ä‘áº·c trÆ°ng cáº¥p cao hÆ¡n (hÃ¬nh dáº¡ng, Ä‘á»‘i tÆ°á»£ng)\n- **Kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u lá»›n:** Hiá»‡u suáº¥t tÄƒng theo lÆ°á»£ng dá»¯ liá»‡u\n- **End-to-end learning:** Há»c trá»±c tiáº¿p tá»« Ä‘áº§u vÃ o thÃ´ Ä‘áº¿n Ä‘áº§u ra mong muá»‘n\n- **Tá»± Ä‘á»™ng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng:** KhÃ´ng cáº§n thiáº¿t káº¿ Ä‘áº·c trÆ°ng thá»§ cÃ´ng\n\n**á»¨ng dá»¥ng Ä‘Ã£ cÃ¡ch máº¡ng hÃ³a:**\n- Thá»‹ giÃ¡c mÃ¡y tÃ­nh (nháº­n dáº¡ng áº£nh, phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng)\n- Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (dá»‹ch mÃ¡y, chatbot, sinh vÄƒn báº£n)\n- Nháº­n dáº¡ng giá»ng nÃ³i (trá»£ lÃ½ áº£o, chuyá»ƒn Ä‘á»•i giá»ng nÃ³i thÃ nh vÄƒn báº£n)\n- Y táº¿ (cháº©n Ä‘oÃ¡n hÃ¬nh áº£nh, phÃ¡t triá»ƒn thuá»‘c)\n- Tá»± Ä‘á»™ng hÃ³a (xe tá»± lÃ¡i, robot)\n\n### Máº¡ng NÆ¡-ron NhÃ¢n Táº¡o (Artificial Neural Networks - ANN)\n\nMáº¡ng nÆ¡-ron nhÃ¢n táº¡o Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a nÃ£o ngÆ°á»i, trong Ä‘Ã³ cÃ¡c nÆ¡-ron sinh há»c truyá»n tÃ­n hiá»‡u cho nhau thÃ´ng qua cÃ¡c synapse.\n\n### Perceptron - ÄÆ¡n Vá»‹ CÆ¡ Báº£n\n\nPerceptron lÃ  Ä‘Æ¡n vá»‹ máº¡ng nÆ¡-ron Ä‘Æ¡n giáº£n nháº¥t, Ä‘Æ°á»£c phÃ¡t minh bá»Ÿi Frank Rosenblatt nÄƒm 1958.\n\n**CÃ´ng thá»©c:**\n$$y = \\sigma(w^Tx + b)$$\n\nTrong Ä‘Ã³:\n- $x = [x_1, x_2, ..., x_n]^T$: Vector Ä‘áº§u vÃ o (cÃ¡c Ä‘áº·c trÆ°ng)\n- $w = [w_1, w_2, ..., w_n]^T$: Vector trá»ng sá»‘ (weights)\n- $b$: Há»‡ sá»‘ Ä‘iá»u chá»‰nh (bias) - cho phÃ©p dá»‹ch chuyá»ƒn hÃ m quyáº¿t Ä‘á»‹nh\n- $\\sigma$: HÃ m kÃ­ch hoáº¡t (activation function)\n- $y$: Äáº§u ra dá»± Ä‘oÃ¡n\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t lÃ  má»™t nhÃ¡nh cá»§a Há»c MÃ¡y, nÆ¡i cÃ¡c mÃ´ hÃ¬nh hoáº·c thuáº­t toÃ¡n há»c tá»« dá»¯ liá»‡u khÃ´ng Ä‘Æ°á»£c gÃ¡n nhÃ£n. Má»¥c tiÃªu chÃ­nh lÃ  tÃ¬m kiáº¿m cÃ¡c cáº¥u trÃºc, máº«u hoáº·c nhÃ³m áº©n trong dá»¯ liá»‡u. CÃ¡c ká»¹ thuáº­t phá»• biáº¿n bao gá»“m phÃ¢n cá»¥m, giáº£m chiá»u dá»¯ liá»‡u vÃ  phÃ¢n tÃ­ch liÃªn káº¿t.\n- Há»c MÃ¡y lÃ  má»™t lÄ©nh vá»±c cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o cho phÃ©p há»‡ thá»‘ng há»c tá»« dá»¯ liá»‡u, xÃ¡c Ä‘á»‹nh cÃ¡c máº«u vÃ  Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh vá»›i sá»± can thiá»‡p tá»‘i thiá»ƒu cá»§a con ngÆ°á»i, mÃ  khÃ´ng cáº§n Ä‘Æ°á»£c láº­p trÃ¬nh rÃµ rÃ ng. NÃ³ bao gá»“m nhiá»u phÆ°Æ¡ng phÃ¡p vÃ  thuáº­t toÃ¡n Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n hoáº·c phÃ¢n loáº¡i, vá»›i cÃ¡c nhÃ¡nh nhÆ° Há»c KhÃ´ng GiÃ¡m SÃ¡t vÃ  Há»c SÃ¢u.\n\n**Má»‘i quan há»‡:**\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t chá»©a Gaussian Mixture Model (GMM) nhÆ° má»™t thuáº­t toÃ¡n phÃ¢n cá»¥m quan trá»ng.\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t sá»­ dá»¥ng Topic Modeling Ä‘á»ƒ khÃ¡m phÃ¡ cÃ¡c chá»§ Ä‘á» trong tÃ i liá»‡u.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Há»c KhÃ´ng GiÃ¡m SÃ¡t (Unsupervised Learning)\n   - Merge chÃºng thÃ nh 1 cluster\n3. Until: Chá»‰ cÃ²n 1 cluster\n\n**Steps chi tiáº¿t:**\n- Initialize: N clusters\n- Iteration 1: N-1 clusters\n- Iteration 2: N-2 clusters\n- ...\n- Final: 1 cluster\n\n**2. Divisive (Top-Down - PhÃ¢n Chia):**\n\n**Thuáº­t toÃ¡n:**\n1. Start: Táº¥t cáº£ Ä‘iá»ƒm trong 1 cluster\n2. Repeat:\n   - Chá»n cluster Ä‘á»ƒ split\n   - Chia thÃ nh 2 sub-clusters\n3. Until: Má»—i Ä‘iá»ƒm lÃ  1 cluster\n\n**Ãt phá»• biáº¿n:** Computationally expensive hÆ¡n\n\n**Linkage Methods (CÃ¡ch Äo Khoáº£ng CÃ¡ch Giá»¯a Clusters):**\n\n**1. Single Linkage (Minimum):**\n$$d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoáº£ng cÃ¡ch giá»¯a 2 Ä‘iá»ƒm gáº§n nháº¥t\n- Táº¡o long, chain-like clusters\n- Sensitive to noise vÃ  outliers\n\n**2. Complete Linkage (Maximum):**\n$$d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j}d(x,y)$$\n- Khoáº£ng cÃ¡ch giá»¯a 2 Ä‘iá»ƒm xa nháº¥t\n- Táº¡o compact, spherical clusters\n- Ãt sensitive to outliers\n\n**3. Average Linkage:**\n$$d(C_i, C_j) = \frac{1}{|C_i||C_j|}\\sum_{x \\in C_i}\\sum_{y \\in C_j}d(x,y)$$\n- Trung bÃ¬nh táº¥t cáº£ pairwise distances\n- Balance giá»¯a single vÃ  complete\n- Phá»• biáº¿n choice\n\n**4. Ward's Method:**\n- Minimize within-cluster variance sau khi merge\n- Maximize between-cluster variance\n- Táº¡o balanced, compact clusters\n- ThÆ°á»ng cho káº¿t quáº£ tá»‘t nháº¥t\n- Phá»• biáº¿n nháº¥t trong thá»±c táº¿\n\n**Dendrogram (Biá»ƒu Äá»“ CÃ¢y):**\n\nTree diagram showing cluster hierarchy.\n\n**Äá»c Dendrogram:**\n- Vertical axis: Distance/dissimilarity\n- Horizontal axis: Samples\n- Height cá»§a merge: Distance giá»¯a clusters\n- CÃ ng cao merge cÃ ng dissimilar\n\n**Cutting Dendrogram:**\n- Váº½ horizontal line\n- Number of intersections = Number of clusters\n- Height cá»§a cut = dissimilarity threshold\n\n**Æ¯u Äiá»ƒm:**\n- KhÃ´ng cáº§n specify K trÆ°á»›c\n- Dendrogram provides insights\n- Flexible - cÃ³ thá»ƒ chá»n K sau\n- Deterministic (no randomness)\n\n**NhÆ°á»£c Äiá»ƒm:**\n- Computationally expensive: O(NÂ²log N) or O(NÂ³)\n- KhÃ´ng scale vá»›i large datasets\n- Má»™t khi merge khÃ´ng thá»ƒ undo\n- Memory intensive\n\n**Khi NÃ o DÃ¹ng:**\n- Small-medium datasets (< 10,000)\n- Cáº§n understand hierarchy\n- KhÃ´ng biáº¿t K optimal\n- Exploratory analysis\n\n### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nNhÃ³m cÃ¡c Ä‘iá»ƒm cÃ³ máº­t Ä‘á»™ cao, robust to outliers vÃ  arbitrary shapes.\n\n**Tham Sá»‘:**\n\n**1. Îµ (epsilon):**\n- Maximum distance giá»¯a 2 Ä‘iá»ƒm Ä‘á»ƒ Ä‘Æ°á»£c coi lÃ  neighbors\n- Äá»‹nh nghÄ©a neighborhood radius\n- QuÃ¡ nhá»: Nhiá»u noise points\n- QuÃ¡ lá»›n: Merge nhiá»u clusters\n\n**2. MinPts (Minimum Points):**\n- Minimum sá»‘ Ä‘iá»ƒm trong Îµ-neighborhood Ä‘á»ƒ lÃ  core point\n- ThÆ°á»ng: 4, 5, hoáº·c 2Ã—dim\n- Larger MinPts: Ãt core points, stricter\n\n**CÃ¡c Loáº¡i Äiá»ƒm:**\n\n**1. Core Point:**\n- CÃ³ â‰¥ MinPts Ä‘iá»ƒm khÃ¡c trong Îµ-neighborhood (bao gá»“m cáº£ chÃ­nh nÃ³)\n- Trung tÃ¢m cá»§a clusters\n- Can form clusters\n\n**2. Border Point:**\n- Náº±m trong Îµ-neighborhood cá»§a core point\n- CÃ³ < MinPts neighbors\n- Thuá»™c cluster nhÆ°ng khÃ´ng core\n- á» biÃªn cá»§a cluster\n\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t lÃ  má»™t nhÃ¡nh cá»§a Há»c MÃ¡y, nÆ¡i cÃ¡c mÃ´ hÃ¬nh hoáº·c thuáº­t toÃ¡n há»c tá»« dá»¯ liá»‡u khÃ´ng Ä‘Æ°á»£c gÃ¡n nhÃ£n. Má»¥c tiÃªu chÃ­nh lÃ  tÃ¬m kiáº¿m cÃ¡c cáº¥u trÃºc, máº«u hoáº·c nhÃ³m áº©n trong dá»¯ liá»‡u. CÃ¡c ká»¹ thuáº­t phá»• biáº¿n bao gá»“m phÃ¢n cá»¥m, giáº£m chiá»u dá»¯ liá»‡u vÃ  phÃ¢n tÃ­ch liÃªn káº¿t.\n- Há»c MÃ¡y lÃ  má»™t lÄ©nh vá»±c cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o cho phÃ©p há»‡ thá»‘ng há»c tá»« dá»¯ liá»‡u, xÃ¡c Ä‘á»‹nh cÃ¡c máº«u vÃ  Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh vá»›i sá»± can thiá»‡p tá»‘i thiá»ƒu cá»§a con ngÆ°á»i, mÃ  khÃ´ng cáº§n Ä‘Æ°á»£c láº­p trÃ¬nh rÃµ rÃ ng. NÃ³ bao gá»“m nhiá»u phÆ°Æ¡ng phÃ¡p vÃ  thuáº­t toÃ¡n Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n hoáº·c phÃ¢n loáº¡i, vá»›i cÃ¡c nhÃ¡nh nhÆ° Há»c KhÃ´ng GiÃ¡m SÃ¡t vÃ  Há»c SÃ¢u.\n\n**Má»‘i quan há»‡:**\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t chá»©a Gaussian Mixture Model (GMM) nhÆ° má»™t thuáº­t toÃ¡n phÃ¢n cá»¥m quan trá»ng.\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t sá»­ dá»¥ng Topic Modeling Ä‘á»ƒ khÃ¡m phÃ¡ cÃ¡c chá»§ Ä‘á» trong tÃ i liá»‡u.\n- Há»c KhÃ´ng GiÃ¡m SÃ¡t sá»­ dá»¥ng Matrix factorization nhÆ° má»™t ká»¹ thuáº­t Ä‘á»ƒ tÃ¬m kiáº¿m cáº¥u trÃºc áº©n trong dá»¯ liá»‡u khÃ´ng nhÃ£n.\n\n**Ná»™i dung tá»« tÃ i liá»‡u:**\n# Há»c MÃ¡y (Machine Learning)\n## Lá»±a Chá»n Äáº·c TrÆ°ng & Tá»‘i Æ¯u HÃ³a MÃ´ HÃ¬nh\n- CV cho stable estimate\n\n**1. K-Fold Cross-Validation:**\n\n**Thuáº­t toÃ¡n:**\n1. Chia data thÃ nh k folds\n2. For i = 1 to k:\n   - Use fold i lÃ m validation\n   - Use k-1 folds cÃ²n láº¡i lÃ m training\n   - Train vÃ  evaluate\n3. Average metrics across k folds\n\n**Chá»n k:**\n- k=5: Standard, good balance\n- k=10: More stable, more computational\n- Larger k: Less bias, more variance, more expensive\n\n**Æ¯u Ä‘iá»ƒm:**\n- Sá»­ dá»¥ng toÃ n bá»™ data\n- Stable estimate\n- Reduce variance\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- k láº§n training (expensive)\n- CÃ³ thá»ƒ cháº­m\n\n**2. Stratified K-Fold:**\n\n**NguyÃªn lÃ½:**\n- Maintain class distribution trong má»—i fold\n- Each fold representative\n\n**Khi nÃ o dÃ¹ng:**\n- Imbalanced datasets\n- Classification tasks\n- Äáº£m báº£o má»—i fold cÃ³ Ä‘á»§ samples má»—i class\n\n**Æ¯u Ä‘iá»ƒm:**\n- Fair evaluation vá»›i imbalanced data\n- Consistent class proportions\n\n**3. Leave-One-Out (LOO):**\n\n**NguyÃªn lÃ½:**\n- k = n (n = sá»‘ samples)\n- Má»—i sample lÃ  má»™t fold\n\n**Æ¯u Ä‘iá»ƒm:**\n- Maximum data cho training\n- No randomness\n- Deterministic\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Ráº¥t cháº­m (n iterations)\n- High variance\n- Chá»‰ kháº£ thi vá»›i small datasets (< 1000)\n\n**Khi nÃ o dÃ¹ng:**\n- Very small datasets\n- Need maximum training data\n- Computational resources available\n\n**4. Time Series Cross-Validation:**\n\n**NguyÃªn lÃ½:**\n- Respect temporal order\n- Train on past, validate on future\n- No data leakage from future\n\n**Expanding Window:**\n```\nFold 1: Train [1:100] â†’ Test [101:120]\nFold 2: Train [1:120] â†’ Test [121:140]\nFold 3: Train [1:140] â†’ Test [141:160]\n```\n\n**Rolling Window:**\n```\nFold 1: Train [1:100] â†’ Test [101:120]\nFold 2: Train [21:120] â†’ Test [121:140]\nFold 3: Train [41:140] â†’ Test [141:160]\n```\n\n**Quan trá»ng:**\n- **KHÃ”NG shuffle data**\n- Maintain temporal order\n- Avoid look-ahead bias\n\n**5. Nested Cross-Validation:**\n\n**NguyÃªn lÃ½:**\n- Outer loop: Model evaluation\n- Inner loop: Hyperparameter tuning\n- Prevents overfitting in parameter selection\n\n**Structure:**\n```\nOuter CV (5-fold):\n  For each outer fold:\n    Inner CV (5-fold):\n      Hyperparameter tuning\n    Train with best params\n    Evaluate on outer fold\n```\n\n**Æ¯u Ä‘iá»ƒm:**\n- Unbiased performance estimate\n- Proper hyperparameter tuning\n- Gold standard\n\n**NhÆ°á»£c Ä‘iá»ƒm:**\n- Very expensive (k_outer Ã— k_inner trainings)\n- Overkill cho simple problems\n\n**Khi nÃ o dÃ¹ng:**\n- Need unbiased estimate\n- Publishing results\n- Critical applications\n- Have computational resources\n\n### Learning Curves (ÄÆ°á»ng Cong Há»c)\n\nPhÃ¢n tÃ­ch hiá»‡u suáº¥t mÃ´ hÃ¬nh vs kÃ­ch thÆ°á»›c training set.\n\n**Váº½ gÃ¬:**\n- X-axis: Training set size\n- Y-axis: Error (hoáº·c Score)\n- Two curves: Training error & Validation error\n\n**Cháº©n ÄoÃ¡n:**\n\n**1. High Bias (Underfitting):**\n```\nTraining error: Cao\nValidation error: Cao\nGap: Nhá»\nBoth plateau at high error\n```\n**Dáº¥u hiá»‡u:**\n- Cáº£ hai curves plateau\n- Performance kÃ©m ngay cáº£ vá»›i nhiá»u data\n- ThÃªm data khÃ´ng giÃºp\n\n**Giáº£i phÃ¡p:**\n- Increase model complexity\n- Add features\n- Reduce regularization\n- Try complex model\n\n**2. High Variance (Overfitting):**\n```\nTraining error: Tháº¥p\nValidation error: Cao\nGap: Lá»›n\nGap doesn't close with more data\n\n**CÃ¡c khÃ¡i niá»‡m quan trá»ng:**\n- Learning Curves (ÄÆ°á»ng cong há»c) lÃ  biá»ƒu Ä‘á»“ phÃ¢n tÃ­ch hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh (thÆ°á»ng lÃ  lá»—i hoáº·c Ä‘iá»ƒm sá»‘) so vá»›i kÃ­ch thÆ°á»›c cá»§a táº­p huáº¥n luyá»‡n. Biá»ƒu Ä‘á»“ nÃ y thÆ°á»ng cÃ³ hai Ä‘Æ°á»ng: Ä‘Æ°á»ng lá»—i huáº¥n luyá»‡n (Training error) vÃ  Ä‘Æ°á»ng lá»—i validation (Validation error). Learning Curves giÃºp cháº©n Ä‘oÃ¡n cÃ¡c váº¥n Ä‘á» nhÆ° High Bias (Underfitting) vÃ  High Variance (Overfitting) cá»§a mÃ´ hÃ¬nh.\n\n**Má»‘i quan há»‡:**\n- Learning Curves cháº©n Ä‘oÃ¡n váº¥n Ä‘á» High Bias (Underfitting) khi cáº£ training error vÃ  validation error Ä‘á»u cao vÃ  cÃ³ khoáº£ng cÃ¡ch nhá», cáº£ hai Ä‘Æ°á»ng Ä‘á»u Ä‘áº¡t Ä‘áº¿n má»™t má»©c cao vÃ  khÃ´ng cáº£i thiá»‡n khi thÃªm dá»¯ liá»‡u."
        },
        "output_without_context": {
            "question": "So sÃ¡nh Æ°u nhÆ°á»£c Ä‘iá»ƒm cá»§a mÃ´ hÃ¬nh phÃ¢n biá»‡t vÃ  mÃ´ hÃ¬nh sinh trong viá»‡c lá»±a chá»n thuáº­t toÃ¡n há»c mÃ¡y nÃ o lÃ  quan trá»ng nháº¥t Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a chÃºng trong cÃ¡c tÃ¬nh huá»‘ng khÃ¡c nhau?",
            "answer": "MÃ´ hÃ¬nh phÃ¢n biá»‡t thÆ°á»ng cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n trong cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i, trong khi mÃ´ hÃ¬nh sinh cÃ³ thá»ƒ táº¡o ra dá»¯ liá»‡u má»›i vÃ  xá»­ lÃ½ cÃ¡c tÃ¬nh huá»‘ng khÃ´ng cháº¯c cháº¯n tá»‘t hÆ¡n."
        }
    }
]