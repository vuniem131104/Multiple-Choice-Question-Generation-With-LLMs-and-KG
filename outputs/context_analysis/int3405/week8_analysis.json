[
    {
        "topic_description": "Chủ đề này tập trung vào việc hiểu Mạng Nơ-ron Tích chập (CNN), bao gồm cấu trúc, chức năng của các lớp trong CNN và các ứng dụng thực tế như phân loại hình ảnh. Học sinh sẽ được đánh giá về khả năng xác định các thành phần của CNN và ứng dụng của chúng trong các bài toán phân loại.",
        "output_with_context": {
            "question": "Mạng Nơ-ron Tích chập (CNN) chủ yếu được sử dụng để xử lý loại dữ liệu nào?",
            "answer": "Dữ liệu dạng lưới như hình ảnh.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**1. Generator (Bộ Sinh):** $G(z) \to fake\\_data$\n\n**Input:**\n- Random noise vector $z \\sim p_z(z)$ (thường Gaussian hoặc Uniform)\n- Dimension thường 100-1000\n\n**Output:**\n- Synthetic data (fake) $G(z)$\n- Cùng kích thước với real data\n- Ví dụ: Ảnh 64×64×3\n\n**Mục tiêu:**\n- **Generate realistic samples** không phân biệt được với real data\n- \"Lừa\" Discriminator tin là real\n\n**Kiến trúc:**\n- Thường là deconvolutional network (transpose convolutions)\n- Batch normalization, ReLU/LeakyReLU\n- Tanh activation ở output (để output trong [-1, 1])\n\n**2. Discriminator (Bộ Phân Biệt):** $D(x) \to [0,1]$\n\n**Input:**\n- Data sample $x$ (có thể real hoặc fake)\n\n**Output:**\n- Scalar trong [0, 1]: Xác suất sample là **real**\n- Gần 1 = tin là real, gần 0 = tin là fake\n\n**Mục tiêu:**\n- **Phân biệt chính xác** real vs fake\n- Maximize classification accuracy\n\n**Kiến trúc:**\n- Thường là CNN (convolutional network)\n- Leaky ReLU, Dropout\n- Sigmoid activation ở output\n\n**Training - Minimax Game:**\n\nĐây là một **two-player game** với objective function:\n\n$$\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1-D(G(z)))]$$\n\n**Giải thích:**\n\n**Discriminator muốn maximize:**\n- $\\mathbb{E}_{x \\sim p_{data}}[\\log D(x)]$: Maximize log probability của real data\n- $\\mathbb{E}_{z \\sim p_z}[\\log(1-D(G(z)))]$: Maximize log probability reject fake data\n\n**Generator muốn minimize:**\n- $\\mathbb{E}_{z \\sim p_z}[\\log(1-D(G(z)))]$: Minimize log probability fake bị reject\n- Tương đương: Maximize $\\mathbb{E}_{z}[\\log D(G(z))]$ (non-saturating loss trong thực tế)\n\n**Thuật Toán Training:**\n\n**Alternating Updates** (training xen kẽ):\n\n**Mỗi iteration:**\n\n1. **Train Discriminator (k steps, thường k=1):**\n   - Sample mini-batch real data $\\{x^{(1)}, ..., x^{(m)}\\}$\n   - Sample mini-batch noise $\\{z^{(1)}, ..., z^{(m)}\\}$\n   - Generate fake data: $\tilde{x}^{(i)} = G(z^{(i)})$\n   - Update D by **ascending** gradient:\n   $$\nabla_{\theta_D} \frac{1}{m}\\sum_{i=1}^{m}[\\log D(x^{(i)}) + \\log(1-D(G(z^{(i)})))]$$\n\n2. **Train Generator (1 step):**\n   - Sample mini-batch noise $\\{z^{(1)}, ..., z^{(m)}\\}$\n   - Update G by **descending** gradient:\n   $$\nabla_{\theta_G} \frac{1}{m}\\sum_{i=1}^{m}\\log(1-D(G(z^{(i)})))$$\n   - Hoặc non-saturating: Ascending $\nabla_{\theta_G} \frac{1}{m}\\sum_{i=1}^{m}\\log D(G(z^{(i)}))$\n\n**Lý do train D nhiều hơn G:**\n- D cần đủ accurate để provide good gradient cho G\n- Nếu D quá yếu, G không học được gì\n\n**Thách Thức trong Training GANs:**\n\n**1. Mode Collapse:**\n- **Vấn đề:** Generator chỉ sinh một vài modes (variations) của data\n\n**Các khái niệm quan trọng:**\n- CNN (Convolutional Neural Network) là một loại mạng nơ-ron thường được sử dụng làm kiến trúc cho Discriminator. CNN hiệu quả trong việc xử lý dữ liệu dạng lưới như hình ảnh, sử dụng các lớp tích chập để trích xuất các đặc trưng phân cấp từ dữ liệu đầu vào.\n- CNN (Convolutional Neural Network) là một loại mạng nơ-ron chuyên dụng để xử lý dữ liệu có cấu trúc lưới như hình ảnh. Trong RL, CNN thường được sử dụng để trích xuất đặc trưng từ các quan sát hình ảnh (ví dụ: khung hình trò chơi, hình ảnh camera) làm đầu vào cho các thuật toán học tăng cường.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n| | L1 | L2 |\n|---|---|---|\n| Sparsity | Có (nhiều weights = 0) | Không |\n| Feature selection | Có | Không |\n| Gradient | Không trơn tại 0 | Trơn mọi nơi |\n| Sử dụng | Nhiều features, cần sparse | Default choice |\n\n**2. Dropout:**\n\nKỹ thuật cực kỳ hiệu quả được giới thiệu bởi Hinton et al. (2012).\n\n**Cơ chế:**\n- Trong mỗi iteration training, **ngẫu nhiên \"tắt\" (drop)** một số nơ-ron\n- Mỗi nơ-ron có xác suất $p$ bị drop (thường $p = 0.2 - 0.5$)\n- Nơ-ron bị drop không tham gia forward và backward pass\n- Tại test time, sử dụng tất cả nơ-ron nhưng scale output với $(1-p)$\n\n**Tại sao hiệu quả:**\n- **Ngăn co-adaptation:** Nơ-ron không thể phụ thuộc quá nhiều vào nơ-ron cụ thể khác\n- **Ensemble effect:** Mỗi iteration training một mạng con khác nhau → giống train nhiều mô hình\n- **Noise injection:** Thêm noise vào training → mô hình robust hơn\n\n**Cách sử dụng:**\n- Áp dụng cho fully connected layers (không dùng cho convolutional layers)\n- Hidden layers: dropout rate 0.5\n- Input layer: dropout rate thấp hơn (0.2) nếu dùng\n- **Không** sử dụng dropout trong test/inference\n\n**Inverted Dropout:**\n```python\n# Training\nmask = (np.random.rand(*shape) > dropout_rate) / (1 - dropout_rate)\noutput = input * mask\n\n# Testing: không cần scale\noutput = input\n```\n\n**Biến thể:**\n- **DropConnect:** Drop connections thay vì neurons\n- **Spatial Dropout:** Drop entire feature maps trong CNN\n- **Variational Dropout:** Dropout mask giống nhau qua time steps trong RNN\n\n**3. Early Stopping:**\n\nDừng training khi validation loss bắt đầu tăng.\n\n**Cách thực hiện:**\n1. Chia data thành train/validation/test\n2. Theo dõi validation loss sau mỗi epoch\n3. Lưu model có validation loss tốt nhất\n4. Dừng nếu validation loss không giảm sau $n$ epochs (patience)\n\n**Ưu điểm:**\n- Đơn giản, hiệu quả\n- Tự động tìm số epoch tối ưu\n- Không cần thêm hyperparameter phức tạp\n\n**Lưu ý:**\n- Cần validation set riêng (không dùng test set!)\n- Patience thường 10-20 epochs\n- Có thể kết hợp với learning rate scheduling\n\n**4. Data Augmentation:**\n\nTăng cường dữ liệu training bằng cách tạo biến thể từ dữ liệu gốc.\n\n**Cho ảnh (Computer Vision):**\n- **Geometric transformations:**\n  - Rotation (xoay): ±15-30 độ\n  - Flip (lật): Horizontal, vertical\n  - Crop (cắt): Random crop, center crop\n  - Zoom: Scale in/out\n  - Translation (dịch chuyển)\n  - Shearing (nghiêng)\n  \n- **Color transformations:**\n  - Brightness adjustment (độ sáng)\n  - Contrast adjustment (độ tương phản)\n  - Saturation adjustment (độ bão hòa)\n  - Hue adjustment (sắc màu)\n  - RGB channel shifts\n  \n- **Noise injection:**\n  - Gaussian noise\n  - Salt and pepper noise\n  - Blur (làm mờ)\n  \n- **Advanced techniques:**\n  - **Mixup:** Trộn hai ảnh: $x = \\lambda x_1 + (1-\\lambda)x_2$\n\n**Các khái niệm quan trọng:**\n- Convolutional layers là các lớp trong mạng nơ-ron tích chập (CNN) thực hiện phép tích chập trên dữ liệu đầu vào. Chúng được sử dụng trong Convolutional Autoencoder để trích xuất các đặc trưng cục bộ và bảo toàn cấu trúc không gian của dữ liệu, đặc biệt hiệu quả với hình ảnh.\n- Convolutional layers (lớp tích chập) là các lớp trong mạng nơ-ron tích chập (CNN) thực hiện phép tích chập trên dữ liệu đầu vào để trích xuất các đặc trưng. Dropout thường không được sử dụng trực tiếp cho các lớp tích chập, thay vào đó, các biến thể như Spatial Dropout có thể được áp dụng.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**Khi nào dùng:**\n- RNNs, LSTMs, GRUs\n- Transformers (BERT, GPT)\n- Batch size nhỏ\n- Online learning\n\n**7. Weight Decay:**\n\nThêm penalty cho magnitude của weights trong update rule.\n\n**Trong optimizer:**\n$$w := w - \\alpha(\frac{\\partial L}{\\partial w} + \\lambda w)$$\n\n**Khác với L2 Regularization:**\n- L2 reg: Thêm vào loss function\n- Weight decay: Thêm trực tiếp vào update rule\n- Với SGD: Tương đương nhau\n- Với Adam/AdamW: **Khác nhau!** AdamW implement weight decay đúng cách\n\n**8. Stochastic Depth:**\n\nNgẫu nhiên skip một số layers trong training (dùng cho ResNets).\n\n**9. Label Smoothing:**\n\nThay vì hard labels (0 hoặc 1), dùng soft labels.\n\n**Ví dụ:** \n- Hard: [0, 1, 0, 0]\n- Soft (ε=0.1): [0.025, 0.925, 0.025, 0.025]\n\n**Công thức:**\n$$y_{smooth} = (1-\\epsilon)y + \frac{\\epsilon}{K}$$\n\n**Ưu điểm:**\n- Ngăn mô hình quá tự tin (overconfident)\n- Improve generalization\n- Thường dùng trong image classification\n\n**10. Gradient Clipping:**\n\nGiới hạn magnitude của gradients, đặc biệt quan trọng cho RNNs.\n\n**Clip by value:**\n$$g = \\max(\\min(g, threshold), -threshold)$$\n\n**Clip by norm:**\n$$g = \frac{threshold \\cdot g}{||g||} \text{ if } ||g|| > threshold$$\n\n**Sử dụng:**\n- RNNs, LSTMs (giải quyết exploding gradients)\n- Transformers với sequence dài\n- Threshold thường: 1.0 hoặc 5.0\n\n### Mạng Nơ-ron Tích Chập (Convolutional Neural Networks - CNN)\n\nCNN là kiến trúc mạng nơ-ron chuyên biệt cho xử lý dữ liệu dạng lưới (grid-like), đặc biệt là ảnh. Được lấy cảm hứng từ vỏ não thị giác (visual cortex) của động vật.\n\n**Đặc điểm chính:**\n- **Locally connected:** Mỗi nơ-ron chỉ kết nối với vùng local của input\n- **Parameter sharing:** Cùng một filter được áp dụng trên toàn bộ input\n- **Translation invariance:** Phát hiện đặc trưng ở bất kỳ vị trí nào\n- **Hierarchical feature learning:** Lớp đầu học low-level features, lớp sau học high-level features\n\n**Các Thành Phần Chính:**\n\n**1. Lớp Tích Chập (Convolutional Layer):**\n\nÁp dụng filters (kernels) trượt trên input để trích xuất features.\n\n**Công thức:**\n$$Output[i,j] = \\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n] \times Kernel[m,n] + bias$$\n\nHoặc dạng tổng quát với nhiều channels:\n$$Output[i,j,d] = \\sum_{c=0}^{C-1}\\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n,c] \times Kernel[m,n,c,d] + bias[d]$$\n\n**Khái niệm quan trọng:**\n\n**Filter/Kernel:**\n- Ma trận nhỏ (thường 3×3, 5×5, 7×7)\n- Chứa weights học được\n- Mỗi filter phát hiện một đặc trưng cụ thể:\n  - Lớp đầu: Edges (cạnh), corners (góc), colors\n\n**Các khái niệm quan trọng:**\n- Convolutional Neural Networks (CNN) là một kiến trúc mạng nơ-ron chuyên biệt được thiết kế để xử lý dữ liệu dạng lưới (grid-like data), đặc biệt là hình ảnh. CNN được lấy cảm hứng từ vỏ não thị giác của động vật và có các đặc điểm chính như kết nối cục bộ (locally connected), chia sẻ tham số (parameter sharing), bất biến dịch chuyển (translation invariance), và học đặc trưng phân cấp (hierarchical feature learning). Các thành phần chính bao gồm Lớp Tích Chập và Lớp Pooling.\n\n**Mối quan hệ:**\n- Convolutional Neural Networks chứa một hoặc nhiều Convolutional Layer làm thành phần chính để trích xuất đặc trưng.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n- Kết hợp tất cả features để ra quyết định cuối cùng\n- Classification head\n\n**Nhược điểm:**\n- **Nhiều parameters:** Có thể chiếm 90% tổng số parameters\n- Dễ overfit\n- Xu hướng thay thế bằng Global Average Pooling + 1 FC nhỏ\n\n**Kiến Trúc Điển Hình:**\n\n**Pattern chung:**\n```\nInput → [Conv → ReLU → Pool] × N → [FC] × M → Output\n```\n\n**Ví dụ cụ thể:**\n```\nInput (224×224×3)\n↓\nConv 64 filters 3×3, ReLU (224×224×64)\n↓\nMaxPool 2×2 (112×112×64)\n↓\nConv 128 filters 3×3, ReLU (112×112×128)\n↓\nMaxPool 2×2 (56×56×128)\n↓\nConv 256 filters 3×3, ReLU (56×56×256)\n↓\nMaxPool 2×2 (28×28×256)\n↓\nFlatten (200,704 dimensions)\n↓\nFC 1024, ReLU\n↓\nDropout 0.5\n↓\nFC 10 (num_classes)\n↓\nSoftmax\n```\n\n**Các Kiến Trúc CNN Kinh Điển:**\n\n**LeNet-5 (1998 - Yann LeCun):**\n- Một trong những CNN đầu tiên\n- Phân loại chữ số viết tay (MNIST)\n- Kiến trúc: Conv → Pool → Conv → Pool → FC → FC\n- Chỉ ~60K parameters\n\n**AlexNet (2012 - Krizhevsky, Sutskever, Hinton):**\n- **Breakthrough moment** trong Deep Learning\n- Thắng ImageNet 2012 với top-5 error 15.3% (giảm 10% so với runner-up)\n- 8 layers, ~60M parameters\n\n**Đóng góp quan trọng:**\n- Sử dụng **ReLU** thay vì tanh/sigmoid\n- **Dropout** regularization\n- **Data augmentation** mạnh\n- **GPU training** (2 GPUs)\n- Local Response Normalization (LRN)\n\n**VGGNet (2014 - Visual Geometry Group, Oxford):**\n- Rất sâu: VGG-16 (16 layers), VGG-19 (19 layers)\n- **Đơn giản và đồng nhất:** Chỉ dùng conv 3×3, pool 2×2\n- ~138M parameters (rất lớn!)\n\n**Kiến trúc VGG-16:**\n```\nInput (224×224×3)\n↓\n[Conv 3×3, 64] × 2 → Pool\n↓\n[Conv 3×3, 128] × 2 → Pool\n↓\n[Conv 3×3, 256] × 3 → Pool\n↓\n[Conv 3×3, 512] × 3 → Pool\n↓\n[Conv 3×3, 512] × 3 → Pool\n↓\nFC 4096 → FC 4096 → FC 1000\n```\n\n**Insight:**\n- Nhiều conv 3×3 stacked = receptive field lớn hơn nhưng ít params hơn\n- 2 conv 3×3 = receptive field 5×5\n- 3 conv 3×3 = receptive field 7×7\n\n**GoogLeNet/Inception (2014 - Google):**\n- Thắng ImageNet 2014\n- 22 layers nhưng chỉ 7M parameters (ít hơn AlexNet!)\n- **Inception module:** Ý tưởng chính\n\n**Inception Module:**\n- Áp dụng **đồng thời** nhiều kích thước filter (1×1, 3×3, 5×5) và pooling\n- Concatenate outputs\n- Network tự học combination nào tốt\n\n```\nInput\n├─ 1×1 conv\n├─ 1×1 conv → 3×3 conv\n├─ 1×1 conv → 5×5 conv\n\n**Các khái niệm quan trọng:**\n- FC (Fully Connected Layer) là một lớp trong mạng nơ-ron nơi mỗi nơ-ron trong lớp hiện tại được kết nối với mọi nơ-ron trong lớp trước đó. Lớp FC thường được sử dụng ở cuối mạng để thực hiện phân loại hoặc hồi quy. Trong kiến trúc CNN điển hình, nó xuất hiện dưới dạng [FC] × M.\n- Lớp Tích Chập (Convolutional Layer, Conv) là thành phần cốt lõi của mạng nơ-ron tích chập (CNN), chịu trách nhiệm trích xuất các đặc trưng từ dữ liệu đầu vào. Nó thực hiện phép tích chập bằng cách áp dụng các bộ lọc (filters hoặc kernels) trượt trên input. Mỗi filter học cách phát hiện một đặc trưng cụ thể như cạnh, góc, hoặc màu sắc. Công thức tính toán đầu ra của một lớp tích chập cho một kênh là $Output[i,j] = \\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n] \times Kernel[m,n] + bias$, và dạng tổng quát với nhiều kênh là $Output[i,j,d] = \\sum_{c=0}^{C-1}\\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n,c] \times Kernel[m,n,c,d] + bias[d]$. Ví dụ, \"Conv 64 filters 3×3\" chỉ ra rằng lớp tích chập này sử dụng 64 bộ lọc có kích thước 3x3.\n\n**Mối quan hệ:**\n- Kiến trúc LeNet-5 chứa các lớp FC.\n- Kiến trúc LeNet-5 chứa các lớp Conv."
        },
        "output_without_context": {
            "question": "Mạng Nơ-ron Tích chập (CNN) chủ yếu được sử dụng trong ứng dụng nào sau đây?",
            "answer": "Phân loại hình ảnh"
        }
    },
    {
        "topic_description": "Chủ đề này khám phá các lớp gộp (Pooling Layers) trong Mạng Nơ-ron Tích chập (CNN). Học sinh sẽ học về sự khác biệt giữa gộp tối đa (Max pooling) và gộp trung bình (Average pooling), cách chúng ảnh hưởng đến kích thước đầu ra và khả năng học của mô hình.",
        "output_with_context": {
            "question": "Sự khác biệt chính giữa gộp tối đa (Max Pooling) và gộp trung bình (Average Pooling) trong mạng nơ-ron tích chập (CNN) là gì?",
            "answer": "Gộp tối đa lấy giá trị lớn nhất từ mỗi vùng con, trong khi gộp trung bình lấy giá trị trung bình từ mỗi vùng con.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n- Kết hợp tất cả features để ra quyết định cuối cùng\n- Classification head\n\n**Nhược điểm:**\n- **Nhiều parameters:** Có thể chiếm 90% tổng số parameters\n- Dễ overfit\n- Xu hướng thay thế bằng Global Average Pooling + 1 FC nhỏ\n\n**Kiến Trúc Điển Hình:**\n\n**Pattern chung:**\n```\nInput → [Conv → ReLU → Pool] × N → [FC] × M → Output\n```\n\n**Ví dụ cụ thể:**\n```\nInput (224×224×3)\n↓\nConv 64 filters 3×3, ReLU (224×224×64)\n↓\nMaxPool 2×2 (112×112×64)\n↓\nConv 128 filters 3×3, ReLU (112×112×128)\n↓\nMaxPool 2×2 (56×56×128)\n↓\nConv 256 filters 3×3, ReLU (56×56×256)\n↓\nMaxPool 2×2 (28×28×256)\n↓\nFlatten (200,704 dimensions)\n↓\nFC 1024, ReLU\n↓\nDropout 0.5\n↓\nFC 10 (num_classes)\n↓\nSoftmax\n```\n\n**Các Kiến Trúc CNN Kinh Điển:**\n\n**LeNet-5 (1998 - Yann LeCun):**\n- Một trong những CNN đầu tiên\n- Phân loại chữ số viết tay (MNIST)\n- Kiến trúc: Conv → Pool → Conv → Pool → FC → FC\n- Chỉ ~60K parameters\n\n**AlexNet (2012 - Krizhevsky, Sutskever, Hinton):**\n- **Breakthrough moment** trong Deep Learning\n- Thắng ImageNet 2012 với top-5 error 15.3% (giảm 10% so với runner-up)\n- 8 layers, ~60M parameters\n\n**Đóng góp quan trọng:**\n- Sử dụng **ReLU** thay vì tanh/sigmoid\n- **Dropout** regularization\n- **Data augmentation** mạnh\n- **GPU training** (2 GPUs)\n- Local Response Normalization (LRN)\n\n**VGGNet (2014 - Visual Geometry Group, Oxford):**\n- Rất sâu: VGG-16 (16 layers), VGG-19 (19 layers)\n- **Đơn giản và đồng nhất:** Chỉ dùng conv 3×3, pool 2×2\n- ~138M parameters (rất lớn!)\n\n**Kiến trúc VGG-16:**\n```\nInput (224×224×3)\n↓\n[Conv 3×3, 64] × 2 → Pool\n↓\n[Conv 3×3, 128] × 2 → Pool\n↓\n[Conv 3×3, 256] × 3 → Pool\n↓\n[Conv 3×3, 512] × 3 → Pool\n↓\n[Conv 3×3, 512] × 3 → Pool\n↓\nFC 4096 → FC 4096 → FC 1000\n```\n\n**Insight:**\n- Nhiều conv 3×3 stacked = receptive field lớn hơn nhưng ít params hơn\n- 2 conv 3×3 = receptive field 5×5\n- 3 conv 3×3 = receptive field 7×7\n\n**GoogLeNet/Inception (2014 - Google):**\n- Thắng ImageNet 2014\n- 22 layers nhưng chỉ 7M parameters (ít hơn AlexNet!)\n- **Inception module:** Ý tưởng chính\n\n**Inception Module:**\n- Áp dụng **đồng thời** nhiều kích thước filter (1×1, 3×3, 5×5) và pooling\n- Concatenate outputs\n- Network tự học combination nào tốt\n\n```\nInput\n├─ 1×1 conv\n├─ 1×1 conv → 3×3 conv\n├─ 1×1 conv → 5×5 conv\n\n**Các khái niệm quan trọng:**\n- Pooling (Pooling Layer) là một thành phần hoặc loại lớp trong mạng nơ-ron tích chập (CNN) được sử dụng để giảm chiều không gian của dữ liệu đầu vào, giảm số lượng tham số và tính toán, đồng thời giúp mô hình trở nên bất biến với các dịch chuyển nhỏ. Ví dụ, \"MaxPool 2×2\" thực hiện phép lấy mẫu tối đa trên cửa sổ 2x2. Trong Inception module, pooling có thể được áp dụng đồng thời với các phép tích chập khác.\n- MaxPool (Max Pooling) là một loại Pooling Layer trong CNN, chọn giá trị lớn nhất từ mỗi vùng con (cửa sổ) của đầu vào. Ví dụ: \"MaxPool 2×2\" sẽ lấy giá trị lớn nhất trong mỗi vùng 2x2, giảm kích thước không gian của feature map.\n\n**Mối quan hệ:**\n- Kiến trúc LeNet-5 chứa các lớp Pool.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n- **KL Divergence:** $KL(q_{\\phi}(z|x) || p(z))$ - Regularization, đảm bảo latent distribution gần prior $p(z) = \\mathcal{N}(0, I)$\n- $\beta$: Hyperparameter kiểm soát trade-off (β-VAE)\n\n**Tại sao KL divergence:**\n- Force latent space có cấu trúc tốt (continuous, smooth)\n- Cho phép generate by sampling $z \\sim \\mathcal{N}(0, I)$\n- Regularization effect\n\n**Ưu điểm VAE:**\n- **Generative:** Có thể tạo dữ liệu mới\n- **Smooth latent space:** Interpolation giữa samples có ý nghĩa\n- **Probabilistic framework:** Principled, có foundation lý thuyết\n\n**Nhược điểm:**\n- Reconstructions thường blurry (do Gaussian assumption)\n- Phức tạp hơn vanilla autoencoder\n\n**Ứng dụng:**\n- Image generation\n- Data augmentation\n- Anomaly detection (outliers có reconstruction error cao)\n- Semi-supervised learning\n\n**5. Contractive Autoencoder:**\n\nThêm penalty trên **derivative của latent representation** theo input.\n\n$$L = ||x - \\hat{x}||^2 + \\lambda ||\frac{\\partial f}{\\partial x}||_F^2$$\n\n**Mục tiêu:**\n- Latent representation **robust to small changes** trong input\n- Locally \"contract\" space\n\n**Lợi ích:**\n- Learn representations insensitive to small perturbations\n- Regularization effect\n\n**6. Convolutional Autoencoder:**\n\nSử dụng convolutional và pooling layers thay vì fully connected.\n\n**Encoder:**\n```\nConv → Pool → Conv → Pool → ...\n```\n\n**Decoder:**\n```\nConvTranspose (Upsampling) → ConvTranspose → ...\n```\n\n**Ưu điểm:**\n- **Ít parameters hơn** với images\n- **Preserve spatial structure**\n- Hiệu quả hơn với image data\n\n**Ứng dụng:**\n- Image compression\n- Image denoising\n- Super-resolution\n\n**Ứng Dụng Thực Tế của Autoencoders:**\n\n**1. Dimensionality Reduction:**\n- Alternative to PCA\n- Non-linear transformations\n- Visualization trong 2D/3D (t-SNE trên latent space)\n\n**2. Anomaly Detection:**\n- Train trên normal data\n- Anomalies có reconstruction error cao\n- Applications: Fraud detection, defect detection, network intrusion\n\n**3. Denoising:**\n- Remove noise từ images, audio, signals\n- Medical imaging\n- Old photo restoration\n\n**4. Feature Learning:**\n- Pre-training cho supervised tasks\n- Transfer learning\n- Extract meaningful representations\n\n**5. Generative Modeling (VAE):**\n- Generate new faces, artwork\n- Data augmentation\n- Creative applications\n\n**6. Image Compression:**\n- Learn compression schemes tốt hơn traditional methods\n- JPEG alternative\n\n**7. Information Retrieval:**\n- Semantic hashing\n- Similar image search (search trong latent space)\n\n**So sánh các loại Autoencoders:**\n\n| Type | Goal | Output | Use Case |\n|------|------|--------|----------|\n| Vanilla | Dim reduction | Deterministic | Compression, features |\n| Sparse | Interpretability | Sparse code | Feature learning |\n| Denoising | Robustness | Denoised | Denoising, robust features |\n| VAE | Generation | Probabilistic | Generation, sampling |\n| Contractive | Stability | Robust code | Robust representations |\n\n### Mạng Đối Sinh (Generative Adversarial Networks - GANs)\n\nGANs là framework cho generative models được phát minh bởi Ian Goodfellow et al. (2014). Ý tưởng: Hai mạng nơ-ron \"cạnh tranh\" với nhau để cải thiện.\n\n**Ẩn dụ:** \n- Generator giống như **tên làm tiền giả**\n- Discriminator giống như **cảnh sát phát hiện tiền giả**\n- Qua thời gian, cả hai đều giỏi hơn → tiền giả ngày càng thật\n\n**Các Thành Phần:**\n\n\n**Các khái niệm quan trọng:**\n- Convolutional layers (lớp tích chập) là các lớp trong mạng nơ-ron tích chập (CNN) thực hiện phép tích chập trên dữ liệu đầu vào để trích xuất các đặc trưng. Dropout thường không được sử dụng trực tiếp cho các lớp tích chập, thay vào đó, các biến thể như Spatial Dropout có thể được áp dụng.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n| | L1 | L2 |\n|---|---|---|\n| Sparsity | Có (nhiều weights = 0) | Không |\n| Feature selection | Có | Không |\n| Gradient | Không trơn tại 0 | Trơn mọi nơi |\n| Sử dụng | Nhiều features, cần sparse | Default choice |\n\n**2. Dropout:**\n\nKỹ thuật cực kỳ hiệu quả được giới thiệu bởi Hinton et al. (2012).\n\n**Cơ chế:**\n- Trong mỗi iteration training, **ngẫu nhiên \"tắt\" (drop)** một số nơ-ron\n- Mỗi nơ-ron có xác suất $p$ bị drop (thường $p = 0.2 - 0.5$)\n- Nơ-ron bị drop không tham gia forward và backward pass\n- Tại test time, sử dụng tất cả nơ-ron nhưng scale output với $(1-p)$\n\n**Tại sao hiệu quả:**\n- **Ngăn co-adaptation:** Nơ-ron không thể phụ thuộc quá nhiều vào nơ-ron cụ thể khác\n- **Ensemble effect:** Mỗi iteration training một mạng con khác nhau → giống train nhiều mô hình\n- **Noise injection:** Thêm noise vào training → mô hình robust hơn\n\n**Cách sử dụng:**\n- Áp dụng cho fully connected layers (không dùng cho convolutional layers)\n- Hidden layers: dropout rate 0.5\n- Input layer: dropout rate thấp hơn (0.2) nếu dùng\n- **Không** sử dụng dropout trong test/inference\n\n**Inverted Dropout:**\n```python\n# Training\nmask = (np.random.rand(*shape) > dropout_rate) / (1 - dropout_rate)\noutput = input * mask\n\n# Testing: không cần scale\noutput = input\n```\n\n**Biến thể:**\n- **DropConnect:** Drop connections thay vì neurons\n- **Spatial Dropout:** Drop entire feature maps trong CNN\n- **Variational Dropout:** Dropout mask giống nhau qua time steps trong RNN\n\n**3. Early Stopping:**\n\nDừng training khi validation loss bắt đầu tăng.\n\n**Cách thực hiện:**\n1. Chia data thành train/validation/test\n2. Theo dõi validation loss sau mỗi epoch\n3. Lưu model có validation loss tốt nhất\n4. Dừng nếu validation loss không giảm sau $n$ epochs (patience)\n\n**Ưu điểm:**\n- Đơn giản, hiệu quả\n- Tự động tìm số epoch tối ưu\n- Không cần thêm hyperparameter phức tạp\n\n**Lưu ý:**\n- Cần validation set riêng (không dùng test set!)\n- Patience thường 10-20 epochs\n- Có thể kết hợp với learning rate scheduling\n\n**4. Data Augmentation:**\n\nTăng cường dữ liệu training bằng cách tạo biến thể từ dữ liệu gốc.\n\n**Cho ảnh (Computer Vision):**\n- **Geometric transformations:**\n  - Rotation (xoay): ±15-30 độ\n  - Flip (lật): Horizontal, vertical\n  - Crop (cắt): Random crop, center crop\n  - Zoom: Scale in/out\n  - Translation (dịch chuyển)\n  - Shearing (nghiêng)\n  \n- **Color transformations:**\n  - Brightness adjustment (độ sáng)\n  - Contrast adjustment (độ tương phản)\n  - Saturation adjustment (độ bão hòa)\n  - Hue adjustment (sắc màu)\n  - RGB channel shifts\n  \n- **Noise injection:**\n  - Gaussian noise\n  - Salt and pepper noise\n  - Blur (làm mờ)\n  \n- **Advanced techniques:**\n  - **Mixup:** Trộn hai ảnh: $x = \\lambda x_1 + (1-\\lambda)x_2$\n\n**Các khái niệm quan trọng:**\n- Convolutional layers (lớp tích chập) là các lớp trong mạng nơ-ron tích chập (CNN) thực hiện phép tích chập trên dữ liệu đầu vào để trích xuất các đặc trưng. Dropout thường không được sử dụng trực tiếp cho các lớp tích chập, thay vào đó, các biến thể như Spatial Dropout có thể được áp dụng.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n  - Lớp giữa: Textures (kết cấu), patterns (mẫu)\n  - Lớp sâu: Parts of objects (phần của vật thể), objects\n\n**Depth (Number of Filters):**\n- Số lượng filters trong một lớp\n- Tạo ra feature maps tương ứng\n- Thường tăng dần qua các lớp: 32 → 64 → 128 → 256\n\n**Stride:**\n- Bước nhảy khi trượt filter\n- Stride = 1: Di chuyển từng pixel\n- Stride = 2: Di chuyển 2 pixels, giảm kích thước output\n- Stride lớn: Giảm spatial dimensions, tăng receptive field\n\n**Padding:**\n- Thêm pixels (thường 0) xung quanh input\n- **VALID (No padding):** Output size giảm\n- **SAME (Zero padding):** Output size = Input size (khi stride=1)\n\n**Tính Output Size:**\n$$Output\\_size = \frac{Input\\_size - Kernel\\_size + 2 \times Padding}{Stride} + 1$$\n\n**Ví dụ:**\n- Input: 32×32×3 (ảnh RGB)\n- 64 filters 3×3, stride=1, padding=1\n- Output: 32×32×64\n\n**Số parameters:**\n- Mỗi filter 3×3×3: 27 weights + 1 bias = 28 params\n- 64 filters: 28 × 64 = 1,792 params\n- **Ít hơn nhiều so với fully connected!**\n\n**Ưu điểm của Convolution:**\n- **Parameter sharing:** Giảm số lượng parameters cần học\n- **Sparse connectivity:** Mỗi output chỉ phụ thuộc vào vùng local\n- **Equivariance to translation:** Nếu input dịch chuyển, output cũng dịch chuyển tương ứng\n\n**2. Lớp Pooling:**\n\nGiảm spatial dimensions, giữ lại thông tin quan trọng.\n\n**Max Pooling:**\n- Lấy giá trị maximum trong mỗi region (thường 2×2)\n- Phổ biến nhất\n- Giữ lại features nổi bật nhất\n- Invariant to small translations\n\n**Công thức:**\n$$Output[i,j] = \\max_{m,n} Input[2i+m, 2j+n]$$\n\n**Ví dụ Max Pooling 2×2:**\n```\nInput 4×4:        Output 2×2:\n1  3  2  4        3  4\n5  6  7  8   →    6  8\n2  1  4  3\n0  9  5  2\n```\n\n**Average Pooling:**\n- Lấy trung bình trong mỗi region\n- Ít phổ biến hơn Max Pooling\n- Smoother, giữ được nhiều thông tin hơn\n\n**Global Average Pooling:**\n- Average trên toàn bộ feature map → 1 số\n- Thay thế fully connected layers\n- Giảm overfitting, ít parameters\n\n**Hyperparameters:**\n- Pool size: Thường 2×2 hoặc 3×3\n- Stride: Thường = pool size\n- Padding: Hiếm khi dùng\n\n**Tác dụng:**\n- **Downsampling:** Giảm spatial dimensions → giảm computation\n- **Increase receptive field:** Mỗi nơ-ron \"nhìn\" được vùng rộng hơn\n- **Translation invariance:** Ít nhạy cảm với vị trí chính xác\n- **Regularization effect:** Giảm overfitting\n\n**3. Lớp Fully Connected (Dense):**\n\nLớp truyền thống, mỗi nơ-ron kết nối với tất cả nơ-ron lớp trước.\n\n**Vị trí:**\n- Thường ở cuối mạng CNN\n- Sau khi flatten feature maps thành vector\n\n**Tác dụng:**\n\n**Các khái niệm quan trọng:**\n- Pooling Layer là một lớp trong mạng nơ-ron tích chập (CNN) được sử dụng để giảm kích thước không gian (chiều rộng và chiều cao) của feature maps bằng cách tổng hợp thông tin từ các vùng nhỏ. Mục đích chính của nó là giảm computation, tăng receptive field, tạo translation invariance, và có tác dụng regularization. Trong Convolutional Autoencoder, pooling layers cũng giúp giảm số lượng tham số và làm cho mô hình ít nhạy cảm hơn với các biến đổi nhỏ trong input, đồng thời giữ lại thông tin quan trọng.\n- Average Pooling là một kỹ thuật pooling trong đó giá trị trung bình được lấy từ mỗi vùng của feature map, tạo ra đầu ra mượt mà hơn và giữ được nhiều thông tin hơn so với Max Pooling. Global Average Pooling (GAP) là một kỹ thuật pooling đặc biệt, thường được sử dụng trong các mạng nơ-ron tích chập (CNN) để giảm chiều dữ liệu trước lớp phân loại cuối cùng. GAP tính giá trị trung bình trên toàn bộ mỗi feature map, tạo ra một số duy nhất cho mỗi map. Kỹ thuật này giúp giảm số lượng tham số và ngăn chặn overfitting, thường được dùng để thay thế các lớp Fully Connected lớn ở cuối mạng.\n\n**Mối quan hệ:**\n- Pooling Layer ngăn chặn Overfitting bằng cách giảm số lượng tham số và tạo ra một hiệu ứng regularization.\n- Pooling Layer làm tăng Receptive field của các nơ-ron trong các lớp sâu hơn.\n- Pooling Layer thực hiện Downsampling để giảm kích thước không gian của feature maps.\n- Pooling Layer cung cấp Regularization effect, giúp giảm overfitting.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**Khi nào dùng:**\n- RNNs, LSTMs, GRUs\n- Transformers (BERT, GPT)\n- Batch size nhỏ\n- Online learning\n\n**7. Weight Decay:**\n\nThêm penalty cho magnitude của weights trong update rule.\n\n**Trong optimizer:**\n$$w := w - \\alpha(\frac{\\partial L}{\\partial w} + \\lambda w)$$\n\n**Khác với L2 Regularization:**\n- L2 reg: Thêm vào loss function\n- Weight decay: Thêm trực tiếp vào update rule\n- Với SGD: Tương đương nhau\n- Với Adam/AdamW: **Khác nhau!** AdamW implement weight decay đúng cách\n\n**8. Stochastic Depth:**\n\nNgẫu nhiên skip một số layers trong training (dùng cho ResNets).\n\n**9. Label Smoothing:**\n\nThay vì hard labels (0 hoặc 1), dùng soft labels.\n\n**Ví dụ:** \n- Hard: [0, 1, 0, 0]\n- Soft (ε=0.1): [0.025, 0.925, 0.025, 0.025]\n\n**Công thức:**\n$$y_{smooth} = (1-\\epsilon)y + \frac{\\epsilon}{K}$$\n\n**Ưu điểm:**\n- Ngăn mô hình quá tự tin (overconfident)\n- Improve generalization\n- Thường dùng trong image classification\n\n**10. Gradient Clipping:**\n\nGiới hạn magnitude của gradients, đặc biệt quan trọng cho RNNs.\n\n**Clip by value:**\n$$g = \\max(\\min(g, threshold), -threshold)$$\n\n**Clip by norm:**\n$$g = \frac{threshold \\cdot g}{||g||} \text{ if } ||g|| > threshold$$\n\n**Sử dụng:**\n- RNNs, LSTMs (giải quyết exploding gradients)\n- Transformers với sequence dài\n- Threshold thường: 1.0 hoặc 5.0\n\n### Mạng Nơ-ron Tích Chập (Convolutional Neural Networks - CNN)\n\nCNN là kiến trúc mạng nơ-ron chuyên biệt cho xử lý dữ liệu dạng lưới (grid-like), đặc biệt là ảnh. Được lấy cảm hứng từ vỏ não thị giác (visual cortex) của động vật.\n\n**Đặc điểm chính:**\n- **Locally connected:** Mỗi nơ-ron chỉ kết nối với vùng local của input\n- **Parameter sharing:** Cùng một filter được áp dụng trên toàn bộ input\n- **Translation invariance:** Phát hiện đặc trưng ở bất kỳ vị trí nào\n- **Hierarchical feature learning:** Lớp đầu học low-level features, lớp sau học high-level features\n\n**Các Thành Phần Chính:**\n\n**1. Lớp Tích Chập (Convolutional Layer):**\n\nÁp dụng filters (kernels) trượt trên input để trích xuất features.\n\n**Công thức:**\n$$Output[i,j] = \\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n] \times Kernel[m,n] + bias$$\n\nHoặc dạng tổng quát với nhiều channels:\n$$Output[i,j,d] = \\sum_{c=0}^{C-1}\\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n,c] \times Kernel[m,n,c,d] + bias[d]$$\n\n**Khái niệm quan trọng:**\n\n**Filter/Kernel:**\n- Ma trận nhỏ (thường 3×3, 5×5, 7×7)\n- Chứa weights học được\n- Mỗi filter phát hiện một đặc trưng cụ thể:\n  - Lớp đầu: Edges (cạnh), corners (góc), colors\n\n**Các khái niệm quan trọng:**\n- Lớp Tích Chập (Convolutional Layer, Conv) là thành phần cốt lõi của mạng nơ-ron tích chập (CNN), chịu trách nhiệm trích xuất các đặc trưng từ dữ liệu đầu vào. Nó thực hiện phép tích chập bằng cách áp dụng các bộ lọc (filters hoặc kernels) trượt trên input. Mỗi filter học cách phát hiện một đặc trưng cụ thể như cạnh, góc, hoặc màu sắc. Công thức tính toán đầu ra của một lớp tích chập cho một kênh là $Output[i,j] = \\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n] \times Kernel[m,n] + bias$, và dạng tổng quát với nhiều kênh là $Output[i,j,d] = \\sum_{c=0}^{C-1}\\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n,c] \times Kernel[m,n,c,d] + bias[d]$. Ví dụ, \"Conv 64 filters 3×3\" chỉ ra rằng lớp tích chập này sử dụng 64 bộ lọc có kích thước 3x3.\n\n**Mối quan hệ:**\n- Convolutional Neural Networks chứa một hoặc nhiều Convolutional Layer làm thành phần chính để trích xuất đặc trưng."
        },
        "output_without_context": {
            "question": "Sự khác biệt chính giữa gộp tối đa (Max pooling) và gộp trung bình (Average pooling) trong các lớp gộp của mạng nơ-ron tích chập (CNN) là gì?",
            "answer": "Gộp tối đa lấy giá trị lớn nhất trong vùng gộp, trong khi gộp trung bình tính giá trị trung bình của vùng gộp."
        }
    },
    {
        "topic_description": "Chủ đề này bàn về các vấn đề liên quan đến gradient trong việc huấn luyện mạng nơ-ron, bao gồm gradient biến mất và gradient bùng nổ. Học sinh sẽ cần hiểu được nguyên nhân của những vấn đề này và cách chúng ảnh hưởng đến khả năng học tập của mô hình, cũng như cách khắc phục chúng.",
        "output_with_context": {
            "question": "Vấn đề gradient biến mất thường xảy ra trong mạng nơ-ron sâu khi nào?",
            "answer": "Khi gradient của hàm loss trở nên cực kỳ nhỏ trong quá trình lan truyền ngược qua nhiều lớp.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n└─ 3×3 pool → 1×1 conv\n↓ (Concatenate)\nOutput\n```\n\n**1×1 Convolutions:**\n- Dimensionality reduction\n- Thêm non-linearity\n- \"Network in Network\"\n\n**ResNet (2015 - Microsoft Research):**\n- **Cách mạng trong deep learning**\n- Rất sâu: ResNet-50, ResNet-101, ResNet-152 (thậm chí 1000+ layers)\n- Thắng ImageNet 2015: Top-5 error 3.6% (better than human!)\n\n**Vấn đề với mạng rất sâu:**\n- **Degradation problem:** Mạng sâu hơn nhưng accuracy giảm (không phải overfitting!)\n- Vanishing gradients\n\n**Giải pháp: Skip Connections (Residual Connections):**\n$$H(x) = F(x) + x$$\n\n**Residual Block:**\n```\nx ─────────────────────→ +\n  ↓                      ↑\n  Conv 3×3, ReLU         │\n  ↓                      │\n  Conv 3×3               │\n  ↓─────────────────────┘\n  (Add)\n  ↓\n  ReLU\n```\n\n**Tại sao hiệu quả:**\n- **Identity mapping:** Nếu thêm layers không giúp ích, học $F(x) = 0$ → $H(x) = x$\n- **Gradient flow:** Gradients có thể flow trực tiếp qua shortcut\n- Dễ optimize hơn\n\n**Bottleneck Design (ResNet-50+):**\n```\n1×1 conv (reduce dim)\n↓\n3×3 conv\n↓\n1×1 conv (restore dim)\n```\nGiảm computational cost\n\n**DenseNet (2017):**\n- Kết nối mọi layer với tất cả layers sau nó\n- Reuse features hiệu quả hơn ResNet\n\n**EfficientNet (2019 - Google):**\n- **Compound scaling:** Scale đồng thời depth, width, và resolution\n- Balancing method\n- State-of-the-art accuracy với ít parameters và FLOPS hơn\n\n**MobileNet:**\n- Thiết kế cho mobile devices và embedded systems\n- **Depthwise Separable Convolutions:**\n  - Depthwise conv: Áp dụng filter riêng cho mỗi channel\n  - Pointwise conv: 1×1 conv để combine channels\n  - Giảm parameters và computation 8-9 lần\n\n**SqueezeNet:**\n- AlexNet-level accuracy với 50× ít parameters hơn\n- Fire modules: Squeeze (1×1 convs) + Expand (1×1 và 3×3 convs)\n\n**Applications of CNNs:**\n\n**Image Classification:**\n- Phân loại ảnh vào các categories\n- ImageNet, CIFAR, etc.\n\n**Object Detection:**\n- Phát hiện và localize objects trong ảnh\n- **R-CNN family:** R-CNN, Fast R-CNN, Faster R-CNN\n- **YOLO (You Only Look Once):** Real-time detection\n- **SSD (Single Shot Detector)**\n\n**Semantic Segmentation:**\n- Phân loại mỗi pixel\n- **FCN (Fully Convolutional Networks)**\n- **U-Net:** Architecture for biomedical images\n- **DeepLab:** Atrous convolution\n\n**Instance Segmentation:**\n- Segment mỗi object instance riêng biệt\n- **Mask R-CNN**\n\n**Face Recognition:**\n- FaceNet, DeepFace\n\n**Style Transfer:**\n- Chuyển style từ ảnh này sang ảnh khác\n\n**Medical Image Analysis:**\n- X-ray, MRI, CT scan analysis\n- Disease detection\n\n### Mạng Nơ-ron Hồi Tiếp (Recurrent Neural Networks - RNN)\n\nRNN được thiết kế đặc biệt cho dữ liệu tuần tự (sequential data) như văn bản, chuỗi thời gian, âm thanh, video.\n\n**Đặc điểm:**\n- **Kết nối recurrent:** Output ở bước $t$ phụ thuộc vào input hiện tại và các bước trước đó\n- **Chia sẻ parameters** qua các time steps\n\n**Các khái niệm quan trọng:**\n- Vanishing gradient là một vấn đề phổ biến trong việc huấn luyện mạng nơ-ron sâu, đặc biệt là Mạng Nơ-ron Hồi quy (RNN). Vấn đề này xảy ra khi gradient của hàm loss trở nên cực kỳ nhỏ (giảm theo cấp số mũ) trong quá trình lan truyền ngược qua nhiều lớp hoặc nhiều bước thời gian. Hậu quả là việc cập nhật trọng số ở các lớp đầu tiên hoặc các bước thời gian xa trong quá khứ trở nên không đáng kể, khiến các lớp này học rất chậm hoặc ngừng học hoàn toàn. Điều này làm cho mô hình khó học được các phụ thuộc dài hạn trong dữ liệu, đặc biệt nghiêm trọng trong RNN khi thông tin từ các bước thời gian xa bị \"quên\".\n\nNguyên nhân chính của vanishing gradient thường bao gồm:\n1.  **Hàm kích hoạt có đạo hàm nhỏ:** Các hàm kích hoạt như sigmoid và tanh có đạo hàm luôn nhỏ hơn 1 (đặc biệt là ở các vùng bão hòa), khi nhân nhiều lần trong quá trình backpropagation sẽ làm gradient giảm dần về 0.\n2.  **Nhân ma trận trọng số:** Trong RNN, việc nhân nhiều lần với ma trận trọng số $W_{hh}$ cũng góp phần làm giảm gradient.\n\nCác giải pháp và kiến trúc được phát triển để giảm thiểu hoặc khắc phục vấn đề vanishing gradient bao gồm:\n*   **Thay đổi hàm kích hoạt:** Sử dụng các hàm kích hoạt có đạo hàm lớn hơn và không bị bão hòa như ReLU (Rectified Linear Unit) và các biến thể của nó.\n*   **Batch Normalization:** Giúp chuẩn hóa đầu vào của mỗi lớp, giảm sự thay đổi phân phối của các kích hoạt và cho phép sử dụng các hàm kích hoạt như sigmoid và tanh hiệu quả hơn.\n*   **Residual Connections:** Cho phép gradient chảy trực tiếp qua các lớp, giúp giảm thiểu sự suy giảm của gradient.\n*   **Kiến trúc mạng chuyên biệt:**\n    *   **LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Unit):** Được thiết kế đặc biệt để giải quyết vanishing gradient trong RNN bằng cách sử dụng các cổng (gate) để kiểm soát luồng thông tin, giúp mạng học và ghi nhớ các phụ thuộc dài hạn.\n    *   **Transformer:** Kiến trúc này được thiết kế để không bị vanishing gradient qua nhiều bước nhờ cơ chế attention, cho phép mô hình xử lý thông tin từ các vị trí xa mà không cần lan truyền qua nhiều bước thời gian tuần tự.\n\nNhờ các giải pháp và kiến trúc hiện đại, vấn đề vanishing gradient đã được giảm thiểu đáng kể trong Deep Learning.\n\n**Mối quan hệ:**\n- ReLU giải quyết vấn đề Vanishing gradients bằng cách cung cấp gradient ổn định hơn cho các giá trị dương, giúp các lớp sâu học hiệu quả hơn.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\nBackpropagation có thể được hiểu thông qua đồ thị tính toán (computational graph), trong đó:\n- Mỗi node là một operation\n- Edges mang giá trị và gradients\n- Forward pass tính giá trị, backward pass tính gradients\n\n**Vấn đề Vanishing/Exploding Gradients:**\n- **Vanishing:** Gradient giảm dần khi lan truyền về các lớp đầu → các lớp đầu học chậm\n  - Nguyên nhân: Hàm kích hoạt có đạo hàm nhỏ (sigmoid, tanh)\n  - Giải pháp: ReLU, batch normalization, residual connections\n- **Exploding:** Gradient tăng dần → weights cập nhật quá mạnh, không ổn định\n  - Giải pháp: Gradient clipping, proper weight initialization\n\n**Lưu ý về hiệu suất:**\n- Độ phức tạp tính toán của backpropagation tương đương forward pass\n- Matrix operations có thể vectorize → tính toán hiệu quả trên GPU\n- Cần lưu trữ activations từ forward pass → tốn memory\n\n### Thuật Toán Tối Ưu (Optimization Algorithms)\n\nCác thuật toán tối ưu quyết định cách cập nhật weights để minimize loss function.\n\n**1. Gradient Descent (Hạ Gradient):**\n$$w := w - \\alpha\frac{\\partial L}{\\partial w}$$\n\n**Đặc điểm:**\n- $\\alpha$ (learning rate): Hyperparameter quan trọng nhất\n- Cập nhật dựa trên toàn bộ training set (batch gradient descent)\n\n**Ưu điểm:**\n- Đơn giản, dễ hiểu\n- Hội tụ ổn định với learning rate phù hợp\n- Đảm bảo tìm được local minimum với hàm convex\n\n**Nhược điểm:**\n- Chậm với dữ liệu lớn (phải xử lý toàn bộ dataset mỗi iteration)\n- Có thể bị kẹt ở local minima hoặc saddle points\n- Learning rate cố định không phù hợp mọi giai đoạn training\n\n**2. Stochastic Gradient Descent (SGD):**\n$$w := w - \\alpha\frac{\\partial L_i}{\\partial w}$$\n\n**Đặc điểm:**\n- Cập nhật sau **mỗi** mẫu dữ liệu (sample)\n- Gradient ước lượng từ 1 sample → noisy nhưng nhanh\n\n**Ưu điểm:**\n- Rất nhanh, có thể train trên dữ liệu lớn\n- Noise giúp thoát khỏi local minima\n- Có thể train online (dữ liệu đến liên tục)\n\n**Nhược điểm:**\n- Quá trình hội tụ không ổn định, dao động mạnh\n- Có thể không hội tụ chính xác đến minimum\n- Khó song song hóa (sequential updates)\n\n**3. Mini-batch Gradient Descent:**\n$$w := w - \\alpha\frac{1}{m}\\sum_{i=1}^{m}\frac{\\partial L_i}{\\partial w}$$\n\n**Đặc điểm:**\n- Cập nhật sau một **batch nhỏ** (thường 32, 64, 128, 256)\n- Kết hợp ưu điểm của batch GD và SGD\n- **Là phương pháp được sử dụng phổ biến nhất trong thực tế**\n\n**Ưu điểm:**\n- Tốc độ nhanh, ổn định hơn SGD\n- Có thể vectorize, tận dụng GPU hiệu quả\n- Gradient ổn định hơn SGD nhưng vẫn có noise tốt\n- Batch size là hyperparameter điều chỉnh được\n\n**Lựa chọn batch size:**\n\n**Các khái niệm quan trọng:**\n- Vanishing gradient là một vấn đề phổ biến trong việc huấn luyện mạng nơ-ron sâu, đặc biệt là Mạng Nơ-ron Hồi quy (RNN). Vấn đề này xảy ra khi gradient của hàm loss trở nên cực kỳ nhỏ (giảm theo cấp số mũ) trong quá trình lan truyền ngược qua nhiều lớp hoặc nhiều bước thời gian. Hậu quả là việc cập nhật trọng số ở các lớp đầu tiên hoặc các bước thời gian xa trong quá khứ trở nên không đáng kể, khiến các lớp này học rất chậm hoặc ngừng học hoàn toàn. Điều này làm cho mô hình khó học được các phụ thuộc dài hạn trong dữ liệu, đặc biệt nghiêm trọng trong RNN khi thông tin từ các bước thời gian xa bị \"quên\".\n\nNguyên nhân chính của vanishing gradient thường bao gồm:\n1.  **Hàm kích hoạt có đạo hàm nhỏ:** Các hàm kích hoạt như sigmoid và tanh có đạo hàm luôn nhỏ hơn 1 (đặc biệt là ở các vùng bão hòa), khi nhân nhiều lần trong quá trình backpropagation sẽ làm gradient giảm dần về 0.\n2.  **Nhân ma trận trọng số:** Trong RNN, việc nhân nhiều lần với ma trận trọng số $W_{hh}$ cũng góp phần làm giảm gradient.\n\nCác giải pháp và kiến trúc được phát triển để giảm thiểu hoặc khắc phục vấn đề vanishing gradient bao gồm:\n*   **Thay đổi hàm kích hoạt:** Sử dụng các hàm kích hoạt có đạo hàm lớn hơn và không bị bão hòa như ReLU (Rectified Linear Unit) và các biến thể của nó.\n*   **Batch Normalization:** Giúp chuẩn hóa đầu vào của mỗi lớp, giảm sự thay đổi phân phối của các kích hoạt và cho phép sử dụng các hàm kích hoạt như sigmoid và tanh hiệu quả hơn.\n*   **Residual Connections:** Cho phép gradient chảy trực tiếp qua các lớp, giúp giảm thiểu sự suy giảm của gradient.\n*   **Kiến trúc mạng chuyên biệt:**\n    *   **LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Unit):** Được thiết kế đặc biệt để giải quyết vanishing gradient trong RNN bằng cách sử dụng các cổng (gate) để kiểm soát luồng thông tin, giúp mạng học và ghi nhớ các phụ thuộc dài hạn.\n    *   **Transformer:** Kiến trúc này được thiết kế để không bị vanishing gradient qua nhiều bước nhờ cơ chế attention, cho phép mô hình xử lý thông tin từ các vị trí xa mà không cần lan truyền qua nhiều bước thời gian tuần tự.\n\nNhờ các giải pháp và kiến trúc hiện đại, vấn đề vanishing gradient đã được giảm thiểu đáng kể trong Deep Learning.\n- Vanishing Gradients là một vấn đề trong huấn luyện GANs, xảy ra khi Discriminator trở nên quá hoàn hảo (D(G(z)) ≈ 0), khiến gradient cho Generator gần bằng 0. Điều này ngăn cản Generator học hỏi và cải thiện khả năng tạo mẫu.\n\n**Mối quan hệ:**\n- Vanishing Gradients có nguyên nhân từ việc sử dụng hàm kích hoạt có đạo hàm nhỏ như sigmoid, khiến gradient giảm dần khi lan truyền về các lớp đầu.\n- Vanishing Gradients có nguyên nhân từ việc sử dụng hàm kích hoạt có đạo hàm nhỏ như tanh, khiến gradient giảm dần khi lan truyền về các lớp đầu.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n- Ví dụ: Generate faces nhưng chỉ 5-10 khuôn mặt khác nhau, không đa dạng\n- **Nguyên nhân:** G tìm được cách \"lừa\" D với một số samples → bỏ qua diversity\n\n**Giải pháp:**\n- Minibatch discrimination\n- Feature matching\n- Unrolled GAN\n\n**2. Training Instability:**\n- Loss oscillates, không converge\n- D quá mạnh → gradient vanishing cho G\n- G quá mạnh → D không học được gì\n- **Cân bằng khó đạt được**\n\n**Giải pháp:**\n- Careful hyperparameter tuning\n- Learning rate schedules\n- Different architectures (DCGAN guidelines)\n\n**3. Vanishing Gradients:**\n- Nếu D perfect (D(G(z)) ≈ 0), gradient cho G ≈ 0\n- G không learn được\n\n**Giải pháp:**\n- Non-saturating loss: Maximize $\\log D(G(z))$ thay vì minimize $\\log(1-D(G(z)))$\n- Wasserstein loss (WGAN)\n\n**4. Difficulty in Convergence:**\n- Không rõ khi nào stop training\n- Loss không phản ánh chất lượng samples\n\n**Giải pháp:**\n- Inception Score (IS)\n- Fréchet Inception Distance (FID)\n- Manual inspection\n\n**Các Biến Thể GAN Quan Trọng:**\n\n**1. DCGAN (Deep Convolutional GAN, 2015):**\n\n**Architectural guidelines** giúp stable training:\n\n**Generator:**\n- Replace pooling với **strided convolutions** (transpose conv)\n- **Batch Normalization** trong cả G và D (không dùng ở output layer của G và input layer của D)\n- Remove fully connected hidden layers\n- **ReLU** activation trong G, **Tanh** ở output\n  \n**Discriminator:**\n- **Strided convolutions** thay vì pooling\n- **Batch Normalization**\n- **LeakyReLU** activation\n\n**Impact:** Chuẩn hóa architecture, foundation cho nhiều GANs sau này\n\n**2. Conditional GAN (cGAN, 2014):**\n\n**Ý tưởng:** Condition generation trên additional information (labels, text, etc.)\n\n**Modification:**\n- Generator: $G(z, y)$ - Input noise + condition\n- Discriminator: $D(x, y)$ - Input data + condition\n\n**Loss:**\n$$\\min_G \\max_D V(D,G) = \\mathbb{E}_{x,y}[\\log D(x,y)] + \\mathbb{E}_{z,y}[\\log(1-D(G(z,y),y))]$$\n\n**Ứng dụng:**\n- Text-to-image: \"a red car\" → generate ảnh xe đỏ\n- Class-conditional generation: Chọn class → generate sample thuộc class đó\n- Image-to-image translation với conditions\n\n**3. Wasserstein GAN (WGAN, 2017):**\n\n**Vấn đề với original GAN:** JS divergence không cung cấp gradient tốt khi distributions không overlap.\n\n**Giải pháp:** Sử dụng **Wasserstein distance** (Earth Mover's Distance)\n\n**New Objective:**\n$$\\min_G \\max_{D \\in \\mathcal{D}} \\mathbb{E}_{x \\sim p_{data}}[D(x)] - \\mathbb{E}_{z \\sim p_z}[D(G(z))]$$\n\n**Khác biệt:**\n- D không output probability nữa, mà output **score** (không bound)\n- D gọi là \"critic\" thay vì discriminator\n- Remove sigmoid ở output của D\n\n**1-Lipschitz Constraint:**\n- D phải thỏa mãn Lipschitz constraint\n- **Weight clipping** (WGAN): Clip weights trong [-c, c]\n- **Gradient penalty** (WGAN-GP, better): Penalize gradient norm khác 1\n\n**Ưu điểm:**\n- **Stable training:** Ít mode collapse, ít training instability\n- **Meaningful loss:** Loss correlate với sample quality → biết khi nào stop\n\n**Các khái niệm quan trọng:**\n- Vanishing gradient là một vấn đề phổ biến trong việc huấn luyện mạng nơ-ron sâu, đặc biệt là Mạng Nơ-ron Hồi quy (RNN). Vấn đề này xảy ra khi gradient của hàm loss trở nên cực kỳ nhỏ (giảm theo cấp số mũ) trong quá trình lan truyền ngược qua nhiều lớp hoặc nhiều bước thời gian. Hậu quả là việc cập nhật trọng số ở các lớp đầu tiên hoặc các bước thời gian xa trong quá khứ trở nên không đáng kể, khiến các lớp này học rất chậm hoặc ngừng học hoàn toàn. Điều này làm cho mô hình khó học được các phụ thuộc dài hạn trong dữ liệu, đặc biệt nghiêm trọng trong RNN khi thông tin từ các bước thời gian xa bị \"quên\".\n\nNguyên nhân chính của vanishing gradient thường bao gồm:\n1.  **Hàm kích hoạt có đạo hàm nhỏ:** Các hàm kích hoạt như sigmoid và tanh có đạo hàm luôn nhỏ hơn 1 (đặc biệt là ở các vùng bão hòa), khi nhân nhiều lần trong quá trình backpropagation sẽ làm gradient giảm dần về 0.\n2.  **Nhân ma trận trọng số:** Trong RNN, việc nhân nhiều lần với ma trận trọng số $W_{hh}$ cũng góp phần làm giảm gradient.\n\nCác giải pháp và kiến trúc được phát triển để giảm thiểu hoặc khắc phục vấn đề vanishing gradient bao gồm:\n*   **Thay đổi hàm kích hoạt:** Sử dụng các hàm kích hoạt có đạo hàm lớn hơn và không bị bão hòa như ReLU (Rectified Linear Unit) và các biến thể của nó.\n*   **Batch Normalization:** Giúp chuẩn hóa đầu vào của mỗi lớp, giảm sự thay đổi phân phối của các kích hoạt và cho phép sử dụng các hàm kích hoạt như sigmoid và tanh hiệu quả hơn.\n*   **Residual Connections:** Cho phép gradient chảy trực tiếp qua các lớp, giúp giảm thiểu sự suy giảm của gradient.\n*   **Kiến trúc mạng chuyên biệt:**\n    *   **LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Unit):** Được thiết kế đặc biệt để giải quyết vanishing gradient trong RNN bằng cách sử dụng các cổng (gate) để kiểm soát luồng thông tin, giúp mạng học và ghi nhớ các phụ thuộc dài hạn.\n    *   **Transformer:** Kiến trúc này được thiết kế để không bị vanishing gradient qua nhiều bước nhờ cơ chế attention, cho phép mô hình xử lý thông tin từ các vị trí xa mà không cần lan truyền qua nhiều bước thời gian tuần tự.\n\nNhờ các giải pháp và kiến trúc hiện đại, vấn đề vanishing gradient đã được giảm thiểu đáng kể trong Deep Learning.\n- Vanishing Gradients là một vấn đề trong huấn luyện GANs, xảy ra khi Discriminator trở nên quá hoàn hảo (D(G(z)) ≈ 0), khiến gradient cho Generator gần bằng 0. Điều này ngăn cản Generator học hỏi và cải thiện khả năng tạo mẫu.\n\n**Mối quan hệ:**\n- Vanishing Gradients có nguyên nhân từ việc sử dụng hàm kích hoạt có đạo hàm nhỏ như sigmoid, khiến gradient giảm dần khi lan truyền về các lớp đầu.\n- Vanishing Gradients có nguyên nhân từ việc sử dụng hàm kích hoạt có đạo hàm nhỏ như tanh, khiến gradient giảm dần khi lan truyền về các lớp đầu.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**Khi nào dùng:**\n- RNNs, LSTMs, GRUs\n- Transformers (BERT, GPT)\n- Batch size nhỏ\n- Online learning\n\n**7. Weight Decay:**\n\nThêm penalty cho magnitude của weights trong update rule.\n\n**Trong optimizer:**\n$$w := w - \\alpha(\frac{\\partial L}{\\partial w} + \\lambda w)$$\n\n**Khác với L2 Regularization:**\n- L2 reg: Thêm vào loss function\n- Weight decay: Thêm trực tiếp vào update rule\n- Với SGD: Tương đương nhau\n- Với Adam/AdamW: **Khác nhau!** AdamW implement weight decay đúng cách\n\n**8. Stochastic Depth:**\n\nNgẫu nhiên skip một số layers trong training (dùng cho ResNets).\n\n**9. Label Smoothing:**\n\nThay vì hard labels (0 hoặc 1), dùng soft labels.\n\n**Ví dụ:** \n- Hard: [0, 1, 0, 0]\n- Soft (ε=0.1): [0.025, 0.925, 0.025, 0.025]\n\n**Công thức:**\n$$y_{smooth} = (1-\\epsilon)y + \frac{\\epsilon}{K}$$\n\n**Ưu điểm:**\n- Ngăn mô hình quá tự tin (overconfident)\n- Improve generalization\n- Thường dùng trong image classification\n\n**10. Gradient Clipping:**\n\nGiới hạn magnitude của gradients, đặc biệt quan trọng cho RNNs.\n\n**Clip by value:**\n$$g = \\max(\\min(g, threshold), -threshold)$$\n\n**Clip by norm:**\n$$g = \frac{threshold \\cdot g}{||g||} \text{ if } ||g|| > threshold$$\n\n**Sử dụng:**\n- RNNs, LSTMs (giải quyết exploding gradients)\n- Transformers với sequence dài\n- Threshold thường: 1.0 hoặc 5.0\n\n### Mạng Nơ-ron Tích Chập (Convolutional Neural Networks - CNN)\n\nCNN là kiến trúc mạng nơ-ron chuyên biệt cho xử lý dữ liệu dạng lưới (grid-like), đặc biệt là ảnh. Được lấy cảm hứng từ vỏ não thị giác (visual cortex) của động vật.\n\n**Đặc điểm chính:**\n- **Locally connected:** Mỗi nơ-ron chỉ kết nối với vùng local của input\n- **Parameter sharing:** Cùng một filter được áp dụng trên toàn bộ input\n- **Translation invariance:** Phát hiện đặc trưng ở bất kỳ vị trí nào\n- **Hierarchical feature learning:** Lớp đầu học low-level features, lớp sau học high-level features\n\n**Các Thành Phần Chính:**\n\n**1. Lớp Tích Chập (Convolutional Layer):**\n\nÁp dụng filters (kernels) trượt trên input để trích xuất features.\n\n**Công thức:**\n$$Output[i,j] = \\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n] \times Kernel[m,n] + bias$$\n\nHoặc dạng tổng quát với nhiều channels:\n$$Output[i,j,d] = \\sum_{c=0}^{C-1}\\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n,c] \times Kernel[m,n,c,d] + bias[d]$$\n\n**Khái niệm quan trọng:**\n\n**Filter/Kernel:**\n- Ma trận nhỏ (thường 3×3, 5×5, 7×7)\n- Chứa weights học được\n- Mỗi filter phát hiện một đặc trưng cụ thể:\n  - Lớp đầu: Edges (cạnh), corners (góc), colors\n\n**Các khái niệm quan trọng:**\n- Exploding Gradients là một vấn đề trong quá trình huấn luyện mạng nơ-ron sâu, đặc biệt là trong Mạng Nơ-ron Hồi quy (RNNs), xảy ra khi các gradient tăng theo cấp số mũ hoặc trở nên cực kỳ lớn trong quá trình lan truyền ngược (backpropagation). Điều này dẫn đến việc cập nhật trọng số (weights) hoặc tham số quá lớn, gây ra sự không ổn định nghiêm trọng trong quá trình huấn luyện, khiến mô hình không thể học, không hội tụ hoặc phân kỳ. Các giải pháp phổ biến để khắc phục vấn đề này bao gồm Gradient Clipping (cắt bớt gradient), khởi tạo trọng số phù hợp (proper weight initialization) và giảm tốc độ học (learning rate).\n\n**Mối quan hệ:**\n- GRUs, tương tự như LSTMs, giúp giải quyết vấn đề exploding gradients trong RNNs bằng cách sử dụng các cổng để điều chỉnh luồng thông tin và gradient, cho phép học các phụ thuộc dài hạn ổn định hơn."
        },
        "output_without_context": {
            "question": "Gradient biến mất trong mạng nơ-ron thường xảy ra do yếu tố nào sau đây?",
            "answer": "Sử dụng hàm kích hoạt có đạo hàm gần bằng 0."
        }
    },
    {
        "topic_description": "Chủ đề này sẽ thảo luận về Bộ nhớ dài-ngắn hạn (LSTM) như một giải pháp cho vấn đề gradient biến mất trong RNN. Học sinh sẽ được đánh giá về cách thức hoạt động của LSTM và cách nó tối ưu hóa quy trình huấn luyện qua các cổng kiểm soát.",
        "output_with_context": {
            "question": "Trong LSTM, cổng nào chịu trách nhiệm kiểm soát thông tin nào sẽ được ghi nhớ và thông tin nào sẽ bị quên trong trạng thái ô?",
            "answer": "Cổng Forget (Forget Gate)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**Khi nào dùng:**\n- RNNs, LSTMs, GRUs\n- Transformers (BERT, GPT)\n- Batch size nhỏ\n- Online learning\n\n**7. Weight Decay:**\n\nThêm penalty cho magnitude của weights trong update rule.\n\n**Trong optimizer:**\n$$w := w - \\alpha(\frac{\\partial L}{\\partial w} + \\lambda w)$$\n\n**Khác với L2 Regularization:**\n- L2 reg: Thêm vào loss function\n- Weight decay: Thêm trực tiếp vào update rule\n- Với SGD: Tương đương nhau\n- Với Adam/AdamW: **Khác nhau!** AdamW implement weight decay đúng cách\n\n**8. Stochastic Depth:**\n\nNgẫu nhiên skip một số layers trong training (dùng cho ResNets).\n\n**9. Label Smoothing:**\n\nThay vì hard labels (0 hoặc 1), dùng soft labels.\n\n**Ví dụ:** \n- Hard: [0, 1, 0, 0]\n- Soft (ε=0.1): [0.025, 0.925, 0.025, 0.025]\n\n**Công thức:**\n$$y_{smooth} = (1-\\epsilon)y + \frac{\\epsilon}{K}$$\n\n**Ưu điểm:**\n- Ngăn mô hình quá tự tin (overconfident)\n- Improve generalization\n- Thường dùng trong image classification\n\n**10. Gradient Clipping:**\n\nGiới hạn magnitude của gradients, đặc biệt quan trọng cho RNNs.\n\n**Clip by value:**\n$$g = \\max(\\min(g, threshold), -threshold)$$\n\n**Clip by norm:**\n$$g = \frac{threshold \\cdot g}{||g||} \text{ if } ||g|| > threshold$$\n\n**Sử dụng:**\n- RNNs, LSTMs (giải quyết exploding gradients)\n- Transformers với sequence dài\n- Threshold thường: 1.0 hoặc 5.0\n\n### Mạng Nơ-ron Tích Chập (Convolutional Neural Networks - CNN)\n\nCNN là kiến trúc mạng nơ-ron chuyên biệt cho xử lý dữ liệu dạng lưới (grid-like), đặc biệt là ảnh. Được lấy cảm hứng từ vỏ não thị giác (visual cortex) của động vật.\n\n**Đặc điểm chính:**\n- **Locally connected:** Mỗi nơ-ron chỉ kết nối với vùng local của input\n- **Parameter sharing:** Cùng một filter được áp dụng trên toàn bộ input\n- **Translation invariance:** Phát hiện đặc trưng ở bất kỳ vị trí nào\n- **Hierarchical feature learning:** Lớp đầu học low-level features, lớp sau học high-level features\n\n**Các Thành Phần Chính:**\n\n**1. Lớp Tích Chập (Convolutional Layer):**\n\nÁp dụng filters (kernels) trượt trên input để trích xuất features.\n\n**Công thức:**\n$$Output[i,j] = \\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n] \times Kernel[m,n] + bias$$\n\nHoặc dạng tổng quát với nhiều channels:\n$$Output[i,j,d] = \\sum_{c=0}^{C-1}\\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n,c] \times Kernel[m,n,c,d] + bias[d]$$\n\n**Khái niệm quan trọng:**\n\n**Filter/Kernel:**\n- Ma trận nhỏ (thường 3×3, 5×5, 7×7)\n- Chứa weights học được\n- Mỗi filter phát hiện một đặc trưng cụ thể:\n  - Lớp đầu: Edges (cạnh), corners (góc), colors\n\n**Các khái niệm quan trọng:**\n- Exploding Gradients là một vấn đề trong quá trình huấn luyện mạng nơ-ron sâu, đặc biệt là trong Mạng Nơ-ron Hồi quy (RNNs), xảy ra khi các gradient tăng theo cấp số mũ hoặc trở nên cực kỳ lớn trong quá trình lan truyền ngược (backpropagation). Điều này dẫn đến việc cập nhật trọng số (weights) hoặc tham số quá lớn, gây ra sự không ổn định nghiêm trọng trong quá trình huấn luyện, khiến mô hình không thể học, không hội tụ hoặc phân kỳ. Các giải pháp phổ biến để khắc phục vấn đề này bao gồm Gradient Clipping (cắt bớt gradient), khởi tạo trọng số phù hợp (proper weight initialization) và giảm tốc độ học (learning rate).\n- LSTM (Long Short-Term Memory) là một loại mạng nơ-ron hồi quy (RNN) được thiết kế để xử lý và dự đoán các chuỗi dữ liệu dài, khắc phục vấn đề vanishing/exploding gradient thường gặp ở RNN truyền thống. LSTM có khả năng học và ghi nhớ các phụ thuộc dài hạn trong dữ liệu tuần tự (như dữ liệu thời gian hoặc văn bản) nhờ vào cấu trúc cổng (gating mechanisms) bao gồm Input Gate, Forget Gate và Output Gate. Các cổng này kiểm soát luồng thông tin vào, ra và qua \"trạng thái ô\" (cell state), đóng vai trò như bộ nhớ dài hạn của mạng. LSTM được phát minh bởi Hochreiter & Schmidhuber (1997) và thường được sử dụng làm thành phần trong các kiến trúc như Seq2Seq.\n\n**Mối quan hệ:**\n- GRUs, tương tự như LSTMs, giúp giải quyết vấn đề exploding gradients trong RNNs bằng cách sử dụng các cổng để điều chỉnh luồng thông tin và gradient, cho phép học các phụ thuộc dài hạn ổn định hơn.\n- LSTMs được thiết kế để giải quyết vấn đề exploding gradients (cũng như vanishing gradients) trong RNNs truyền thống thông qua kiến trúc cổng phức tạp của chúng, cho phép kiểm soát luồng thông tin và gradient tốt hơn."
        },
        "output_without_context": {
            "question": "Trong mô hình LSTM, cổng nào chịu trách nhiệm quyết định thông tin nào sẽ được lưu trữ trong trạng thái bộ nhớ?",
            "answer": "Cổng ghi (input gate)"
        }
    },
    {
        "topic_description": "Chủ đề này tập trung vào việc tìm hiểu về Mạng Nơ-ron Hồi quy (RNN) và cách chúng xử lý dữ liệu tuần tự. Học sinh sẽ được yêu cầu mô tả chức năng của RNN và đưa ra ví dụ về ứng dụng thực tế của chúng như phân tích cảm xúc.",
        "output_with_context": {
            "question": "Mạng Nơ-ron Hồi quy (RNN) được thiết kế đặc biệt để xử lý loại dữ liệu nào?",
            "answer": "Dữ liệu tuần tự như văn bản, chuỗi thời gian, âm thanh và video.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n└─ 3×3 pool → 1×1 conv\n↓ (Concatenate)\nOutput\n```\n\n**1×1 Convolutions:**\n- Dimensionality reduction\n- Thêm non-linearity\n- \"Network in Network\"\n\n**ResNet (2015 - Microsoft Research):**\n- **Cách mạng trong deep learning**\n- Rất sâu: ResNet-50, ResNet-101, ResNet-152 (thậm chí 1000+ layers)\n- Thắng ImageNet 2015: Top-5 error 3.6% (better than human!)\n\n**Vấn đề với mạng rất sâu:**\n- **Degradation problem:** Mạng sâu hơn nhưng accuracy giảm (không phải overfitting!)\n- Vanishing gradients\n\n**Giải pháp: Skip Connections (Residual Connections):**\n$$H(x) = F(x) + x$$\n\n**Residual Block:**\n```\nx ─────────────────────→ +\n  ↓                      ↑\n  Conv 3×3, ReLU         │\n  ↓                      │\n  Conv 3×3               │\n  ↓─────────────────────┘\n  (Add)\n  ↓\n  ReLU\n```\n\n**Tại sao hiệu quả:**\n- **Identity mapping:** Nếu thêm layers không giúp ích, học $F(x) = 0$ → $H(x) = x$\n- **Gradient flow:** Gradients có thể flow trực tiếp qua shortcut\n- Dễ optimize hơn\n\n**Bottleneck Design (ResNet-50+):**\n```\n1×1 conv (reduce dim)\n↓\n3×3 conv\n↓\n1×1 conv (restore dim)\n```\nGiảm computational cost\n\n**DenseNet (2017):**\n- Kết nối mọi layer với tất cả layers sau nó\n- Reuse features hiệu quả hơn ResNet\n\n**EfficientNet (2019 - Google):**\n- **Compound scaling:** Scale đồng thời depth, width, và resolution\n- Balancing method\n- State-of-the-art accuracy với ít parameters và FLOPS hơn\n\n**MobileNet:**\n- Thiết kế cho mobile devices và embedded systems\n- **Depthwise Separable Convolutions:**\n  - Depthwise conv: Áp dụng filter riêng cho mỗi channel\n  - Pointwise conv: 1×1 conv để combine channels\n  - Giảm parameters và computation 8-9 lần\n\n**SqueezeNet:**\n- AlexNet-level accuracy với 50× ít parameters hơn\n- Fire modules: Squeeze (1×1 convs) + Expand (1×1 và 3×3 convs)\n\n**Applications of CNNs:**\n\n**Image Classification:**\n- Phân loại ảnh vào các categories\n- ImageNet, CIFAR, etc.\n\n**Object Detection:**\n- Phát hiện và localize objects trong ảnh\n- **R-CNN family:** R-CNN, Fast R-CNN, Faster R-CNN\n- **YOLO (You Only Look Once):** Real-time detection\n- **SSD (Single Shot Detector)**\n\n**Semantic Segmentation:**\n- Phân loại mỗi pixel\n- **FCN (Fully Convolutional Networks)**\n- **U-Net:** Architecture for biomedical images\n- **DeepLab:** Atrous convolution\n\n**Instance Segmentation:**\n- Segment mỗi object instance riêng biệt\n- **Mask R-CNN**\n\n**Face Recognition:**\n- FaceNet, DeepFace\n\n**Style Transfer:**\n- Chuyển style từ ảnh này sang ảnh khác\n\n**Medical Image Analysis:**\n- X-ray, MRI, CT scan analysis\n- Disease detection\n\n### Mạng Nơ-ron Hồi Tiếp (Recurrent Neural Networks - RNN)\n\nRNN được thiết kế đặc biệt cho dữ liệu tuần tự (sequential data) như văn bản, chuỗi thời gian, âm thanh, video.\n\n**Đặc điểm:**\n- **Kết nối recurrent:** Output ở bước $t$ phụ thuộc vào input hiện tại và các bước trước đó\n- **Chia sẻ parameters** qua các time steps\n\n**Các khái niệm quan trọng:**\n- Sequential data là loại dữ liệu mà các phần tử có thứ tự phụ thuộc lẫn nhau, ví dụ như văn bản (chuỗi từ), chuỗi thời gian (giá cổ phiếu theo ngày), âm thanh (chuỗi tín hiệu), hoặc video (chuỗi khung hình). RNN được thiết kế để xử lý hiệu quả loại dữ liệu này.\n\n**Mối quan hệ:**\n- Recurrent Neural Networks (RNN) được thiết kế để xử lý sequential data như văn bản, chuỗi thời gian, âm thanh và video.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**Khi nào dùng:**\n- RNNs, LSTMs, GRUs\n- Transformers (BERT, GPT)\n- Batch size nhỏ\n- Online learning\n\n**7. Weight Decay:**\n\nThêm penalty cho magnitude của weights trong update rule.\n\n**Trong optimizer:**\n$$w := w - \\alpha(\frac{\\partial L}{\\partial w} + \\lambda w)$$\n\n**Khác với L2 Regularization:**\n- L2 reg: Thêm vào loss function\n- Weight decay: Thêm trực tiếp vào update rule\n- Với SGD: Tương đương nhau\n- Với Adam/AdamW: **Khác nhau!** AdamW implement weight decay đúng cách\n\n**8. Stochastic Depth:**\n\nNgẫu nhiên skip một số layers trong training (dùng cho ResNets).\n\n**9. Label Smoothing:**\n\nThay vì hard labels (0 hoặc 1), dùng soft labels.\n\n**Ví dụ:** \n- Hard: [0, 1, 0, 0]\n- Soft (ε=0.1): [0.025, 0.925, 0.025, 0.025]\n\n**Công thức:**\n$$y_{smooth} = (1-\\epsilon)y + \frac{\\epsilon}{K}$$\n\n**Ưu điểm:**\n- Ngăn mô hình quá tự tin (overconfident)\n- Improve generalization\n- Thường dùng trong image classification\n\n**10. Gradient Clipping:**\n\nGiới hạn magnitude của gradients, đặc biệt quan trọng cho RNNs.\n\n**Clip by value:**\n$$g = \\max(\\min(g, threshold), -threshold)$$\n\n**Clip by norm:**\n$$g = \frac{threshold \\cdot g}{||g||} \text{ if } ||g|| > threshold$$\n\n**Sử dụng:**\n- RNNs, LSTMs (giải quyết exploding gradients)\n- Transformers với sequence dài\n- Threshold thường: 1.0 hoặc 5.0\n\n### Mạng Nơ-ron Tích Chập (Convolutional Neural Networks - CNN)\n\nCNN là kiến trúc mạng nơ-ron chuyên biệt cho xử lý dữ liệu dạng lưới (grid-like), đặc biệt là ảnh. Được lấy cảm hứng từ vỏ não thị giác (visual cortex) của động vật.\n\n**Đặc điểm chính:**\n- **Locally connected:** Mỗi nơ-ron chỉ kết nối với vùng local của input\n- **Parameter sharing:** Cùng một filter được áp dụng trên toàn bộ input\n- **Translation invariance:** Phát hiện đặc trưng ở bất kỳ vị trí nào\n- **Hierarchical feature learning:** Lớp đầu học low-level features, lớp sau học high-level features\n\n**Các Thành Phần Chính:**\n\n**1. Lớp Tích Chập (Convolutional Layer):**\n\nÁp dụng filters (kernels) trượt trên input để trích xuất features.\n\n**Công thức:**\n$$Output[i,j] = \\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n] \times Kernel[m,n] + bias$$\n\nHoặc dạng tổng quát với nhiều channels:\n$$Output[i,j,d] = \\sum_{c=0}^{C-1}\\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Input[i+m,j+n,c] \times Kernel[m,n,c,d] + bias[d]$$\n\n**Khái niệm quan trọng:**\n\n**Filter/Kernel:**\n- Ma trận nhỏ (thường 3×3, 5×5, 7×7)\n- Chứa weights học được\n- Mỗi filter phát hiện một đặc trưng cụ thể:\n  - Lớp đầu: Edges (cạnh), corners (góc), colors\n\n**Các khái niệm quan trọng:**\n- Recurrent Neural Networks (RNNs) là một loại mạng nơ-ron được thiết kế đặc biệt để xử lý dữ liệu tuần tự (sequential data) như văn bản, chuỗi thời gian, âm thanh và video. Đặc điểm chính của RNN là có các kết nối recurrent, nghĩa là đầu ra ở một bước thời gian phụ thuộc vào đầu ra hoặc trạng thái ẩn từ các bước thời gian trước đó, cho phép nó nắm bắt các phụ thuộc tuần tự. RNN duy trì một \"hidden state\" (bộ nhớ) từ các time step trước, sử dụng các ma trận trọng số được chia sẻ qua tất cả các time steps và áp dụng hàm kích hoạt (thường là tanh) để tạo ra hidden state và output mới. Chúng thường được sử dụng trong các bài toán như xử lý ngôn ngữ tự nhiên và nhận dạng giọng nói, và cũng là thành phần trong các kiến trúc như Seq2Seq và Transformers. Các biến thể phổ biến của RNN bao gồm LSTMs và GRUs, và các dạng kiến trúc RNN bao gồm One-to-One, One-to-Many, Many-to-One, Many-to-Many (synced) và Many-to-Many (seq2seq).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**Cấu trúc:**\n- **Forward RNN:** Xử lý từ trái sang phải ($\\overrightarrow{h}_t$)\n- **Backward RNN:** Xử lý từ phải sang trái ($\\overleftarrow{h}_t$)\n- **Kết hợp:** $h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t]$ (concatenate)\n\n**Ưu điểm:**\n- Có thông tin từ **cả quá khứ và tương lai**\n- Hiệu suất tốt hơn đáng kể trong nhiều tasks\n\n**Nhược điểm:**\n- **Không real-time:** Cần toàn bộ sequence trước khi xử lý\n- **Gấp đôi parameters và computation**\n\n**Ứng dụng:**\n- NLP tasks (POS tagging, NER, sentiment analysis)\n- Speech recognition\n- Protein structure prediction\n\n**Sequence-to-Sequence (Seq2Seq):**\n\nKiến trúc cho tasks với input và output sequences có độ dài khác nhau.\n\n**Cấu trúc:**\n- **Encoder:** RNN/LSTM/GRU xử lý input sequence → context vector\n- **Decoder:** RNN/LSTM/GRU generate output sequence từ context vector\n\n**Ứng dụng:**\n- Machine translation\n- Text summarization\n- Question answering\n- Chatbots\n\n**Vấn đề:**\n- **Bottleneck:** Context vector cố định phải chứa toàn bộ thông tin\n- Khó với sequences dài\n\n**Giải pháp: Attention Mechanism** (dẫn đến Transformers)\n\n### Transformers\n\nKiến trúc cách mạng sử dụng cơ chế self-attention, được giới thiệu trong paper \"Attention is All You Need\" (Vaswani et al., 2017).\n\n**Bối cảnh:**\n- RNN/LSTM xử lý tuần tự → chậm, khó song song hóa\n- Khó học dependencies rất dài\n- Transformers giải quyết bằng cách loại bỏ recurrence, chỉ dùng attention\n\n**Self-Attention (Scaled Dot-Product Attention):**\n\n$$Attention(Q, K, V) = softmax(\frac{QK^T}{\\sqrt{d_k}})V$$\n\nTrong đó:\n- $Q$ (Queries): \"Tôi đang tìm gì?\" - ma trận queries $(n \times d_k)$\n- $K$ (Keys): \"Tôi có thông tin gì?\" - ma trận keys $(m \times d_k)$\n- $V$ (Values): \"Thông tin chi tiết là gì?\" - ma trận values $(m \times d_v)$\n- $d_k$: Dimension của keys (dùng để scale)\n- $n$: Số lượng queries (thường = độ dài sequence)\n- $m$: Số lượng keys/values\n\n**Cách hoạt động từng bước:**\n\n1. **Tính Attention Scores:**\n   $$scores = \frac{QK^T}{\\sqrt{d_k}}$$\n   - Nhân $Q$ với $K^T$ để tính similarity\n   - Chia cho $\\sqrt{d_k}$ để scale (tránh softmax saturation)\n\n2. **Áp dụng Softmax:**\n   $$attention\\_weights = softmax(scores)$$\n   - Chuyển scores thành phân phối xác suất\n   - Mỗi hàng tổng = 1\n\n3. **Weighted Sum của Values:**\n   $$output = attention\\_weights \\cdot V$$\n   - Kết hợp values theo trọng số attention\n\n**Ví dụ minh họa:**\nCâu: \"The cat sat on the mat\"\n- Khi xử lý \"sat\", attention có thể chú ý nhiều đến \"cat\" (subject) và \"mat\" (location)\n- Attention weights cao cho những từ liên quan, thấp cho những từ không liên quan\n\n**Multi-Head Attention:**\n\n\n**Các khái niệm quan trọng:**\n- Recurrence là một kỹ thuật trong các mạng nơ-ron hồi quy (RNN, LSTM, GRU) nơi đầu ra hoặc trạng thái ẩn của một bước thời gian được đưa trở lại làm đầu vào cho bước thời gian tiếp theo, cho phép mô hình xử lý dữ liệu tuần tự và nắm bắt các phụ thuộc theo thời gian. Transformers loại bỏ kỹ thuật này.\n- Forward RNN xử lý chuỗi dữ liệu từ trái sang phải, tạo ra biểu diễn ẩn $\\overrightarrow{h}_t$ tại mỗi bước thời gian t để nắm bắt thông tin từ quá khứ. Ngược lại, Backward RNN xử lý chuỗi dữ liệu từ phải sang trái, tạo ra biểu diễn ẩn $\\overleftarrow{h}_t$ tại mỗi bước thời gian t để nắm bắt thông tin từ tương lai. Cả hai đều là thành phần của mô hình RNN.\n\n**Mối quan hệ:**\n- Forward RNN tính toán $\\overrightarrow{h}_t$, một thành phần của $h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t]$, đại diện cho thông tin từ quá khứ trong chuỗi.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\nBackpropagation là thuật toán cốt lõi để huấn luyện mạng nơ-ron sâu, cho phép tính gradient một cách hiệu quả thông qua quy tắc chuỗi (chain rule).\n\n**Ý tưởng cơ bản:**\n- Tính toán gradient của loss function theo tất cả các tham số (weights và biases)\n- Lan truyền gradient từ output về input qua các lớp\n- Sử dụng quy tắc chuỗi để phân rã gradient phức tạp thành các phần đơn giản\n\n**Quy tắc chuỗi (Chain Rule):**\n$$\frac{\\partial L}{\\partial w^{[l]}} = \frac{\\partial L}{\\partial a^{[l]}} \\cdot \frac{\\partial a^{[l]}}{\\partial z^{[l]}} \\cdot \frac{\\partial z^{[l]}}{\\partial w^{[l]}}$$\n\nTrong đó:\n- $\frac{\\partial L}{\\partial a^{[l]}}$: Gradient của loss theo activation\n- $\frac{\\partial a^{[l]}}{\\partial z^{[l]}}$: Đạo hàm của hàm kích hoạt\n- $\frac{\\partial z^{[l]}}{\\partial w^{[l]}}$: Gradient của pre-activation theo weights\n\n**Các bước chi tiết:**\n\n**1. Forward Pass (Lan truyền xuôi):**\n- Tính toán output của mỗi lớp từ input đến output\n- Lưu trữ tất cả các giá trị $z^{[l]}$ và $a^{[l]}$ (cần cho backward pass)\n\n**2. Tính Loss:**\n- So sánh prediction với ground truth\n- Tính giá trị loss: $L = Loss(y, \\hat{y})$\n\n**3. Backward Pass (Lan truyền ngược):**\n- Bắt đầu từ lớp output, tính gradient của loss theo output\n- Với mỗi lớp từ L về 1:\n  - Tính $\frac{\\partial L}{\\partial z^{[l]}} = \frac{\\partial L}{\\partial a^{[l]}} \\odot \\sigma'(z^{[l]})$ (element-wise product)\n  - Tính $\frac{\\partial L}{\\partial W^{[l]}} = \frac{\\partial L}{\\partial z^{[l]}} \\cdot (a^{[l-1]})^T$\n  - Tính $\frac{\\partial L}{\\partial b^{[l]}} = \frac{\\partial L}{\\partial z^{[l]}}$\n  - Lan truyền về lớp trước: $\frac{\\partial L}{\\partial a^{[l-1]}} = (W^{[l]})^T \\cdot \frac{\\partial L}{\\partial z^{[l]}}$\n\n**4. Cập nhật Weights:**\n- Sử dụng gradient descent hoặc các optimizer khác\n- $W^{[l]} := W^{[l]} - \\alpha \frac{\\partial L}{\\partial W^{[l]}}$\n- $b^{[l]} := b^{[l]} - \\alpha \frac{\\partial L}{\\partial b^{[l]}}$\n\n**Ví dụ minh họa:**\nMạng 2 lớp: Input → Hidden → Output\n- Forward: $a^{[1]} = \\sigma(W^{[1]}x + b^{[1]})$, $\\hat{y} = \\sigma(W^{[2]}a^{[1]} + b^{[2]})$\n- Loss: $L = (y - \\hat{y})^2$\n- Backward:\n  - $\frac{\\partial L}{\\partial \\hat{y}} = -2(y - \\hat{y})$\n  - $\frac{\\partial L}{\\partial W^{[2]}} = \frac{\\partial L}{\\partial \\hat{y}} \\cdot \\sigma'(z^{[2]}) \\cdot a^{[1]}$\n  - Lan truyền về hidden layer tương tự\n\n**Computational Graph:**\n\n**Các khái niệm quan trọng:**\n- Neural Network (Mạng nơ-ron) là một mô hình học máy lấy cảm hứng từ cấu trúc não bộ, bao gồm nhiều lớp nơ-ron được kết nối với nhau. Mỗi nơ-ron thực hiện một phép biến đổi tuyến tính và áp dụng một hàm kích hoạt phi tuyến. Mạng nơ-ron có khả năng học các mối quan hệ phức tạp trong dữ liệu, ánh xạ từ dữ liệu đầu vào sang đầu ra mong muốn thông qua quá trình huấn luyện, thường sử dụng thuật toán Backpropagation để cập nhật các trọng số và bias. Nó được sử dụng rộng rãi trong các bài toán phân loại, hồi quy và học sâu.\n\n**Mối quan hệ:**\n- Neural network sử dụng Backpropagation để huấn luyện và cập nhật các tham số của nó.\n- Forward Pass tính toán output của mỗi lớp trong neural network từ input đến output.\n- Neural network chứa các weights là các tham số có thể học được giữa các nơ-ron.\n- Neural network chứa các biases là các tham số có thể học được trong mỗi lớp."
        },
        "output_without_context": {
            "question": "Mạng Nơ-ron Hồi quy (RNN) chủ yếu được sử dụng để xử lý loại dữ liệu nào?",
            "answer": "Dữ liệu tuần tự."
        }
    },
    {
        "topic_description": "Chủ đề này sẽ tích hợp kiến thức từ tuần 8 về CNN và kiến thức từ tuần 7 về RNN, đánh giá khả năng của học sinh trong việc xác định cách mà CNN và RNN có thể được sử dụng cùng nhau trong các ứng dụng như chú thích hình ảnh. Học sinh sẽ khám phá được ưu điểm của sự kết hợp này.",
        "output_with_context": {
            "question": "Trong ứng dụng chú thích hình ảnh, cách nào mà CNN và RNN có thể được kết hợp để cải thiện hiệu suất?",
            "answer": "CNN được sử dụng để trích xuất đặc trưng từ hình ảnh, trong khi RNN xử lý chuỗi từ để tạo ra mô tả văn bản.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n- **Hidden state** lưu trữ thông tin từ quá khứ\n\n**Kiến trúc Cơ Bản:**\n\n$$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$\n$$y_t = W_{hy}h_t + b_y$$\n\nTrong đó:\n- $x_t$: Input tại time step $t$\n- $h_t$: Hidden state tại time step $t$ (bộ nhớ của mạng)\n- $h_{t-1}$: Hidden state từ time step trước\n- $y_t$: Output tại time step $t$\n- $W_{hh}, W_{xh}, W_{hy}$: Weight matrices (được chia sẻ qua tất cả time steps)\n\n**Cách hoạt động:**\n1. Nhận input $x_t$ và hidden state $h_{t-1}$ từ bước trước\n2. Kết hợp chúng qua linear transformation\n3. Áp dụng activation (tanh)\n4. Tạo output $y_t$ và hidden state mới $h_t$\n5. $h_t$ được truyền đến time step tiếp theo\n\n**Các Dạng RNN:**\n\n**1. One-to-One:**\n- Input: 1 vector\n- Output: 1 vector\n- Giống feedforward network (không phải RNN thực sự)\n\n**2. One-to-Many:**\n- Input: 1 vector\n- Output: Sequence of vectors\n- **Ứng dụng:** Image captioning (ảnh → mô tả văn bản)\n\n**3. Many-to-One:**\n- Input: Sequence of vectors\n- Output: 1 vector\n- **Ứng dụng:** Sentiment analysis (câu → cảm xúc), video classification\n\n**4. Many-to-Many (synced):**\n- Input và output có cùng độ dài, aligned\n- **Ứng dụng:** Video classification per frame, POS tagging\n\n**5. Many-to-Many (seq2seq):**\n- Input và output có độ dài khác nhau\n- **Ứng dụng:** Machine translation, speech recognition, text summarization\n\n**Backpropagation Through Time (BPTT):**\n- Mở rộng backpropagation cho sequences\n- \"Unfold\" RNN qua time → feed forward network lớn\n- Tính gradients và backprop qua tất cả time steps\n- Computational cost cao cho sequences dài\n\n**Vấn Đề Nghiêm Trọng:**\n\n**1. Vanishing Gradients:**\n- Gradients giảm mũ khi backprop qua nhiều time steps\n- Thông tin từ xa trong quá khứ bị \"quên\"\n- Khó học long-term dependencies\n- Nguyên nhân: Nhân nhiều lần với $W_{hh}$ và đạo hàm của tanh (< 1)\n\n**2. Exploding Gradients:**\n- Gradients tăng mũ\n- Weights update quá lớn, training không ổn định\n- Giải pháp: **Gradient clipping**\n\n**Long Short-Term Memory (LSTM):**\n\nLSTM giải quyết vanishing gradient bằng **gating mechanisms** để kiểm soát luồng thông tin. Được phát minh bởi Hochreiter & Schmidhuber (1997).\n\n**Ý tưởng chính:**\n- **Cell state** $C_t$: \"Conveyor belt\" chạy suốt chuỗi, ít thay đổi\n- **Gates:** Các cơ chế điều khiển thông tin vào/ra cell state\n\n**Các Gates:**\n\n**1. Forget Gate $f_t$:**\n$$f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)$$\n- Quyết định **quên** bao nhiêu thông tin từ cell state cũ\n- Output: Vector trong [0,1]\n- 0 = quên hoàn toàn, 1 = giữ hoàn toàn\n\n**2. Input Gate $i_t$:**\n\n**Các khái niệm quan trọng:**\n- Many-to-Many (synced) RNN là một dạng kiến trúc mạng nơ-ron trong đó cả input và output đều là chuỗi có cùng độ dài và được căn chỉnh (aligned). Ứng dụng bao gồm Video Classification per frame và POS tagging (gán nhãn từ loại).\n- One-to-Many RNN là một kiến trúc mạng nơ-ron nhận một input vector duy nhất và tạo ra một chuỗi các output vector, điển hình là trong Image Captioning. Many-to-One RNN là một kiến trúc mạng nơ-ron nhận một chuỗi các input vector và tạo ra một output vector duy nhất, được ứng dụng trong Sentiment Analysis và Video Classification.\n- Many-to-Many (seq2seq) RNN là một dạng kiến trúc mạng nơ-ron trong đó input và output đều là chuỗi nhưng có thể có độ dài khác nhau. Ứng dụng phổ biến bao gồm Machine Translation (dịch máy), Speech Recognition (nhận dạng giọng nói) và Text Summarization (tóm tắt văn bản).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**1. Generator (Bộ Sinh):** $G(z) \to fake\\_data$\n\n**Input:**\n- Random noise vector $z \\sim p_z(z)$ (thường Gaussian hoặc Uniform)\n- Dimension thường 100-1000\n\n**Output:**\n- Synthetic data (fake) $G(z)$\n- Cùng kích thước với real data\n- Ví dụ: Ảnh 64×64×3\n\n**Mục tiêu:**\n- **Generate realistic samples** không phân biệt được với real data\n- \"Lừa\" Discriminator tin là real\n\n**Kiến trúc:**\n- Thường là deconvolutional network (transpose convolutions)\n- Batch normalization, ReLU/LeakyReLU\n- Tanh activation ở output (để output trong [-1, 1])\n\n**2. Discriminator (Bộ Phân Biệt):** $D(x) \to [0,1]$\n\n**Input:**\n- Data sample $x$ (có thể real hoặc fake)\n\n**Output:**\n- Scalar trong [0, 1]: Xác suất sample là **real**\n- Gần 1 = tin là real, gần 0 = tin là fake\n\n**Mục tiêu:**\n- **Phân biệt chính xác** real vs fake\n- Maximize classification accuracy\n\n**Kiến trúc:**\n- Thường là CNN (convolutional network)\n- Leaky ReLU, Dropout\n- Sigmoid activation ở output\n\n**Training - Minimax Game:**\n\nĐây là một **two-player game** với objective function:\n\n$$\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1-D(G(z)))]$$\n\n**Giải thích:**\n\n**Discriminator muốn maximize:**\n- $\\mathbb{E}_{x \\sim p_{data}}[\\log D(x)]$: Maximize log probability của real data\n- $\\mathbb{E}_{z \\sim p_z}[\\log(1-D(G(z)))]$: Maximize log probability reject fake data\n\n**Generator muốn minimize:**\n- $\\mathbb{E}_{z \\sim p_z}[\\log(1-D(G(z)))]$: Minimize log probability fake bị reject\n- Tương đương: Maximize $\\mathbb{E}_{z}[\\log D(G(z))]$ (non-saturating loss trong thực tế)\n\n**Thuật Toán Training:**\n\n**Alternating Updates** (training xen kẽ):\n\n**Mỗi iteration:**\n\n1. **Train Discriminator (k steps, thường k=1):**\n   - Sample mini-batch real data $\\{x^{(1)}, ..., x^{(m)}\\}$\n   - Sample mini-batch noise $\\{z^{(1)}, ..., z^{(m)}\\}$\n   - Generate fake data: $\tilde{x}^{(i)} = G(z^{(i)})$\n   - Update D by **ascending** gradient:\n   $$\nabla_{\theta_D} \frac{1}{m}\\sum_{i=1}^{m}[\\log D(x^{(i)}) + \\log(1-D(G(z^{(i)})))]$$\n\n2. **Train Generator (1 step):**\n   - Sample mini-batch noise $\\{z^{(1)}, ..., z^{(m)}\\}$\n   - Update G by **descending** gradient:\n   $$\nabla_{\theta_G} \frac{1}{m}\\sum_{i=1}^{m}\\log(1-D(G(z^{(i)})))$$\n   - Hoặc non-saturating: Ascending $\nabla_{\theta_G} \frac{1}{m}\\sum_{i=1}^{m}\\log D(G(z^{(i)}))$\n\n**Lý do train D nhiều hơn G:**\n- D cần đủ accurate để provide good gradient cho G\n- Nếu D quá yếu, G không học được gì\n\n**Thách Thức trong Training GANs:**\n\n**1. Mode Collapse:**\n- **Vấn đề:** Generator chỉ sinh một vài modes (variations) của data\n\n**Các khái niệm quan trọng:**\n- CNN (Convolutional Neural Network) là một loại mạng nơ-ron thường được sử dụng làm kiến trúc cho Discriminator. CNN hiệu quả trong việc xử lý dữ liệu dạng lưới như hình ảnh, sử dụng các lớp tích chập để trích xuất các đặc trưng phân cấp từ dữ liệu đầu vào.\n- CNN (Convolutional Neural Network) là một loại mạng nơ-ron chuyên dụng để xử lý dữ liệu có cấu trúc lưới như hình ảnh. Trong RL, CNN thường được sử dụng để trích xuất đặc trưng từ các quan sát hình ảnh (ví dụ: khung hình trò chơi, hình ảnh camera) làm đầu vào cho các thuật toán học tăng cường.\n\n**Mối quan hệ:**\n- DQN trên Atari sử dụng CNN để xử lý đầu vào là các khung hình trò chơi và trích xuất đặc trưng hình ảnh.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Value Function Approximation - Xấp Xỉ Hàm Giá Trị\n         ↑ Chọn action với θ    ↑ Evaluate với θ⁻\n```\n→ Giảm overestimation\n\n**Dueling DQN**:\n```\nQ(s, a) = V(s) + A(s, a)\nV(s): State value stream\nA(s, a): Advantage stream\n```\n→ Better representation\n\n**Prioritized Experience Replay**:\n```\nPriority = |TD error|\nSample theo priority thay vì uniform\n```\n→ Học từ important transitions\n\n**Rainbow DQN**: Kết hợp tất cả improvements\n\n### 9. Policy Approximation Preview\n\n#### 9.1. Parameterized Policies\n\n**Discrete Actions**:\n```\nπ(a|s; θ) = softmax(preference(s, a; θ))\n          = exp(h(s,a;θ)) / Σ_b exp(h(s,b;θ))\n```\n\n**Continuous Actions**:\n```\nπ(a|s; θ) = N(μ(s; θ), σ(s; θ)²)\n```\n\n#### 9.2. Advantages của Policy Approximation\n\n✅ Trực tiếp học policy\n✅ Stochastic policies tự nhiên\n✅ Hiệu quả với continuous actions\n✅ Tốt với partially observable environments\n\n### 10. Ứng dụng thực tế\n\n#### 10.1. Atari Games\n\n**DQN trên Atari**:\n- Input: 84×84×4 grayscale frames\n- CNN: 3 conv layers + 2 FC layers\n- Output: Q-values cho 18 actions\n- Training: 50M frames ≈ 38 days gameplay\n\n**Kết quả**:\n- Human-level performance trên 29/49 games\n- Superhuman trên một số games (Breakout, Pong)\n\n#### 10.2. Robotics\n\n**Robot Manipulation**:\n- State: Joint angles, end-effector pose, camera images\n- Action: Joint velocities/torques\n- FA: Deep networks để process visual input\n\n**Challenges**:\n- Sample efficiency\n- Safe exploration\n- Sim-to-real transfer\n\n#### 10.3. Autonomous Systems\n\n**Self-driving Cars**:\n- State: Camera, LIDAR, GPS, sensors\n- Action: Steering, acceleration, braking\n- FA: CNN cho perception + FC cho control\n\n### 11. Code Implementation\n\n#### 11.1. Linear Function Approximation\n```python\nclass LinearVFA:\n    def __init__(self, num_features):\n        self.w = np.zeros(num_features)\n    \n    def predict(self, features):\n        \"\"\"V̂(s) = w^T φ(s)\"\"\"\n        return np.dot(self.w, features)\n    \n    def update(self, features, target, alpha):\n        \"\"\"w ← w + α[target - V̂(s)]φ(s)\"\"\"\n        prediction = self.predict(features)\n        self.w += alpha * (target - prediction) * features\n        return prediction\n\ndef semi_gradient_td0(env, policy, vfa, num_episodes, \n                      alpha=0.01, gamma=0.99):\n    for episode in range(num_episodes):\n        state = env.reset()\n        features = extract_features(state)\n        \n        done = False\n        while not done:\n            action = policy(state)\n            next_state, reward, done, _ = env.step(action)\n            next_features = extract_features(next_state)\n            \n            # TD target\n            if done:\n                target = reward\n            else:\n                target = reward + gamma * vfa.predict(next_features)\n            \n            # Update\n            vfa.update(features, target, alpha)\n            \n            state = next_state\n            features = next_features\n    \n    return vfa\n```\n\n#### 11.2. Tile Coding\n```python\nclass TileCoding:\n    def __init__(self, num_tilings, tiles_per_dim, \n                 state_ranges, num_actions):\n        self.num_tilings = num_tilings\n        self.tiles_per_dim = tiles_per_dim\n        self.state_ranges = state_ranges\n        self.num_actions = num_actions\n        \n        # Calculate feature vector size\n        tiles_per_tiling = np.prod(tiles_per_dim)\n        self.feature_size = num_tilings * tiles_per_tiling * num_actions\n    \n    def get_features(self, state, action):\n        \"\"\"Convert (state, action) to feature vector\"\"\"\n        features = np.zeros(self.feature_size)\n        \n        for tiling_idx in range(self.num_tilings):\n\n**Các khái niệm quan trọng:**\n- CNN (Convolutional Neural Network) là một loại mạng nơ-ron thường được sử dụng làm kiến trúc cho Discriminator. CNN hiệu quả trong việc xử lý dữ liệu dạng lưới như hình ảnh, sử dụng các lớp tích chập để trích xuất các đặc trưng phân cấp từ dữ liệu đầu vào.\n- CNN (Convolutional Neural Network) là một loại mạng nơ-ron chuyên dụng để xử lý dữ liệu có cấu trúc lưới như hình ảnh. Trong RL, CNN thường được sử dụng để trích xuất đặc trưng từ các quan sát hình ảnh (ví dụ: khung hình trò chơi, hình ảnh camera) làm đầu vào cho các thuật toán học tăng cường.\n\n**Mối quan hệ:**\n- DQN trên Atari sử dụng CNN để xử lý đầu vào là các khung hình trò chơi và trích xuất đặc trưng hình ảnh.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n**Cấu trúc:**\n- **Forward RNN:** Xử lý từ trái sang phải ($\\overrightarrow{h}_t$)\n- **Backward RNN:** Xử lý từ phải sang trái ($\\overleftarrow{h}_t$)\n- **Kết hợp:** $h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t]$ (concatenate)\n\n**Ưu điểm:**\n- Có thông tin từ **cả quá khứ và tương lai**\n- Hiệu suất tốt hơn đáng kể trong nhiều tasks\n\n**Nhược điểm:**\n- **Không real-time:** Cần toàn bộ sequence trước khi xử lý\n- **Gấp đôi parameters và computation**\n\n**Ứng dụng:**\n- NLP tasks (POS tagging, NER, sentiment analysis)\n- Speech recognition\n- Protein structure prediction\n\n**Sequence-to-Sequence (Seq2Seq):**\n\nKiến trúc cho tasks với input và output sequences có độ dài khác nhau.\n\n**Cấu trúc:**\n- **Encoder:** RNN/LSTM/GRU xử lý input sequence → context vector\n- **Decoder:** RNN/LSTM/GRU generate output sequence từ context vector\n\n**Ứng dụng:**\n- Machine translation\n- Text summarization\n- Question answering\n- Chatbots\n\n**Vấn đề:**\n- **Bottleneck:** Context vector cố định phải chứa toàn bộ thông tin\n- Khó với sequences dài\n\n**Giải pháp: Attention Mechanism** (dẫn đến Transformers)\n\n### Transformers\n\nKiến trúc cách mạng sử dụng cơ chế self-attention, được giới thiệu trong paper \"Attention is All You Need\" (Vaswani et al., 2017).\n\n**Bối cảnh:**\n- RNN/LSTM xử lý tuần tự → chậm, khó song song hóa\n- Khó học dependencies rất dài\n- Transformers giải quyết bằng cách loại bỏ recurrence, chỉ dùng attention\n\n**Self-Attention (Scaled Dot-Product Attention):**\n\n$$Attention(Q, K, V) = softmax(\frac{QK^T}{\\sqrt{d_k}})V$$\n\nTrong đó:\n- $Q$ (Queries): \"Tôi đang tìm gì?\" - ma trận queries $(n \times d_k)$\n- $K$ (Keys): \"Tôi có thông tin gì?\" - ma trận keys $(m \times d_k)$\n- $V$ (Values): \"Thông tin chi tiết là gì?\" - ma trận values $(m \times d_v)$\n- $d_k$: Dimension của keys (dùng để scale)\n- $n$: Số lượng queries (thường = độ dài sequence)\n- $m$: Số lượng keys/values\n\n**Cách hoạt động từng bước:**\n\n1. **Tính Attention Scores:**\n   $$scores = \frac{QK^T}{\\sqrt{d_k}}$$\n   - Nhân $Q$ với $K^T$ để tính similarity\n   - Chia cho $\\sqrt{d_k}$ để scale (tránh softmax saturation)\n\n2. **Áp dụng Softmax:**\n   $$attention\\_weights = softmax(scores)$$\n   - Chuyển scores thành phân phối xác suất\n   - Mỗi hàng tổng = 1\n\n3. **Weighted Sum của Values:**\n   $$output = attention\\_weights \\cdot V$$\n   - Kết hợp values theo trọng số attention\n\n**Ví dụ minh họa:**\nCâu: \"The cat sat on the mat\"\n- Khi xử lý \"sat\", attention có thể chú ý nhiều đến \"cat\" (subject) và \"mat\" (location)\n- Attention weights cao cho những từ liên quan, thấp cho những từ không liên quan\n\n**Multi-Head Attention:**\n\n\n**Các khái niệm quan trọng:**\n- Công thức $h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t]$ mô tả cách kết hợp biểu diễn ẩn từ Forward RNN ($\\overrightarrow{h}_t$) và Backward RNN ($\\overleftarrow{h}_t$) bằng cách nối (concatenate) chúng lại với nhau để tạo ra một biểu diễn ngữ cảnh đầy đủ hơn tại thời điểm t, chứa thông tin từ cả quá khứ và tương lai.\n- Forward RNN xử lý chuỗi dữ liệu từ trái sang phải, tạo ra biểu diễn ẩn $\\overrightarrow{h}_t$ tại mỗi bước thời gian t để nắm bắt thông tin từ quá khứ. Ngược lại, Backward RNN xử lý chuỗi dữ liệu từ phải sang trái, tạo ra biểu diễn ẩn $\\overleftarrow{h}_t$ tại mỗi bước thời gian t để nắm bắt thông tin từ tương lai. Cả hai đều là thành phần của mô hình RNN.\n\n**Mối quan hệ:**\n- Backward RNN tính toán $\\overleftarrow{h}_t$, một thành phần của $h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t]$, đại diện cho thông tin từ tương lai trong chuỗi.\n- Forward RNN tính toán $\\overrightarrow{h}_t$, một thành phần của $h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t]$, đại diện cho thông tin từ quá khứ trong chuỗi."
        },
        "output_without_context": {
            "question": "Trong ứng dụng chú thích hình ảnh, sự kết hợp giữa CNN và RNN thường được sử dụng để làm gì?",
            "answer": "Tạo ra các mô tả văn bản cho hình ảnh."
        }
    },
    {
        "topic_description": "Chủ đề này sẽ cung cấp tổng quan về các phương pháp giảm thiểu overfitting, như Dropout và Batch Normalization. Học sinh sẽ cần hiểu các kỹ thuật này và ứng dụng của chúng trong việc cải thiện hiệu suất của mô hình học sâu.",
        "output_with_context": {
            "question": "Kỹ thuật nào trong học sâu giúp ngăn chặn overfitting bằng cách ngẫu nhiên \"tắt\" một số nơ-ron trong mỗi bước huấn luyện?",
            "answer": "Dropout",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Lựa Chọn Đặc Trưng & Tối Ưu Hóa Mô Hình\n```\n**Dấu hiệu:**\n- Large gap giữa curves\n- Training error tiếp tục giảm\n- Validation error không cải thiện\n\n**Giải pháp:**\n- Get more training data\n- Reduce model complexity\n- Increase regularization\n- Feature selection\n- Dropout, early stopping\n\n**3. Good Fit:**\n```\nTraining error: Thấp\nValidation error: Thấp\nGap: Nhỏ\nBoth converge\n```\n**Dấu hiệu:**\n- Small gap\n- Both errors low\n- Converged performance\n\n**4. More Data Helps:**\n```\nValidation error giảm khi tăng data\nGap đang đóng lại\nChưa plateau\n```\n**Hành động:** Get more data!\n\n**5. More Data Doesn't Help:**\n```\nBoth curves plateau\nAdding data không cải thiện\n```\n**Hành động:** Improve features hoặc model\n\n### Bias-Variance Tradeoff (Sự Đánh Đổi Bias-Variance)\n\n**Công thức:**\n$$Expected\\ Error = Bias^2 + Variance + Irreducible\\ Error$$\n\n**Bias (Thiên Lệch):**\n- Error từ giả định đơn giản hóa\n- Underfitting\n- Model không capture được patterns\n- High bias → Systematic errors\n\n**Variance (Phương Sai):**\n- Error từ sensitivity to training data\n- Overfitting\n- Model learns noise\n- High variance → Different results với different data\n\n**Irreducible Error:**\n- Noise trong data\n- Không thể giảm\n- Comes from data collection\n\n**Tradeoff:**\n- Decrease bias → Increase variance\n- Decrease variance → Increase bias\n- Cần balance\n\n**Strategies:**\n\n**Giảm High Bias:**\n1. Increase model complexity\n2. Add more features/polynomial features\n3. Decrease regularization\n4. Train longer\n5. Use ensemble methods\n\n**Giảm High Variance:**\n1. Get more training data\n2. Reduce model complexity\n3. Increase regularization (L1, L2, dropout)\n4. Feature selection\n5. Early stopping\n6. Ensemble methods (bagging)\n\n**Sweet Spot:**\n- Minimize total error\n- Balance bias và variance\n- Depends on problem và data\n\n**Visualize:**\n```\nTotal Error\n    |     \\\n    |      \\___Bias²\n    |___________\\\n    |            \\___\n    |Variance_____\\___Total\n    |________________\\___\n    |___________________\\___\n    +----------------------->\n    Simple          Complex\n            Model Complexity\n```\n\n### Phương Pháp Ensemble\n\nKết hợp nhiều models để cải thiện hiệu suất.\n\n**\"Wisdom of crowds\"**\n\n**Tại sao hoạt động:**\n- Errors của individual models cancel out\n- Diverse models capture different patterns\n- Reduce variance\n- More robust\n\n**1. Bagging (Bootstrap Aggregating):**\n\n**Nguyên lý:**\n- Train multiple models trên bootstrap samples\n- Average predictions (regression) hoặc vote (classification)\n\n**Bootstrap Sampling:**\n- Sample with replacement\n- Same size as original\n- ~63% unique samples mỗi bootstrap\n\n**Thuật toán:**\n1. For i = 1 to M:\n   - Create bootstrap sample $D_i$\n   - Train model $M_i$ on $D_i$\n2. Combine:\n   - Regression: $\\hat{y} = \frac{1}{M}\\sum_{i=1}^{M}M_i(x)$\n   - Classification: Majority vote\n\n**Ưu điểm:**\n- Reduce variance\n- Parallel training\n- Works với high-variance models\n\n**Nhược điểm:**\n- Không giảm bias\n- Có thể chậm (many models)\n\n**Ví dụ:** Random Forest\n\n**2. Boosting:**\n\n**Nguyên lý:**\n- Sequential training\n- Each model corrects errors của previous models\n- Weighted combination\n\n**Thuật toán (general):**\n1. Initialize equal weights\n2. For i = 1 to M:\n   - Train model $M_i$ on weighted data\n   - Tính error\n   - Update weights (increase for misclassified)\n\n**Các khái niệm quan trọng:**\n- Overfitting (học quá mức/quá khớp) là một vấn đề phổ biến trong học máy khi mô hình học quá sát với dữ liệu huấn luyện, bao gồm cả nhiễu, các mẫu ngẫu nhiên, và các chi tiết không tổng quát. Điều này dẫn đến hiệu suất rất tốt (độ lỗi thấp, độ chính xác cao) trên tập huấn luyện nhưng lại kém (độ lỗi cao, độ chính xác thấp) trên dữ liệu mới, chưa từng thấy (tập kiểm tra/validation).\n\nCác dấu hiệu của overfitting bao gồm:\n- Độ lỗi thấp trên tập huấn luyện nhưng độ lỗi cao trên tập kiểm tra.\n- Khoảng cách lớn giữa learning curves của training và validation.\n- Training error tiếp tục giảm nhưng validation error không cải thiện hoặc thậm chí tăng.\n- Mô hình có high variance và generalization kém.\n\nCác mô hình phức tạp, đặc biệt là các mô hình có nhiều tham số như Deep Learning, Transformers, hoặc Cây Quyết định quá sâu/phức tạp, có nguy cơ cao bị overfitting nếu không có đủ dữ liệu để huấn luyện tốt.\n\nĐể ngăn chặn overfitting, các kỹ thuật sau thường được sử dụng:\n- **Regularization**: Bao gồm L1 Regularization (Lasso), L2 Regularization, và Dropout, giúp đơn giản hóa mô hình và khuyến khích tính tổng quát hóa tốt hơn.\n- **Early Stopping**: Dừng quá trình huấn luyện khi hiệu suất trên tập validation bắt đầu giảm hoặc không cải thiện.\n- **Data Augmentation**: Tăng cường dữ liệu huấn luyện bằng cách tạo ra các biến thể của dữ liệu hiện có.\n- **Giảm độ phức tạp của mô hình**: Ví dụ, giảm số lượng đặc trưng, hoặc cắt tỉa (pruning) cây quyết định.\n- **Thêm dữ liệu huấn luyện**: Cung cấp nhiều dữ liệu hơn để mô hình học các mẫu tổng quát thay vì ghi nhớ nhiễu.\n- **Các kỹ thuật khác**: Pooling và Global Average Pooling cũng có thể giúp giảm overfitting.\n\nNgược lại, Underfitting (học dưới mức) là vấn đề khi mô hình quá đơn giản để nắm bắt các mẫu cơ bản trong dữ liệu, dẫn đến hiệu suất kém trên cả dữ liệu huấn luyện và dữ liệu mới. Underfitting liên quan đến High Bias, với dấu hiệu là training error cao và validation error cao, cùng với một khoảng cách nhỏ giữa chúng trên Learning Curves.\n- Dropout là một kỹ thuật regularization được giới thiệu bởi Hinton et al. (2012) và được sử dụng rộng rãi trong mạng nơ-ron, bao gồm cả kiến trúc như AlexNet và Discriminator, để ngăn chặn overfitting và giảm variance. Kỹ thuật này hoạt động bằng cách ngẫu nhiên \"tắt\" (đặt đầu ra về 0) một phần các nơ-ron cùng với các kết nối của chúng trong mỗi bước huấn luyện. Các nơ-ron bị tắt sẽ không tham gia vào quá trình forward pass và backpropagation. Tỷ lệ dropout (xác suất p) thường dao động từ 0.2 đến 0.5, với 0.5 cho các lớp Fully Connected và 0.2-0.3 cho các lớp khác. Việc này buộc mạng phải học các biểu diễn mạnh mẽ hơn, ít phụ thuộc vào bất kỳ nơ-ron cụ thể nào, ngăn chặn sự thích nghi lẫn nhau (co-adaptation) giữa các nơ-ron, tạo hiệu ứng ensemble và thêm nhiễu, từ đó cải thiện khả năng tổng quát hóa và làm cho mô hình trở nên robust hơn. Trong quá trình kiểm tra (test time), tất cả các nơ-ron được sử dụng nhưng đầu ra của chúng được scale với (1-p).\n\n**Mối quan hệ:**\n- Dropout là một kỹ thuật ngăn chặn Overfitting bằng cách ngẫu nhiên tắt các nơ-ron trong quá trình huấn luyện, buộc mạng phải học các biểu diễn mạnh mẽ hơn.\n- Dropout ngăn chặn Overfitting bằng cách buộc mạng học các biểu diễn dự phòng và giảm sự đồng thích nghi giữa các neuron, từ đó cải thiện khả năng tổng quát hóa của mô hình.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n  - Exponential decay: Giảm dần liên tục\n  - Cosine annealing: Smooth decrease\n  - ReduceLROnPlateau: Giảm khi validation loss plateau\n\n**Batch Size:**\n- **Larger batches (128-512):**\n  - Pros: Faster training (parallelization), stable gradients\n  - Cons: Cần nhiều memory, có thể generalize kém hơn\n- **Smaller batches (32-64):**\n  - Pros: Regularization effect, better generalization, ít memory\n  - Cons: Noisy gradients, train chậm hơn\n- **Rule of thumb:** Bắt đầu 32-64, tăng nếu có memory\n\n**Number of Epochs:**\n- Start với nhiều epochs, dùng early stopping\n- Monitor validation loss\n\n**Network Architecture:**\n- Number of layers: Start shallow, thêm depth nếu underfitting\n- Number of neurons: Enough capacity nhưng không quá\n- Dropout rate: 0.2-0.5\n\n**Regularization Strength:**\n- Weight decay (L2): 1e-4 hoặc 1e-5\n- Dropout: 0.5 for FC, 0.2-0.3 for others\n\n**Hyperparameter Tuning Strategies:**\n- **Manual tuning:** Hiểu behavior, instructive\n- **Grid search:** Exhaustive nhưng expensive\n- **Random search:** Often better than grid\n- **Bayesian optimization:** Intelligent search (Optuna, Hyperopt)\n- **Learning rate first!** Tune LR trước, sau đó others\n\n**4. Tips Trong Training:**\n\n**Monitor Metrics:**\n- **Training loss:** Phải giảm\n- **Validation loss:** Quan trọng nhất, measure generalization\n- **Train/val gap:** Lớn → overfitting\n- **Accuracy, F1, etc.:** Task-specific metrics\n\n**Visualization:**\n- Plot loss curves (train vs validation)\n- Learning rate vs loss\n- Gradient norms\n- Activation distributions\n- Attention weights (interpretability)\n\n**Learning Rate Scheduling:**\n- Essential cho converge tốt\n- Warmup (increase gradually) cho transformers\n- Decay về cuối training\n\n**Gradient Clipping:**\n- **Critical for RNNs** (prevent exploding gradients)\n- Useful cho transformers\n- Typical value: 1.0 hoặc 5.0\n\n**Checkpoint Best Models:**\n- Save model với best validation metric\n- Checkpoint mỗi N epochs hoặc khi improve\n- Resume training nếu crash\n\n**Early Stopping:**\n- Stop nếu val loss không improve sau N epochs (patience=10-20)\n- Saves time, prevents overfitting\n\n**Mixed Precision Training:**\n- FP16 thay vì FP32\n- **2×** faster, ít memory hơn\n- Nvidia GPUs (Tensor Cores)\n\n**5. Debugging Deep Learning Models:**\n\n**Start Small:**\n- **Overfit một batch nhỏ:** Nếu không overfit được → bug trong model/training\n- Nếu overfit được → model có capacity, issue là generalization\n\n**Check Data:**\n- Visualize samples: Correct labels? Reasonable augmentation?\n- Check data pipeline: No bugs?\n- Shuffle properly?\n\n**Check Gradients:**\n- Vanishing gradients: Gradients → 0, không học được\n  - Solutions: Change activation (ReLU), batch norm, residual connections\n- Exploding gradients: Gradients → ∞\n  - Solutions: Gradient clipping, lower learning rate\n- Use gradient visualization tools\n\n**Sanity Checks:**\n- Disable regularization (dropout, weight decay): Should overfit\n- Train trên random labels: Should fit (báo model có capacity)\n- Check loss: Không phải NaN, không explode\n\n**Common Issues:**\n\n| Symptom | Possible Cause | Solution |\n|---------|---------------|----------|\n| Loss is NaN | Learning rate too high, numerical instability | Lower LR, gradient clipping, check data |\n| Loss không giảm | Learning rate too low, bad initialization | Increase LR, check data/labels |\n| Train loss giảm, val loss không giảm | Overfitting | Regularization, data augmentation, smaller model |\n| Both losses cao | Underfitting | Bigger model, more layers, train longer |\n| Loss oscillates | Batch size too small, LR too high | Increase batch size, lower LR |\n\n**Use Tools:**\n\n**Các khái niệm quan trọng:**\n- Dropout là một kỹ thuật regularization được giới thiệu bởi Hinton et al. (2012) và được sử dụng rộng rãi trong mạng nơ-ron, bao gồm cả kiến trúc như AlexNet và Discriminator, để ngăn chặn overfitting và giảm variance. Kỹ thuật này hoạt động bằng cách ngẫu nhiên \"tắt\" (đặt đầu ra về 0) một phần các nơ-ron cùng với các kết nối của chúng trong mỗi bước huấn luyện. Các nơ-ron bị tắt sẽ không tham gia vào quá trình forward pass và backpropagation. Tỷ lệ dropout (xác suất p) thường dao động từ 0.2 đến 0.5, với 0.5 cho các lớp Fully Connected và 0.2-0.3 cho các lớp khác. Việc này buộc mạng phải học các biểu diễn mạnh mẽ hơn, ít phụ thuộc vào bất kỳ nơ-ron cụ thể nào, ngăn chặn sự thích nghi lẫn nhau (co-adaptation) giữa các nơ-ron, tạo hiệu ứng ensemble và thêm nhiễu, từ đó cải thiện khả năng tổng quát hóa và làm cho mô hình trở nên robust hơn. Trong quá trình kiểm tra (test time), tất cả các nơ-ron được sử dụng nhưng đầu ra của chúng được scale với (1-p).\n\n**Mối quan hệ:**\n- Dropout là một kỹ thuật ngăn chặn Overfitting bằng cách ngẫu nhiên tắt các nơ-ron trong quá trình huấn luyện, buộc mạng phải học các biểu diễn mạnh mẽ hơn.\n- Dropout ngăn chặn Overfitting bằng cách buộc mạng học các biểu diễn dự phòng và giảm sự đồng thích nghi giữa các neuron, từ đó cải thiện khả năng tổng quát hóa của mô hình.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n| | L1 | L2 |\n|---|---|---|\n| Sparsity | Có (nhiều weights = 0) | Không |\n| Feature selection | Có | Không |\n| Gradient | Không trơn tại 0 | Trơn mọi nơi |\n| Sử dụng | Nhiều features, cần sparse | Default choice |\n\n**2. Dropout:**\n\nKỹ thuật cực kỳ hiệu quả được giới thiệu bởi Hinton et al. (2012).\n\n**Cơ chế:**\n- Trong mỗi iteration training, **ngẫu nhiên \"tắt\" (drop)** một số nơ-ron\n- Mỗi nơ-ron có xác suất $p$ bị drop (thường $p = 0.2 - 0.5$)\n- Nơ-ron bị drop không tham gia forward và backward pass\n- Tại test time, sử dụng tất cả nơ-ron nhưng scale output với $(1-p)$\n\n**Tại sao hiệu quả:**\n- **Ngăn co-adaptation:** Nơ-ron không thể phụ thuộc quá nhiều vào nơ-ron cụ thể khác\n- **Ensemble effect:** Mỗi iteration training một mạng con khác nhau → giống train nhiều mô hình\n- **Noise injection:** Thêm noise vào training → mô hình robust hơn\n\n**Cách sử dụng:**\n- Áp dụng cho fully connected layers (không dùng cho convolutional layers)\n- Hidden layers: dropout rate 0.5\n- Input layer: dropout rate thấp hơn (0.2) nếu dùng\n- **Không** sử dụng dropout trong test/inference\n\n**Inverted Dropout:**\n```python\n# Training\nmask = (np.random.rand(*shape) > dropout_rate) / (1 - dropout_rate)\noutput = input * mask\n\n# Testing: không cần scale\noutput = input\n```\n\n**Biến thể:**\n- **DropConnect:** Drop connections thay vì neurons\n- **Spatial Dropout:** Drop entire feature maps trong CNN\n- **Variational Dropout:** Dropout mask giống nhau qua time steps trong RNN\n\n**3. Early Stopping:**\n\nDừng training khi validation loss bắt đầu tăng.\n\n**Cách thực hiện:**\n1. Chia data thành train/validation/test\n2. Theo dõi validation loss sau mỗi epoch\n3. Lưu model có validation loss tốt nhất\n4. Dừng nếu validation loss không giảm sau $n$ epochs (patience)\n\n**Ưu điểm:**\n- Đơn giản, hiệu quả\n- Tự động tìm số epoch tối ưu\n- Không cần thêm hyperparameter phức tạp\n\n**Lưu ý:**\n- Cần validation set riêng (không dùng test set!)\n- Patience thường 10-20 epochs\n- Có thể kết hợp với learning rate scheduling\n\n**4. Data Augmentation:**\n\nTăng cường dữ liệu training bằng cách tạo biến thể từ dữ liệu gốc.\n\n**Cho ảnh (Computer Vision):**\n- **Geometric transformations:**\n  - Rotation (xoay): ±15-30 độ\n  - Flip (lật): Horizontal, vertical\n  - Crop (cắt): Random crop, center crop\n  - Zoom: Scale in/out\n  - Translation (dịch chuyển)\n  - Shearing (nghiêng)\n  \n- **Color transformations:**\n  - Brightness adjustment (độ sáng)\n  - Contrast adjustment (độ tương phản)\n  - Saturation adjustment (độ bão hòa)\n  - Hue adjustment (sắc màu)\n  - RGB channel shifts\n  \n- **Noise injection:**\n  - Gaussian noise\n  - Salt and pepper noise\n  - Blur (làm mờ)\n  \n- **Advanced techniques:**\n  - **Mixup:** Trộn hai ảnh: $x = \\lambda x_1 + (1-\\lambda)x_2$\n\n**Các khái niệm quan trọng:**\n- Dropout là một kỹ thuật regularization được giới thiệu bởi Hinton et al. (2012) và được sử dụng rộng rãi trong mạng nơ-ron, bao gồm cả kiến trúc như AlexNet và Discriminator, để ngăn chặn overfitting và giảm variance. Kỹ thuật này hoạt động bằng cách ngẫu nhiên \"tắt\" (đặt đầu ra về 0) một phần các nơ-ron cùng với các kết nối của chúng trong mỗi bước huấn luyện. Các nơ-ron bị tắt sẽ không tham gia vào quá trình forward pass và backpropagation. Tỷ lệ dropout (xác suất p) thường dao động từ 0.2 đến 0.5, với 0.5 cho các lớp Fully Connected và 0.2-0.3 cho các lớp khác. Việc này buộc mạng phải học các biểu diễn mạnh mẽ hơn, ít phụ thuộc vào bất kỳ nơ-ron cụ thể nào, ngăn chặn sự thích nghi lẫn nhau (co-adaptation) giữa các nơ-ron, tạo hiệu ứng ensemble và thêm nhiễu, từ đó cải thiện khả năng tổng quát hóa và làm cho mô hình trở nên robust hơn. Trong quá trình kiểm tra (test time), tất cả các nơ-ron được sử dụng nhưng đầu ra của chúng được scale với (1-p).\n- Overfitting (học quá mức/quá khớp) là một vấn đề phổ biến trong học máy khi mô hình học quá sát với dữ liệu huấn luyện, bao gồm cả nhiễu, các mẫu ngẫu nhiên, và các chi tiết không tổng quát. Điều này dẫn đến hiệu suất rất tốt (độ lỗi thấp, độ chính xác cao) trên tập huấn luyện nhưng lại kém (độ lỗi cao, độ chính xác thấp) trên dữ liệu mới, chưa từng thấy (tập kiểm tra/validation).\n\nCác dấu hiệu của overfitting bao gồm:\n- Độ lỗi thấp trên tập huấn luyện nhưng độ lỗi cao trên tập kiểm tra.\n- Khoảng cách lớn giữa learning curves của training và validation.\n- Training error tiếp tục giảm nhưng validation error không cải thiện hoặc thậm chí tăng.\n- Mô hình có high variance và generalization kém.\n\nCác mô hình phức tạp, đặc biệt là các mô hình có nhiều tham số như Deep Learning, Transformers, hoặc Cây Quyết định quá sâu/phức tạp, có nguy cơ cao bị overfitting nếu không có đủ dữ liệu để huấn luyện tốt.\n\nĐể ngăn chặn overfitting, các kỹ thuật sau thường được sử dụng:\n- **Regularization**: Bao gồm L1 Regularization (Lasso), L2 Regularization, và Dropout, giúp đơn giản hóa mô hình và khuyến khích tính tổng quát hóa tốt hơn.\n- **Early Stopping**: Dừng quá trình huấn luyện khi hiệu suất trên tập validation bắt đầu giảm hoặc không cải thiện.\n- **Data Augmentation**: Tăng cường dữ liệu huấn luyện bằng cách tạo ra các biến thể của dữ liệu hiện có.\n- **Giảm độ phức tạp của mô hình**: Ví dụ, giảm số lượng đặc trưng, hoặc cắt tỉa (pruning) cây quyết định.\n- **Thêm dữ liệu huấn luyện**: Cung cấp nhiều dữ liệu hơn để mô hình học các mẫu tổng quát thay vì ghi nhớ nhiễu.\n- **Các kỹ thuật khác**: Pooling và Global Average Pooling cũng có thể giúp giảm overfitting.\n\nNgược lại, Underfitting (học dưới mức) là vấn đề khi mô hình quá đơn giản để nắm bắt các mẫu cơ bản trong dữ liệu, dẫn đến hiệu suất kém trên cả dữ liệu huấn luyện và dữ liệu mới. Underfitting liên quan đến High Bias, với dấu hiệu là training error cao và validation error cao, cùng với một khoảng cách nhỏ giữa chúng trên Learning Curves.\n\n**Mối quan hệ:**\n- Dropout là một kỹ thuật ngăn chặn Overfitting bằng cách ngẫu nhiên tắt các nơ-ron trong quá trình huấn luyện, buộc mạng phải học các biểu diễn mạnh mẽ hơn.\n- Batch Normalization ngăn chặn overfitting thông qua tác dụng regularization, có thể giảm nhu cầu sử dụng dropout.\n- Dropout ngăn chặn Overfitting bằng cách buộc mạng học các biểu diễn dự phòng và giảm sự đồng thích nghi giữa các neuron, từ đó cải thiện khả năng tổng quát hóa của mô hình.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Học Sâu (Deep Learning)\n  - **CutMix:** Cắt và dán vùng từ ảnh khác\n  - **CutOut:** Che ngẫu nhiên một vùng của ảnh\n  - **AutoAugment:** Tự động học policy augmentation tốt nhất\n\n**Cho văn bản (NLP):**\n- Synonym replacement (thay từ đồng nghĩa)\n- Random insertion/deletion\n- Back-translation (dịch qua lại)\n- Paraphrasing\n\n**Cho âm thanh:**\n- Time stretching\n- Pitch shifting\n- Adding noise\n- Time masking, frequency masking\n\n**Lưu ý:**\n- Augmentation phải giữ nguyên nhãn\n- Cần domain knowledge (ví dụ: không flip chữ số)\n- On-the-fly augmentation trong training (không pre-generate)\n\n**5. Batch Normalization:**\n\nKỹ thuật chuẩn hóa activation của mỗi layer, được giới thiệu bởi Ioffe & Szegedy (2015).\n\n**Công thức:**\n\nVới một mini-batch:\n$$\\mu_B = \frac{1}{m}\\sum_{i=1}^{m}x_i \\quad \text{(mean của batch)}$$\n$$\\sigma_B^2 = \frac{1}{m}\\sum_{i=1}^{m}(x_i - \\mu_B)^2 \\quad \text{(variance của batch)}$$\n\n**Normalize:**\n$$\\hat{x}_i = \frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n\n**Scale and shift** (learnable parameters):\n$$y_i = \\gamma\\hat{x}_i + \beta$$\n\n**Tại sao hiệu quả:**\n- **Giảm Internal Covariate Shift:** Phân phối input của mỗi layer ổn định hơn\n- **Cho phép learning rate cao hơn:** Training nhanh hơn 5-10 lần\n- **Regularization effect:** Thêm noise từ batch statistics\n- **Giảm nhạy cảm với weight initialization**\n\n**Vị trí đặt:**\n- Thường đặt **sau** linear/conv layer, **trước** activation function\n- Có thể đặt sau activation (ít phổ biến hơn)\n\n**Ưu điểm:**\n- Tăng tốc training đáng kể\n- Cho phép sử dụng activation functions có vanishing gradient (sigmoid, tanh)\n- Giảm overfitting (có thể giảm dropout)\n- Mô hình ít nhạy cảm với hyperparameters\n\n**Nhược điểm:**\n- **Phụ thuộc batch size:** Không hoạt động tốt với batch nhỏ\n- Khác nhau giữa training và inference (cần lưu running statistics)\n- Không phù hợp với RNNs\n- Thêm computational cost\n\n**Inference time:**\nSử dụng moving average của mean và variance từ training:\n$$\\mu_{test} = E[\\mu_B], \\quad \\sigma_{test}^2 = E[\\sigma_B^2]$$\n\n**6. Layer Normalization:**\n\nThay thế cho Batch Normalization, chuẩn hóa theo features thay vì batch.\n\n**Công thức:**\nVới một sample, normalize across features:\n$$\\mu = \frac{1}{H}\\sum_{i=1}^{H}x_i$$\n$$\\sigma^2 = \frac{1}{H}\\sum_{i=1}^{H}(x_i - \\mu)^2$$\n$$\\hat{x}_i = \frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n$$y_i = \\gamma\\hat{x}_i + \beta$$\n\n**Ưu điểm so với Batch Norm:**\n- **Không phụ thuộc batch size** → hoạt động với batch size = 1\n- **Giống nhau giữa training và inference** → không cần lưu running statistics\n- **Phù hợp với RNNs** và sequence models\n- Hoạt động tốt với Transformers\n\n\n**Các khái niệm quan trọng:**\n- Overfitting (học quá mức/quá khớp) là một vấn đề phổ biến trong học máy khi mô hình học quá sát với dữ liệu huấn luyện, bao gồm cả nhiễu, các mẫu ngẫu nhiên, và các chi tiết không tổng quát. Điều này dẫn đến hiệu suất rất tốt (độ lỗi thấp, độ chính xác cao) trên tập huấn luyện nhưng lại kém (độ lỗi cao, độ chính xác thấp) trên dữ liệu mới, chưa từng thấy (tập kiểm tra/validation).\n\nCác dấu hiệu của overfitting bao gồm:\n- Độ lỗi thấp trên tập huấn luyện nhưng độ lỗi cao trên tập kiểm tra.\n- Khoảng cách lớn giữa learning curves của training và validation.\n- Training error tiếp tục giảm nhưng validation error không cải thiện hoặc thậm chí tăng.\n- Mô hình có high variance và generalization kém.\n\nCác mô hình phức tạp, đặc biệt là các mô hình có nhiều tham số như Deep Learning, Transformers, hoặc Cây Quyết định quá sâu/phức tạp, có nguy cơ cao bị overfitting nếu không có đủ dữ liệu để huấn luyện tốt.\n\nĐể ngăn chặn overfitting, các kỹ thuật sau thường được sử dụng:\n- **Regularization**: Bao gồm L1 Regularization (Lasso), L2 Regularization, và Dropout, giúp đơn giản hóa mô hình và khuyến khích tính tổng quát hóa tốt hơn.\n- **Early Stopping**: Dừng quá trình huấn luyện khi hiệu suất trên tập validation bắt đầu giảm hoặc không cải thiện.\n- **Data Augmentation**: Tăng cường dữ liệu huấn luyện bằng cách tạo ra các biến thể của dữ liệu hiện có.\n- **Giảm độ phức tạp của mô hình**: Ví dụ, giảm số lượng đặc trưng, hoặc cắt tỉa (pruning) cây quyết định.\n- **Thêm dữ liệu huấn luyện**: Cung cấp nhiều dữ liệu hơn để mô hình học các mẫu tổng quát thay vì ghi nhớ nhiễu.\n- **Các kỹ thuật khác**: Pooling và Global Average Pooling cũng có thể giúp giảm overfitting.\n\nNgược lại, Underfitting (học dưới mức) là vấn đề khi mô hình quá đơn giản để nắm bắt các mẫu cơ bản trong dữ liệu, dẫn đến hiệu suất kém trên cả dữ liệu huấn luyện và dữ liệu mới. Underfitting liên quan đến High Bias, với dấu hiệu là training error cao và validation error cao, cùng với một khoảng cách nhỏ giữa chúng trên Learning Curves.\n\n**Mối quan hệ:**\n- Batch Normalization ngăn chặn overfitting thông qua tác dụng regularization, có thể giảm nhu cầu sử dụng dropout."
        },
        "output_without_context": {
            "question": "Kỹ thuật nào trong học sâu giúp giảm thiểu overfitting bằng cách ngẫu nhiên bỏ qua một số nút trong mạng nơ-ron trong quá trình huấn luyện?",
            "answer": "Dropout"
        }
    },
    {
        "topic_description": "Chủ đề này tích hợp các khái niệm từ tuần 5 và tuần 8 để cùng xem xét các kỹ thuật bảo vệ nhân mẫu như Early Stopping và Cross-Validation. Học sinh sẽ được đánh giá về khả năng áp dụng các kỹ thuật này để tối ưu hóa mô hình và tránh overfitting trong học sâu.",
        "output_with_context": {
            "question": "Kỹ thuật nào trong học máy được sử dụng để ngăn chặn overfitting bằng cách dừng quá trình huấn luyện khi hiệu suất trên tập kiểm định bắt đầu giảm?",
            "answer": "Early Stopping",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Lựa Chọn Đặc Trưng & Tối Ưu Hóa Mô Hình\n- CV cho stable estimate\n\n**1. K-Fold Cross-Validation:**\n\n**Thuật toán:**\n1. Chia data thành k folds\n2. For i = 1 to k:\n   - Use fold i làm validation\n   - Use k-1 folds còn lại làm training\n   - Train và evaluate\n3. Average metrics across k folds\n\n**Chọn k:**\n- k=5: Standard, good balance\n- k=10: More stable, more computational\n- Larger k: Less bias, more variance, more expensive\n\n**Ưu điểm:**\n- Sử dụng toàn bộ data\n- Stable estimate\n- Reduce variance\n\n**Nhược điểm:**\n- k lần training (expensive)\n- Có thể chậm\n\n**2. Stratified K-Fold:**\n\n**Nguyên lý:**\n- Maintain class distribution trong mỗi fold\n- Each fold representative\n\n**Khi nào dùng:**\n- Imbalanced datasets\n- Classification tasks\n- Đảm bảo mỗi fold có đủ samples mỗi class\n\n**Ưu điểm:**\n- Fair evaluation với imbalanced data\n- Consistent class proportions\n\n**3. Leave-One-Out (LOO):**\n\n**Nguyên lý:**\n- k = n (n = số samples)\n- Mỗi sample là một fold\n\n**Ưu điểm:**\n- Maximum data cho training\n- No randomness\n- Deterministic\n\n**Nhược điểm:**\n- Rất chậm (n iterations)\n- High variance\n- Chỉ khả thi với small datasets (< 1000)\n\n**Khi nào dùng:**\n- Very small datasets\n- Need maximum training data\n- Computational resources available\n\n**4. Time Series Cross-Validation:**\n\n**Nguyên lý:**\n- Respect temporal order\n- Train on past, validate on future\n- No data leakage from future\n\n**Expanding Window:**\n```\nFold 1: Train [1:100] → Test [101:120]\nFold 2: Train [1:120] → Test [121:140]\nFold 3: Train [1:140] → Test [141:160]\n```\n\n**Rolling Window:**\n```\nFold 1: Train [1:100] → Test [101:120]\nFold 2: Train [21:120] → Test [121:140]\nFold 3: Train [41:140] → Test [141:160]\n```\n\n**Quan trọng:**\n- **KHÔNG shuffle data**\n- Maintain temporal order\n- Avoid look-ahead bias\n\n**5. Nested Cross-Validation:**\n\n**Nguyên lý:**\n- Outer loop: Model evaluation\n- Inner loop: Hyperparameter tuning\n- Prevents overfitting in parameter selection\n\n**Structure:**\n```\nOuter CV (5-fold):\n  For each outer fold:\n    Inner CV (5-fold):\n      Hyperparameter tuning\n    Train with best params\n    Evaluate on outer fold\n```\n\n**Ưu điểm:**\n- Unbiased performance estimate\n- Proper hyperparameter tuning\n- Gold standard\n\n**Nhược điểm:**\n- Very expensive (k_outer × k_inner trainings)\n- Overkill cho simple problems\n\n**Khi nào dùng:**\n- Need unbiased estimate\n- Publishing results\n- Critical applications\n- Have computational resources\n\n### Learning Curves (Đường Cong Học)\n\nPhân tích hiệu suất mô hình vs kích thước training set.\n\n**Vẽ gì:**\n- X-axis: Training set size\n- Y-axis: Error (hoặc Score)\n- Two curves: Training error & Validation error\n\n**Chẩn Đoán:**\n\n**1. High Bias (Underfitting):**\n```\nTraining error: Cao\nValidation error: Cao\nGap: Nhỏ\nBoth plateau at high error\n```\n**Dấu hiệu:**\n- Cả hai curves plateau\n- Performance kém ngay cả với nhiều data\n- Thêm data không giúp\n\n**Giải pháp:**\n- Increase model complexity\n- Add features\n- Reduce regularization\n- Try complex model\n\n**2. High Variance (Overfitting):**\n```\nTraining error: Thấp\nValidation error: Cao\nGap: Lớn\nGap doesn't close with more data\n\n**Các khái niệm quan trọng:**\n- Nested Cross-Validation là một phương pháp đánh giá mô hình mạnh mẽ bao gồm hai vòng lặp cross-validation lồng vào nhau. Vòng lặp bên ngoài dùng để đánh giá hiệu suất mô hình một cách không thiên vị, trong khi vòng lặp bên trong dùng để tinh chỉnh siêu tham số (hyperparameter tuning). Kỹ thuật này ngăn chặn overfitting trong quá trình lựa chọn tham số và cung cấp ước lượng hiệu suất đáng tin cậy hơn.\n\n**Mối quan hệ:**\n- Nested Cross-Validation chứa một vòng lặp bên trong dành riêng cho việc tinh chỉnh siêu tham số (Hyperparameter tuning) để tìm ra các tham số tối ưu cho mô hình.\n- Nested Cross-Validation ngăn chặn overfitting trong quá trình lựa chọn tham số bằng cách tách biệt quá trình tinh chỉnh siêu tham số khỏi quá trình đánh giá hiệu suất cuối cùng của mô hình.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Lựa Chọn Đặc Trưng & Tối Ưu Hóa Mô Hình\n```\n**Dấu hiệu:**\n- Large gap giữa curves\n- Training error tiếp tục giảm\n- Validation error không cải thiện\n\n**Giải pháp:**\n- Get more training data\n- Reduce model complexity\n- Increase regularization\n- Feature selection\n- Dropout, early stopping\n\n**3. Good Fit:**\n```\nTraining error: Thấp\nValidation error: Thấp\nGap: Nhỏ\nBoth converge\n```\n**Dấu hiệu:**\n- Small gap\n- Both errors low\n- Converged performance\n\n**4. More Data Helps:**\n```\nValidation error giảm khi tăng data\nGap đang đóng lại\nChưa plateau\n```\n**Hành động:** Get more data!\n\n**5. More Data Doesn't Help:**\n```\nBoth curves plateau\nAdding data không cải thiện\n```\n**Hành động:** Improve features hoặc model\n\n### Bias-Variance Tradeoff (Sự Đánh Đổi Bias-Variance)\n\n**Công thức:**\n$$Expected\\ Error = Bias^2 + Variance + Irreducible\\ Error$$\n\n**Bias (Thiên Lệch):**\n- Error từ giả định đơn giản hóa\n- Underfitting\n- Model không capture được patterns\n- High bias → Systematic errors\n\n**Variance (Phương Sai):**\n- Error từ sensitivity to training data\n- Overfitting\n- Model learns noise\n- High variance → Different results với different data\n\n**Irreducible Error:**\n- Noise trong data\n- Không thể giảm\n- Comes from data collection\n\n**Tradeoff:**\n- Decrease bias → Increase variance\n- Decrease variance → Increase bias\n- Cần balance\n\n**Strategies:**\n\n**Giảm High Bias:**\n1. Increase model complexity\n2. Add more features/polynomial features\n3. Decrease regularization\n4. Train longer\n5. Use ensemble methods\n\n**Giảm High Variance:**\n1. Get more training data\n2. Reduce model complexity\n3. Increase regularization (L1, L2, dropout)\n4. Feature selection\n5. Early stopping\n6. Ensemble methods (bagging)\n\n**Sweet Spot:**\n- Minimize total error\n- Balance bias và variance\n- Depends on problem và data\n\n**Visualize:**\n```\nTotal Error\n    |     \\\n    |      \\___Bias²\n    |___________\\\n    |            \\___\n    |Variance_____\\___Total\n    |________________\\___\n    |___________________\\___\n    +----------------------->\n    Simple          Complex\n            Model Complexity\n```\n\n### Phương Pháp Ensemble\n\nKết hợp nhiều models để cải thiện hiệu suất.\n\n**\"Wisdom of crowds\"**\n\n**Tại sao hoạt động:**\n- Errors của individual models cancel out\n- Diverse models capture different patterns\n- Reduce variance\n- More robust\n\n**1. Bagging (Bootstrap Aggregating):**\n\n**Nguyên lý:**\n- Train multiple models trên bootstrap samples\n- Average predictions (regression) hoặc vote (classification)\n\n**Bootstrap Sampling:**\n- Sample with replacement\n- Same size as original\n- ~63% unique samples mỗi bootstrap\n\n**Thuật toán:**\n1. For i = 1 to M:\n   - Create bootstrap sample $D_i$\n   - Train model $M_i$ on $D_i$\n2. Combine:\n   - Regression: $\\hat{y} = \frac{1}{M}\\sum_{i=1}^{M}M_i(x)$\n   - Classification: Majority vote\n\n**Ưu điểm:**\n- Reduce variance\n- Parallel training\n- Works với high-variance models\n\n**Nhược điểm:**\n- Không giảm bias\n- Có thể chậm (many models)\n\n**Ví dụ:** Random Forest\n\n**2. Boosting:**\n\n**Nguyên lý:**\n- Sequential training\n- Each model corrects errors của previous models\n- Weighted combination\n\n**Thuật toán (general):**\n1. Initialize equal weights\n2. For i = 1 to M:\n   - Train model $M_i$ on weighted data\n   - Tính error\n   - Update weights (increase for misclassified)\n\n**Các khái niệm quan trọng:**\n- Early Stopping là một kỹ thuật regularization và tối ưu hóa hiệu quả, được sử dụng rộng rãi trong các thuật toán lặp (như gradient descent, boosting) để ngăn chặn overfitting. Kỹ thuật này hoạt động bằng cách theo dõi hiệu suất của mô hình trên tập dữ liệu kiểm định (validation set) sau mỗi epoch. Quá trình huấn luyện sẽ dừng lại khi hiệu suất trên tập validation bắt đầu giảm hoặc không cải thiện trong một số epoch nhất định (được xác định bởi tham số `patience`, ví dụ 10-20), ngay cả khi hiệu suất trên tập huấn luyện vẫn đang tăng hoặc training error vẫn đang giảm. Điều này báo hiệu rằng mô hình đang bắt đầu overfitting vào dữ liệu huấn luyện. Early Stopping giúp tìm ra điểm tối ưu giữa underfitting và overfitting, tự động điều chỉnh số epoch huấn luyện tối ưu, tiết kiệm thời gian và tài nguyên tính toán. Để triển khai, cần chia dữ liệu thành tập train/validation/test, theo dõi validation loss (hoặc một chỉ số hiệu suất khác) sau mỗi epoch, và lưu lại mô hình có hiệu suất validation tốt nhất. Mặc dù không có cơ chế Early Stopping trực tiếp trong SVM tiêu chuẩn, có thể giới hạn số lần lặp bằng tham số `max_iter`.\n\n**Mối quan hệ:**\n- Early stopping được áp dụng trên validation set để theo dõi hiệu suất và ngăn chặn overfitting.\n- Early stopping ngăn chặn Overfitting bằng cách dừng quá trình huấn luyện khi hiệu suất trên tập kiểm định bắt đầu giảm, ngay cả khi training error vẫn đang giảm."
        },
        "output_without_context": {
            "question": "Khi nào nên áp dụng kỹ thuật Early Stopping trong quá trình huấn luyện mô hình học sâu để tối ưu hóa hiệu suất và tránh overfitting?",
            "answer": "Khi mô hình bắt đầu có dấu hiệu giảm hiệu suất trên tập kiểm tra trong khi vẫn cải thiện trên tập huấn luyện."
        }
    }
]