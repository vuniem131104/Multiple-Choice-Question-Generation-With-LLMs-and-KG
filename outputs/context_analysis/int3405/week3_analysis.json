[
    {
        "topic_description": "Khảo sát về cây quyết định như một mô hình phân loại. Học sinh cần hiểu cấu trúc của cây quyết định, bao gồm các nút quyết định, nút gốc và nút lá. Các câu hỏi sẽ tập trung vào việc xác định các thành phần của cây quyết định và đưa ra ví dụ thực tế.",
        "output_with_context": {
            "question": "Cấu trúc của cây quyết định bao gồm các thành phần nào sau đây?",
            "answer": "Nút gốc, nút nội bộ, nhánh và nút lá.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n1. Xây dựng cây đầy đủ\n2. Tính $\\alpha$ cho mỗi subtree\n3. Loại bỏ subtree có $\\alpha$ nhỏ nhất (đóng góp ít nhất)\n4. Lặp lại cho đến khi còn gốc\n5. Chọn cây tối ưu bằng cross-validation\n\n**Ưu điểm:**\n- Thường cho kết quả tốt hơn pre-pruning\n- Không bỏ lỡ phân chia tốt\n\n**Nhược điểm:**\n- Tốn thời gian (xây dựng cây đầy đủ trước)\n- Phức tạp hơn\n\n### Feature Importance (Tầm Quan Trọng Đặc Trưng)\n\nCây quyết định tự động tính toán mức độ quan trọng của mỗi đặc trưng.\n\n**Công Thức:**\n$$Importance(f) = \\sum_{t \\in T} p(t) \\cdot \\Delta impurity(t, f)$$\n\nTrong đó:\n- $p(t)$ là tỷ lệ mẫu tại nút $t$\n- $\\Delta impurity(t, f)$ là giảm impurity khi phân chia theo đặc trưng $f$ tại nút $t$\n- Tổng trên tất cả nút sử dụng đặc trưng $f$\n\n**Diễn giải:**\n- Giá trị càng cao, đặc trưng càng quan trọng\n- Tổng tất cả importance = 1\n- Đặc trưng không xuất hiện có importance = 0\n\n**Ứng dụng:**\n- Feature selection\n- Hiểu mô hình\n- Phát hiện đặc trưng không cần thiết\n- Giải thích cho stakeholders\n\n**Lưu ý:**\n- Thiên vị về đặc trưng có nhiều giá trị unique\n- Đặc trưng tương quan cao có importance phân tán\n- Sử dụng permutation importance để khắc phục\n\n### Ưu Điểm Của Cây Quyết Định\n\n**1. Dễ hiểu và diễn giải:**\n- Trực quan, giống cách con người quyết định\n- Có thể vẽ và giải thích bằng lời\n- Không cần kiến thức thống kê sâu\n- Phù hợp cho business users\n\n**2. Yêu cầu ít tiền xử lý dữ liệu:**\n- Không cần feature scaling\n- Không cần one-hot encoding cho categorical\n- Xử lý được missing values (surrogate splits)\n- Không cần assumption về phân phối\n\n**3. Xử lý dữ liệu số và phân loại:**\n- Linh hoạt với nhiều loại đặc trưng\n- Không cần encoding phức tạp\n- Mixed data types\n\n**4. Non-parametric:**\n- Không giả định về phân phối dữ liệu\n- Linh hoạt với mọi dạng data\n- Không cần chọn hàm phân phối\n\n**5. Bắt được mối quan hệ phi tuyến:**\n- Decision boundary phức tạp\n- Tương tác giữa các đặc trưng\n- Không giới hạn bởi tuyến tính\n\n**6. Tính toán Feature Importance tự nhiên:**\n- Không cần phương pháp bên ngoài\n- Tích hợp trong thuật toán\n\n**7. Nhanh với dự đoán:**\n- Độ phức tạp: O(log n) với cây cân bằng\n- Hiệu quả cho production\n\n**Các khái niệm quan trọng:**\n- Cây Quyết Định là một mô hình học máy có giám sát đa năng, được sử dụng cho cả bài toán phân loại và hồi quy. Mô hình này hoạt động bằng cách học các quy tắc quyết định từ các đặc trưng của dữ liệu và biểu diễn chúng dưới dạng cấu trúc cây phân cấp để dự đoán giá trị mục tiêu. Cấu trúc cây bao gồm Nút Gốc, Nút Nội Bộ (mỗi nút đại diện cho một kiểm tra trên một thuộc tính), Nhánh (mỗi nhánh đại diện cho kết quả của kiểm tra), và Nút Lá (mỗi nút lá đại diện cho một nhãn lớp hoặc giá trị số). Quá trình xây dựng cây bao gồm việc phân chia dữ liệu dựa trên các đặc trưng để tối đa hóa sự giảm impurity. Cây Quyết Định có thể được xây dựng đầy đủ và sau đó được cắt tỉa (pruning) để chọn cây tối ưu bằng cross-validation. Ưu điểm của mô hình này là dễ hiểu, dễ diễn giải, yêu cầu ít tiền xử lý dữ liệu, xử lý được cả dữ liệu số và phân loại, là non-parametric, bắt được mối quan hệ phi tuyến, và tính toán Feature Importance tự nhiên. Tuy nhiên, việc xây dựng cây đầy đủ có thể tốn thời gian và phức tạp hơn so với pre-pruning.\n\n**Mối quan hệ:**\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Phân loại, gán nhãn lớp cho dữ liệu.\n- Cây Quyết Định chứa Nhánh, đại diện cho kết quả của các quyết định và kết nối các nút.\n- Cây Quyết Định chứa Nút Lá, nơi chứa dự đoán cuối cùng của mô hình.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n\n### Giới Thiệu Về Cây Quyết Định\n\nCây quyết định là thuật toán học có giám sát đa năng có thể thực hiện cả tác vụ phân loại và hồi quy. Chúng học các quy tắc quyết định từ các đặc trưng để dự đoán giá trị mục tiêu thông qua cấu trúc dạng cây.\n\n**Ứng dụng thực tế:**\n- Chẩn đoán y tế (chuỗi quyết định dựa trên triệu chứng)\n- Đánh giá rủi ro tín dụng\n- Dự đoán churn khách hàng\n- Phát hiện gian lận\n- Hệ thống chuyên gia\n- Phân loại email spam\n\n**Tại sao gọi là \"cây\":**\n- Cấu trúc phân cấp giống cây ngược\n- Gốc ở trên, lá ở dưới\n- Quyết định được đưa ra tại mỗi nút nội bộ\n- Kết quả cuối cùng ở nút lá\n\n### Cấu Trúc Cây\n\n**1. Nút Gốc (Root Node):**\n- Nút trên cùng đại diện cho toàn bộ tập dữ liệu\n- Chứa tất cả mẫu training\n- Điểm bắt đầu của quá trình quyết định\n- Có phân chia đầu tiên dựa trên đặc trưng quan trọng nhất\n\n**2. Nút Nội Bộ (Internal Nodes):**\n- Các nút quyết định dựa trên kiểm tra đặc trưng\n- Mỗi nút thực hiện một câu hỏi yes/no về đặc trưng\n- Ví dụ: \"Tuổi > 30?\", \"Thu nhập < 50,000?\"\n- Chia dữ liệu thành các tập con\n\n**3. Nhánh (Branches):**\n- Kết quả của các quyết định\n- Kết nối nút cha với nút con\n- Đại diện cho giá trị hoặc phạm vi giá trị của đặc trưng\n\n**4. Nút Lá (Leaf Nodes):**\n- Nút cuối cùng không có nhánh con\n- Chứa dự đoán cuối cùng\n- Phân loại: Nhãn lớp\n- Hồi quy: Giá trị số\n\n**Ví dụ minh họa - Quyết định mua nhà:**\n```\n                 [Thu nhập > 50K?]\n                /                 \\\n            YES                    NO\n           /                         \\\n   [Tuổi > 30?]                [Không mua]\n    /        \\\n  YES        NO\n  /            \\\n[Mua]      [Thuê]\n```\n\n### Xây Dựng Cây Quyết Định\n\n**Tiêu Chí Phân Chia (Splitting Criteria):**\n\nMục tiêu: Tìm phân chia tốt nhất làm tăng \"độ thuần khiết\" (purity) của các tập con.\n\n**Cho Phân Loại:**\n\n**1. Gini Impurity (Chỉ Số Gini):**\n$$Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$$\n\nTrong đó:\n- $p_i$ là tỷ lệ mẫu thuộc lớp $i$ tại nút $t$\n- $C$ là số lớp\n- Gini = 0: Nút hoàn toàn thuần khiết (tất cả mẫu cùng lớp)\n\n**Các khái niệm quan trọng:**\n- Nhánh là kết quả của các quyết định tại các nút trong Cây Quyết Định, kết nối nút cha với nút con. Mỗi nhánh đại diện cho một giá trị hoặc phạm vi giá trị của đặc trưng được kiểm tra.\n- Cây Quyết Định là một mô hình học máy có giám sát đa năng, được sử dụng cho cả bài toán phân loại và hồi quy. Mô hình này hoạt động bằng cách học các quy tắc quyết định từ các đặc trưng của dữ liệu và biểu diễn chúng dưới dạng cấu trúc cây phân cấp để dự đoán giá trị mục tiêu. Cấu trúc cây bao gồm Nút Gốc, Nút Nội Bộ (mỗi nút đại diện cho một kiểm tra trên một thuộc tính), Nhánh (mỗi nhánh đại diện cho kết quả của kiểm tra), và Nút Lá (mỗi nút lá đại diện cho một nhãn lớp hoặc giá trị số). Quá trình xây dựng cây bao gồm việc phân chia dữ liệu dựa trên các đặc trưng để tối đa hóa sự giảm impurity. Cây Quyết Định có thể được xây dựng đầy đủ và sau đó được cắt tỉa (pruning) để chọn cây tối ưu bằng cross-validation. Ưu điểm của mô hình này là dễ hiểu, dễ diễn giải, yêu cầu ít tiền xử lý dữ liệu, xử lý được cả dữ liệu số và phân loại, là non-parametric, bắt được mối quan hệ phi tuyến, và tính toán Feature Importance tự nhiên. Tuy nhiên, việc xây dựng cây đầy đủ có thể tốn thời gian và phức tạp hơn so với pre-pruning.\n- Phân loại là một tác vụ trong học máy nhằm gán một nhãn lớp cụ thể cho các mẫu dữ liệu đầu vào dựa trên các đặc trưng của chúng. Mục tiêu là xây dựng một mô hình, ví dụ như Cây Quyết Định, có khả năng dự đoán chính xác nhãn lớp của các mẫu mới, với kết quả dự đoán ở nút lá là một nhãn lớp cụ thể.\n- Nút Lá là nút cuối cùng trong Cây Quyết Định, không có nhánh con. Nút lá chứa dự đoán cuối cùng của mô hình, đó có thể là nhãn lớp cho bài toán phân loại hoặc một giá trị số cho bài toán hồi quy.\n\n**Mối quan hệ:**\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Phân loại, gán nhãn lớp cho dữ liệu.\n- Cây Quyết Định chứa Nhánh, đại diện cho kết quả của các quyết định và kết nối các nút.\n- Cây Quyết Định chứa Nút Lá, nơi chứa dự đoán cuối cùng của mô hình.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Phân Loại (Classification)\n- Ưu điểm: Mỗi mô hình đơn giản hơn, cân bằng hơn\n- Nhược điểm: Nhiều mô hình (phức tạp khi K lớn)\n\n**Ví dụ:** 3 lớp (A, B, C)\n- Mô hình 1: A vs B\n- Mô hình 2: A vs C\n- Mô hình 3: B vs C\n\n**3. Softmax Regression (Multinomial Logistic Regression):**\n\nMở rộng trực tiếp của logistic regression cho đa lớp.\n\n**Công thức:**\n$$P(y=k|x) = \frac{e^{z_k}}{\\sum_{j=1}^{K}e^{z_j}}$$\n\nTrong đó: $z_k = \beta_k^Tx$ với $\beta_k$ là vector hệ số cho lớp $k$\n\n**Đặc điểm:**\n- Tổng các xác suất = 1: $\\sum_{k=1}^{K}P(y=k|x) = 1$\n- Output là phân phối xác suất trên tất cả lớp\n- Huấn luyện đồng thời tất cả lớp\n\n**Hàm chi phí (Categorical Cross-Entropy):**\n$$J(\beta) = -\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}\\log(P(y=k|x^{(i)}))$$\n\nTrong đó $y_k^{(i)}$ là one-hot encoding của nhãn.\n\n**Lựa chọn giữa OvR, OvO, và Softmax:**\n- **Softmax:** Tốt nhất khi cần xác suất, K không quá lớn\n- **OvR:** Đơn giản, hiệu quả với K lớn\n- **OvO:** Tốt với SVM, K nhỏ/trung bình\n\n### Naive Bayes Classifier (Bộ Phân Loại Naive Bayes)\n\nDựa trên định lý Bayes với giả định \"ngây thơ\" (naive) về tính độc lập đặc trưng.\n\n**Định Lý Bayes:**\n$$P(C_k|x) = \frac{P(x|C_k)P(C_k)}{P(x)}$$\n\nTrong đó:\n- $P(C_k|x)$: Xác suất hậu nghiệm (posterior) - xác suất lớp $C_k$ cho trước $x$\n- $P(x|C_k)$: Likelihood - xác suất của $x$ trong lớp $C_k$\n- $P(C_k)$: Xác suất tiên nghiệm (prior) của lớp $C_k$\n- $P(x)$: Evidence - xác suất của $x$\n\n**Giả Định Naive (Độc Lập Điều Kiện):**\n$$P(x_1, x_2, ..., x_n|C_k) = \\prod_{i=1}^{n}P(x_i|C_k)$$\n\nCác đặc trưng độc lập với nhau khi biết lớp.\n\n**Công Thức Đầy Đủ:**\n$$P(C_k|x_1,...,x_n) = \frac{P(C_k)\\prod_{i=1}^{n}P(x_i|C_k)}{P(x_1,...,x_n)}$$\n\n**Quyết Định:**\n$$\\hat{y} = \\arg\\max_{k} P(C_k)\\prod_{i=1}^{n}P(x_i|C_k)$$\n\nKhông cần tính $P(x)$ vì nó giống nhau cho tất cả lớp.\n\n**Các Biến Thể:**\n\n**1. Gaussian Naive Bayes:**\n- Cho đặc trưng liên tục\n- Giả định phân phối Gaussian (chuẩn)\n\n**Các khái niệm quan trọng:**\n- Phân loại là một tác vụ trong học máy nhằm gán một nhãn lớp cụ thể cho các mẫu dữ liệu đầu vào dựa trên các đặc trưng của chúng. Mục tiêu là xây dựng một mô hình, ví dụ như Cây Quyết Định, có khả năng dự đoán chính xác nhãn lớp của các mẫu mới, với kết quả dự đoán ở nút lá là một nhãn lớp cụ thể.\n\n**Mối quan hệ:**\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Phân loại, gán nhãn lớp cho dữ liệu."
        },
        "output_without_context": {
            "question": "Trong cây quyết định, nút nào được gọi là nút gốc?",
            "answer": "Nút gốc là nút đầu tiên trong cây quyết định, nơi bắt đầu quá trình phân loại."
        }
    },
    {
        "topic_description": "Giới thiệu về thuật toán Hunt dùng để xây dựng cây quyết định. Học sinh sẽ được kiểm tra hiểu biết về quy trình và từng bước trong thuật toán này. Các câu hỏi có thể yêu cầu thực hành về việc áp dụng thuật toán trong các tình huống thực tế.",
        "output_with_context": {
            "question": "Trong thuật toán Hunt để xây dựng cây quyết định, nút nào đại diện cho toàn bộ tập dữ liệu và là điểm bắt đầu của quá trình quyết định?",
            "answer": "Nút gốc (Root Node)",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n\n### Giới Thiệu Về Cây Quyết Định\n\nCây quyết định là thuật toán học có giám sát đa năng có thể thực hiện cả tác vụ phân loại và hồi quy. Chúng học các quy tắc quyết định từ các đặc trưng để dự đoán giá trị mục tiêu thông qua cấu trúc dạng cây.\n\n**Ứng dụng thực tế:**\n- Chẩn đoán y tế (chuỗi quyết định dựa trên triệu chứng)\n- Đánh giá rủi ro tín dụng\n- Dự đoán churn khách hàng\n- Phát hiện gian lận\n- Hệ thống chuyên gia\n- Phân loại email spam\n\n**Tại sao gọi là \"cây\":**\n- Cấu trúc phân cấp giống cây ngược\n- Gốc ở trên, lá ở dưới\n- Quyết định được đưa ra tại mỗi nút nội bộ\n- Kết quả cuối cùng ở nút lá\n\n### Cấu Trúc Cây\n\n**1. Nút Gốc (Root Node):**\n- Nút trên cùng đại diện cho toàn bộ tập dữ liệu\n- Chứa tất cả mẫu training\n- Điểm bắt đầu của quá trình quyết định\n- Có phân chia đầu tiên dựa trên đặc trưng quan trọng nhất\n\n**2. Nút Nội Bộ (Internal Nodes):**\n- Các nút quyết định dựa trên kiểm tra đặc trưng\n- Mỗi nút thực hiện một câu hỏi yes/no về đặc trưng\n- Ví dụ: \"Tuổi > 30?\", \"Thu nhập < 50,000?\"\n- Chia dữ liệu thành các tập con\n\n**3. Nhánh (Branches):**\n- Kết quả của các quyết định\n- Kết nối nút cha với nút con\n- Đại diện cho giá trị hoặc phạm vi giá trị của đặc trưng\n\n**4. Nút Lá (Leaf Nodes):**\n- Nút cuối cùng không có nhánh con\n- Chứa dự đoán cuối cùng\n- Phân loại: Nhãn lớp\n- Hồi quy: Giá trị số\n\n**Ví dụ minh họa - Quyết định mua nhà:**\n```\n                 [Thu nhập > 50K?]\n                /                 \\\n            YES                    NO\n           /                         \\\n   [Tuổi > 30?]                [Không mua]\n    /        \\\n  YES        NO\n  /            \\\n[Mua]      [Thuê]\n```\n\n### Xây Dựng Cây Quyết Định\n\n**Tiêu Chí Phân Chia (Splitting Criteria):**\n\nMục tiêu: Tìm phân chia tốt nhất làm tăng \"độ thuần khiết\" (purity) của các tập con.\n\n**Cho Phân Loại:**\n\n**1. Gini Impurity (Chỉ Số Gini):**\n$$Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$$\n\nTrong đó:\n- $p_i$ là tỷ lệ mẫu thuộc lớp $i$ tại nút $t$\n- $C$ là số lớp\n- Gini = 0: Nút hoàn toàn thuần khiết (tất cả mẫu cùng lớp)\n\n**Các khái niệm quan trọng:**\n- Cây Quyết Định là một mô hình học máy có giám sát đa năng, được sử dụng cho cả bài toán phân loại và hồi quy. Mô hình này hoạt động bằng cách học các quy tắc quyết định từ các đặc trưng của dữ liệu và biểu diễn chúng dưới dạng cấu trúc cây phân cấp để dự đoán giá trị mục tiêu. Cấu trúc cây bao gồm Nút Gốc, Nút Nội Bộ (mỗi nút đại diện cho một kiểm tra trên một thuộc tính), Nhánh (mỗi nhánh đại diện cho kết quả của kiểm tra), và Nút Lá (mỗi nút lá đại diện cho một nhãn lớp hoặc giá trị số). Quá trình xây dựng cây bao gồm việc phân chia dữ liệu dựa trên các đặc trưng để tối đa hóa sự giảm impurity. Cây Quyết Định có thể được xây dựng đầy đủ và sau đó được cắt tỉa (pruning) để chọn cây tối ưu bằng cross-validation. Ưu điểm của mô hình này là dễ hiểu, dễ diễn giải, yêu cầu ít tiền xử lý dữ liệu, xử lý được cả dữ liệu số và phân loại, là non-parametric, bắt được mối quan hệ phi tuyến, và tính toán Feature Importance tự nhiên. Tuy nhiên, việc xây dựng cây đầy đủ có thể tốn thời gian và phức tạp hơn so với pre-pruning.\n- Hồi quy là một tác vụ trong học máy mà Cây Quyết Định có thể thực hiện, nhằm dự đoán một giá trị số liên tục cho các mẫu dữ liệu dựa trên các đặc trưng đầu vào. Kết quả dự đoán ở nút lá là một giá trị số.\n\n**Mối quan hệ:**\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Phân loại, gán nhãn lớp cho dữ liệu.\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Hồi quy, dự đoán giá trị số liên tục.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n1. Xây dựng cây đầy đủ\n2. Tính $\\alpha$ cho mỗi subtree\n3. Loại bỏ subtree có $\\alpha$ nhỏ nhất (đóng góp ít nhất)\n4. Lặp lại cho đến khi còn gốc\n5. Chọn cây tối ưu bằng cross-validation\n\n**Ưu điểm:**\n- Thường cho kết quả tốt hơn pre-pruning\n- Không bỏ lỡ phân chia tốt\n\n**Nhược điểm:**\n- Tốn thời gian (xây dựng cây đầy đủ trước)\n- Phức tạp hơn\n\n### Feature Importance (Tầm Quan Trọng Đặc Trưng)\n\nCây quyết định tự động tính toán mức độ quan trọng của mỗi đặc trưng.\n\n**Công Thức:**\n$$Importance(f) = \\sum_{t \\in T} p(t) \\cdot \\Delta impurity(t, f)$$\n\nTrong đó:\n- $p(t)$ là tỷ lệ mẫu tại nút $t$\n- $\\Delta impurity(t, f)$ là giảm impurity khi phân chia theo đặc trưng $f$ tại nút $t$\n- Tổng trên tất cả nút sử dụng đặc trưng $f$\n\n**Diễn giải:**\n- Giá trị càng cao, đặc trưng càng quan trọng\n- Tổng tất cả importance = 1\n- Đặc trưng không xuất hiện có importance = 0\n\n**Ứng dụng:**\n- Feature selection\n- Hiểu mô hình\n- Phát hiện đặc trưng không cần thiết\n- Giải thích cho stakeholders\n\n**Lưu ý:**\n- Thiên vị về đặc trưng có nhiều giá trị unique\n- Đặc trưng tương quan cao có importance phân tán\n- Sử dụng permutation importance để khắc phục\n\n### Ưu Điểm Của Cây Quyết Định\n\n**1. Dễ hiểu và diễn giải:**\n- Trực quan, giống cách con người quyết định\n- Có thể vẽ và giải thích bằng lời\n- Không cần kiến thức thống kê sâu\n- Phù hợp cho business users\n\n**2. Yêu cầu ít tiền xử lý dữ liệu:**\n- Không cần feature scaling\n- Không cần one-hot encoding cho categorical\n- Xử lý được missing values (surrogate splits)\n- Không cần assumption về phân phối\n\n**3. Xử lý dữ liệu số và phân loại:**\n- Linh hoạt với nhiều loại đặc trưng\n- Không cần encoding phức tạp\n- Mixed data types\n\n**4. Non-parametric:**\n- Không giả định về phân phối dữ liệu\n- Linh hoạt với mọi dạng data\n- Không cần chọn hàm phân phối\n\n**5. Bắt được mối quan hệ phi tuyến:**\n- Decision boundary phức tạp\n- Tương tác giữa các đặc trưng\n- Không giới hạn bởi tuyến tính\n\n**6. Tính toán Feature Importance tự nhiên:**\n- Không cần phương pháp bên ngoài\n- Tích hợp trong thuật toán\n\n**7. Nhanh với dự đoán:**\n- Độ phức tạp: O(log n) với cây cân bằng\n- Hiệu quả cho production\n\n**Các khái niệm quan trọng:**\n- Cây Quyết Định là một mô hình học máy có giám sát đa năng, được sử dụng cho cả bài toán phân loại và hồi quy. Mô hình này hoạt động bằng cách học các quy tắc quyết định từ các đặc trưng của dữ liệu và biểu diễn chúng dưới dạng cấu trúc cây phân cấp để dự đoán giá trị mục tiêu. Cấu trúc cây bao gồm Nút Gốc, Nút Nội Bộ (mỗi nút đại diện cho một kiểm tra trên một thuộc tính), Nhánh (mỗi nhánh đại diện cho kết quả của kiểm tra), và Nút Lá (mỗi nút lá đại diện cho một nhãn lớp hoặc giá trị số). Quá trình xây dựng cây bao gồm việc phân chia dữ liệu dựa trên các đặc trưng để tối đa hóa sự giảm impurity. Cây Quyết Định có thể được xây dựng đầy đủ và sau đó được cắt tỉa (pruning) để chọn cây tối ưu bằng cross-validation. Ưu điểm của mô hình này là dễ hiểu, dễ diễn giải, yêu cầu ít tiền xử lý dữ liệu, xử lý được cả dữ liệu số và phân loại, là non-parametric, bắt được mối quan hệ phi tuyến, và tính toán Feature Importance tự nhiên. Tuy nhiên, việc xây dựng cây đầy đủ có thể tốn thời gian và phức tạp hơn so với pre-pruning.\n\n**Mối quan hệ:**\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Phân loại, gán nhãn lớp cho dữ liệu.\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Hồi quy, dự đoán giá trị số liên tục.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n- Gini = 0.5: Nút hỗn loạn nhất (phân bố đều giữa các lớp)\n\n**Ví dụ:**\n- Nút có 100 mẫu: 80 lớp A, 20 lớp B\n- $Gini = 1 - (0.8^2 + 0.2^2) = 1 - (0.64 + 0.04) = 0.32$\n\n**2. Entropy và Information Gain (Độ Lợi Thông Tin):**\n$$Entropy(t) = -\\sum_{i=1}^{C}p_i\\log_2(p_i)$$\n\n- Entropy đo lường độ hỗn loạn/không chắc chắn\n- Entropy = 0: Thuần khiết hoàn toàn\n- Entropy cao: Hỗn loạn\n\n**Information Gain:**\n$$IG(D_p, f) = Entropy(D_p) - \\sum_{j=1}^{m}\frac{N_j}{N_p}Entropy(D_j)$$\n\nTrong đó:\n- $D_p$ là tập dữ liệu cha\n- $D_j$ là tập con thứ $j$ sau phân chia\n- $N_j$ là số mẫu trong $D_j$\n- $N_p$ là tổng số mẫu trong $D_p$\n- $m$ là số tập con\n\n**Information Gain đo lường:**\n- Giảm entropy sau khi phân chia\n- Giá trị càng cao càng tốt\n- Phân chia tốt nhất có IG cao nhất\n\n**So sánh Gini vs Entropy:**\n| Tiêu chí | Gini | Entropy |\n|----------|------|---------|\n| Tính toán | Nhanh hơn | Chậm hơn (có log) |\n| Độ nhạy | Ít nhạy | Nhạy hơn với thay đổi |\n| Kết quả | Tương tự trong thực tế | Tương tự |\n| Mặc định | Sklearn default | ID3, C4.5 |\n\n**Cho Hồi Quy:**\n\n**3. Mean Squared Error (MSE):**\n$$MSE = \frac{1}{N}\\sum_{i=1}^{N}(y_i - \bar{y})^2$$\n\n- $\bar{y}$ là giá trị trung bình của mục tiêu tại nút\n- Đo lường phương sai của giá trị mục tiêu\n- Mục tiêu: Giảm MSE sau phân chia\n\n**4. Mean Absolute Error (MAE):**\n$$MAE = \frac{1}{N}\\sum_{i=1}^{N}|y_i - \bar{y}|$$\n\n- Ít nhạy cảm với outliers hơn MSE\n- Median thay vì mean làm giá trị dự đoán\n\n### Thuật Toán CART (Classification and Regression Trees)\n\nCART là thuật toán phổ biến nhất để xây dựng cây quyết định, tạo ra cây nhị phân.\n\n**Các Bước:**\n\n**1. Bắt đầu với tất cả dữ liệu training tại gốc:**\n- Tính impurity ban đầu\n- Xem xét tất cả đặc trưng và ngưỡng có thể\n\n**2. Chọn đặc trưng và ngưỡng tốt nhất:**\n- Thử tất cả đặc trưng\n- Với mỗi đặc trưng, thử nhiều ngưỡng\n- Tính information gain hoặc giảm Gini\n- Chọn phân chia có improvement cao nhất\n\n**3. Phân chia tập dữ liệu thành các tập con:**\n- Tạo nhánh trái và phải\n- Mẫu thỏa điều kiện đi trái\n- Mẫu không thỏa đi phải\n\n\n**Các khái niệm quan trọng:**\n- CART (Classification and Regression Trees) là thuật toán phổ biến nhất để xây dựng cây quyết định, tạo ra cây nhị phân. Thuật toán này bắt đầu với tất cả dữ liệu training tại gốc, sau đó chọn đặc trưng và ngưỡng tốt nhất dựa trên việc tính toán Information Gain hoặc giảm Gini Impurity (cho phân loại) hoặc giảm MSE/MAE (cho hồi quy). Dữ liệu được phân chia thành các tập con (nhánh trái và phải) dựa trên điều kiện đã chọn, và quá trình này lặp lại cho đến khi đạt được điều kiện dừng.\n- Decision Tree là một mô hình học máy được sử dụng cho cả bài toán phân loại và hồi quy, hoạt động bằng cách xây dựng một cấu trúc cây phân cấp để đưa ra quyết định. Cây quyết định phân chia dữ liệu dựa trên các đặc trưng và ngưỡng để tạo ra các nút lá chứa dự đoán cuối cùng. Thuật toán CART là một trong những thuật toán phổ biến nhất để xây dựng cây quyết định nhị phân.\n\n**Mối quan hệ:**\n- Thuật toán CART được sử dụng để xây dựng mô hình Decision Tree, tạo ra một cây nhị phân.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n\n### Nhược Điểm\n\n**1. Dễ Overfitting:**\n- Cây sâu học cả noise\n- Mô hình phức tạp không generalize tốt\n- Cần pruning hoặc ensemble\n\n**2. Không Ổn Định:**\n- Thay đổi nhỏ trong dữ liệu → cây hoàn toàn khác\n- High variance\n- Giải pháp: Ensemble methods (Random Forest)\n\n**3. Thiên Vị Về Đặc Trưng Có Nhiều Mức:**\n- Đặc trưng với nhiều giá trị unique được ưu tiên\n- Information Gain thiên vị\n- Giải pháp: Gain Ratio (C4.5)\n\n**4. Không Tối Ưu Cho Extrapolation:**\n- Hồi quy chỉ dự đoán trong phạm vi training data\n- Không thể dự đoán ngoài min/max đã thấy\n- Dự đoán là hằng số ở nút lá\n\n**5. Tạo Cây Thiên Vị Với Imbalanced Data:**\n- Ưu tiên lớp đa số\n- Cần class_weight hoặc resampling\n\n**6. Greedy Algorithm:**\n- Chọn phân chia tốt nhất tại thời điểm hiện tại\n- Không đảm bảo cây tối ưu toàn cục\n- Có thể bỏ lỡ cây tốt hơn\n\n**7. Khó Bắt Mối Quan Hệ Tuyến Tính:**\n- Cần nhiều phân chia để xấp xỉ đường thẳng\n- Linear model đơn giản hơn cho quan hệ tuyến tính\n\n### Phương Pháp Ensemble Với Cây\n\n**1. Random Forest (Rừng Ngẫu Nhiên):**\n\n**Nguyên lý:**\n- Xây dựng nhiều cây quyết định\n- Mỗi cây trên bootstrap sample khác nhau\n- Random subset đặc trưng tại mỗi split\n- Kết hợp dự đoán: Voting (classification) hoặc averaging (regression)\n\n**Tham số chính:**\n- `n_estimators`: Số cây (50-500)\n- `max_features`: Số đặc trưng xem xét (sqrt(n) cho classification, n/3 cho regression)\n- `max_depth`: Độ sâu mỗi cây\n- `min_samples_split`, `min_samples_leaf`\n\n**Ưu điểm:**\n- Giảm variance, ít overfitting\n- Ổn định hơn cây đơn\n- Feature importance đáng tin cậy hơn\n- Xử lý tốt high-dimensional data\n- Out-of-bag error estimation\n\n**2. Gradient Boosting:**\n\n**Nguyên lý:**\n- Xây dựng cây tuần tự\n- Mỗi cây học sửa lỗi của cây trước\n- Mỗi cây nhỏ (weak learner)\n- Kết hợp có trọng số\n\n**Công thức:**\n$$F_m(x) = F_{m-1}(x) + \nu \\cdot h_m(x)$$\n\nTrong đó:\n- $F_m$ là mô hình tại iteration $m$\n- $h_m$ là cây mới\n- $\nu$ là learning rate\n\n**Implementations phổ biến:**\n- **XGBoost:** Nhanh, regularization tốt, xử lý missing values\n- **LightGBM:** Rất nhanh, hiệu quả bộ nhớ, leaf-wise growth\n- **CatBoost:** Tốt cho categorical features, ít overfitting\n\n**Ưu điểm:**\n- Hiệu suất cao nhất trong nhiều competition\n- Có thể đạt accuracy rất cao\n- Xử lý tốt heterogeneous features\n\n**Nhược điểm:**\n\n**Các khái niệm quan trọng:**\n- Greedy Algorithm là một thuật toán đưa ra lựa chọn tối ưu cục bộ tại mỗi bước với hy vọng tìm được giải pháp tối ưu toàn cục. Trong bối cảnh Cây Quyết định, thuật toán xây dựng cây là greedy vì nó chọn phân chia tốt nhất tại thời điểm hiện tại dựa trên một tiêu chí (ví dụ: Information Gain). Tuy nhiên, việc này không đảm bảo rằng cây được tạo ra là tối ưu toàn cục, và có thể bỏ lỡ một cấu trúc cây tốt hơn nếu xét đến các bước sau.\n- Decision Tree là một mô hình học máy được sử dụng cho cả bài toán phân loại và hồi quy, hoạt động bằng cách xây dựng một cấu trúc cây phân cấp để đưa ra quyết định. Cây quyết định phân chia dữ liệu dựa trên các đặc trưng và ngưỡng để tạo ra các nút lá chứa dự đoán cuối cùng. Thuật toán CART là một trong những thuật toán phổ biến nhất để xây dựng cây quyết định nhị phân.\n\n**Mối quan hệ:**\n- Thuật toán CART được sử dụng để xây dựng mô hình Decision Tree, tạo ra một cây nhị phân.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n- Dễ overfit nếu không tune cẩn thận\n- Chậm hơn Random Forest (sequential)\n- Khó tune (nhiều hyperparameters)\n\n**3. AdaBoost (Adaptive Boosting):**\n\n**Nguyên lý:**\n- Tăng trọng số cho mẫu bị misclassified\n- Mỗi cây tập trung vào mẫu khó\n- Trọng số cho mô hình dựa trên accuracy\n\n**Ưu điểm:**\n- Đơn giản, hiệu quả\n- Ít tham số hơn Gradient Boosting\n- Tốt cho binary classification\n\n**Nhược điểm:**\n- Nhạy cảm với noise và outliers\n- Có thể overfit\n\n### Điều Chỉnh Hyperparameters\n\n**Tham số quan trọng:**\n\n**1. max_depth (Độ sâu tối đa):**\n- Giá trị nhỏ: Underfitting, mô hình đơn giản\n- Giá trị lớn: Overfitting, mô hình phức tạp\n- Thường: 3-10 cho cây đơn, 5-20 cho ensemble\n\n**2. min_samples_split:**\n- Số mẫu tối thiểu để split nút\n- Tăng lên: Giảm overfitting, cây đơn giản hơn\n- Thường: 2-100\n\n**3. min_samples_leaf:**\n- Số mẫu tối thiểu trong mỗi lá\n- Làm mượt decision boundary\n- Thường: 1-50\n\n**4. max_features:**\n- Số đặc trưng xem xét cho mỗi split\n- 'auto'/'sqrt': √n (cho classification)\n- 'log2': log₂(n)\n- None: Tất cả đặc trưng\n\n**5. criterion:**\n- 'gini': Gini impurity (mặc định, nhanh)\n- 'entropy': Information gain (chậm hơn)\n- 'squared_error': Cho regression\n\n**6. splitter:**\n- 'best': Chọn phân chia tốt nhất (mặc định)\n- 'random': Chọn ngẫu nhiên (nhanh hơn, thêm randomness)\n\n**Strategies cho Tuning:**\n- **Grid Search:** Thử tất cả combinations\n- **Random Search:** Sample ngẫu nhiên, hiệu quả hơn\n- **Bayesian Optimization:** Thông minh, ít iterations\n- **Cross-validation:** Luôn dùng CV để đánh giá\n\n**Tips:**\n- Bắt đầu với default parameters\n- Tune max_depth trước\n- Sau đó min_samples_split và min_samples_leaf\n- Cuối cùng các tham số khác\n- Monitor training vs validation performance\n\n### Ứng Dụng Thực Tế\n\n**1. Chẩn Đoán Y Tế:**\n- Chuỗi quyết định dựa trên triệu chứng\n- Dự đoán bệnh từ test results\n- Giải thích dễ dàng cho bác sĩ\n\n**2. Đánh Giá Rủi Ro Tín Dụng:**\n- Quyết định cho vay\n- Dự đoán default risk\n- Tuân thủ quy định (interpretability)\n\n**3. Dự Đoán Customer Churn:**\n- Xác định khách hàng có khả năng rời đi\n- Hành động marketing có mục tiêu\n- Hiểu lý do churn\n\n**4. Phát Hiện Gian Lận:**\n- Phát hiện transactions đáng ngờ\n- Real-time scoring\n- Giải thích cho investigation team\n\n**5. Feature Selection:**\n- Xác định đặc trưng quan trọng\n- Giảm dimensionality\n- Chuẩn bị cho mô hình khác\n\n**6. Hệ Thống Gợi Ý:**\n- Quyết định sản phẩm recommend\n- Personalization rules\n\n**Các khái niệm quan trọng:**\n- Decision Tree là một mô hình học máy được sử dụng cho cả bài toán phân loại và hồi quy, hoạt động bằng cách xây dựng một cấu trúc cây phân cấp để đưa ra quyết định. Cây quyết định phân chia dữ liệu dựa trên các đặc trưng và ngưỡng để tạo ra các nút lá chứa dự đoán cuối cùng. Thuật toán CART là một trong những thuật toán phổ biến nhất để xây dựng cây quyết định nhị phân.\n\n**Mối quan hệ:**\n- Thuật toán CART được sử dụng để xây dựng mô hình Decision Tree, tạo ra một cây nhị phân."
        },
        "output_without_context": {
            "question": "Thuật toán Hunt được sử dụng để làm gì trong lĩnh vực học máy?",
            "answer": "Xây dựng cây quyết định."
        }
    },
    {
        "topic_description": "Tìm hiểu về các phương pháp đo độ không thuần khiết của nút trong cây quyết định bằng các chỉ số như Gini và Entropy. Câu hỏi sẽ yêu cầu so sánh hai phương pháp và tính toán chỉ số cho các ví dụ cụ thể.",
        "output_with_context": {
            "question": "Khi tính toán Gini Impurity cho một nút trong cây quyết định, công thức nào được sử dụng để đo lường độ không thuần khiết của nút đó?",
            "answer": "Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n\n### Giới Thiệu Về Cây Quyết Định\n\nCây quyết định là thuật toán học có giám sát đa năng có thể thực hiện cả tác vụ phân loại và hồi quy. Chúng học các quy tắc quyết định từ các đặc trưng để dự đoán giá trị mục tiêu thông qua cấu trúc dạng cây.\n\n**Ứng dụng thực tế:**\n- Chẩn đoán y tế (chuỗi quyết định dựa trên triệu chứng)\n- Đánh giá rủi ro tín dụng\n- Dự đoán churn khách hàng\n- Phát hiện gian lận\n- Hệ thống chuyên gia\n- Phân loại email spam\n\n**Tại sao gọi là \"cây\":**\n- Cấu trúc phân cấp giống cây ngược\n- Gốc ở trên, lá ở dưới\n- Quyết định được đưa ra tại mỗi nút nội bộ\n- Kết quả cuối cùng ở nút lá\n\n### Cấu Trúc Cây\n\n**1. Nút Gốc (Root Node):**\n- Nút trên cùng đại diện cho toàn bộ tập dữ liệu\n- Chứa tất cả mẫu training\n- Điểm bắt đầu của quá trình quyết định\n- Có phân chia đầu tiên dựa trên đặc trưng quan trọng nhất\n\n**2. Nút Nội Bộ (Internal Nodes):**\n- Các nút quyết định dựa trên kiểm tra đặc trưng\n- Mỗi nút thực hiện một câu hỏi yes/no về đặc trưng\n- Ví dụ: \"Tuổi > 30?\", \"Thu nhập < 50,000?\"\n- Chia dữ liệu thành các tập con\n\n**3. Nhánh (Branches):**\n- Kết quả của các quyết định\n- Kết nối nút cha với nút con\n- Đại diện cho giá trị hoặc phạm vi giá trị của đặc trưng\n\n**4. Nút Lá (Leaf Nodes):**\n- Nút cuối cùng không có nhánh con\n- Chứa dự đoán cuối cùng\n- Phân loại: Nhãn lớp\n- Hồi quy: Giá trị số\n\n**Ví dụ minh họa - Quyết định mua nhà:**\n```\n                 [Thu nhập > 50K?]\n                /                 \\\n            YES                    NO\n           /                         \\\n   [Tuổi > 30?]                [Không mua]\n    /        \\\n  YES        NO\n  /            \\\n[Mua]      [Thuê]\n```\n\n### Xây Dựng Cây Quyết Định\n\n**Tiêu Chí Phân Chia (Splitting Criteria):**\n\nMục tiêu: Tìm phân chia tốt nhất làm tăng \"độ thuần khiết\" (purity) của các tập con.\n\n**Cho Phân Loại:**\n\n**1. Gini Impurity (Chỉ Số Gini):**\n$$Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$$\n\nTrong đó:\n- $p_i$ là tỷ lệ mẫu thuộc lớp $i$ tại nút $t$\n- $C$ là số lớp\n- Gini = 0: Nút hoàn toàn thuần khiết (tất cả mẫu cùng lớp)\n\n**Các khái niệm quan trọng:**\n- Gini Impurity (Chỉ Số Gini) là một tiêu chí phân chia được sử dụng trong Cây Quyết Định cho bài toán phân loại, được tính bằng công thức $Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$. Trong đó, $p_i$ là tỷ lệ mẫu thuộc lớp $i$ tại nút $t$, và $C$ là số lớp. Gini Impurity đo lường mức độ không thuần khiết của một nút; giá trị Gini = 0 cho biết nút hoàn toàn thuần khiết (tất cả mẫu cùng lớp).\n- Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2 là công thức toán học dùng để tính Gini Impurity tại nút $t$. Công thức này đo lường xác suất mà một mẫu được chọn ngẫu nhiên từ nút $t$ sẽ bị phân loại sai nếu nó được gán nhãn ngẫu nhiên theo phân phối lớp hiện có tại nút đó. Giá trị càng nhỏ, nút càng thuần khiết.\n\n**Mối quan hệ:**\n- Gini Impurity được tính toán bằng công thức Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2 để đo lường độ không thuần khiết của một nút.\n- Tiêu Chí Phân Chia sử dụng Gini Impurity làm một trong các phương pháp để đánh giá chất lượng phân chia cho bài toán phân loại.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n- Gini = 0.5: Nút hỗn loạn nhất (phân bố đều giữa các lớp)\n\n**Ví dụ:**\n- Nút có 100 mẫu: 80 lớp A, 20 lớp B\n- $Gini = 1 - (0.8^2 + 0.2^2) = 1 - (0.64 + 0.04) = 0.32$\n\n**2. Entropy và Information Gain (Độ Lợi Thông Tin):**\n$$Entropy(t) = -\\sum_{i=1}^{C}p_i\\log_2(p_i)$$\n\n- Entropy đo lường độ hỗn loạn/không chắc chắn\n- Entropy = 0: Thuần khiết hoàn toàn\n- Entropy cao: Hỗn loạn\n\n**Information Gain:**\n$$IG(D_p, f) = Entropy(D_p) - \\sum_{j=1}^{m}\frac{N_j}{N_p}Entropy(D_j)$$\n\nTrong đó:\n- $D_p$ là tập dữ liệu cha\n- $D_j$ là tập con thứ $j$ sau phân chia\n- $N_j$ là số mẫu trong $D_j$\n- $N_p$ là tổng số mẫu trong $D_p$\n- $m$ là số tập con\n\n**Information Gain đo lường:**\n- Giảm entropy sau khi phân chia\n- Giá trị càng cao càng tốt\n- Phân chia tốt nhất có IG cao nhất\n\n**So sánh Gini vs Entropy:**\n| Tiêu chí | Gini | Entropy |\n|----------|------|---------|\n| Tính toán | Nhanh hơn | Chậm hơn (có log) |\n| Độ nhạy | Ít nhạy | Nhạy hơn với thay đổi |\n| Kết quả | Tương tự trong thực tế | Tương tự |\n| Mặc định | Sklearn default | ID3, C4.5 |\n\n**Cho Hồi Quy:**\n\n**3. Mean Squared Error (MSE):**\n$$MSE = \frac{1}{N}\\sum_{i=1}^{N}(y_i - \bar{y})^2$$\n\n- $\bar{y}$ là giá trị trung bình của mục tiêu tại nút\n- Đo lường phương sai của giá trị mục tiêu\n- Mục tiêu: Giảm MSE sau phân chia\n\n**4. Mean Absolute Error (MAE):**\n$$MAE = \frac{1}{N}\\sum_{i=1}^{N}|y_i - \bar{y}|$$\n\n- Ít nhạy cảm với outliers hơn MSE\n- Median thay vì mean làm giá trị dự đoán\n\n### Thuật Toán CART (Classification and Regression Trees)\n\nCART là thuật toán phổ biến nhất để xây dựng cây quyết định, tạo ra cây nhị phân.\n\n**Các Bước:**\n\n**1. Bắt đầu với tất cả dữ liệu training tại gốc:**\n- Tính impurity ban đầu\n- Xem xét tất cả đặc trưng và ngưỡng có thể\n\n**2. Chọn đặc trưng và ngưỡng tốt nhất:**\n- Thử tất cả đặc trưng\n- Với mỗi đặc trưng, thử nhiều ngưỡng\n- Tính information gain hoặc giảm Gini\n- Chọn phân chia có improvement cao nhất\n\n**3. Phân chia tập dữ liệu thành các tập con:**\n- Tạo nhánh trái và phải\n- Mẫu thỏa điều kiện đi trái\n- Mẫu không thỏa đi phải\n\n\n**Các khái niệm quan trọng:**\n- Gini (Gini Impurity) là một thước đo độ hỗn loạn hoặc không thuần khiết của một nút hoặc một tập con dữ liệu trong cây quyết định, được sử dụng chủ yếu cho bài toán phân loại. Nó đóng vai trò là tiêu chí phân chia trong Decision Tree, thường là mặc định và nhanh hơn entropy. Công thức tính Gini cho một nút là $Gini = 1 - \\sum_{i=1}^{C}p_i^2$, trong đó $p_i$ là tỷ lệ mẫu thuộc lớp $i$ trong nút đó. Giá trị Gini = 0.5 cho thấy nút hỗn loạn nhất (phân bố đều giữa các lớp), trong khi Gini = 0 cho thấy nút hoàn toàn thuần khiết (chỉ chứa một lớp).\n- Entropy là một thước đo độ hỗn loạn hoặc không chắc chắn của một nút trong cây quyết định, được sử dụng chủ yếu cho bài toán phân loại. Công thức tính Entropy cho một nút là $Entropy(t) = -\\sum_{i=1}^{C}p_i\\log_2(p_i)$, trong đó $p_i$ là tỷ lệ mẫu thuộc lớp $i$ trong nút đó. Entropy = 0 khi nút hoàn toàn thuần khiết, và Entropy cao khi nút hỗn loạn.\n\n**Mối quan hệ:**\n- Gini Impurity được áp dụng làm tiêu chí phân chia cho các bài toán phân loại trong cây quyết định.\n- Decision Tree sử dụng Gini Impurity làm tiêu chí để đánh giá độ thuần khiết của các nút và chọn phân chia tốt nhất trong bài toán phân loại.\n- Decision Tree sử dụng Entropy làm tiêu chí để đánh giá độ hỗn loạn của các nút và chọn phân chia tốt nhất trong bài toán phân loại.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n- Dễ overfit nếu không tune cẩn thận\n- Chậm hơn Random Forest (sequential)\n- Khó tune (nhiều hyperparameters)\n\n**3. AdaBoost (Adaptive Boosting):**\n\n**Nguyên lý:**\n- Tăng trọng số cho mẫu bị misclassified\n- Mỗi cây tập trung vào mẫu khó\n- Trọng số cho mô hình dựa trên accuracy\n\n**Ưu điểm:**\n- Đơn giản, hiệu quả\n- Ít tham số hơn Gradient Boosting\n- Tốt cho binary classification\n\n**Nhược điểm:**\n- Nhạy cảm với noise và outliers\n- Có thể overfit\n\n### Điều Chỉnh Hyperparameters\n\n**Tham số quan trọng:**\n\n**1. max_depth (Độ sâu tối đa):**\n- Giá trị nhỏ: Underfitting, mô hình đơn giản\n- Giá trị lớn: Overfitting, mô hình phức tạp\n- Thường: 3-10 cho cây đơn, 5-20 cho ensemble\n\n**2. min_samples_split:**\n- Số mẫu tối thiểu để split nút\n- Tăng lên: Giảm overfitting, cây đơn giản hơn\n- Thường: 2-100\n\n**3. min_samples_leaf:**\n- Số mẫu tối thiểu trong mỗi lá\n- Làm mượt decision boundary\n- Thường: 1-50\n\n**4. max_features:**\n- Số đặc trưng xem xét cho mỗi split\n- 'auto'/'sqrt': √n (cho classification)\n- 'log2': log₂(n)\n- None: Tất cả đặc trưng\n\n**5. criterion:**\n- 'gini': Gini impurity (mặc định, nhanh)\n- 'entropy': Information gain (chậm hơn)\n- 'squared_error': Cho regression\n\n**6. splitter:**\n- 'best': Chọn phân chia tốt nhất (mặc định)\n- 'random': Chọn ngẫu nhiên (nhanh hơn, thêm randomness)\n\n**Strategies cho Tuning:**\n- **Grid Search:** Thử tất cả combinations\n- **Random Search:** Sample ngẫu nhiên, hiệu quả hơn\n- **Bayesian Optimization:** Thông minh, ít iterations\n- **Cross-validation:** Luôn dùng CV để đánh giá\n\n**Tips:**\n- Bắt đầu với default parameters\n- Tune max_depth trước\n- Sau đó min_samples_split và min_samples_leaf\n- Cuối cùng các tham số khác\n- Monitor training vs validation performance\n\n### Ứng Dụng Thực Tế\n\n**1. Chẩn Đoán Y Tế:**\n- Chuỗi quyết định dựa trên triệu chứng\n- Dự đoán bệnh từ test results\n- Giải thích dễ dàng cho bác sĩ\n\n**2. Đánh Giá Rủi Ro Tín Dụng:**\n- Quyết định cho vay\n- Dự đoán default risk\n- Tuân thủ quy định (interpretability)\n\n**3. Dự Đoán Customer Churn:**\n- Xác định khách hàng có khả năng rời đi\n- Hành động marketing có mục tiêu\n- Hiểu lý do churn\n\n**4. Phát Hiện Gian Lận:**\n- Phát hiện transactions đáng ngờ\n- Real-time scoring\n- Giải thích cho investigation team\n\n**5. Feature Selection:**\n- Xác định đặc trưng quan trọng\n- Giảm dimensionality\n- Chuẩn bị cho mô hình khác\n\n**6. Hệ Thống Gợi Ý:**\n- Quyết định sản phẩm recommend\n- Personalization rules\n\n**Các khái niệm quan trọng:**\n- criterion là tham số xác định hàm để đo chất lượng của một phân chia (split). Các lựa chọn bao gồm 'gini' (Gini impurity, thường là mặc định và nhanh hơn), 'entropy' (Information gain, chậm hơn) cho bài toán phân loại, và 'squared_error' cho bài toán hồi quy.\n- Information gain (dựa trên entropy) là một thước đo được sử dụng làm tiêu chí phân chia trong Decision Tree, chậm hơn Gini impurity. Nó đo lường sự giảm độ không chắc chắn sau khi một nút được chia.\n\n**Mối quan hệ:**\n- Tham số criterion sử dụng entropy (Information gain) để đo chất lượng phân chia trong Decision Tree.\n- Tham số criterion sử dụng Gini impurity để đo chất lượng phân chia trong Decision Tree.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n1. Xây dựng cây đầy đủ\n2. Tính $\\alpha$ cho mỗi subtree\n3. Loại bỏ subtree có $\\alpha$ nhỏ nhất (đóng góp ít nhất)\n4. Lặp lại cho đến khi còn gốc\n5. Chọn cây tối ưu bằng cross-validation\n\n**Ưu điểm:**\n- Thường cho kết quả tốt hơn pre-pruning\n- Không bỏ lỡ phân chia tốt\n\n**Nhược điểm:**\n- Tốn thời gian (xây dựng cây đầy đủ trước)\n- Phức tạp hơn\n\n### Feature Importance (Tầm Quan Trọng Đặc Trưng)\n\nCây quyết định tự động tính toán mức độ quan trọng của mỗi đặc trưng.\n\n**Công Thức:**\n$$Importance(f) = \\sum_{t \\in T} p(t) \\cdot \\Delta impurity(t, f)$$\n\nTrong đó:\n- $p(t)$ là tỷ lệ mẫu tại nút $t$\n- $\\Delta impurity(t, f)$ là giảm impurity khi phân chia theo đặc trưng $f$ tại nút $t$\n- Tổng trên tất cả nút sử dụng đặc trưng $f$\n\n**Diễn giải:**\n- Giá trị càng cao, đặc trưng càng quan trọng\n- Tổng tất cả importance = 1\n- Đặc trưng không xuất hiện có importance = 0\n\n**Ứng dụng:**\n- Feature selection\n- Hiểu mô hình\n- Phát hiện đặc trưng không cần thiết\n- Giải thích cho stakeholders\n\n**Lưu ý:**\n- Thiên vị về đặc trưng có nhiều giá trị unique\n- Đặc trưng tương quan cao có importance phân tán\n- Sử dụng permutation importance để khắc phục\n\n### Ưu Điểm Của Cây Quyết Định\n\n**1. Dễ hiểu và diễn giải:**\n- Trực quan, giống cách con người quyết định\n- Có thể vẽ và giải thích bằng lời\n- Không cần kiến thức thống kê sâu\n- Phù hợp cho business users\n\n**2. Yêu cầu ít tiền xử lý dữ liệu:**\n- Không cần feature scaling\n- Không cần one-hot encoding cho categorical\n- Xử lý được missing values (surrogate splits)\n- Không cần assumption về phân phối\n\n**3. Xử lý dữ liệu số và phân loại:**\n- Linh hoạt với nhiều loại đặc trưng\n- Không cần encoding phức tạp\n- Mixed data types\n\n**4. Non-parametric:**\n- Không giả định về phân phối dữ liệu\n- Linh hoạt với mọi dạng data\n- Không cần chọn hàm phân phối\n\n**5. Bắt được mối quan hệ phi tuyến:**\n- Decision boundary phức tạp\n- Tương tác giữa các đặc trưng\n- Không giới hạn bởi tuyến tính\n\n**6. Tính toán Feature Importance tự nhiên:**\n- Không cần phương pháp bên ngoài\n- Tích hợp trong thuật toán\n\n**7. Nhanh với dự đoán:**\n- Độ phức tạp: O(log n) với cây cân bằng\n- Hiệu quả cho production\n\n**Các khái niệm quan trọng:**\n- Δ impurity(t, f) là tham số đại diện cho sự giảm impurity khi phân chia theo đặc trưng f tại nút t trong Cây Quyết Định. Impurity có thể là Gini impurity hoặc entropy. Giá trị này đo lường mức độ \"sạch\" hơn của các nút con sau khi phân chia so với nút cha."
        },
        "output_without_context": {
            "question": "Khi so sánh chỉ số Gini và Entropy trong việc đo độ không thuần khiết của nút trong cây quyết định, chỉ số nào thường được ưa chuộng hơn do tính toán đơn giản hơn?",
            "answer": "Chỉ số Gini"
        }
    },
    {
        "topic_description": "Khám phá Gain Ratio và cách nó điều chỉnh Information Gain. Học sinh sẽ kiểm tra hiểu biết về lý thuyết và cách áp dụng Gain Ratio trong thuật toán C4.5.",
        "output_with_context": {
            "question": "Gain Ratio được sử dụng trong thuật toán C4.5 để khắc phục nhược điểm nào của Information Gain?",
            "answer": "Thiên vị các đặc trưng có nhiều giá trị unique.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n- Gini = 0.5: Nút hỗn loạn nhất (phân bố đều giữa các lớp)\n\n**Ví dụ:**\n- Nút có 100 mẫu: 80 lớp A, 20 lớp B\n- $Gini = 1 - (0.8^2 + 0.2^2) = 1 - (0.64 + 0.04) = 0.32$\n\n**2. Entropy và Information Gain (Độ Lợi Thông Tin):**\n$$Entropy(t) = -\\sum_{i=1}^{C}p_i\\log_2(p_i)$$\n\n- Entropy đo lường độ hỗn loạn/không chắc chắn\n- Entropy = 0: Thuần khiết hoàn toàn\n- Entropy cao: Hỗn loạn\n\n**Information Gain:**\n$$IG(D_p, f) = Entropy(D_p) - \\sum_{j=1}^{m}\frac{N_j}{N_p}Entropy(D_j)$$\n\nTrong đó:\n- $D_p$ là tập dữ liệu cha\n- $D_j$ là tập con thứ $j$ sau phân chia\n- $N_j$ là số mẫu trong $D_j$\n- $N_p$ là tổng số mẫu trong $D_p$\n- $m$ là số tập con\n\n**Information Gain đo lường:**\n- Giảm entropy sau khi phân chia\n- Giá trị càng cao càng tốt\n- Phân chia tốt nhất có IG cao nhất\n\n**So sánh Gini vs Entropy:**\n| Tiêu chí | Gini | Entropy |\n|----------|------|---------|\n| Tính toán | Nhanh hơn | Chậm hơn (có log) |\n| Độ nhạy | Ít nhạy | Nhạy hơn với thay đổi |\n| Kết quả | Tương tự trong thực tế | Tương tự |\n| Mặc định | Sklearn default | ID3, C4.5 |\n\n**Cho Hồi Quy:**\n\n**3. Mean Squared Error (MSE):**\n$$MSE = \frac{1}{N}\\sum_{i=1}^{N}(y_i - \bar{y})^2$$\n\n- $\bar{y}$ là giá trị trung bình của mục tiêu tại nút\n- Đo lường phương sai của giá trị mục tiêu\n- Mục tiêu: Giảm MSE sau phân chia\n\n**4. Mean Absolute Error (MAE):**\n$$MAE = \frac{1}{N}\\sum_{i=1}^{N}|y_i - \bar{y}|$$\n\n- Ít nhạy cảm với outliers hơn MSE\n- Median thay vì mean làm giá trị dự đoán\n\n### Thuật Toán CART (Classification and Regression Trees)\n\nCART là thuật toán phổ biến nhất để xây dựng cây quyết định, tạo ra cây nhị phân.\n\n**Các Bước:**\n\n**1. Bắt đầu với tất cả dữ liệu training tại gốc:**\n- Tính impurity ban đầu\n- Xem xét tất cả đặc trưng và ngưỡng có thể\n\n**2. Chọn đặc trưng và ngưỡng tốt nhất:**\n- Thử tất cả đặc trưng\n- Với mỗi đặc trưng, thử nhiều ngưỡng\n- Tính information gain hoặc giảm Gini\n- Chọn phân chia có improvement cao nhất\n\n**3. Phân chia tập dữ liệu thành các tập con:**\n- Tạo nhánh trái và phải\n- Mẫu thỏa điều kiện đi trái\n- Mẫu không thỏa đi phải\n\n\n**Các khái niệm quan trọng:**\n- Information Gain (Độ Lợi Thông Tin) là một thước đo quan trọng trong thuật toán Cây Quyết định, được sử dụng để chọn đặc trưng và ngưỡng phân chia tốt nhất cho dữ liệu tại mỗi nút. Nó định lượng sự giảm Entropy (hoặc sự tăng thông tin) sau khi phân chia dữ liệu dựa trên một đặc trưng cụ thể. Công thức tính Information Gain là $IG(D_p, f) = Entropy(D_p) - \\sum_{j=1}^{m}\frac{N_j}{N_p}Entropy(D_j)$, trong đó $D_p$ là tập dữ liệu cha, $D_j$ là tập con thứ $j$ sau phân chia, $N_j$ là số mẫu trong $D_j$, $N_p$ là tổng số mẫu trong $D_p$, và $m$ là số tập con. Giá trị Information Gain càng cao cho thấy một phân chia càng hiệu quả. Tuy nhiên, Information Gain có nhược điểm là có thể thiên vị các đặc trưng có nhiều giá trị unique, dẫn đến việc ưu tiên chúng một cách không công bằng; giải pháp cho vấn đề này là sử dụng Gain Ratio.\n\n**Mối quan hệ:**\n- Thuật toán CART sử dụng Information Gain để chọn phân chia tốt nhất trong các bài toán phân loại.\n- Information Gain được tính toán dựa trên giá trị Entropy của tập dữ liệu cha và các tập con sau phân chia.\n- Gain Ratio được dẫn xuất từ Information Gain để khắc phục sự thiên vị của nó đối với các đặc trưng có nhiều mức bằng cách chuẩn hóa.\n- Information-Theoretic Exploration sử dụng Information Gain để chọn hành động tiết lộ nhiều thông tin nhất.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n\n### Nhược Điểm\n\n**1. Dễ Overfitting:**\n- Cây sâu học cả noise\n- Mô hình phức tạp không generalize tốt\n- Cần pruning hoặc ensemble\n\n**2. Không Ổn Định:**\n- Thay đổi nhỏ trong dữ liệu → cây hoàn toàn khác\n- High variance\n- Giải pháp: Ensemble methods (Random Forest)\n\n**3. Thiên Vị Về Đặc Trưng Có Nhiều Mức:**\n- Đặc trưng với nhiều giá trị unique được ưu tiên\n- Information Gain thiên vị\n- Giải pháp: Gain Ratio (C4.5)\n\n**4. Không Tối Ưu Cho Extrapolation:**\n- Hồi quy chỉ dự đoán trong phạm vi training data\n- Không thể dự đoán ngoài min/max đã thấy\n- Dự đoán là hằng số ở nút lá\n\n**5. Tạo Cây Thiên Vị Với Imbalanced Data:**\n- Ưu tiên lớp đa số\n- Cần class_weight hoặc resampling\n\n**6. Greedy Algorithm:**\n- Chọn phân chia tốt nhất tại thời điểm hiện tại\n- Không đảm bảo cây tối ưu toàn cục\n- Có thể bỏ lỡ cây tốt hơn\n\n**7. Khó Bắt Mối Quan Hệ Tuyến Tính:**\n- Cần nhiều phân chia để xấp xỉ đường thẳng\n- Linear model đơn giản hơn cho quan hệ tuyến tính\n\n### Phương Pháp Ensemble Với Cây\n\n**1. Random Forest (Rừng Ngẫu Nhiên):**\n\n**Nguyên lý:**\n- Xây dựng nhiều cây quyết định\n- Mỗi cây trên bootstrap sample khác nhau\n- Random subset đặc trưng tại mỗi split\n- Kết hợp dự đoán: Voting (classification) hoặc averaging (regression)\n\n**Tham số chính:**\n- `n_estimators`: Số cây (50-500)\n- `max_features`: Số đặc trưng xem xét (sqrt(n) cho classification, n/3 cho regression)\n- `max_depth`: Độ sâu mỗi cây\n- `min_samples_split`, `min_samples_leaf`\n\n**Ưu điểm:**\n- Giảm variance, ít overfitting\n- Ổn định hơn cây đơn\n- Feature importance đáng tin cậy hơn\n- Xử lý tốt high-dimensional data\n- Out-of-bag error estimation\n\n**2. Gradient Boosting:**\n\n**Nguyên lý:**\n- Xây dựng cây tuần tự\n- Mỗi cây học sửa lỗi của cây trước\n- Mỗi cây nhỏ (weak learner)\n- Kết hợp có trọng số\n\n**Công thức:**\n$$F_m(x) = F_{m-1}(x) + \nu \\cdot h_m(x)$$\n\nTrong đó:\n- $F_m$ là mô hình tại iteration $m$\n- $h_m$ là cây mới\n- $\nu$ là learning rate\n\n**Implementations phổ biến:**\n- **XGBoost:** Nhanh, regularization tốt, xử lý missing values\n- **LightGBM:** Rất nhanh, hiệu quả bộ nhớ, leaf-wise growth\n- **CatBoost:** Tốt cho categorical features, ít overfitting\n\n**Ưu điểm:**\n- Hiệu suất cao nhất trong nhiều competition\n- Có thể đạt accuracy rất cao\n- Xử lý tốt heterogeneous features\n\n**Nhược điểm:**\n\n**Các khái niệm quan trọng:**\n- Gain Ratio là một thước đo được sử dụng trong thuật toán Cây Quyết định (ví dụ: C4.5) để khắc phục sự thiên vị của Information Gain đối với các đặc trưng có nhiều mức. Gain Ratio chuẩn hóa Information Gain bằng cách chia cho Split Information (đo lường sự phân tán của các giá trị của đặc trưng). Điều này giúp chọn các đặc trưng phân chia dữ liệu một cách cân bằng hơn, không ưu tiên các đặc trưng có nhiều giá trị unique.\n- Information Gain (Độ Lợi Thông Tin) là một thước đo quan trọng trong thuật toán Cây Quyết định, được sử dụng để chọn đặc trưng và ngưỡng phân chia tốt nhất cho dữ liệu tại mỗi nút. Nó định lượng sự giảm Entropy (hoặc sự tăng thông tin) sau khi phân chia dữ liệu dựa trên một đặc trưng cụ thể. Công thức tính Information Gain là $IG(D_p, f) = Entropy(D_p) - \\sum_{j=1}^{m}\frac{N_j}{N_p}Entropy(D_j)$, trong đó $D_p$ là tập dữ liệu cha, $D_j$ là tập con thứ $j$ sau phân chia, $N_j$ là số mẫu trong $D_j$, $N_p$ là tổng số mẫu trong $D_p$, và $m$ là số tập con. Giá trị Information Gain càng cao cho thấy một phân chia càng hiệu quả. Tuy nhiên, Information Gain có nhược điểm là có thể thiên vị các đặc trưng có nhiều giá trị unique, dẫn đến việc ưu tiên chúng một cách không công bằng; giải pháp cho vấn đề này là sử dụng Gain Ratio.\n\n**Mối quan hệ:**\n- Thuật toán CART sử dụng Information Gain để chọn phân chia tốt nhất trong các bài toán phân loại.\n- Information Gain được tính toán dựa trên giá trị Entropy của tập dữ liệu cha và các tập con sau phân chia.\n- Gain Ratio được dẫn xuất từ Information Gain để khắc phục sự thiên vị của nó đối với các đặc trưng có nhiều mức bằng cách chuẩn hóa.\n- Information-Theoretic Exploration sử dụng Information Gain để chọn hành động tiết lộ nhiều thông tin nhất.\n\n**Nội dung từ tài liệu:**\n# Reinforcement Learning - Học Tăng Cường\n## Exploration and Exploitation - Khám Phá và Khai Thác\n    Failure: β_a ← β_a + 1\n```\n\n**Properties**:\n- Optimal regret bounds\n- Often better than UCB trong practice\n\n### 5. Count-Based Exploration\n\n#### 5.1. Exploration Bonuses\n\n**Idea**: Add bonus reward for visiting rare states\n\n**Intrinsic Motivation**:\n```\nr_total = r_extrinsic + β · r_intrinsic\n\nr_intrinsic = 1/√N(s)  hoặc  1/√N(s,a)\n```\n\n**Effect**: Encourage visiting under-explored regions\n\n#### 5.2. Pseudo-Count Methods\n\n**For large state spaces**: Cannot count exactly\n\n**Density Models**:\n```\nρ(s): Estimate density/frequency of state s\nPseudo-count: N(s) ∝ ρ(s) / (1 - ρ(s))\nBonus: r_intrinsic = β/√N(s)\n```\n\n**Examples**:\n- Context Tree Switching (CTS)\n- Neural density models\n\n### 6. Curiosity-Driven Exploration\n\n#### 6.1. Intrinsic Curiosity Module (ICM)\n\n**Components**:\n```\n1. Forward Model: Predict next state feature\n   ŝ_{t+1} = f(s_t, a_t)\n\n2. Inverse Model: Predict action from states\n   â_t = g(s_t, s_{t+1})\n\n3. Intrinsic Reward: Prediction error\n   r_intrinsic = ||ŝ_{t+1} - s_{t+1}||²\n```\n\n**Intuition**: States that are hard to predict are \"interesting\"\n\n#### 6.2. Random Network Distillation (RND)\n\n**Setup**:\n```\nFixed random network: f_target(s)\nLearned predictor: f_pred(s; θ)\n```\n\n**Intrinsic Reward**:\n```\nr_intrinsic = ||f_target(s) - f_pred(s; θ)||²\n```\n\n**Properties**:\n- Novel states have high prediction error\n- Visited states have low error\n- Non-stationary targets avoided\n\n#### 6.3. Never Give Up (NGU)\n\n**Combines**:\n- Episodic novelty (memory-based)\n- Life-long novelty (RND-based)\n\n**Two-timescale curiosity**:\n```\nr_episodic: Within episode novelty\nr_lifelong: Across episodes novelty\n```\n\n### 7. Information-Theoretic Exploration\n\n#### 7.1. Information Gain\n\n**Maximum Information Gain**:\n```\nMaximize: I(Θ; O | a) = H(Θ) - H(Θ|O,a)\nΘ: Parameters/state of world\nO: Observations\n```\n\n**Intuition**: Choose actions that reveal most information\n\n#### 7.2. Entropy Maximization\n\n**Maximum Entropy RL** (covered in SAC):\n```\nπ* = argmax_π E[Σ_t (R_t + α H(π(·|S_t)))]\n```\n\n**Benefits**:\n- Natural exploration\n- Robust policies\n- Multiple solutions\n\n#### 7.3. Empowerment\n\n**Definition**: Mutual information between actions và future states\n```\nEmpowerment = I(A_t; S_{t+k} | S_t)\n```\n\n**Intuition**: Maximize control over future\n\n### 8. Goal-Driven Exploration\n\n#### 8.1. Hindsight Experience Replay (HER)\n\n**Problem**: Sparse rewards → most episodes fail → little learning\n\n**Idea**: Learn from failures by relabeling goals\n```\nOriginal: Goal = g, achieved = g', reward = 0 (failure)\nHER: Goal = g', achieved = g', reward = 0 (success!)\n```\n\n**Algorithm**:\n```\nStore transition (s, a, r, s', g) vào replay buffer\n\nAdditionally store:\n    (s, a, r', s', g') where g' = achieved_goal(s')\n    r' = reward(s', a, g')\n```\n\n**Effect**: Every trajectory teaches something\n\n#### 8.2. Curriculum Learning\n\n**Progressive Difficulty**:\n```\nEasy tasks → Medium tasks → Hard tasks\n```\n\n**Automatic Curriculum**:\n- Track success rates\n- Adjust task distribution\n- Focus on \"frontier\" of capability\n\n### 9. Multi-Agent Exploration\n\n#### 9.1. Population-Based Training\n\n**Idea**: Train population of agents với different hyperparameters\n\n**Process**:\n```\nPopulation: {Agent_1, ..., Agent_N}\n\nPeriodically:\n\n**Các khái niệm quan trọng:**\n- Information-Theoretic Exploration là một nhóm các kỹ thuật khám phá trong RL dựa trên lý thuyết thông tin, nhằm mục đích tối đa hóa lượng thông tin thu được về môi trường hoặc tối đa hóa entropy của chính sách để khuyến khích sự đa dạng trong hành vi.\n\n**Mối quan hệ:**\n- Information-Theoretic Exploration sử dụng Information Gain để chọn hành động tiết lộ nhiều thông tin nhất."
        },
        "output_without_context": {
            "question": "Gain Ratio được sử dụng để điều chỉnh Information Gain trong thuật toán C4.5 nhằm mục đích gì?",
            "answer": "Giảm thiểu sự thiên lệch của Information Gain đối với các thuộc tính có nhiều giá trị."
        }
    },
    {
        "topic_description": "Tìm hiểu về sai số phân loại tại một nút trong cây quyết định. Câu hỏi sẽ yêu cầu học sinh tính toán sai số phân loại dựa trên tần suất của các lớp trong nút.",
        "output_with_context": {
            "question": "Tại một nút trong cây quyết định, nếu có 100 mẫu với 80 lớp A và 20 lớp B, hãy tính Gini Impurity của nút đó.",
            "answer": "Gini = 0.32",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n- Gini = 0.5: Nút hỗn loạn nhất (phân bố đều giữa các lớp)\n\n**Ví dụ:**\n- Nút có 100 mẫu: 80 lớp A, 20 lớp B\n- $Gini = 1 - (0.8^2 + 0.2^2) = 1 - (0.64 + 0.04) = 0.32$\n\n**2. Entropy và Information Gain (Độ Lợi Thông Tin):**\n$$Entropy(t) = -\\sum_{i=1}^{C}p_i\\log_2(p_i)$$\n\n- Entropy đo lường độ hỗn loạn/không chắc chắn\n- Entropy = 0: Thuần khiết hoàn toàn\n- Entropy cao: Hỗn loạn\n\n**Information Gain:**\n$$IG(D_p, f) = Entropy(D_p) - \\sum_{j=1}^{m}\frac{N_j}{N_p}Entropy(D_j)$$\n\nTrong đó:\n- $D_p$ là tập dữ liệu cha\n- $D_j$ là tập con thứ $j$ sau phân chia\n- $N_j$ là số mẫu trong $D_j$\n- $N_p$ là tổng số mẫu trong $D_p$\n- $m$ là số tập con\n\n**Information Gain đo lường:**\n- Giảm entropy sau khi phân chia\n- Giá trị càng cao càng tốt\n- Phân chia tốt nhất có IG cao nhất\n\n**So sánh Gini vs Entropy:**\n| Tiêu chí | Gini | Entropy |\n|----------|------|---------|\n| Tính toán | Nhanh hơn | Chậm hơn (có log) |\n| Độ nhạy | Ít nhạy | Nhạy hơn với thay đổi |\n| Kết quả | Tương tự trong thực tế | Tương tự |\n| Mặc định | Sklearn default | ID3, C4.5 |\n\n**Cho Hồi Quy:**\n\n**3. Mean Squared Error (MSE):**\n$$MSE = \frac{1}{N}\\sum_{i=1}^{N}(y_i - \bar{y})^2$$\n\n- $\bar{y}$ là giá trị trung bình của mục tiêu tại nút\n- Đo lường phương sai của giá trị mục tiêu\n- Mục tiêu: Giảm MSE sau phân chia\n\n**4. Mean Absolute Error (MAE):**\n$$MAE = \frac{1}{N}\\sum_{i=1}^{N}|y_i - \bar{y}|$$\n\n- Ít nhạy cảm với outliers hơn MSE\n- Median thay vì mean làm giá trị dự đoán\n\n### Thuật Toán CART (Classification and Regression Trees)\n\nCART là thuật toán phổ biến nhất để xây dựng cây quyết định, tạo ra cây nhị phân.\n\n**Các Bước:**\n\n**1. Bắt đầu với tất cả dữ liệu training tại gốc:**\n- Tính impurity ban đầu\n- Xem xét tất cả đặc trưng và ngưỡng có thể\n\n**2. Chọn đặc trưng và ngưỡng tốt nhất:**\n- Thử tất cả đặc trưng\n- Với mỗi đặc trưng, thử nhiều ngưỡng\n- Tính information gain hoặc giảm Gini\n- Chọn phân chia có improvement cao nhất\n\n**3. Phân chia tập dữ liệu thành các tập con:**\n- Tạo nhánh trái và phải\n- Mẫu thỏa điều kiện đi trái\n- Mẫu không thỏa đi phải\n\n\n**Các khái niệm quan trọng:**\n- Phân loại là một loại bài toán học máy trong đó mục tiêu là dự đoán nhãn lớp rời rạc cho một mẫu dữ liệu. Trong cây quyết định, các tiêu chí như Gini và Entropy được sử dụng để phân chia dữ liệu cho các bài toán phân loại.\n- Entropy là một thước đo độ hỗn loạn hoặc không chắc chắn của một nút trong cây quyết định, được sử dụng chủ yếu cho bài toán phân loại. Công thức tính Entropy cho một nút là $Entropy(t) = -\\sum_{i=1}^{C}p_i\\log_2(p_i)$, trong đó $p_i$ là tỷ lệ mẫu thuộc lớp $i$ trong nút đó. Entropy = 0 khi nút hoàn toàn thuần khiết, và Entropy cao khi nút hỗn loạn.\n\n**Mối quan hệ:**\n- Gini Impurity được áp dụng làm tiêu chí phân chia cho các bài toán phân loại trong cây quyết định.\n- Entropy được áp dụng làm tiêu chí phân chia cho các bài toán phân loại trong cây quyết định.\n- Công thức Entropy(t) = -∑ᵢpᵢlog₂(pᵢ) định nghĩa cách tính Entropy của một nút.\n- Decision Tree sử dụng Entropy làm tiêu chí để đánh giá độ hỗn loạn của các nút và chọn phân chia tốt nhất trong bài toán phân loại.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Hồi Quy Tuyến Tính (Linear Regression)\n**Underfitting (High Bias):**\n- Training error cao\n- Validation error cao\n- Mô hình quá đơn giản\n- Giải pháp: Thêm đặc trưng, tăng độ phức tạp, giảm regularization\n\n**Overfitting (High Variance):**\n- Training error thấp\n- Validation error cao (chênh lệch lớn)\n- Mô hình quá phức tạp\n- Giải pháp: Thêm dữ liệu, regularization, giảm đặc trưng, early stopping\n\n**Good fit:**\n- Training error thấp\n- Validation error thấp\n- Chênh lệch nhỏ giữa hai errors\n\n\n---\n\n## Phân Loại (Classification)\n\n### Giới Thiệu Về Phân Loại\n\nPhân loại là một tác vụ học có giám sát trong đó mục tiêu là dự đoán nhãn lớp rời rạc. Khác với hồi quy dự đoán giá trị liên tục, phân loại gán các đầu vào vào các danh mục được định nghĩa trước.\n\n**Ứng dụng thực tế:**\n- Phát hiện thư rác (spam/không spam)\n- Chẩn đoán bệnh (bệnh/không bệnh)\n- Nhận dạng chữ viết tay\n- Phân tích cảm xúc (tích cực/tiêu cực/trung lập)\n- Phát hiện gian lận thẻ tín dụng\n- Nhận dạng khuôn mặt\n- Phân loại văn bản, hình ảnh\n\n### Các Loại Bài Toán Phân Loại\n\n**1. Phân Loại Nhị Phân (Binary Classification):**\n- Hai lớp duy nhất\n- Ví dụ: Email spam/không spam, Bệnh/khỏe mạnh\n- Mã hóa nhãn: 0 và 1, hoặc -1 và +1\n\n**2. Phân Loại Đa Lớp (Multiclass Classification):**\n- Nhiều hơn hai lớp\n- Mỗi mẫu thuộc đúng một lớp\n- Ví dụ: Nhận dạng chữ số (0-9), Phân loại loại hoa\n- Mã hóa nhãn: One-hot encoding\n\n**3. Phân Loại Đa Nhãn (Multilabel Classification):**\n- Mỗi mẫu có thể thuộc nhiều lớp\n- Ví dụ: Gắn thẻ bài viết (công nghệ, kinh tế, chính trị), Phân loại thể loại phim\n\n### Hồi Quy Logistic (Logistic Regression)\n\nMặc dù có tên là \"regression\", hồi quy logistic là thuật toán phân loại mô hình hóa xác suất của kết quả nhị phân.\n\n**Hàm Sigmoid (Logistic Function):**\n$$\\sigma(z) = \frac{1}{1 + e^{-z}}$$\n\nTrong đó: $z = \beta_0 + \beta_1x_1 + ... + \beta_nx_n = \beta^Tx$\n\n**Đặc điểm hàm Sigmoid:**\n- Miền giá trị: $(0, 1)$ - phù hợp để biểu diễn xác suất\n- $\\sigma(0) = 0.5$\n- $\\sigma(z) \to 1$ khi $z \to \\infty$\n- $\\sigma(z) \to 0$ khi $z \to -\\infty$\n- Đạo hàm: $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n\n**Diễn Giải:**\n\n**Các khái niệm quan trọng:**\n- Phân loại là một tác vụ học có giám sát trong đó mục tiêu là dự đoán nhãn lớp rời rạc. Khác với hồi quy dự đoán giá trị liên tục, phân loại gán các đầu vào vào các danh mục được định nghĩa trước. Các ứng dụng bao gồm phát hiện thư rác, chẩn đoán bệnh, nhận dạng chữ viết tay, phân tích cảm xúc, phát hiện gian lận thẻ tín dụng, nhận dạng khuôn mặt và phân loại văn bản, hình ảnh.\n\n**Mối quan hệ:**\n- Phân loại chứa bài toán Phân loại Nhị phân, nơi mục tiêu là dự đoán một trong hai lớp.\n- Phân loại chứa bài toán Phân loại Đa Lớp, nơi mục tiêu là dự đoán một trong nhiều lớp.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n\n### Giới Thiệu Về Cây Quyết Định\n\nCây quyết định là thuật toán học có giám sát đa năng có thể thực hiện cả tác vụ phân loại và hồi quy. Chúng học các quy tắc quyết định từ các đặc trưng để dự đoán giá trị mục tiêu thông qua cấu trúc dạng cây.\n\n**Ứng dụng thực tế:**\n- Chẩn đoán y tế (chuỗi quyết định dựa trên triệu chứng)\n- Đánh giá rủi ro tín dụng\n- Dự đoán churn khách hàng\n- Phát hiện gian lận\n- Hệ thống chuyên gia\n- Phân loại email spam\n\n**Tại sao gọi là \"cây\":**\n- Cấu trúc phân cấp giống cây ngược\n- Gốc ở trên, lá ở dưới\n- Quyết định được đưa ra tại mỗi nút nội bộ\n- Kết quả cuối cùng ở nút lá\n\n### Cấu Trúc Cây\n\n**1. Nút Gốc (Root Node):**\n- Nút trên cùng đại diện cho toàn bộ tập dữ liệu\n- Chứa tất cả mẫu training\n- Điểm bắt đầu của quá trình quyết định\n- Có phân chia đầu tiên dựa trên đặc trưng quan trọng nhất\n\n**2. Nút Nội Bộ (Internal Nodes):**\n- Các nút quyết định dựa trên kiểm tra đặc trưng\n- Mỗi nút thực hiện một câu hỏi yes/no về đặc trưng\n- Ví dụ: \"Tuổi > 30?\", \"Thu nhập < 50,000?\"\n- Chia dữ liệu thành các tập con\n\n**3. Nhánh (Branches):**\n- Kết quả của các quyết định\n- Kết nối nút cha với nút con\n- Đại diện cho giá trị hoặc phạm vi giá trị của đặc trưng\n\n**4. Nút Lá (Leaf Nodes):**\n- Nút cuối cùng không có nhánh con\n- Chứa dự đoán cuối cùng\n- Phân loại: Nhãn lớp\n- Hồi quy: Giá trị số\n\n**Ví dụ minh họa - Quyết định mua nhà:**\n```\n                 [Thu nhập > 50K?]\n                /                 \\\n            YES                    NO\n           /                         \\\n   [Tuổi > 30?]                [Không mua]\n    /        \\\n  YES        NO\n  /            \\\n[Mua]      [Thuê]\n```\n\n### Xây Dựng Cây Quyết Định\n\n**Tiêu Chí Phân Chia (Splitting Criteria):**\n\nMục tiêu: Tìm phân chia tốt nhất làm tăng \"độ thuần khiết\" (purity) của các tập con.\n\n**Cho Phân Loại:**\n\n**1. Gini Impurity (Chỉ Số Gini):**\n$$Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$$\n\nTrong đó:\n- $p_i$ là tỷ lệ mẫu thuộc lớp $i$ tại nút $t$\n- $C$ là số lớp\n- Gini = 0: Nút hoàn toàn thuần khiết (tất cả mẫu cùng lớp)\n\n**Các khái niệm quan trọng:**\n- Phân loại là một tác vụ trong học máy nhằm gán một nhãn lớp cụ thể cho các mẫu dữ liệu đầu vào dựa trên các đặc trưng của chúng. Mục tiêu là xây dựng một mô hình, ví dụ như Cây Quyết Định, có khả năng dự đoán chính xác nhãn lớp của các mẫu mới, với kết quả dự đoán ở nút lá là một nhãn lớp cụ thể.\n\n**Mối quan hệ:**\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Phân loại, gán nhãn lớp cho dữ liệu.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Phân Loại (Classification)\n**3. Minkowski Distance:**\n$$d(x,y) = \\left(\\sum_{i=1}^{n}|x_i-y_i|^p\right)^{1/p}$$\n- Tổng quát hóa Euclidean (p=2) và Manhattan (p=1)\n- p=∞: Chebyshev distance\n\n**4. Cosine Similarity:**\n$$similarity = \frac{x \\cdot y}{||x|| \times ||y||}$$\n- Đo góc giữa vectors\n- Tốt cho văn bản, high-dimensional sparse data\n- Không phụ thuộc vào độ lớn\n\n**5. Hamming Distance:**\n- Số vị trí khác nhau giữa hai chuỗi\n- Cho dữ liệu categorical hoặc binary\n\n**Chọn Giá Trị k:**\n\n**k quá nhỏ (k=1):**\n- Nhạy cảm với noise\n- Overfitting\n- Decision boundary phức tạp, không mượt\n\n**k quá lớn:**\n- Underfitting\n- Decision boundary quá mượt\n- Tính toán chậm\n- Có thể bị ảnh hưởng bởi lớp đa số\n\n**Lựa chọn k tối ưu:**\n- Cross-validation\n- Thử các giá trị k khác nhau (1, 3, 5, 7, 11, ...)\n- Chọn k lẻ để tránh tie trong binary classification\n- Thường k = √n (n là số mẫu training)\n- Elbow method: Vẽ error vs k\n\n**Weighted k-NN:**\n- Gán trọng số cho láng giềng dựa trên khoảng cách\n- Láng giềng gần hơn có ảnh hưởng lớn hơn\n$$weight = \frac{1}{distance}$$ hoặc $$weight = e^{-distance}$$\n\n**Ưu Điểm:**\n- Đơn giản, trực quan\n- Không có giai đoạn training (lazy learning)\n- Tự nhiên xử lý đa lớp\n- Hiệu quả với decision boundary phức tạp\n- Không giả định về phân phối dữ liệu\n\n**Nhược Điểm:**\n- Dự đoán chậm (O(n) cho mỗi dự đoán)\n- Yêu cầu lưu trữ toàn bộ training data\n- Nhạy cảm với scale của đặc trưng (cần scaling)\n- Hiệu suất giảm với high-dimensional data (curse of dimensionality)\n- Không xử lý tốt imbalanced data\n- Khó xử lý missing values\n\n**Tối Ưu Hóa k-NN:**\n- **KD-Tree, Ball Tree:** Cấu trúc dữ liệu để tìm kiếm nhanh\n- **Approximate Nearest Neighbors:** Hy sinh chút độ chính xác để tăng tốc\n- **Feature Selection:** Giảm dimensionality\n- **Dimensionality Reduction:** PCA, t-SNE\n\n### Các Chỉ Số Đánh Giá Phân Loại\n\n**Ma Trận Nhầm Lẫn (Confusion Matrix):**\n\n```\n                    Dự Đoán\n                Positive  Negative\nThực Tế  Pos      TP        FN\n         Neg      FP        TN\n```\n\n- **TP (True Positive):** Dự đoán đúng lớp dương\n- **TN (True Negative):** Dự đoán đúng lớp âm\n- **FP (False Positive):** Dự đoán sai là dương (Type I error)\n- **FN (False Negative):** Dự đoán sai là âm (Type II error)\n\n**Các Chỉ Số Chính:**\n\n**1. Accuracy (Độ Chính Xác):**\n$$Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$\n\n- Tỷ lệ dự đoán đúng tổng thể\n\n**Các khái niệm quan trọng:**\n- Accuracy là một thước đo hiệu suất của mô hình phân loại, tính bằng tỷ lệ số lượng dự đoán đúng trên tổng số dự đoán. Nó được cải thiện khi loại bỏ noise và các đặc trưng không liên quan.\n- Accuracy (Độ Chính Xác) là một thước đo hiệu suất phân loại, được tính bằng tỷ lệ tổng số dự đoán đúng (bao gồm cả dương tính thật - TP và âm tính thật - TN) trên tổng số tất cả các dự đoán (TP + TN + FP + FN). Công thức tính là: Accuracy = (TP + TN) / (TP + TN + FP + FN). Chỉ số này cho biết tỷ lệ dự đoán đúng tổng thể của mô hình, nhưng có thể gây hiểu lầm hoặc không phù hợp khi làm việc với dữ liệu mất cân bằng lớp (imbalanced data).\n\n**Mối quan hệ:**\n- Accuracy được dẫn xuất từ các giá trị trong Confusion Matrix (TP, TN, FP, FN)."
        },
        "output_without_context": {
            "question": "Giả sử trong một nút của cây quyết định, tần suất của lớp A là 30, lớp B là 10 và lớp C là 10. Tính sai số phân loại tại nút này.",
            "answer": "0.5"
        }
    },
    {
        "topic_description": "Khảo sát các ứng dụng thực tế của cây quyết định trong việc phân loại, như phân loại khách hàng vỡ nợ. Các câu hỏi sẽ yêu cầu phân tích tình huống và đưa ra nhận xét về hiệu suất của mô hình.",
        "output_with_context": {
            "question": "Trong việc đánh giá rủi ro tín dụng bằng cây quyết định, tiêu chí nào được sử dụng để tối ưu hóa độ thuần khiết của các tập con sau mỗi lần phân chia?",
            "answer": "Gini impurity",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n- Dễ overfit nếu không tune cẩn thận\n- Chậm hơn Random Forest (sequential)\n- Khó tune (nhiều hyperparameters)\n\n**3. AdaBoost (Adaptive Boosting):**\n\n**Nguyên lý:**\n- Tăng trọng số cho mẫu bị misclassified\n- Mỗi cây tập trung vào mẫu khó\n- Trọng số cho mô hình dựa trên accuracy\n\n**Ưu điểm:**\n- Đơn giản, hiệu quả\n- Ít tham số hơn Gradient Boosting\n- Tốt cho binary classification\n\n**Nhược điểm:**\n- Nhạy cảm với noise và outliers\n- Có thể overfit\n\n### Điều Chỉnh Hyperparameters\n\n**Tham số quan trọng:**\n\n**1. max_depth (Độ sâu tối đa):**\n- Giá trị nhỏ: Underfitting, mô hình đơn giản\n- Giá trị lớn: Overfitting, mô hình phức tạp\n- Thường: 3-10 cho cây đơn, 5-20 cho ensemble\n\n**2. min_samples_split:**\n- Số mẫu tối thiểu để split nút\n- Tăng lên: Giảm overfitting, cây đơn giản hơn\n- Thường: 2-100\n\n**3. min_samples_leaf:**\n- Số mẫu tối thiểu trong mỗi lá\n- Làm mượt decision boundary\n- Thường: 1-50\n\n**4. max_features:**\n- Số đặc trưng xem xét cho mỗi split\n- 'auto'/'sqrt': √n (cho classification)\n- 'log2': log₂(n)\n- None: Tất cả đặc trưng\n\n**5. criterion:**\n- 'gini': Gini impurity (mặc định, nhanh)\n- 'entropy': Information gain (chậm hơn)\n- 'squared_error': Cho regression\n\n**6. splitter:**\n- 'best': Chọn phân chia tốt nhất (mặc định)\n- 'random': Chọn ngẫu nhiên (nhanh hơn, thêm randomness)\n\n**Strategies cho Tuning:**\n- **Grid Search:** Thử tất cả combinations\n- **Random Search:** Sample ngẫu nhiên, hiệu quả hơn\n- **Bayesian Optimization:** Thông minh, ít iterations\n- **Cross-validation:** Luôn dùng CV để đánh giá\n\n**Tips:**\n- Bắt đầu với default parameters\n- Tune max_depth trước\n- Sau đó min_samples_split và min_samples_leaf\n- Cuối cùng các tham số khác\n- Monitor training vs validation performance\n\n### Ứng Dụng Thực Tế\n\n**1. Chẩn Đoán Y Tế:**\n- Chuỗi quyết định dựa trên triệu chứng\n- Dự đoán bệnh từ test results\n- Giải thích dễ dàng cho bác sĩ\n\n**2. Đánh Giá Rủi Ro Tín Dụng:**\n- Quyết định cho vay\n- Dự đoán default risk\n- Tuân thủ quy định (interpretability)\n\n**3. Dự Đoán Customer Churn:**\n- Xác định khách hàng có khả năng rời đi\n- Hành động marketing có mục tiêu\n- Hiểu lý do churn\n\n**4. Phát Hiện Gian Lận:**\n- Phát hiện transactions đáng ngờ\n- Real-time scoring\n- Giải thích cho investigation team\n\n**5. Feature Selection:**\n- Xác định đặc trưng quan trọng\n- Giảm dimensionality\n- Chuẩn bị cho mô hình khác\n\n**6. Hệ Thống Gợi Ý:**\n- Quyết định sản phẩm recommend\n- Personalization rules\n\n**Các khái niệm quan trọng:**\n- Credit Risk Assessment là bài toán đánh giá rủi ro tín dụng để đưa ra quyết định cho vay. Decision Tree có thể dự đoán rủi ro vỡ nợ và tuân thủ các quy định nhờ khả năng giải thích (interpretability).\n- Customer Churn Prediction là bài toán dự đoán khách hàng có khả năng rời bỏ dịch vụ hoặc sản phẩm. Decision Tree có thể được ứng dụng để giải quyết bài toán này, giúp xác định khách hàng có nguy cơ và đưa ra các hành động marketing mục tiêu.\n- Decision Tree là một mô hình học máy được sử dụng cho cả bài toán phân loại và hồi quy, hoạt động bằng cách xây dựng một cấu trúc cây phân cấp để đưa ra quyết định. Cây quyết định phân chia dữ liệu dựa trên các đặc trưng và ngưỡng để tạo ra các nút lá chứa dự đoán cuối cùng. Thuật toán CART là một trong những thuật toán phổ biến nhất để xây dựng cây quyết định nhị phân.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n\n### Giới Thiệu Về Cây Quyết Định\n\nCây quyết định là thuật toán học có giám sát đa năng có thể thực hiện cả tác vụ phân loại và hồi quy. Chúng học các quy tắc quyết định từ các đặc trưng để dự đoán giá trị mục tiêu thông qua cấu trúc dạng cây.\n\n**Ứng dụng thực tế:**\n- Chẩn đoán y tế (chuỗi quyết định dựa trên triệu chứng)\n- Đánh giá rủi ro tín dụng\n- Dự đoán churn khách hàng\n- Phát hiện gian lận\n- Hệ thống chuyên gia\n- Phân loại email spam\n\n**Tại sao gọi là \"cây\":**\n- Cấu trúc phân cấp giống cây ngược\n- Gốc ở trên, lá ở dưới\n- Quyết định được đưa ra tại mỗi nút nội bộ\n- Kết quả cuối cùng ở nút lá\n\n### Cấu Trúc Cây\n\n**1. Nút Gốc (Root Node):**\n- Nút trên cùng đại diện cho toàn bộ tập dữ liệu\n- Chứa tất cả mẫu training\n- Điểm bắt đầu của quá trình quyết định\n- Có phân chia đầu tiên dựa trên đặc trưng quan trọng nhất\n\n**2. Nút Nội Bộ (Internal Nodes):**\n- Các nút quyết định dựa trên kiểm tra đặc trưng\n- Mỗi nút thực hiện một câu hỏi yes/no về đặc trưng\n- Ví dụ: \"Tuổi > 30?\", \"Thu nhập < 50,000?\"\n- Chia dữ liệu thành các tập con\n\n**3. Nhánh (Branches):**\n- Kết quả của các quyết định\n- Kết nối nút cha với nút con\n- Đại diện cho giá trị hoặc phạm vi giá trị của đặc trưng\n\n**4. Nút Lá (Leaf Nodes):**\n- Nút cuối cùng không có nhánh con\n- Chứa dự đoán cuối cùng\n- Phân loại: Nhãn lớp\n- Hồi quy: Giá trị số\n\n**Ví dụ minh họa - Quyết định mua nhà:**\n```\n                 [Thu nhập > 50K?]\n                /                 \\\n            YES                    NO\n           /                         \\\n   [Tuổi > 30?]                [Không mua]\n    /        \\\n  YES        NO\n  /            \\\n[Mua]      [Thuê]\n```\n\n### Xây Dựng Cây Quyết Định\n\n**Tiêu Chí Phân Chia (Splitting Criteria):**\n\nMục tiêu: Tìm phân chia tốt nhất làm tăng \"độ thuần khiết\" (purity) của các tập con.\n\n**Cho Phân Loại:**\n\n**1. Gini Impurity (Chỉ Số Gini):**\n$$Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$$\n\nTrong đó:\n- $p_i$ là tỷ lệ mẫu thuộc lớp $i$ tại nút $t$\n- $C$ là số lớp\n- Gini = 0: Nút hoàn toàn thuần khiết (tất cả mẫu cùng lớp)\n\n**Các khái niệm quan trọng:**\n- Cây Quyết Định là một mô hình học máy có giám sát đa năng, được sử dụng cho cả bài toán phân loại và hồi quy. Mô hình này hoạt động bằng cách học các quy tắc quyết định từ các đặc trưng của dữ liệu và biểu diễn chúng dưới dạng cấu trúc cây phân cấp để dự đoán giá trị mục tiêu. Cấu trúc cây bao gồm Nút Gốc, Nút Nội Bộ (mỗi nút đại diện cho một kiểm tra trên một thuộc tính), Nhánh (mỗi nhánh đại diện cho kết quả của kiểm tra), và Nút Lá (mỗi nút lá đại diện cho một nhãn lớp hoặc giá trị số). Quá trình xây dựng cây bao gồm việc phân chia dữ liệu dựa trên các đặc trưng để tối đa hóa sự giảm impurity. Cây Quyết Định có thể được xây dựng đầy đủ và sau đó được cắt tỉa (pruning) để chọn cây tối ưu bằng cross-validation. Ưu điểm của mô hình này là dễ hiểu, dễ diễn giải, yêu cầu ít tiền xử lý dữ liệu, xử lý được cả dữ liệu số và phân loại, là non-parametric, bắt được mối quan hệ phi tuyến, và tính toán Feature Importance tự nhiên. Tuy nhiên, việc xây dựng cây đầy đủ có thể tốn thời gian và phức tạp hơn so với pre-pruning.\n- Phân loại là một tác vụ trong học máy nhằm gán một nhãn lớp cụ thể cho các mẫu dữ liệu đầu vào dựa trên các đặc trưng của chúng. Mục tiêu là xây dựng một mô hình, ví dụ như Cây Quyết Định, có khả năng dự đoán chính xác nhãn lớp của các mẫu mới, với kết quả dự đoán ở nút lá là một nhãn lớp cụ thể.\n- Tiêu Chí Phân Chia là kỹ thuật được sử dụng trong quá trình xây dựng Cây Quyết Định để tìm ra cách phân chia dữ liệu tốt nhất tại mỗi nút. Mục tiêu của tiêu chí này là làm tăng \"độ thuần khiết\" (purity) của các tập con được tạo ra sau khi phân chia.\n\n**Mối quan hệ:**\n- Cây Quyết Định sử dụng Tiêu Chí Phân Chia để xác định cách tốt nhất để phân tách dữ liệu tại mỗi nút.\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Phân loại, gán nhãn lớp cho dữ liệu.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n1. Xây dựng cây đầy đủ\n2. Tính $\\alpha$ cho mỗi subtree\n3. Loại bỏ subtree có $\\alpha$ nhỏ nhất (đóng góp ít nhất)\n4. Lặp lại cho đến khi còn gốc\n5. Chọn cây tối ưu bằng cross-validation\n\n**Ưu điểm:**\n- Thường cho kết quả tốt hơn pre-pruning\n- Không bỏ lỡ phân chia tốt\n\n**Nhược điểm:**\n- Tốn thời gian (xây dựng cây đầy đủ trước)\n- Phức tạp hơn\n\n### Feature Importance (Tầm Quan Trọng Đặc Trưng)\n\nCây quyết định tự động tính toán mức độ quan trọng của mỗi đặc trưng.\n\n**Công Thức:**\n$$Importance(f) = \\sum_{t \\in T} p(t) \\cdot \\Delta impurity(t, f)$$\n\nTrong đó:\n- $p(t)$ là tỷ lệ mẫu tại nút $t$\n- $\\Delta impurity(t, f)$ là giảm impurity khi phân chia theo đặc trưng $f$ tại nút $t$\n- Tổng trên tất cả nút sử dụng đặc trưng $f$\n\n**Diễn giải:**\n- Giá trị càng cao, đặc trưng càng quan trọng\n- Tổng tất cả importance = 1\n- Đặc trưng không xuất hiện có importance = 0\n\n**Ứng dụng:**\n- Feature selection\n- Hiểu mô hình\n- Phát hiện đặc trưng không cần thiết\n- Giải thích cho stakeholders\n\n**Lưu ý:**\n- Thiên vị về đặc trưng có nhiều giá trị unique\n- Đặc trưng tương quan cao có importance phân tán\n- Sử dụng permutation importance để khắc phục\n\n### Ưu Điểm Của Cây Quyết Định\n\n**1. Dễ hiểu và diễn giải:**\n- Trực quan, giống cách con người quyết định\n- Có thể vẽ và giải thích bằng lời\n- Không cần kiến thức thống kê sâu\n- Phù hợp cho business users\n\n**2. Yêu cầu ít tiền xử lý dữ liệu:**\n- Không cần feature scaling\n- Không cần one-hot encoding cho categorical\n- Xử lý được missing values (surrogate splits)\n- Không cần assumption về phân phối\n\n**3. Xử lý dữ liệu số và phân loại:**\n- Linh hoạt với nhiều loại đặc trưng\n- Không cần encoding phức tạp\n- Mixed data types\n\n**4. Non-parametric:**\n- Không giả định về phân phối dữ liệu\n- Linh hoạt với mọi dạng data\n- Không cần chọn hàm phân phối\n\n**5. Bắt được mối quan hệ phi tuyến:**\n- Decision boundary phức tạp\n- Tương tác giữa các đặc trưng\n- Không giới hạn bởi tuyến tính\n\n**6. Tính toán Feature Importance tự nhiên:**\n- Không cần phương pháp bên ngoài\n- Tích hợp trong thuật toán\n\n**7. Nhanh với dự đoán:**\n- Độ phức tạp: O(log n) với cây cân bằng\n- Hiệu quả cho production\n\n**Các khái niệm quan trọng:**\n- Cây Quyết Định là một mô hình học máy có giám sát đa năng, được sử dụng cho cả bài toán phân loại và hồi quy. Mô hình này hoạt động bằng cách học các quy tắc quyết định từ các đặc trưng của dữ liệu và biểu diễn chúng dưới dạng cấu trúc cây phân cấp để dự đoán giá trị mục tiêu. Cấu trúc cây bao gồm Nút Gốc, Nút Nội Bộ (mỗi nút đại diện cho một kiểm tra trên một thuộc tính), Nhánh (mỗi nhánh đại diện cho kết quả của kiểm tra), và Nút Lá (mỗi nút lá đại diện cho một nhãn lớp hoặc giá trị số). Quá trình xây dựng cây bao gồm việc phân chia dữ liệu dựa trên các đặc trưng để tối đa hóa sự giảm impurity. Cây Quyết Định có thể được xây dựng đầy đủ và sau đó được cắt tỉa (pruning) để chọn cây tối ưu bằng cross-validation. Ưu điểm của mô hình này là dễ hiểu, dễ diễn giải, yêu cầu ít tiền xử lý dữ liệu, xử lý được cả dữ liệu số và phân loại, là non-parametric, bắt được mối quan hệ phi tuyến, và tính toán Feature Importance tự nhiên. Tuy nhiên, việc xây dựng cây đầy đủ có thể tốn thời gian và phức tạp hơn so với pre-pruning.\n\n**Mối quan hệ:**\n- Cây Quyết Định sử dụng Tiêu Chí Phân Chia để xác định cách tốt nhất để phân tách dữ liệu tại mỗi nút.\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Phân loại, gán nhãn lớp cho dữ liệu.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Phân Loại (Classification)\n- Ưu điểm: Mỗi mô hình đơn giản hơn, cân bằng hơn\n- Nhược điểm: Nhiều mô hình (phức tạp khi K lớn)\n\n**Ví dụ:** 3 lớp (A, B, C)\n- Mô hình 1: A vs B\n- Mô hình 2: A vs C\n- Mô hình 3: B vs C\n\n**3. Softmax Regression (Multinomial Logistic Regression):**\n\nMở rộng trực tiếp của logistic regression cho đa lớp.\n\n**Công thức:**\n$$P(y=k|x) = \frac{e^{z_k}}{\\sum_{j=1}^{K}e^{z_j}}$$\n\nTrong đó: $z_k = \beta_k^Tx$ với $\beta_k$ là vector hệ số cho lớp $k$\n\n**Đặc điểm:**\n- Tổng các xác suất = 1: $\\sum_{k=1}^{K}P(y=k|x) = 1$\n- Output là phân phối xác suất trên tất cả lớp\n- Huấn luyện đồng thời tất cả lớp\n\n**Hàm chi phí (Categorical Cross-Entropy):**\n$$J(\beta) = -\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}\\log(P(y=k|x^{(i)}))$$\n\nTrong đó $y_k^{(i)}$ là one-hot encoding của nhãn.\n\n**Lựa chọn giữa OvR, OvO, và Softmax:**\n- **Softmax:** Tốt nhất khi cần xác suất, K không quá lớn\n- **OvR:** Đơn giản, hiệu quả với K lớn\n- **OvO:** Tốt với SVM, K nhỏ/trung bình\n\n### Naive Bayes Classifier (Bộ Phân Loại Naive Bayes)\n\nDựa trên định lý Bayes với giả định \"ngây thơ\" (naive) về tính độc lập đặc trưng.\n\n**Định Lý Bayes:**\n$$P(C_k|x) = \frac{P(x|C_k)P(C_k)}{P(x)}$$\n\nTrong đó:\n- $P(C_k|x)$: Xác suất hậu nghiệm (posterior) - xác suất lớp $C_k$ cho trước $x$\n- $P(x|C_k)$: Likelihood - xác suất của $x$ trong lớp $C_k$\n- $P(C_k)$: Xác suất tiên nghiệm (prior) của lớp $C_k$\n- $P(x)$: Evidence - xác suất của $x$\n\n**Giả Định Naive (Độc Lập Điều Kiện):**\n$$P(x_1, x_2, ..., x_n|C_k) = \\prod_{i=1}^{n}P(x_i|C_k)$$\n\nCác đặc trưng độc lập với nhau khi biết lớp.\n\n**Công Thức Đầy Đủ:**\n$$P(C_k|x_1,...,x_n) = \frac{P(C_k)\\prod_{i=1}^{n}P(x_i|C_k)}{P(x_1,...,x_n)}$$\n\n**Quyết Định:**\n$$\\hat{y} = \\arg\\max_{k} P(C_k)\\prod_{i=1}^{n}P(x_i|C_k)$$\n\nKhông cần tính $P(x)$ vì nó giống nhau cho tất cả lớp.\n\n**Các Biến Thể:**\n\n**1. Gaussian Naive Bayes:**\n- Cho đặc trưng liên tục\n- Giả định phân phối Gaussian (chuẩn)\n\n**Các khái niệm quan trọng:**\n- Phân loại là một tác vụ trong học máy nhằm gán một nhãn lớp cụ thể cho các mẫu dữ liệu đầu vào dựa trên các đặc trưng của chúng. Mục tiêu là xây dựng một mô hình, ví dụ như Cây Quyết Định, có khả năng dự đoán chính xác nhãn lớp của các mẫu mới, với kết quả dự đoán ở nút lá là một nhãn lớp cụ thể.\n\n**Mối quan hệ:**\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Phân loại, gán nhãn lớp cho dữ liệu."
        },
        "output_without_context": {
            "question": "Khi đánh giá hiệu suất của mô hình cây quyết định trong việc phân loại khách hàng vỡ nợ, yếu tố nào là quan trọng nhất để xem xét?",
            "answer": "Độ chính xác của mô hình."
        }
    },
    {
        "topic_description": "Khảo sát sự khác biệt và tương đồng giữa hồi quy logistic và cây quyết định trong phân loại. Câu hỏi sẽ tập trung vào hiểu biết về ưu và nhược điểm của cả hai phương pháp đã học từ tuần trước.",
        "output_with_context": {
            "question": "So với hồi quy logistic, ưu điểm nào của cây quyết định trong phân loại là dễ hiểu và dễ diễn giải?",
            "answer": "Cây quyết định",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n- Gini = 0.5: Nút hỗn loạn nhất (phân bố đều giữa các lớp)\n\n**Ví dụ:**\n- Nút có 100 mẫu: 80 lớp A, 20 lớp B\n- $Gini = 1 - (0.8^2 + 0.2^2) = 1 - (0.64 + 0.04) = 0.32$\n\n**2. Entropy và Information Gain (Độ Lợi Thông Tin):**\n$$Entropy(t) = -\\sum_{i=1}^{C}p_i\\log_2(p_i)$$\n\n- Entropy đo lường độ hỗn loạn/không chắc chắn\n- Entropy = 0: Thuần khiết hoàn toàn\n- Entropy cao: Hỗn loạn\n\n**Information Gain:**\n$$IG(D_p, f) = Entropy(D_p) - \\sum_{j=1}^{m}\frac{N_j}{N_p}Entropy(D_j)$$\n\nTrong đó:\n- $D_p$ là tập dữ liệu cha\n- $D_j$ là tập con thứ $j$ sau phân chia\n- $N_j$ là số mẫu trong $D_j$\n- $N_p$ là tổng số mẫu trong $D_p$\n- $m$ là số tập con\n\n**Information Gain đo lường:**\n- Giảm entropy sau khi phân chia\n- Giá trị càng cao càng tốt\n- Phân chia tốt nhất có IG cao nhất\n\n**So sánh Gini vs Entropy:**\n| Tiêu chí | Gini | Entropy |\n|----------|------|---------|\n| Tính toán | Nhanh hơn | Chậm hơn (có log) |\n| Độ nhạy | Ít nhạy | Nhạy hơn với thay đổi |\n| Kết quả | Tương tự trong thực tế | Tương tự |\n| Mặc định | Sklearn default | ID3, C4.5 |\n\n**Cho Hồi Quy:**\n\n**3. Mean Squared Error (MSE):**\n$$MSE = \frac{1}{N}\\sum_{i=1}^{N}(y_i - \bar{y})^2$$\n\n- $\bar{y}$ là giá trị trung bình của mục tiêu tại nút\n- Đo lường phương sai của giá trị mục tiêu\n- Mục tiêu: Giảm MSE sau phân chia\n\n**4. Mean Absolute Error (MAE):**\n$$MAE = \frac{1}{N}\\sum_{i=1}^{N}|y_i - \bar{y}|$$\n\n- Ít nhạy cảm với outliers hơn MSE\n- Median thay vì mean làm giá trị dự đoán\n\n### Thuật Toán CART (Classification and Regression Trees)\n\nCART là thuật toán phổ biến nhất để xây dựng cây quyết định, tạo ra cây nhị phân.\n\n**Các Bước:**\n\n**1. Bắt đầu với tất cả dữ liệu training tại gốc:**\n- Tính impurity ban đầu\n- Xem xét tất cả đặc trưng và ngưỡng có thể\n\n**2. Chọn đặc trưng và ngưỡng tốt nhất:**\n- Thử tất cả đặc trưng\n- Với mỗi đặc trưng, thử nhiều ngưỡng\n- Tính information gain hoặc giảm Gini\n- Chọn phân chia có improvement cao nhất\n\n**3. Phân chia tập dữ liệu thành các tập con:**\n- Tạo nhánh trái và phải\n- Mẫu thỏa điều kiện đi trái\n- Mẫu không thỏa đi phải\n\n\n**Các khái niệm quan trọng:**\n- Phân loại là một loại bài toán học máy trong đó mục tiêu là dự đoán nhãn lớp rời rạc cho một mẫu dữ liệu. Trong cây quyết định, các tiêu chí như Gini và Entropy được sử dụng để phân chia dữ liệu cho các bài toán phân loại.\n- Hồi quy (regression) là một loại bài toán trong Machine Learning nhằm dự đoán một giá trị đầu ra liên tục dựa trên các biến đầu vào. Các ví dụ bao gồm dự đoán giá nhà, nhiệt độ hoặc doanh số bán hàng. Trong các bài toán hồi quy, các hàm mất mát phổ biến như Mean Squared Error (MSE) và Mean Absolute Error (MAE) được sử dụng, đặc biệt là trong cây quyết định để phân chia dữ liệu.\n- Decision Tree là một mô hình học máy được sử dụng cho cả bài toán phân loại và hồi quy, hoạt động bằng cách xây dựng một cấu trúc cây phân cấp để đưa ra quyết định. Cây quyết định phân chia dữ liệu dựa trên các đặc trưng và ngưỡng để tạo ra các nút lá chứa dự đoán cuối cùng. Thuật toán CART là một trong những thuật toán phổ biến nhất để xây dựng cây quyết định nhị phân.\n\n**Mối quan hệ:**\n- Gini Impurity được áp dụng làm tiêu chí phân chia cho các bài toán phân loại trong cây quyết định.\n- Entropy được áp dụng làm tiêu chí phân chia cho các bài toán phân loại trong cây quyết định.\n- Mean Squared Error được áp dụng làm tiêu chí phân chia cho các bài toán hồi quy trong cây quyết định.\n- Decision Tree sử dụng Entropy làm tiêu chí để đánh giá độ hỗn loạn của các nút và chọn phân chia tốt nhất trong bài toán phân loại.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Phân Loại (Classification)\n$$P(y=1|x) = \\sigma(\beta^Tx) = \frac{1}{1 + e^{-\beta^Tx}}$$\n- Dự đoán xác suất lớp dương (y=1)\n- Nếu $P(y=1|x) \\geq 0.5$: Dự đoán lớp 1\n- Nếu $P(y=1|x) < 0.5$: Dự đoán lớp 0\n\n**Quyết Định Ranh Giới (Decision Boundary):**\n- Tuyến tính trong không gian đặc trưng: $\beta^Tx = 0$\n- Phi tuyến có thể đạt được bằng polynomial features\n- Ngưỡng quyết định có thể điều chỉnh (không nhất thiết 0.5)\n\n**Hàm Chi Phí (Cross-Entropy Loss):**\n$$J(\beta) = -\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_\beta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\beta(x^{(i)}))]$$\n\n**Lý do không dùng MSE:**\n- MSE với sigmoid tạo hàm non-convex\n- Nhiều cực tiểu địa phương\n- Gradient descent khó hội tụ\n\n**Cross-entropy:**\n- Hàm convex\n- Gradient descent hội tụ toàn cục\n- Phạt nặng dự đoán sai với confidence cao\n\n**Tối Ưu Hóa:**\n- Gradient descent (hoặc các biến thể)\n- Đạo hàm: $\frac{\\partial J}{\\partial\beta_j} = \frac{1}{m}\\sum_{i=1}^{m}(h_\beta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n- Giống với linear regression nhưng $h_\beta$ khác\n\n**Regularization trong Logistic Regression:**\n\n**L2 (Ridge):**\n$$J(\beta) = -\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_\beta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\beta(x^{(i)}))] + \frac{\\lambda}{2m}\\sum_{j=1}^{n}\beta_j^2$$\n\n**L1 (Lasso):**\n$$J(\beta) = -\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_\beta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\beta(x^{(i)}))] + \frac{\\lambda}{m}\\sum_{j=1}^{n}|\beta_j|$$\n\n### Phân Loại Đa Lớp\n\n**1. One-vs-Rest (OvR) / One-vs-All (OvA):**\n- Huấn luyện K bộ phân loại nhị phân cho K lớp\n- Mỗi bộ phân loại: Một lớp vs tất cả lớp còn lại\n- Dự đoán: Chọn lớp có confidence cao nhất\n- Ưu điểm: Đơn giản, phù hợp với mọi thuật toán nhị phân\n- Nhược điểm: Mất cân bằng lớp, K mô hình riêng biệt\n\n**Ví dụ:** 3 lớp (A, B, C)\n- Mô hình 1: A vs (B, C)\n- Mô hình 2: B vs (A, C)\n- Mô hình 3: C vs (A, B)\n\n**2. One-vs-One (OvO):**\n- Huấn luyện $\frac{K(K-1)}{2}$ bộ phân loại nhị phân\n- Mỗi cặp lớp có một bộ phân loại\n- Dự đoán: Voting - lớp thắng nhiều nhất\n\n**Các khái niệm quan trọng:**\n- Đây là công thức của Cross-Entropy Loss cho bài toán phân loại nhị phân, được sử dụng làm hàm mục tiêu trong Logistic Regression. Nó tính toán trung bình log-likelihood âm trên toàn bộ tập huấn luyện, đo lường sự khác biệt giữa nhãn thực y^(i) và xác suất dự đoán h_β(x^(i)).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Hồi Quy Tuyến Tính (Linear Regression)\n**Underfitting (High Bias):**\n- Training error cao\n- Validation error cao\n- Mô hình quá đơn giản\n- Giải pháp: Thêm đặc trưng, tăng độ phức tạp, giảm regularization\n\n**Overfitting (High Variance):**\n- Training error thấp\n- Validation error cao (chênh lệch lớn)\n- Mô hình quá phức tạp\n- Giải pháp: Thêm dữ liệu, regularization, giảm đặc trưng, early stopping\n\n**Good fit:**\n- Training error thấp\n- Validation error thấp\n- Chênh lệch nhỏ giữa hai errors\n\n\n---\n\n## Phân Loại (Classification)\n\n### Giới Thiệu Về Phân Loại\n\nPhân loại là một tác vụ học có giám sát trong đó mục tiêu là dự đoán nhãn lớp rời rạc. Khác với hồi quy dự đoán giá trị liên tục, phân loại gán các đầu vào vào các danh mục được định nghĩa trước.\n\n**Ứng dụng thực tế:**\n- Phát hiện thư rác (spam/không spam)\n- Chẩn đoán bệnh (bệnh/không bệnh)\n- Nhận dạng chữ viết tay\n- Phân tích cảm xúc (tích cực/tiêu cực/trung lập)\n- Phát hiện gian lận thẻ tín dụng\n- Nhận dạng khuôn mặt\n- Phân loại văn bản, hình ảnh\n\n### Các Loại Bài Toán Phân Loại\n\n**1. Phân Loại Nhị Phân (Binary Classification):**\n- Hai lớp duy nhất\n- Ví dụ: Email spam/không spam, Bệnh/khỏe mạnh\n- Mã hóa nhãn: 0 và 1, hoặc -1 và +1\n\n**2. Phân Loại Đa Lớp (Multiclass Classification):**\n- Nhiều hơn hai lớp\n- Mỗi mẫu thuộc đúng một lớp\n- Ví dụ: Nhận dạng chữ số (0-9), Phân loại loại hoa\n- Mã hóa nhãn: One-hot encoding\n\n**3. Phân Loại Đa Nhãn (Multilabel Classification):**\n- Mỗi mẫu có thể thuộc nhiều lớp\n- Ví dụ: Gắn thẻ bài viết (công nghệ, kinh tế, chính trị), Phân loại thể loại phim\n\n### Hồi Quy Logistic (Logistic Regression)\n\nMặc dù có tên là \"regression\", hồi quy logistic là thuật toán phân loại mô hình hóa xác suất của kết quả nhị phân.\n\n**Hàm Sigmoid (Logistic Function):**\n$$\\sigma(z) = \frac{1}{1 + e^{-z}}$$\n\nTrong đó: $z = \beta_0 + \beta_1x_1 + ... + \beta_nx_n = \beta^Tx$\n\n**Đặc điểm hàm Sigmoid:**\n- Miền giá trị: $(0, 1)$ - phù hợp để biểu diễn xác suất\n- $\\sigma(0) = 0.5$\n- $\\sigma(z) \to 1$ khi $z \to \\infty$\n- $\\sigma(z) \to 0$ khi $z \to -\\infty$\n- Đạo hàm: $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n\n**Diễn Giải:**\n\n**Các khái niệm quan trọng:**\n- Logistic Regression là một mô hình phân loại tuyến tính được sử dụng để dự đoán xác suất của một sự kiện hoặc một mẫu thuộc về một lớp cụ thể. Mặc dù tên có \"Regression\", đây là một thuật toán phân loại nhị phân hoặc đa lớp, thường được sử dụng khi đầu ra là một biến phân loại. Nó sử dụng hàm sigmoid để ánh xạ đầu ra tuyến tính vào một phạm vi từ 0 đến 1, biểu thị xác suất, và được thiết kế để cung cấp xác suất trực tiếp.\n\n**Mối quan hệ:**\n- Logistic Regression sử dụng Sigmoid Function để ánh xạ đầu ra tuyến tính thành xác suất trong khoảng (0, 1).\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n\n### Giới Thiệu Về Cây Quyết Định\n\nCây quyết định là thuật toán học có giám sát đa năng có thể thực hiện cả tác vụ phân loại và hồi quy. Chúng học các quy tắc quyết định từ các đặc trưng để dự đoán giá trị mục tiêu thông qua cấu trúc dạng cây.\n\n**Ứng dụng thực tế:**\n- Chẩn đoán y tế (chuỗi quyết định dựa trên triệu chứng)\n- Đánh giá rủi ro tín dụng\n- Dự đoán churn khách hàng\n- Phát hiện gian lận\n- Hệ thống chuyên gia\n- Phân loại email spam\n\n**Tại sao gọi là \"cây\":**\n- Cấu trúc phân cấp giống cây ngược\n- Gốc ở trên, lá ở dưới\n- Quyết định được đưa ra tại mỗi nút nội bộ\n- Kết quả cuối cùng ở nút lá\n\n### Cấu Trúc Cây\n\n**1. Nút Gốc (Root Node):**\n- Nút trên cùng đại diện cho toàn bộ tập dữ liệu\n- Chứa tất cả mẫu training\n- Điểm bắt đầu của quá trình quyết định\n- Có phân chia đầu tiên dựa trên đặc trưng quan trọng nhất\n\n**2. Nút Nội Bộ (Internal Nodes):**\n- Các nút quyết định dựa trên kiểm tra đặc trưng\n- Mỗi nút thực hiện một câu hỏi yes/no về đặc trưng\n- Ví dụ: \"Tuổi > 30?\", \"Thu nhập < 50,000?\"\n- Chia dữ liệu thành các tập con\n\n**3. Nhánh (Branches):**\n- Kết quả của các quyết định\n- Kết nối nút cha với nút con\n- Đại diện cho giá trị hoặc phạm vi giá trị của đặc trưng\n\n**4. Nút Lá (Leaf Nodes):**\n- Nút cuối cùng không có nhánh con\n- Chứa dự đoán cuối cùng\n- Phân loại: Nhãn lớp\n- Hồi quy: Giá trị số\n\n**Ví dụ minh họa - Quyết định mua nhà:**\n```\n                 [Thu nhập > 50K?]\n                /                 \\\n            YES                    NO\n           /                         \\\n   [Tuổi > 30?]                [Không mua]\n    /        \\\n  YES        NO\n  /            \\\n[Mua]      [Thuê]\n```\n\n### Xây Dựng Cây Quyết Định\n\n**Tiêu Chí Phân Chia (Splitting Criteria):**\n\nMục tiêu: Tìm phân chia tốt nhất làm tăng \"độ thuần khiết\" (purity) của các tập con.\n\n**Cho Phân Loại:**\n\n**1. Gini Impurity (Chỉ Số Gini):**\n$$Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$$\n\nTrong đó:\n- $p_i$ là tỷ lệ mẫu thuộc lớp $i$ tại nút $t$\n- $C$ là số lớp\n- Gini = 0: Nút hoàn toàn thuần khiết (tất cả mẫu cùng lớp)\n\n**Các khái niệm quan trọng:**\n- Cây Quyết Định là một mô hình học máy có giám sát đa năng, được sử dụng cho cả bài toán phân loại và hồi quy. Mô hình này hoạt động bằng cách học các quy tắc quyết định từ các đặc trưng của dữ liệu và biểu diễn chúng dưới dạng cấu trúc cây phân cấp để dự đoán giá trị mục tiêu. Cấu trúc cây bao gồm Nút Gốc, Nút Nội Bộ (mỗi nút đại diện cho một kiểm tra trên một thuộc tính), Nhánh (mỗi nhánh đại diện cho kết quả của kiểm tra), và Nút Lá (mỗi nút lá đại diện cho một nhãn lớp hoặc giá trị số). Quá trình xây dựng cây bao gồm việc phân chia dữ liệu dựa trên các đặc trưng để tối đa hóa sự giảm impurity. Cây Quyết Định có thể được xây dựng đầy đủ và sau đó được cắt tỉa (pruning) để chọn cây tối ưu bằng cross-validation. Ưu điểm của mô hình này là dễ hiểu, dễ diễn giải, yêu cầu ít tiền xử lý dữ liệu, xử lý được cả dữ liệu số và phân loại, là non-parametric, bắt được mối quan hệ phi tuyến, và tính toán Feature Importance tự nhiên. Tuy nhiên, việc xây dựng cây đầy đủ có thể tốn thời gian và phức tạp hơn so với pre-pruning.\n- Hồi quy là một tác vụ trong học máy mà Cây Quyết Định có thể thực hiện, nhằm dự đoán một giá trị số liên tục cho các mẫu dữ liệu dựa trên các đặc trưng đầu vào. Kết quả dự đoán ở nút lá là một giá trị số.\n\n**Mối quan hệ:**\n- Cây Quyết Định sử dụng Tiêu Chí Phân Chia để xác định cách tốt nhất để phân tách dữ liệu tại mỗi nút.\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Hồi quy, dự đoán giá trị số liên tục.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n1. Xây dựng cây đầy đủ\n2. Tính $\\alpha$ cho mỗi subtree\n3. Loại bỏ subtree có $\\alpha$ nhỏ nhất (đóng góp ít nhất)\n4. Lặp lại cho đến khi còn gốc\n5. Chọn cây tối ưu bằng cross-validation\n\n**Ưu điểm:**\n- Thường cho kết quả tốt hơn pre-pruning\n- Không bỏ lỡ phân chia tốt\n\n**Nhược điểm:**\n- Tốn thời gian (xây dựng cây đầy đủ trước)\n- Phức tạp hơn\n\n### Feature Importance (Tầm Quan Trọng Đặc Trưng)\n\nCây quyết định tự động tính toán mức độ quan trọng của mỗi đặc trưng.\n\n**Công Thức:**\n$$Importance(f) = \\sum_{t \\in T} p(t) \\cdot \\Delta impurity(t, f)$$\n\nTrong đó:\n- $p(t)$ là tỷ lệ mẫu tại nút $t$\n- $\\Delta impurity(t, f)$ là giảm impurity khi phân chia theo đặc trưng $f$ tại nút $t$\n- Tổng trên tất cả nút sử dụng đặc trưng $f$\n\n**Diễn giải:**\n- Giá trị càng cao, đặc trưng càng quan trọng\n- Tổng tất cả importance = 1\n- Đặc trưng không xuất hiện có importance = 0\n\n**Ứng dụng:**\n- Feature selection\n- Hiểu mô hình\n- Phát hiện đặc trưng không cần thiết\n- Giải thích cho stakeholders\n\n**Lưu ý:**\n- Thiên vị về đặc trưng có nhiều giá trị unique\n- Đặc trưng tương quan cao có importance phân tán\n- Sử dụng permutation importance để khắc phục\n\n### Ưu Điểm Của Cây Quyết Định\n\n**1. Dễ hiểu và diễn giải:**\n- Trực quan, giống cách con người quyết định\n- Có thể vẽ và giải thích bằng lời\n- Không cần kiến thức thống kê sâu\n- Phù hợp cho business users\n\n**2. Yêu cầu ít tiền xử lý dữ liệu:**\n- Không cần feature scaling\n- Không cần one-hot encoding cho categorical\n- Xử lý được missing values (surrogate splits)\n- Không cần assumption về phân phối\n\n**3. Xử lý dữ liệu số và phân loại:**\n- Linh hoạt với nhiều loại đặc trưng\n- Không cần encoding phức tạp\n- Mixed data types\n\n**4. Non-parametric:**\n- Không giả định về phân phối dữ liệu\n- Linh hoạt với mọi dạng data\n- Không cần chọn hàm phân phối\n\n**5. Bắt được mối quan hệ phi tuyến:**\n- Decision boundary phức tạp\n- Tương tác giữa các đặc trưng\n- Không giới hạn bởi tuyến tính\n\n**6. Tính toán Feature Importance tự nhiên:**\n- Không cần phương pháp bên ngoài\n- Tích hợp trong thuật toán\n\n**7. Nhanh với dự đoán:**\n- Độ phức tạp: O(log n) với cây cân bằng\n- Hiệu quả cho production\n\n**Các khái niệm quan trọng:**\n- Cây Quyết Định là một mô hình học máy có giám sát đa năng, được sử dụng cho cả bài toán phân loại và hồi quy. Mô hình này hoạt động bằng cách học các quy tắc quyết định từ các đặc trưng của dữ liệu và biểu diễn chúng dưới dạng cấu trúc cây phân cấp để dự đoán giá trị mục tiêu. Cấu trúc cây bao gồm Nút Gốc, Nút Nội Bộ (mỗi nút đại diện cho một kiểm tra trên một thuộc tính), Nhánh (mỗi nhánh đại diện cho kết quả của kiểm tra), và Nút Lá (mỗi nút lá đại diện cho một nhãn lớp hoặc giá trị số). Quá trình xây dựng cây bao gồm việc phân chia dữ liệu dựa trên các đặc trưng để tối đa hóa sự giảm impurity. Cây Quyết Định có thể được xây dựng đầy đủ và sau đó được cắt tỉa (pruning) để chọn cây tối ưu bằng cross-validation. Ưu điểm của mô hình này là dễ hiểu, dễ diễn giải, yêu cầu ít tiền xử lý dữ liệu, xử lý được cả dữ liệu số và phân loại, là non-parametric, bắt được mối quan hệ phi tuyến, và tính toán Feature Importance tự nhiên. Tuy nhiên, việc xây dựng cây đầy đủ có thể tốn thời gian và phức tạp hơn so với pre-pruning.\n\n**Mối quan hệ:**\n- Cây Quyết Định sử dụng Tiêu Chí Phân Chia để xác định cách tốt nhất để phân tách dữ liệu tại mỗi nút."
        },
        "output_without_context": {
            "question": "So với hồi quy logistic, cây quyết định có ưu điểm gì nổi bật trong việc xử lý dữ liệu không tuyến tính?",
            "answer": "Cây quyết định có khả năng xử lý dữ liệu không tuyến tính mà không cần biến đổi đặc trưng."
        }
    },
    {
        "topic_description": "Kết nối các khái niệm tối ưu hóa từ hồi quy tuyến tính đến cây quyết định. Học sinh sẽ kiểm tra hiểu biết về cách tối ưu hóa hàm mục tiêu trong cả hai mô hình, tập trung vào Gradient Descent và độ không thuần khiết trong cây quyết định.",
        "output_with_context": {
            "question": "Trong cây quyết định, chỉ số Gini được sử dụng để đo lường điều gì trong quá trình phân chia các nút?",
            "answer": "Độ không thuần khiết (impurity) của các tập con.",
            "rag_context": "**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Hồi Quy Tuyến Tính (Linear Regression)\n- Giải pháp nếu vi phạm: Biến đổi đặc trưng (log, căn bậc hai, đa thức)\n\n**2. Tính Độc Lập (Independence):**\n- Các quan sát độc lập với nhau\n- Quan trọng với dữ liệu chuỗi thời gian\n- Vi phạm: Tự tương quan (autocorrelation)\n- Kiểm tra: Durbin-Watson test\n\n**3. Phương Sai Đồng Nhất (Homoscedasticity):**\n- Phương sai của phần dư không đổi theo giá trị dự đoán\n- Kiểm tra: Vẽ biểu đồ phần dư vs giá trị dự đoán\n- Nếu vi phạm (heteroscedasticity): Sử dụng weighted least squares hoặc biến đổi log\n\n**4. Tính Chuẩn (Normality):**\n- Phần dư tuân theo phân phối chuẩn\n- Kiểm tra: Q-Q plot, Shapiro-Wilk test\n- Quan trọng cho suy diễn thống kê (khoảng tin cậy, kiểm định giả thuyết)\n\n**5. Không Có Đa Cộng Tuyến (No Multicollinearity):**\n- Các đặc trưng không tương quan cao với nhau\n- Kiểm tra: VIF (Variance Inflation Factor)\n- VIF > 10 cho thấy đa cộng tuyến nghiêm trọng\n- Giải pháp: Loại bỏ đặc trưng tương quan cao, PCA, regularization\n\n**Công Thức VIF:**\n$$VIF_j = \frac{1}{1 - R_j^2}$$\nTrong đó $R_j^2$ là $R^2$ khi hồi quy $x_j$ với các đặc trưng còn lại.\n\n### Tối Ưu Hóa Bằng Gradient Descent\n\nGradient Descent là phương pháp lặp để tìm hệ số tối ưu, đặc biệt hữu ích với dữ liệu lớn.\n\n**Thuật Toán:**\n$$\beta_j := \beta_j - \\alpha\frac{\\partial J(\beta)}{\\partial\beta_j}$$\n\nTrong đó:\n- $\\alpha$ là tốc độ học (learning rate)\n- $\frac{\\partial J(\beta)}{\\partial\beta_j}$ là đạo hàm riêng của hàm chi phí\n\n**Đạo Hàm Riêng:**\n$$\frac{\\partial J(\beta)}{\\partial\beta_j} = \frac{1}{m}\\sum_{i=1}^{m}(h_\beta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n\n**Các Loại Gradient Descent:**\n\n**1. Batch Gradient Descent:**\n- Sử dụng toàn bộ tập dữ liệu trong mỗi lần cập nhật\n- Ưu điểm: Hội tụ ổn định, tối ưu toàn cục\n- Nhược điểm: Chậm với dữ liệu lớn\n- Công thức cập nhật: $\beta := \beta - \\alpha\nabla J(\beta)$\n\n**2. Stochastic Gradient Descent (SGD):**\n- Sử dụng từng mẫu một để cập nhật\n- Ưu điểm: Nhanh, có thể thoát khỏi cực tiểu địa phương\n- Nhược điểm: Dao động nhiều, không hội tụ chính xác\n- Phù hợp: Dữ liệu rất lớn, học trực tuyến\n\n**3. Mini-batch Gradient Descent:**\n- Sử dụng các batch nhỏ (thường 32-256 mẫu)\n\n**Các khái niệm quan trọng:**\n- Gradient Descent là một thuật toán tối ưu hóa lặp đi lặp lại, cơ bản được sử dụng để tìm cực tiểu (minimize) một hàm mục tiêu (hàm loss hoặc hàm chi phí) bằng cách di chuyển theo hướng ngược lại của gradient của hàm đó. Thuật toán này cập nhật các tham số mô hình (weights $w$, $W^{[l]}$, $b^{[l]}$, hoặc hệ số $\\beta$) theo công thức chung: $w := w - \\alpha \\cdot \\nabla L$ (hoặc $w := w - \\eta \\cdot \\nabla L$, $W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial L}{\\partial W^{[l]}}$, $b^{[l]} := b^{[l]} - \\alpha \\frac{\\partial L}{\\partial b^{[l]}}$, $\\beta_j := \\beta_j - \\alpha \\cdot \\frac{\\partial J(\\beta)}{\\partial \\beta_j}$), trong đó $\\alpha$ (hoặc $\\eta$) là learning rate và $\\nabla L$ (hoặc $\\frac{\\partial L}{\\partial w}$, $\\frac{\\partial J(\\beta)}{\\partial \\beta_j}$) là gradient của hàm loss $L$ (hoặc hàm chi phí $J$) đối với các tham số. Thuật toán này được áp dụng rộng rãi trong huấn luyện các mô hình học máy, đặc biệt là neural networks và Logistic Regression (để cực tiểu hóa Cross-Entropy Loss, với công thức đạo hàm $\\frac{\\partial J}{\\partial \\beta_j} = \\frac{1}{m} \\sum (h_\\beta(x^{(i)}) - y^{(i)})x_j^{(i)}$). Khi cập nhật dựa trên toàn bộ training set (batch gradient descent), nó đảm bảo tìm được local minimum với hàm convex nhưng có thể chậm với dữ liệu lớn và có nguy cơ bị kẹt ở local minima hoặc saddle points. Gradient Descent đặc biệt hữu ích với dữ liệu lớn.\n\n**Mối quan hệ:**\n- Gradient Descent là một thuật toán tối ưu được sử dụng để minimize loss function bằng cách di chuyển theo hướng ngược lại của gradient.\n- Gradient Descent tối ưu hóa hàm chi phí Cross-Entropy Loss bằng cách cập nhật các tham số mô hình theo hướng gradient âm.\n- Gradient Descent tối ưu hóa hàm mục tiêu J(w) bằng cách di chuyển theo hướng ngược lại với gradient của nó.\n- Gradient Descent cập nhật các tham số β của mô hình dựa trên đạo hàm của hàm chi phí để tìm ra giá trị tối ưu.\n- Gradient Descent cực tiểu hóa hàm chi phí J(β) để tìm hệ số tối ưu.\n- Gradient Descent là một optimizer được sử dụng để cực tiểu hóa loss function bằng cách điều chỉnh tham số theo hướng gradient âm.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n\n### Giới Thiệu Về Cây Quyết Định\n\nCây quyết định là thuật toán học có giám sát đa năng có thể thực hiện cả tác vụ phân loại và hồi quy. Chúng học các quy tắc quyết định từ các đặc trưng để dự đoán giá trị mục tiêu thông qua cấu trúc dạng cây.\n\n**Ứng dụng thực tế:**\n- Chẩn đoán y tế (chuỗi quyết định dựa trên triệu chứng)\n- Đánh giá rủi ro tín dụng\n- Dự đoán churn khách hàng\n- Phát hiện gian lận\n- Hệ thống chuyên gia\n- Phân loại email spam\n\n**Tại sao gọi là \"cây\":**\n- Cấu trúc phân cấp giống cây ngược\n- Gốc ở trên, lá ở dưới\n- Quyết định được đưa ra tại mỗi nút nội bộ\n- Kết quả cuối cùng ở nút lá\n\n### Cấu Trúc Cây\n\n**1. Nút Gốc (Root Node):**\n- Nút trên cùng đại diện cho toàn bộ tập dữ liệu\n- Chứa tất cả mẫu training\n- Điểm bắt đầu của quá trình quyết định\n- Có phân chia đầu tiên dựa trên đặc trưng quan trọng nhất\n\n**2. Nút Nội Bộ (Internal Nodes):**\n- Các nút quyết định dựa trên kiểm tra đặc trưng\n- Mỗi nút thực hiện một câu hỏi yes/no về đặc trưng\n- Ví dụ: \"Tuổi > 30?\", \"Thu nhập < 50,000?\"\n- Chia dữ liệu thành các tập con\n\n**3. Nhánh (Branches):**\n- Kết quả của các quyết định\n- Kết nối nút cha với nút con\n- Đại diện cho giá trị hoặc phạm vi giá trị của đặc trưng\n\n**4. Nút Lá (Leaf Nodes):**\n- Nút cuối cùng không có nhánh con\n- Chứa dự đoán cuối cùng\n- Phân loại: Nhãn lớp\n- Hồi quy: Giá trị số\n\n**Ví dụ minh họa - Quyết định mua nhà:**\n```\n                 [Thu nhập > 50K?]\n                /                 \\\n            YES                    NO\n           /                         \\\n   [Tuổi > 30?]                [Không mua]\n    /        \\\n  YES        NO\n  /            \\\n[Mua]      [Thuê]\n```\n\n### Xây Dựng Cây Quyết Định\n\n**Tiêu Chí Phân Chia (Splitting Criteria):**\n\nMục tiêu: Tìm phân chia tốt nhất làm tăng \"độ thuần khiết\" (purity) của các tập con.\n\n**Cho Phân Loại:**\n\n**1. Gini Impurity (Chỉ Số Gini):**\n$$Gini(t) = 1 - \\sum_{i=1}^{C}p_i^2$$\n\nTrong đó:\n- $p_i$ là tỷ lệ mẫu thuộc lớp $i$ tại nút $t$\n- $C$ là số lớp\n- Gini = 0: Nút hoàn toàn thuần khiết (tất cả mẫu cùng lớp)\n\n**Các khái niệm quan trọng:**\n- Cây Quyết Định là một mô hình học máy có giám sát đa năng, được sử dụng cho cả bài toán phân loại và hồi quy. Mô hình này hoạt động bằng cách học các quy tắc quyết định từ các đặc trưng của dữ liệu và biểu diễn chúng dưới dạng cấu trúc cây phân cấp để dự đoán giá trị mục tiêu. Cấu trúc cây bao gồm Nút Gốc, Nút Nội Bộ (mỗi nút đại diện cho một kiểm tra trên một thuộc tính), Nhánh (mỗi nhánh đại diện cho kết quả của kiểm tra), và Nút Lá (mỗi nút lá đại diện cho một nhãn lớp hoặc giá trị số). Quá trình xây dựng cây bao gồm việc phân chia dữ liệu dựa trên các đặc trưng để tối đa hóa sự giảm impurity. Cây Quyết Định có thể được xây dựng đầy đủ và sau đó được cắt tỉa (pruning) để chọn cây tối ưu bằng cross-validation. Ưu điểm của mô hình này là dễ hiểu, dễ diễn giải, yêu cầu ít tiền xử lý dữ liệu, xử lý được cả dữ liệu số và phân loại, là non-parametric, bắt được mối quan hệ phi tuyến, và tính toán Feature Importance tự nhiên. Tuy nhiên, việc xây dựng cây đầy đủ có thể tốn thời gian và phức tạp hơn so với pre-pruning.\n- Hồi quy là một tác vụ trong học máy mà Cây Quyết Định có thể thực hiện, nhằm dự đoán một giá trị số liên tục cho các mẫu dữ liệu dựa trên các đặc trưng đầu vào. Kết quả dự đoán ở nút lá là một giá trị số.\n\n**Mối quan hệ:**\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Hồi quy, dự đoán giá trị số liên tục.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Cây Quyết Định (Decision Tree)\n1. Xây dựng cây đầy đủ\n2. Tính $\\alpha$ cho mỗi subtree\n3. Loại bỏ subtree có $\\alpha$ nhỏ nhất (đóng góp ít nhất)\n4. Lặp lại cho đến khi còn gốc\n5. Chọn cây tối ưu bằng cross-validation\n\n**Ưu điểm:**\n- Thường cho kết quả tốt hơn pre-pruning\n- Không bỏ lỡ phân chia tốt\n\n**Nhược điểm:**\n- Tốn thời gian (xây dựng cây đầy đủ trước)\n- Phức tạp hơn\n\n### Feature Importance (Tầm Quan Trọng Đặc Trưng)\n\nCây quyết định tự động tính toán mức độ quan trọng của mỗi đặc trưng.\n\n**Công Thức:**\n$$Importance(f) = \\sum_{t \\in T} p(t) \\cdot \\Delta impurity(t, f)$$\n\nTrong đó:\n- $p(t)$ là tỷ lệ mẫu tại nút $t$\n- $\\Delta impurity(t, f)$ là giảm impurity khi phân chia theo đặc trưng $f$ tại nút $t$\n- Tổng trên tất cả nút sử dụng đặc trưng $f$\n\n**Diễn giải:**\n- Giá trị càng cao, đặc trưng càng quan trọng\n- Tổng tất cả importance = 1\n- Đặc trưng không xuất hiện có importance = 0\n\n**Ứng dụng:**\n- Feature selection\n- Hiểu mô hình\n- Phát hiện đặc trưng không cần thiết\n- Giải thích cho stakeholders\n\n**Lưu ý:**\n- Thiên vị về đặc trưng có nhiều giá trị unique\n- Đặc trưng tương quan cao có importance phân tán\n- Sử dụng permutation importance để khắc phục\n\n### Ưu Điểm Của Cây Quyết Định\n\n**1. Dễ hiểu và diễn giải:**\n- Trực quan, giống cách con người quyết định\n- Có thể vẽ và giải thích bằng lời\n- Không cần kiến thức thống kê sâu\n- Phù hợp cho business users\n\n**2. Yêu cầu ít tiền xử lý dữ liệu:**\n- Không cần feature scaling\n- Không cần one-hot encoding cho categorical\n- Xử lý được missing values (surrogate splits)\n- Không cần assumption về phân phối\n\n**3. Xử lý dữ liệu số và phân loại:**\n- Linh hoạt với nhiều loại đặc trưng\n- Không cần encoding phức tạp\n- Mixed data types\n\n**4. Non-parametric:**\n- Không giả định về phân phối dữ liệu\n- Linh hoạt với mọi dạng data\n- Không cần chọn hàm phân phối\n\n**5. Bắt được mối quan hệ phi tuyến:**\n- Decision boundary phức tạp\n- Tương tác giữa các đặc trưng\n- Không giới hạn bởi tuyến tính\n\n**6. Tính toán Feature Importance tự nhiên:**\n- Không cần phương pháp bên ngoài\n- Tích hợp trong thuật toán\n\n**7. Nhanh với dự đoán:**\n- Độ phức tạp: O(log n) với cây cân bằng\n- Hiệu quả cho production\n\n**Các khái niệm quan trọng:**\n- Cây Quyết Định là một mô hình học máy có giám sát đa năng, được sử dụng cho cả bài toán phân loại và hồi quy. Mô hình này hoạt động bằng cách học các quy tắc quyết định từ các đặc trưng của dữ liệu và biểu diễn chúng dưới dạng cấu trúc cây phân cấp để dự đoán giá trị mục tiêu. Cấu trúc cây bao gồm Nút Gốc, Nút Nội Bộ (mỗi nút đại diện cho một kiểm tra trên một thuộc tính), Nhánh (mỗi nhánh đại diện cho kết quả của kiểm tra), và Nút Lá (mỗi nút lá đại diện cho một nhãn lớp hoặc giá trị số). Quá trình xây dựng cây bao gồm việc phân chia dữ liệu dựa trên các đặc trưng để tối đa hóa sự giảm impurity. Cây Quyết Định có thể được xây dựng đầy đủ và sau đó được cắt tỉa (pruning) để chọn cây tối ưu bằng cross-validation. Ưu điểm của mô hình này là dễ hiểu, dễ diễn giải, yêu cầu ít tiền xử lý dữ liệu, xử lý được cả dữ liệu số và phân loại, là non-parametric, bắt được mối quan hệ phi tuyến, và tính toán Feature Importance tự nhiên. Tuy nhiên, việc xây dựng cây đầy đủ có thể tốn thời gian và phức tạp hơn so với pre-pruning.\n\n**Mối quan hệ:**\n- Cây Quyết Định là một thuật toán học có giám sát có thể thực hiện tác vụ Hồi quy, dự đoán giá trị số liên tục.\n\n**Nội dung từ tài liệu:**\n# Học Máy (Machine Learning)\n## Hồi Quy Tuyến Tính (Linear Regression)\n$$MAPE = \frac{100\\%}{m}\\sum_{i=1}^{m}\\left|\frac{y_i - \\hat{y}_i}{y_i}\right|$$\n- Sai số phần trăm trung bình\n- Dễ diễn giải cho người không chuyên\n- Vấn đề: Không xác định khi $y_i = 0$\n\n### Kỹ Thuật Regularization\n\nRegularization giúp giảm overfitting bằng cách thêm penalty vào hàm chi phí.\n\n**1. Ridge Regression (L2 Regularization - Hồi Quy Ridge):**\n$$J(\beta) = \frac{1}{2m}\\sum_{i=1}^{m}(h_\beta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^{n}\beta_j^2$$\n\n**Đặc điểm:**\n- Thêm penalty là tổng bình phương các hệ số\n- Làm co nhỏ (shrink) các hệ số về gần 0\n- Không đưa hệ số về chính xác 0\n- Hiệu quả với đa cộng tuyến\n- Nghiệm dạng đóng: $\boldsymbol{\beta} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}$\n\n**Khi nào sử dụng:**\n- Nhiều đặc trưng tương quan\n- Muốn giữ tất cả đặc trưng\n- Dữ liệu có đa cộng tuyến\n\n**2. Lasso Regression (L1 Regularization - Hồi Quy Lasso):**\n$$J(\beta) = \frac{1}{2m}\\sum_{i=1}^{m}(h_\beta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^{n}|\beta_j|$$\n\n**Đặc điểm:**\n- Thêm penalty là tổng giá trị tuyệt đối các hệ số\n- Có thể đưa một số hệ số về chính xác 0\n- Thực hiện feature selection tự động\n- Tạo ra mô hình sparse (thưa)\n- Không có nghiệm dạng đóng\n\n**Khi nào sử dụng:**\n- Muốn loại bỏ đặc trưng không quan trọng\n- Cần mô hình đơn giản, dễ diễn giải\n- Có nhiều đặc trưng nhưng ít quan trọng\n\n**3. Elastic Net:**\n$$J(\beta) = \frac{1}{2m}\\sum_{i=1}^{m}(h_\beta(x^{(i)}) - y^{(i)})^2 + \\lambda_1\\sum_{j=1}^{n}|\beta_j| + \\lambda_2\\sum_{j=1}^{n}\beta_j^2$$\n\n**Đặc điểm:**\n- Kết hợp L1 và L2\n- Cân bằng giữa feature selection và shrinkage\n- Tốt với các đặc trưng tương quan nhóm\n- Ổn định hơn Lasso khi đặc trưng tương quan cao\n\n**Tham số $\\lambda$ (Lambda):**\n- $\\lambda = 0$: Không có regularization (hồi quy tuyến tính thông thường)\n- $\\lambda$ nhỏ: Ít regularization\n- $\\lambda$ lớn: Nhiều regularization, hệ số bị co nhỏ mạnh\n- Chọn $\\lambda$: Cross-validation\n\n**So Sánh Ridge vs Lasso:**\n| Tiêu chí | Ridge (L2) | Lasso (L1) |\n|----------|-----------|------------|\n| Feature Selection | Không | Có |\n\n**Các khái niệm quan trọng:**\n- Hàm chi phí (còn gọi là hàm loss hoặc hàm mục tiêu) là một hàm toán học đo lường sự khác biệt giữa giá trị dự đoán của mô hình và giá trị thực tế. Mục tiêu của quá trình huấn luyện mô hình là cực tiểu hóa hàm chi phí này. Trong hồi quy tuyến tính, hàm chi phí thường là MSE, và các kỹ thuật regularization thêm penalty vào hàm này.\n\n**Mối quan hệ:**\n- Ridge Regression cực tiểu hóa hàm chi phí $J(\beta) = \frac{1}{2m}\\sum_{i=1}^{m}(h_\beta(x^{(i)}) - y^{(^{(i)})})^2 + \\lambda\\sum_{j=1}^{n}\beta_j^2$ để tìm các hệ số tối ưu."
        },
        "output_without_context": {
            "question": "Trong quá trình tối ưu hóa hàm mục tiêu của một mô hình hồi quy tuyến tính, phương pháp nào được sử dụng để cập nhật các trọng số nhằm giảm thiểu sai số dự đoán?",
            "answer": "Gradient Descent"
        }
    }
]