{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c49760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neo4jService(settings=Neo4jSetting(uri='bolt://localhost:17687', username='neo4j', password='4_Kz1pLYqtmVsxFJED_gxTN8rBcu4oQKAEqw9mm6zUHY'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graph_db import Neo4jService, Neo4jSetting\n",
    "import chromadb\n",
    "from lite_llm import LiteLLMEmbeddingInput, LiteLLMService, LiteLLMSetting, LiteLLMInput\n",
    "from pydantic import HttpUrl, SecretStr\n",
    "\n",
    "litellm_setting=LiteLLMSetting(\n",
    "    url=HttpUrl(\"http://localhost:9510\"),\n",
    "    token=SecretStr(\"abc123\"),\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    frequency_penalty=0.0,\n",
    "    n=1,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_completion_tokens=10000,\n",
    "    dimension=1024,\n",
    "    embedding_model=\"qwen3-embedding:0.6b\"\n",
    ")\n",
    "\n",
    "litellm_service = LiteLLMService(litellm_setting=litellm_setting)\n",
    "\n",
    "neo4j_service = Neo4jService(\n",
    "    settings=Neo4jSetting(\n",
    "        uri=\"bolt://localhost:17687\",\n",
    "        username=\"neo4j\",\n",
    "        password=\"4_Kz1pLYqtmVsxFJED_gxTN8rBcu4oQKAEqw9mm6zUHY\"\n",
    "    )\n",
    ")\n",
    "\n",
    "neo4j_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE VECTOR INDEX description_index\n",
    "# FOR (d:Description)\n",
    "# ON (d.embedding)\n",
    "# OPTIONS {\n",
    "#   indexConfig: {\n",
    "#     `vector.dimensions`: 1024,\n",
    "#     `vector.similarity_function`: 'cosine'\n",
    "#   }\n",
    "# };\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c10dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:28<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Quality Metrics over Course: 0.7983775346998184\n",
      "STD Quality Metrics over Course: 0.03625653219966605\n",
      "Relevance Ratio over Course: 0.47619047619047616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import ollama \n",
    "\n",
    "def embedding_ollama(text): \n",
    "    response = ollama.embed(\"qwen3-embedding:0.6b\", text)\n",
    "    return response.embeddings[0]\n",
    "\n",
    "course_code = \"dsa2025\"\n",
    "course_results = []\n",
    "topic_descs = []\n",
    "for week_number in range(1, 9):\n",
    "    with open(f'/home/lehoangvu/KLTN/outputs/gpt-4o-mini/{course_code}/week{week_number}_pipeline.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    questions = data['questions']\n",
    "    tmp = [q['topic']['description'] for q in questions]\n",
    "    topic_descs.extend(tmp)\n",
    "\n",
    "for query in tqdm(topic_descs):\n",
    "    embedding = embedding_ollama(query)\n",
    "    SIMILARITY_GETTING = \"\"\"CALL db.index.vector.queryNodes($index_name, $query_nodes, $embedding)\n",
    "    YIELD node, score WHERE node.type = 'ENTITY'\n",
    "    MATCH (node)<-[:DESCRIBED]-(e:Entity)\n",
    "    RETURN e.name AS name, e.type AS type, node.uid AS description_id, node.text AS description, node.chunk_uid AS chunk_id, score ORDER BY score DESC limit $k\n",
    "    \"\"\"\n",
    "    results = await neo4j_service.execute_query(\n",
    "        cypher=SIMILARITY_GETTING,\n",
    "        parameters={\n",
    "            'index_name': 'description_index',\n",
    "            'embedding': embedding,\n",
    "            'k': 10,\n",
    "            'query_nodes': 100,\n",
    "        },\n",
    "        output_format='pandas',\n",
    "    )\n",
    "    \n",
    "    list_results = results.to_dict(orient='records')\n",
    "    course_results.extend([tmp['score'] for tmp in list_results])\n",
    "    \n",
    "mean_similarity = np.mean(course_results)  \n",
    "print(f\"Mean Quality Metrics over Course: {mean_similarity}\")\n",
    "std_similarity = np.std(course_results)\n",
    "print(f\"STD Quality Metrics over Course: {std_similarity}\")\n",
    "relevance_ratio = sum(1 for s in course_results if s >= 0.80) / len(course_results)\n",
    "print(f\"Relevance Ratio over Course: {relevance_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc3f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Gradient Descent',\n",
       "  'type': 'technique',\n",
       "  'description_id': 'a64c332d-ba73-4b6a-890a-3d55e3ee9dc7',\n",
       "  'description': 'Gradient Descent là một thuật toán tối ưu hóa lặp đi lặp lại, cơ bản được sử dụng để tìm cực tiểu (minimize) một hàm mục tiêu (hàm loss hoặc hàm chi phí) bằng cách di chuyển theo hướng ngược lại của gradient của hàm đó. Thuật toán này cập nhật các tham số mô hình (weights $w$, $W^{[l]}$, $b^{[l]}$, hoặc hệ số $\\\\beta$) theo công thức chung: $w := w - \\\\alpha \\\\cdot \\\\nabla L$ (hoặc $w := w - \\\\eta \\\\cdot \\\\nabla L$, $W^{[l]} := W^{[l]} - \\\\alpha \\\\frac{\\\\partial L}{\\\\partial W^{[l]}}$, $b^{[l]} := b^{[l]} - \\\\alpha \\\\frac{\\\\partial L}{\\\\partial b^{[l]}}$, $\\\\beta_j := \\\\beta_j - \\\\alpha \\\\cdot \\\\frac{\\\\partial J(\\\\beta)}{\\\\partial \\\\beta_j}$), trong đó $\\\\alpha$ (hoặc $\\\\eta$) là learning rate và $\\\\nabla L$ (hoặc $\\\\frac{\\\\partial L}{\\\\partial w}$, $\\\\frac{\\\\partial J(\\\\beta)}{\\\\partial \\\\beta_j}$) là gradient của hàm loss $L$ (hoặc hàm chi phí $J$) đối với các tham số. Thuật toán này được áp dụng rộng rãi trong huấn luyện các mô hình học máy, đặc biệt là neural networks và Logistic Regression (để cực tiểu hóa Cross-Entropy Loss, với công thức đạo hàm $\\\\frac{\\\\partial J}{\\\\partial \\\\beta_j} = \\\\frac{1}{m} \\\\sum (h_\\\\beta(x^{(i)}) - y^{(i)})x_j^{(i)}$). Khi cập nhật dựa trên toàn bộ training set (batch gradient descent), nó đảm bảo tìm được local minimum với hàm convex nhưng có thể chậm với dữ liệu lớn và có nguy cơ bị kẹt ở local minima hoặc saddle points. Gradient Descent đặc biệt hữu ích với dữ liệu lớn.',\n",
       "  'chunk_id': '3742d85e-c5fa-4421-ad74-ca25d1bfc8a5|93daf18b-7550-43c1-a390-85ad238e2aa2|4632f48a-8868-42f7-b812-44eeabe27e3d|62d6597a-60c2-4d65-9632-aec223345f36|1b36ddb8-64f0-4876-bb2f-61c51b10d01a',\n",
       "  'score': 0.8568469882011414},\n",
       " {'name': 'β_1',\n",
       "  'type': 'parameter',\n",
       "  'description_id': 'bd42e9c4-0985-40b7-a5e7-befcdf267eae',\n",
       "  'description': 'β_1 và β_2 là các hệ số làm mượt (decay rate) trong thuật toán Adam, đóng vai trò là các hyperparameter. β_1 là hệ số làm mượt cho moment bậc 1 (trung bình động của gradient), thường có giá trị mặc định là 0.9. Nó kiểm soát mức độ ảnh hưởng của các gradient trước đó đến ước lượng moment bậc 1 hiện tại, giúp tăng tốc độ hội tụ theo hướng nhất quán; giá trị β_1 cao hơn làm cho moment bậc 1 phản ứng chậm hơn với các thay đổi gradient mới. β_2 là hệ số làm mượt cho moment bậc 2 (trung bình động của bình phương gradient), thường có giá trị mặc định là 0.999. Nó kiểm soát mức độ ảnh hưởng của các bình phương gradient trước đó đến ước lượng moment bậc 2 hiện tại, giúp điều chỉnh learning rate cho từng tham số; giá trị β_2 cao hơn làm cho moment bậc 2 phản ứng chậm hơn với các thay đổi về độ lớn gradient mới.',\n",
       "  'chunk_id': '81274a9d-37c0-4540-9c6b-70b86711531e|81274a9d-37c0-4540-9c6b-70b86711531e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e',\n",
       "  'score': 0.8437535762786865},\n",
       " {'name': 'β_2',\n",
       "  'type': 'parameter',\n",
       "  'description_id': 'bd42e9c4-0985-40b7-a5e7-befcdf267eae',\n",
       "  'description': 'β_1 và β_2 là các hệ số làm mượt (decay rate) trong thuật toán Adam, đóng vai trò là các hyperparameter. β_1 là hệ số làm mượt cho moment bậc 1 (trung bình động của gradient), thường có giá trị mặc định là 0.9. Nó kiểm soát mức độ ảnh hưởng của các gradient trước đó đến ước lượng moment bậc 1 hiện tại, giúp tăng tốc độ hội tụ theo hướng nhất quán; giá trị β_1 cao hơn làm cho moment bậc 1 phản ứng chậm hơn với các thay đổi gradient mới. β_2 là hệ số làm mượt cho moment bậc 2 (trung bình động của bình phương gradient), thường có giá trị mặc định là 0.999. Nó kiểm soát mức độ ảnh hưởng của các bình phương gradient trước đó đến ước lượng moment bậc 2 hiện tại, giúp điều chỉnh learning rate cho từng tham số; giá trị β_2 cao hơn làm cho moment bậc 2 phản ứng chậm hơn với các thay đổi về độ lớn gradient mới.',\n",
       "  'chunk_id': '81274a9d-37c0-4540-9c6b-70b86711531e|81274a9d-37c0-4540-9c6b-70b86711531e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e',\n",
       "  'score': 0.8437535762786865},\n",
       " {'name': 'β₂',\n",
       "  'type': 'parameter',\n",
       "  'description_id': 'bd42e9c4-0985-40b7-a5e7-befcdf267eae',\n",
       "  'description': 'β_1 và β_2 là các hệ số làm mượt (decay rate) trong thuật toán Adam, đóng vai trò là các hyperparameter. β_1 là hệ số làm mượt cho moment bậc 1 (trung bình động của gradient), thường có giá trị mặc định là 0.9. Nó kiểm soát mức độ ảnh hưởng của các gradient trước đó đến ước lượng moment bậc 1 hiện tại, giúp tăng tốc độ hội tụ theo hướng nhất quán; giá trị β_1 cao hơn làm cho moment bậc 1 phản ứng chậm hơn với các thay đổi gradient mới. β_2 là hệ số làm mượt cho moment bậc 2 (trung bình động của bình phương gradient), thường có giá trị mặc định là 0.999. Nó kiểm soát mức độ ảnh hưởng của các bình phương gradient trước đó đến ước lượng moment bậc 2 hiện tại, giúp điều chỉnh learning rate cho từng tham số; giá trị β_2 cao hơn làm cho moment bậc 2 phản ứng chậm hơn với các thay đổi về độ lớn gradient mới.',\n",
       "  'chunk_id': '81274a9d-37c0-4540-9c6b-70b86711531e|81274a9d-37c0-4540-9c6b-70b86711531e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e',\n",
       "  'score': 0.8437535762786865},\n",
       " {'name': 'β₁',\n",
       "  'type': 'parameter',\n",
       "  'description_id': 'bd42e9c4-0985-40b7-a5e7-befcdf267eae',\n",
       "  'description': 'β_1 và β_2 là các hệ số làm mượt (decay rate) trong thuật toán Adam, đóng vai trò là các hyperparameter. β_1 là hệ số làm mượt cho moment bậc 1 (trung bình động của gradient), thường có giá trị mặc định là 0.9. Nó kiểm soát mức độ ảnh hưởng của các gradient trước đó đến ước lượng moment bậc 1 hiện tại, giúp tăng tốc độ hội tụ theo hướng nhất quán; giá trị β_1 cao hơn làm cho moment bậc 1 phản ứng chậm hơn với các thay đổi gradient mới. β_2 là hệ số làm mượt cho moment bậc 2 (trung bình động của bình phương gradient), thường có giá trị mặc định là 0.999. Nó kiểm soát mức độ ảnh hưởng của các bình phương gradient trước đó đến ước lượng moment bậc 2 hiện tại, giúp điều chỉnh learning rate cho từng tham số; giá trị β_2 cao hơn làm cho moment bậc 2 phản ứng chậm hơn với các thay đổi về độ lớn gradient mới.',\n",
       "  'chunk_id': '81274a9d-37c0-4540-9c6b-70b86711531e|81274a9d-37c0-4540-9c6b-70b86711531e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e',\n",
       "  'score': 0.8437535762786865},\n",
       " {'name': 'alpha',\n",
       "  'type': 'parameter',\n",
       "  'description_id': 'a1732e1c-b1a4-402e-8fb1-ce352b6b69ea',\n",
       "  'description': 'alpha là tham số complexity trong Cost Complexity Pruning. Tham số này kiểm soát mức độ cắt tỉa: giá trị $\\\\alpha$ càng lớn thì cây càng bị cắt nhiều hơn, dẫn đến một cây đơn giản hơn. Việc chọn giá trị $\\\\alpha$ phù hợp là rất quan trọng để đạt được sự cân bằng giữa bias và variance.',\n",
       "  'chunk_id': 'd79bd610-5ae7-41b8-8b48-3a802be9b277',\n",
       "  'score': 0.8299418091773987},\n",
       " {'name': 'α_t = α_0 / (1 + decay × t)',\n",
       "  'type': 'formula',\n",
       "  'description_id': 'ff19279f-0a39-4ca4-a12e-c3395fb4e746',\n",
       "  'description': \"Công thức α_t = α_0 / (1 + decay × t) mô tả cách learning rate (α_t) giảm dần theo thời gian (t). Trong đó, α_0 là learning rate ban đầu và 'decay' là một hằng số điều khiển tốc độ giảm. Công thức này được sử dụng trong kỹ thuật Learning rate decay để điều chỉnh learning rate trong quá trình huấn luyện mô hình.\",\n",
       "  'chunk_id': '2947b2f5-0b73-4c82-81fa-ead4476b8e0d',\n",
       "  'score': 0.8283376097679138},\n",
       " {'name': 'β := β - α∇J(β)',\n",
       "  'type': 'formula',\n",
       "  'description_id': 'c467f823-1c58-4e83-be45-d1ab69c4432d',\n",
       "  'description': 'β := β - α∇J(β) là công thức cập nhật tham số trong thuật toán Gradient Descent (bao gồm cả Batch Gradient Descent). Công thức này có thể được biểu diễn cho từng tham số riêng lẻ là β_j := β_j - α * ∂J(β)/∂β_j, hoặc dưới dạng vector là β := β - α∇J(β). Trong đó, β (hoặc β_j) là tham số (hoặc vector tham số) hiện tại, α là learning rate (tốc độ học), ∂J(β)/∂β_j là đạo hàm riêng của hàm chi phí J(β) theo tham số β_j, và ∇J(β) là vector gradient của hàm chi phí J(β) theo tất cả các tham số β. Công thức này điều chỉnh các tham số theo hướng ngược lại với gradient để cực tiểu hóa hàm chi phí J(β), cập nhật tất cả các tham số đồng thời dựa trên gradient tính toán từ toàn bộ tập dữ liệu.',\n",
       "  'chunk_id': '3742d85e-c5fa-4421-ad74-ca25d1bfc8a5|3742d85e-c5fa-4421-ad74-ca25d1bfc8a5',\n",
       "  'score': 0.8268836736679077},\n",
       " {'name': 'β_j := β_j - α * ∂J(β)/∂β_j',\n",
       "  'type': 'formula',\n",
       "  'description_id': 'c467f823-1c58-4e83-be45-d1ab69c4432d',\n",
       "  'description': 'β := β - α∇J(β) là công thức cập nhật tham số trong thuật toán Gradient Descent (bao gồm cả Batch Gradient Descent). Công thức này có thể được biểu diễn cho từng tham số riêng lẻ là β_j := β_j - α * ∂J(β)/∂β_j, hoặc dưới dạng vector là β := β - α∇J(β). Trong đó, β (hoặc β_j) là tham số (hoặc vector tham số) hiện tại, α là learning rate (tốc độ học), ∂J(β)/∂β_j là đạo hàm riêng của hàm chi phí J(β) theo tham số β_j, và ∇J(β) là vector gradient của hàm chi phí J(β) theo tất cả các tham số β. Công thức này điều chỉnh các tham số theo hướng ngược lại với gradient để cực tiểu hóa hàm chi phí J(β), cập nhật tất cả các tham số đồng thời dựa trên gradient tính toán từ toàn bộ tập dữ liệu.',\n",
       "  'chunk_id': '3742d85e-c5fa-4421-ad74-ca25d1bfc8a5|3742d85e-c5fa-4421-ad74-ca25d1bfc8a5',\n",
       "  'score': 0.8268836736679077},\n",
       " {'name': 'Gradient Descent',\n",
       "  'type': 'technique',\n",
       "  'description_id': '73153cce-80f0-42ec-b6ba-e3ec2dcc44ac',\n",
       "  'description': 'Gradient Descent là một thuật toán tối ưu được sử dụng để huấn luyện Generator. Generator cập nhật tham số θ_G bằng cách giảm gradient của hàm mục tiêu của nó: θ_G ← θ_G - η∇θ_G V(D,G). Cụ thể, Generator cố gắng giảm log xác suất mà dữ liệu giả của nó bị Discriminator từ chối, hoặc tương đương, tăng log xác suất mà dữ liệu giả của nó được Discriminator chấp nhận (non-saturating loss).',\n",
       "  'chunk_id': '5f0c45f5-7c26-48aa-b834-1916c523b906',\n",
       "  'score': 0.8258001804351807}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = results.to_dict(orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
