{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c3dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lite_llm import LiteLLMService, LiteLLMEmbeddingInput, LiteLLMSetting \n",
    "from pydantic import HttpUrl, SecretStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e22cb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiteLLMService(litellm_setting=LiteLLMSetting(url=HttpUrl('http://localhost:9510/'), token=SecretStr('**********'), model='gemini-2.5-flash', frequency_penalty=0.0, n=1, temperature=0.0, top_p=1.0, max_completion_tokens=10000, dimension=768, embedding_model='embeddinggemma'), async_client=None, client=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm_service=LiteLLMService(\n",
    "    litellm_setting=LiteLLMSetting(\n",
    "        url=HttpUrl(\"http://localhost:9510\"),\n",
    "        token=SecretStr(\"abc123\"),\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        frequency_penalty=0.0,\n",
    "        n=1,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_completion_tokens=10000,\n",
    "        dimension=768,\n",
    "        embedding_model=\"embeddinggemma\"\n",
    "    )\n",
    ")\n",
    "litellm_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd0b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def cosine_similarity(vector1: np.ndarray, vector2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        vector1: First vector as numpy array\n",
    "        vector2: Second vector as numpy array\n",
    "    \n",
    "    Returns:\n",
    "        Cosine similarity value between -1 and 1\n",
    "    \"\"\"\n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    \n",
    "    # Calculate magnitudes (L2 norms)\n",
    "    magnitude1 = np.linalg.norm(vector1)\n",
    "    magnitude2 = np.linalg.norm(vector2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5dd4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = await litellm_service.embedding_llm_async(\n",
    "    inputs=LiteLLMEmbeddingInput(\n",
    "        text=\"Hồi quy tuyến tính là một trong những kỹ thuật thống kê cơ bản và được sử dụng rộng rãi nhất trong học máy. Nó mô hình hóa mối quan hệ giữa một biến phụ thuộc (biến mục tiêu) và một hoặc nhiều biến độc lập (đặc trưng) bằng cách khớp một phương trình tuyến tính với dữ liệu quan sát được.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "output2 = await litellm_service.embedding_llm_async(\n",
    "    inputs=LiteLLMEmbeddingInput(\n",
    "        text=\"Hồi quy tuyến tính là một trong những kỹ thuật thống kê cơ bản và được sử dụng rộng rãi nhất trong học máy. Nó mô hình hóa mối quan hệ giữa một biến phụ thuộc (biến mục tiêu) và một hoặc nhiều biến độc lập bằng cách khớp một phương trình tuyến tính với dữ liệu quan sát được.\",\n",
    "    )\n",
    ")\n",
    "\n",
    "cosine = cosine_similarity(np.array(output1.embedding), np.array(output2.embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3acbc55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9938122685828052)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0253c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 embeddings, each with dimension 768\n"
     ]
    }
   ],
   "source": [
    "options = [\n",
    "    \"By explicitly computing feature vectors in a higher-dimensional space and then applying a linear SVM.\",\n",
    "    \"By replacing the inner product of mapped feature vectors with a kernel function, avoiding explicit computation in high-dimensional space.\",  # correct\n",
    "    \"By directly adjusting the weights `w` and bias `b` to create a non-linear decision boundary in the input space.\",\n",
    "    \"By reducing the dimensionality of the input data before applying a linear SVM.\"\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get embeddings for each option\n",
    "option_embeddings = []\n",
    "for option in options:\n",
    "    embedding_output = await litellm_service.embedding_llm_async(\n",
    "        inputs=LiteLLMEmbeddingInput(text=option)\n",
    "    )\n",
    "    option_embeddings.append(np.array(embedding_output.embedding))\n",
    "\n",
    "print(f\"Generated {len(option_embeddings)} embeddings, each with dimension {len(option_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e17509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def cosine_similarity(vector1: np.ndarray, vector2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        vector1: First vector as numpy array\n",
    "        vector2: Second vector as numpy array\n",
    "    \n",
    "    Returns:\n",
    "        Cosine similarity value between -1 and 1\n",
    "    \"\"\"\n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    \n",
    "    # Calculate magnitudes (L2 norms)\n",
    "    magnitude1 = np.linalg.norm(vector1)\n",
    "    magnitude2 = np.linalg.norm(vector2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def compute_similarity_matrix(texts: List[str], embeddings: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a similarity matrix for a list of text embeddings.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        embeddings: List of embedding vectors\n",
    "    \n",
    "    Returns:\n",
    "        NxN similarity matrix where N is the number of texts\n",
    "    \"\"\"\n",
    "    n = len(embeddings)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            similarity_matrix[i, j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0ab60f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4x4 Cosine Similarity Matrix:\n",
      "==================================================\n",
      "Option 1: 1.0000  0.8497  0.6905  0.7976  \n",
      "Option 2: 0.8497  1.0000  0.6641  0.7329  \n",
      "Option 3: 0.6905  0.6641  1.0000  0.6729  \n",
      "Option 4: 0.7976  0.7329  0.6729  1.0000  \n",
      "\n",
      "Detailed Matrix with Labels:\n",
      "================================================================================\n",
      "\n",
      "Option 1: By explicitly computing feature vectors in a highe...\n",
      "  vs Option 1: 1.0000\n",
      "  vs Option 2: 0.8497\n",
      "  vs Option 3: 0.6905\n",
      "  vs Option 4: 0.7976\n",
      "\n",
      "Option 2: By replacing the inner product of mapped feature v...\n",
      "  vs Option 1: 0.8497\n",
      "  vs Option 2: 1.0000\n",
      "  vs Option 3: 0.6641\n",
      "  vs Option 4: 0.7329\n",
      "\n",
      "Option 3: By directly adjusting the weights `w` and bias `b`...\n",
      "  vs Option 1: 0.6905\n",
      "  vs Option 2: 0.6641\n",
      "  vs Option 3: 1.0000\n",
      "  vs Option 4: 0.6729\n",
      "\n",
      "Option 4: By reducing the dimensionality of the input data b...\n",
      "  vs Option 1: 0.7976\n",
      "  vs Option 2: 0.7329\n",
      "  vs Option 3: 0.6729\n",
      "  vs Option 4: 1.0000\n",
      "\n",
      "Numpy Array:\n",
      "[[1.         0.84968432 0.69050761 0.79757521]\n",
      " [0.84968432 1.         0.66406493 0.73291624]\n",
      " [0.69050761 0.66406493 1.         0.67288202]\n",
      " [0.79757521 0.73291624 0.67288202 1.        ]]\n",
      "\n",
      "Matrix Shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Compute the 4x4 similarity matrix\n",
    "similarity_matrix = compute_similarity_matrix(options, option_embeddings)\n",
    "\n",
    "print(\"4x4 Cosine Similarity Matrix:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Print matrix with proper formatting\n",
    "for i in range(len(options)):\n",
    "    row_str = \"\"\n",
    "    for j in range(len(options)):\n",
    "        row_str += f\"{similarity_matrix[i, j]:.4f}  \"\n",
    "    print(f\"Option {i+1}: {row_str}\")\n",
    "\n",
    "print(\"\\nDetailed Matrix with Labels:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print with option labels for clarity\n",
    "for i, option1 in enumerate(options):\n",
    "    print(f\"\\nOption {i+1}: {option1[:50]}...\")\n",
    "    for j, option2 in enumerate(options):\n",
    "        print(f\"  vs Option {j+1}: {similarity_matrix[i, j]:.4f}\")\n",
    "\n",
    "# Also display as a clean numpy array\n",
    "print(f\"\\nNumpy Array:\\n{similarity_matrix}\")\n",
    "print(f\"\\nMatrix Shape: {similarity_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74af856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from lite_llm import LiteLLMEmbeddingInput, LiteLLMService, LiteLLMSetting\n",
    "from pydantic import HttpUrl, SecretStr\n",
    "\n",
    "litellm_setting=LiteLLMSetting(\n",
    "    url=HttpUrl(\"http://localhost:9510\"),\n",
    "    token=SecretStr(\"abc123\"),\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    frequency_penalty=0.0,\n",
    "    n=1,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_completion_tokens=10000,\n",
    "    dimension=768,\n",
    "    embedding_model=\"embeddinggemma\"\n",
    ")\n",
    "\n",
    "litellm_service = LiteLLMService(litellm_setting=litellm_setting)\n",
    "client = chromadb.PersistentClient(path=\"./vector_database/rl2025\")\n",
    "collection = client.get_or_create_collection(name=\"questions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b8574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lehoangvu/KLTN/rl2025/mcqs.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"rl2025/mcqs.json\")\n",
    "absolute_path = file_path.resolve()\n",
    "print(absolute_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1d98e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [01:13<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from uuid import uuid4\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "\n",
    "\n",
    "with open(\"/home/lehoangvu/KLTN/services/generation/src/generation/shared/static_files/rl2025/mcqs.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item in tqdm(data):\n",
    "    embedding = await litellm_service.embedding_llm_async(\n",
    "        inputs=LiteLLMEmbeddingInput(\n",
    "            text=item['question']\n",
    "        )\n",
    "    )\n",
    "    options = [\n",
    "        f\"{key}. {value}\"\n",
    "        for key, value in item['options'].items()\n",
    "    ]\n",
    "    collection.add(\n",
    "        ids=[str(uuid4())],\n",
    "        embeddings=[embedding.embedding],\n",
    "        documents=[item['question']],   # text để search\n",
    "        metadatas=[{\n",
    "            \"options\": \"\\n\".join(options),\n",
    "            \"answer\": item['answer'],\n",
    "            \"explanation\": item['explanation'],\n",
    "        }]\n",
    "    )\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8727082f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['17c7414e-5923-4042-9319-d0cf48f2ad6f',\n",
       "   'a55249c6-1670-4abb-acba-5649f88c7a1c',\n",
       "   '8b244a37-c701-4cfa-834a-6df2e32cbda2']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Agent trong học tăng cường là gì?',\n",
       "   'Thành phần nào trong học tăng cường xác định hành vi của agent?',\n",
       "   'Bạn biểu diễn trạng thái của agent trong học tăng cường bằng gì?']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'explanation': 'Agent là một thực thể thực hiện hành động và khám phá môi trường.',\n",
       "    'answer': 'Agent là một thực thể khám phá môi trường.',\n",
       "    'options': 'A. Agent là tình huống mà trong đó phần thưởng được trao đổi\\nB. Agent là một giá trị đơn giản trong học tăng cường.\\nC. Agent là một thực thể khám phá môi trường.'},\n",
       "   {'options': 'A. Chính sách\\nB. Tín hiệu phần thưởng\\nC. Hàm giá trị\\nD. Mô hình môi trường',\n",
       "    'answer': 'Chính sách',\n",
       "    'explanation': 'Chính sách (Policy) xác định hành vi của agent.'},\n",
       "   {'explanation': 'Trạng thái của agent thường được biểu diễn bằng trạng thái Markov.',\n",
       "    'options': 'A. Trạng thái chiết khấu\\nB. Hệ số chiết khấu\\nC. Trạng thái Markov',\n",
       "    'answer': 'Trạng thái Markov'}]],\n",
       " 'distances': [[0.06585059314966202, 0.15563052892684937, 0.1893446296453476]]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Agent trong reinforcement learning là gì?\"\n",
    "\n",
    "embeddings = await litellm_service.embedding_llm_async(\n",
    "    inputs=LiteLLMEmbeddingInput(\n",
    "        text=query\n",
    "    )\n",
    ")\n",
    "results = collection.query(\n",
    "    query_embeddings=[embeddings.embedding],\n",
    "    n_results=3,\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2cf2cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hàm merge trong MergeSort thực hiện hành động chính gì?',\n",
       " 'Tại sao QuickSort thường được ưa thích hơn MergeSort khi sắp xếp mảng trong thực tế?',\n",
       " 'Nguyên tắc mà các thuật toán chia để trị (divide and conquer) sử dụng là gì?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "941ce8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'Gộp các mảng con đã sắp xếp',\n",
       "  'explanation': 'Hàm merge kết hợp hai mảng con đã sắp xếp thành một mảng duy nhất đã được sắp xếp, đây là bước then chốt để đạt được thứ tự tổng thể.',\n",
       "  'options': 'A. Chia mảng\\nB. Sắp xếp các mảng con\\nC. Gộp các mảng con đã sắp xếp\\nD. So sánh từng phần tử'},\n",
       " {'explanation': 'QuickSort thường được ưa chuộng vì khả năng sắp xếp tại chỗ (in-place), cung cấp độ phức tạp không gian thấp hơn so với MergeSort, vốn cần bộ nhớ phụ để hợp nhất.',\n",
       "  'options': 'A. Độ phức tạp không gian thấp hơn\\nB. Thời gian trung bình nhanh hơn\\nC. Dễ cài đặt hơn\\nD. Đảm bảo sắp xếp ổn định',\n",
       "  'answer': 'Độ phức tạp không gian thấp hơn'},\n",
       " {'explanation': 'Các thuật toán như MergeSort và QuickSort chia dữ liệu thành các phần nhỏ, sắp xếp độc lập các phần đó và sau đó kết hợp chúng để tạo thành kết quả đã sắp xếp.',\n",
       "  'options': 'A. Chia dữ liệu thành các phần nhỏ hơn và sắp xếp từng phần độc lập\\nB. Chọn phần tử ngẫu nhiên\\nC. Duyệt tuyến tính\\nD. Đổi chỗ trực tiếp các phần tử',\n",
       "  'answer': 'Chia dữ liệu thành các phần nhỏ hơn và sắp xếp từng phần độc lập'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['metadatas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c49760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neo4jService(settings=Neo4jSetting(uri='bolt://localhost:17687', username='neo4j', password='4_Kz1pLYqtmVsxFJED_gxTN8rBcu4oQKAEqw9mm6zUHY'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graph_db import Neo4jService, Neo4jSetting\n",
    "import chromadb\n",
    "from lite_llm import LiteLLMEmbeddingInput, LiteLLMService, LiteLLMSetting, LiteLLMInput\n",
    "from pydantic import HttpUrl, SecretStr\n",
    "\n",
    "litellm_setting=LiteLLMSetting(\n",
    "    url=HttpUrl(\"http://localhost:9510\"),\n",
    "    token=SecretStr(\"abc123\"),\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    frequency_penalty=0.0,\n",
    "    n=1,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_completion_tokens=10000,\n",
    "    dimension=1024,\n",
    "    embedding_model=\"qwen3-embedding:0.6b\"\n",
    ")\n",
    "\n",
    "litellm_service = LiteLLMService(litellm_setting=litellm_setting)\n",
    "\n",
    "neo4j_service = Neo4jService(\n",
    "    settings=Neo4jSetting(\n",
    "        uri=\"bolt://localhost:17687\",\n",
    "        username=\"neo4j\",\n",
    "        password=\"4_Kz1pLYqtmVsxFJED_gxTN8rBcu4oQKAEqw9mm6zUHY\"\n",
    "    )\n",
    ")\n",
    "\n",
    "neo4j_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f27d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiteLLMOutput(response='Hello! How can I help you today?', completion_tokens=9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lite_llm import CompletionMessage, Role\n",
    "output = await litellm_service.process_async(\n",
    "    inputs=LiteLLMInput(\n",
    "        messages=[\n",
    "            CompletionMessage(\n",
    "                role=Role.USER,\n",
    "                content=\"Hello\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca933f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = await neo4j_service.execute_query(\n",
    "#     cypher=\"\"\"\n",
    "#     Match (d:Document {file_name: 'Reinforcement Learning'})-[:CONTAINED]->(c:Chunk)\n",
    "#     MATCH (c)-[:MENTIONED]->(e:Entity)\n",
    "#     MATCH (e)-[:DESCRIBED]->(desc:Description)\n",
    "#     return desc\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "result = await neo4j_service.execute_query(\n",
    "    cypher=\"\"\"\n",
    "    Match (c:Chunk) return c\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE VECTOR INDEX description_index\n",
    "# FOR (d:Description)\n",
    "# ON (d.embedding)\n",
    "# OPTIONS {\n",
    "#   indexConfig: {\n",
    "#     `vector.dimensions`: 768,\n",
    "#     `vector.similarity_function`: 'cosine'\n",
    "#   }\n",
    "# };\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f53fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7f832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama \n",
    "# t = ollama.embed(\"embeddinggemma\", \"Hồi quy tuyến tính là một trong những kỹ thuật thống kê cơ bản và được sử dụng rộng rãi nhất trong học máy. Nó mô hình hóa mối quan hệ giữa một biến phụ thuộc (biến mục tiêu) và một hoặc nhiều biến độc lập (đặc trưng) bằng cách khớp một phương trình tuyến tính với dữ liệu quan sát được.\")\n",
    "# print(len(t.embeddings[0]))\n",
    "def embedding_ollama(text): \n",
    "    response = ollama.embed(\"qwen3-embedding:0.6b\", text)\n",
    "    return response.embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36bce376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "for i, d in enumerate(result.data):\n",
    "    d = d['c']\n",
    "    embedding = embedding_ollama(d['text'])\n",
    "    cypher = f\"Match (c:Chunk {{uid: '{d['uid']}'}}) set c.embedding = $embedding\"\n",
    "    await neo4j_service.execute_query(\n",
    "        cypher=cypher,\n",
    "        parameters={\"embedding\": embedding},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c10dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:28<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Quality Metrics over Course: 0.7983775346998184\n",
      "STD Quality Metrics over Course: 0.03625653219966605\n",
      "Relevance Ratio over Course: 0.47619047619047616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "course_code = \"dsa2025\"\n",
    "course_results = []\n",
    "topic_descs = []\n",
    "for week_number in range(1, 9):\n",
    "    with open(f'/home/lehoangvu/KLTN/outputs/gpt-4o-mini/{course_code}/week{week_number}_pipeline.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    questions = data['questions']\n",
    "    tmp = [q['topic']['description'] for q in questions]\n",
    "    topic_descs.extend(tmp)\n",
    "\n",
    "for query in tqdm(topic_descs):\n",
    "    embedding = embedding_ollama(query)\n",
    "    SIMILARITY_GETTING = \"\"\"CALL db.index.vector.queryNodes($index_name, $query_nodes, $embedding)\n",
    "    YIELD node, score WHERE node.type = 'ENTITY'\n",
    "    MATCH (node)<-[:DESCRIBED]-(e:Entity)\n",
    "    RETURN e.name AS name, e.type AS type, node.uid AS description_id, node.text AS description, node.chunk_uid AS chunk_id, score ORDER BY score DESC limit $k\n",
    "    \"\"\"\n",
    "    results = await neo4j_service.execute_query(\n",
    "        cypher=SIMILARITY_GETTING,\n",
    "        parameters={\n",
    "            'index_name': 'description_index',\n",
    "            'embedding': embedding,\n",
    "            'k': 10,\n",
    "            'query_nodes': 100,\n",
    "        },\n",
    "        output_format='pandas',\n",
    "    )\n",
    "    \n",
    "    list_results = results.to_dict(orient='records')\n",
    "    course_results.extend([tmp['score'] for tmp in list_results])\n",
    "    \n",
    "mean_similarity = np.mean(course_results)  \n",
    "print(f\"Mean Quality Metrics over Course: {mean_similarity}\")\n",
    "std_similarity = np.std(course_results)\n",
    "print(f\"STD Quality Metrics over Course: {std_similarity}\")\n",
    "relevance_ratio = sum(1 for s in course_results if s >= 0.80) / len(course_results)\n",
    "print(f\"Relevance Ratio over Course: {relevance_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc3f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Gradient Descent',\n",
       "  'type': 'technique',\n",
       "  'description_id': 'a64c332d-ba73-4b6a-890a-3d55e3ee9dc7',\n",
       "  'description': 'Gradient Descent là một thuật toán tối ưu hóa lặp đi lặp lại, cơ bản được sử dụng để tìm cực tiểu (minimize) một hàm mục tiêu (hàm loss hoặc hàm chi phí) bằng cách di chuyển theo hướng ngược lại của gradient của hàm đó. Thuật toán này cập nhật các tham số mô hình (weights $w$, $W^{[l]}$, $b^{[l]}$, hoặc hệ số $\\\\beta$) theo công thức chung: $w := w - \\\\alpha \\\\cdot \\\\nabla L$ (hoặc $w := w - \\\\eta \\\\cdot \\\\nabla L$, $W^{[l]} := W^{[l]} - \\\\alpha \\\\frac{\\\\partial L}{\\\\partial W^{[l]}}$, $b^{[l]} := b^{[l]} - \\\\alpha \\\\frac{\\\\partial L}{\\\\partial b^{[l]}}$, $\\\\beta_j := \\\\beta_j - \\\\alpha \\\\cdot \\\\frac{\\\\partial J(\\\\beta)}{\\\\partial \\\\beta_j}$), trong đó $\\\\alpha$ (hoặc $\\\\eta$) là learning rate và $\\\\nabla L$ (hoặc $\\\\frac{\\\\partial L}{\\\\partial w}$, $\\\\frac{\\\\partial J(\\\\beta)}{\\\\partial \\\\beta_j}$) là gradient của hàm loss $L$ (hoặc hàm chi phí $J$) đối với các tham số. Thuật toán này được áp dụng rộng rãi trong huấn luyện các mô hình học máy, đặc biệt là neural networks và Logistic Regression (để cực tiểu hóa Cross-Entropy Loss, với công thức đạo hàm $\\\\frac{\\\\partial J}{\\\\partial \\\\beta_j} = \\\\frac{1}{m} \\\\sum (h_\\\\beta(x^{(i)}) - y^{(i)})x_j^{(i)}$). Khi cập nhật dựa trên toàn bộ training set (batch gradient descent), nó đảm bảo tìm được local minimum với hàm convex nhưng có thể chậm với dữ liệu lớn và có nguy cơ bị kẹt ở local minima hoặc saddle points. Gradient Descent đặc biệt hữu ích với dữ liệu lớn.',\n",
       "  'chunk_id': '3742d85e-c5fa-4421-ad74-ca25d1bfc8a5|93daf18b-7550-43c1-a390-85ad238e2aa2|4632f48a-8868-42f7-b812-44eeabe27e3d|62d6597a-60c2-4d65-9632-aec223345f36|1b36ddb8-64f0-4876-bb2f-61c51b10d01a',\n",
       "  'score': 0.8568469882011414},\n",
       " {'name': 'β_1',\n",
       "  'type': 'parameter',\n",
       "  'description_id': 'bd42e9c4-0985-40b7-a5e7-befcdf267eae',\n",
       "  'description': 'β_1 và β_2 là các hệ số làm mượt (decay rate) trong thuật toán Adam, đóng vai trò là các hyperparameter. β_1 là hệ số làm mượt cho moment bậc 1 (trung bình động của gradient), thường có giá trị mặc định là 0.9. Nó kiểm soát mức độ ảnh hưởng của các gradient trước đó đến ước lượng moment bậc 1 hiện tại, giúp tăng tốc độ hội tụ theo hướng nhất quán; giá trị β_1 cao hơn làm cho moment bậc 1 phản ứng chậm hơn với các thay đổi gradient mới. β_2 là hệ số làm mượt cho moment bậc 2 (trung bình động của bình phương gradient), thường có giá trị mặc định là 0.999. Nó kiểm soát mức độ ảnh hưởng của các bình phương gradient trước đó đến ước lượng moment bậc 2 hiện tại, giúp điều chỉnh learning rate cho từng tham số; giá trị β_2 cao hơn làm cho moment bậc 2 phản ứng chậm hơn với các thay đổi về độ lớn gradient mới.',\n",
       "  'chunk_id': '81274a9d-37c0-4540-9c6b-70b86711531e|81274a9d-37c0-4540-9c6b-70b86711531e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e',\n",
       "  'score': 0.8437535762786865},\n",
       " {'name': 'β_2',\n",
       "  'type': 'parameter',\n",
       "  'description_id': 'bd42e9c4-0985-40b7-a5e7-befcdf267eae',\n",
       "  'description': 'β_1 và β_2 là các hệ số làm mượt (decay rate) trong thuật toán Adam, đóng vai trò là các hyperparameter. β_1 là hệ số làm mượt cho moment bậc 1 (trung bình động của gradient), thường có giá trị mặc định là 0.9. Nó kiểm soát mức độ ảnh hưởng của các gradient trước đó đến ước lượng moment bậc 1 hiện tại, giúp tăng tốc độ hội tụ theo hướng nhất quán; giá trị β_1 cao hơn làm cho moment bậc 1 phản ứng chậm hơn với các thay đổi gradient mới. β_2 là hệ số làm mượt cho moment bậc 2 (trung bình động của bình phương gradient), thường có giá trị mặc định là 0.999. Nó kiểm soát mức độ ảnh hưởng của các bình phương gradient trước đó đến ước lượng moment bậc 2 hiện tại, giúp điều chỉnh learning rate cho từng tham số; giá trị β_2 cao hơn làm cho moment bậc 2 phản ứng chậm hơn với các thay đổi về độ lớn gradient mới.',\n",
       "  'chunk_id': '81274a9d-37c0-4540-9c6b-70b86711531e|81274a9d-37c0-4540-9c6b-70b86711531e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e',\n",
       "  'score': 0.8437535762786865},\n",
       " {'name': 'β₂',\n",
       "  'type': 'parameter',\n",
       "  'description_id': 'bd42e9c4-0985-40b7-a5e7-befcdf267eae',\n",
       "  'description': 'β_1 và β_2 là các hệ số làm mượt (decay rate) trong thuật toán Adam, đóng vai trò là các hyperparameter. β_1 là hệ số làm mượt cho moment bậc 1 (trung bình động của gradient), thường có giá trị mặc định là 0.9. Nó kiểm soát mức độ ảnh hưởng của các gradient trước đó đến ước lượng moment bậc 1 hiện tại, giúp tăng tốc độ hội tụ theo hướng nhất quán; giá trị β_1 cao hơn làm cho moment bậc 1 phản ứng chậm hơn với các thay đổi gradient mới. β_2 là hệ số làm mượt cho moment bậc 2 (trung bình động của bình phương gradient), thường có giá trị mặc định là 0.999. Nó kiểm soát mức độ ảnh hưởng của các bình phương gradient trước đó đến ước lượng moment bậc 2 hiện tại, giúp điều chỉnh learning rate cho từng tham số; giá trị β_2 cao hơn làm cho moment bậc 2 phản ứng chậm hơn với các thay đổi về độ lớn gradient mới.',\n",
       "  'chunk_id': '81274a9d-37c0-4540-9c6b-70b86711531e|81274a9d-37c0-4540-9c6b-70b86711531e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e',\n",
       "  'score': 0.8437535762786865},\n",
       " {'name': 'β₁',\n",
       "  'type': 'parameter',\n",
       "  'description_id': 'bd42e9c4-0985-40b7-a5e7-befcdf267eae',\n",
       "  'description': 'β_1 và β_2 là các hệ số làm mượt (decay rate) trong thuật toán Adam, đóng vai trò là các hyperparameter. β_1 là hệ số làm mượt cho moment bậc 1 (trung bình động của gradient), thường có giá trị mặc định là 0.9. Nó kiểm soát mức độ ảnh hưởng của các gradient trước đó đến ước lượng moment bậc 1 hiện tại, giúp tăng tốc độ hội tụ theo hướng nhất quán; giá trị β_1 cao hơn làm cho moment bậc 1 phản ứng chậm hơn với các thay đổi gradient mới. β_2 là hệ số làm mượt cho moment bậc 2 (trung bình động của bình phương gradient), thường có giá trị mặc định là 0.999. Nó kiểm soát mức độ ảnh hưởng của các bình phương gradient trước đó đến ước lượng moment bậc 2 hiện tại, giúp điều chỉnh learning rate cho từng tham số; giá trị β_2 cao hơn làm cho moment bậc 2 phản ứng chậm hơn với các thay đổi về độ lớn gradient mới.',\n",
       "  'chunk_id': '81274a9d-37c0-4540-9c6b-70b86711531e|81274a9d-37c0-4540-9c6b-70b86711531e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e|5576b4bf-a8d8-4ce9-9851-ba56b8543d6e',\n",
       "  'score': 0.8437535762786865},\n",
       " {'name': 'alpha',\n",
       "  'type': 'parameter',\n",
       "  'description_id': 'a1732e1c-b1a4-402e-8fb1-ce352b6b69ea',\n",
       "  'description': 'alpha là tham số complexity trong Cost Complexity Pruning. Tham số này kiểm soát mức độ cắt tỉa: giá trị $\\\\alpha$ càng lớn thì cây càng bị cắt nhiều hơn, dẫn đến một cây đơn giản hơn. Việc chọn giá trị $\\\\alpha$ phù hợp là rất quan trọng để đạt được sự cân bằng giữa bias và variance.',\n",
       "  'chunk_id': 'd79bd610-5ae7-41b8-8b48-3a802be9b277',\n",
       "  'score': 0.8299418091773987},\n",
       " {'name': 'α_t = α_0 / (1 + decay × t)',\n",
       "  'type': 'formula',\n",
       "  'description_id': 'ff19279f-0a39-4ca4-a12e-c3395fb4e746',\n",
       "  'description': \"Công thức α_t = α_0 / (1 + decay × t) mô tả cách learning rate (α_t) giảm dần theo thời gian (t). Trong đó, α_0 là learning rate ban đầu và 'decay' là một hằng số điều khiển tốc độ giảm. Công thức này được sử dụng trong kỹ thuật Learning rate decay để điều chỉnh learning rate trong quá trình huấn luyện mô hình.\",\n",
       "  'chunk_id': '2947b2f5-0b73-4c82-81fa-ead4476b8e0d',\n",
       "  'score': 0.8283376097679138},\n",
       " {'name': 'β := β - α∇J(β)',\n",
       "  'type': 'formula',\n",
       "  'description_id': 'c467f823-1c58-4e83-be45-d1ab69c4432d',\n",
       "  'description': 'β := β - α∇J(β) là công thức cập nhật tham số trong thuật toán Gradient Descent (bao gồm cả Batch Gradient Descent). Công thức này có thể được biểu diễn cho từng tham số riêng lẻ là β_j := β_j - α * ∂J(β)/∂β_j, hoặc dưới dạng vector là β := β - α∇J(β). Trong đó, β (hoặc β_j) là tham số (hoặc vector tham số) hiện tại, α là learning rate (tốc độ học), ∂J(β)/∂β_j là đạo hàm riêng của hàm chi phí J(β) theo tham số β_j, và ∇J(β) là vector gradient của hàm chi phí J(β) theo tất cả các tham số β. Công thức này điều chỉnh các tham số theo hướng ngược lại với gradient để cực tiểu hóa hàm chi phí J(β), cập nhật tất cả các tham số đồng thời dựa trên gradient tính toán từ toàn bộ tập dữ liệu.',\n",
       "  'chunk_id': '3742d85e-c5fa-4421-ad74-ca25d1bfc8a5|3742d85e-c5fa-4421-ad74-ca25d1bfc8a5',\n",
       "  'score': 0.8268836736679077},\n",
       " {'name': 'β_j := β_j - α * ∂J(β)/∂β_j',\n",
       "  'type': 'formula',\n",
       "  'description_id': 'c467f823-1c58-4e83-be45-d1ab69c4432d',\n",
       "  'description': 'β := β - α∇J(β) là công thức cập nhật tham số trong thuật toán Gradient Descent (bao gồm cả Batch Gradient Descent). Công thức này có thể được biểu diễn cho từng tham số riêng lẻ là β_j := β_j - α * ∂J(β)/∂β_j, hoặc dưới dạng vector là β := β - α∇J(β). Trong đó, β (hoặc β_j) là tham số (hoặc vector tham số) hiện tại, α là learning rate (tốc độ học), ∂J(β)/∂β_j là đạo hàm riêng của hàm chi phí J(β) theo tham số β_j, và ∇J(β) là vector gradient của hàm chi phí J(β) theo tất cả các tham số β. Công thức này điều chỉnh các tham số theo hướng ngược lại với gradient để cực tiểu hóa hàm chi phí J(β), cập nhật tất cả các tham số đồng thời dựa trên gradient tính toán từ toàn bộ tập dữ liệu.',\n",
       "  'chunk_id': '3742d85e-c5fa-4421-ad74-ca25d1bfc8a5|3742d85e-c5fa-4421-ad74-ca25d1bfc8a5',\n",
       "  'score': 0.8268836736679077},\n",
       " {'name': 'Gradient Descent',\n",
       "  'type': 'technique',\n",
       "  'description_id': '73153cce-80f0-42ec-b6ba-e3ec2dcc44ac',\n",
       "  'description': 'Gradient Descent là một thuật toán tối ưu được sử dụng để huấn luyện Generator. Generator cập nhật tham số θ_G bằng cách giảm gradient của hàm mục tiêu của nó: θ_G ← θ_G - η∇θ_G V(D,G). Cụ thể, Generator cố gắng giảm log xác suất mà dữ liệu giả của nó bị Discriminator từ chối, hoặc tương đương, tăng log xác suất mà dữ liệu giả của nó được Discriminator chấp nhận (non-saturating loss).',\n",
       "  'chunk_id': '5f0c45f5-7c26-48aa-b834-1916c523b906',\n",
       "  'score': 0.8258001804351807}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = results.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c178e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.8315550999870504)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def cosine_similarity(vector1: np.ndarray, vector2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        vector1: First vector as numpy array\n",
    "        vector2: Second vector as numpy array\n",
    "    \n",
    "    Returns:\n",
    "        Cosine similarity value between -1 and 1\n",
    "    \"\"\"\n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    \n",
    "    # Calculate magnitudes (L2 norms)\n",
    "    magnitude1 = np.linalg.norm(vector1)\n",
    "    magnitude2 = np.linalg.norm(vector2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "text1 = \"Feature Selection trong Machine Learning là gì\"\n",
    "text2 = \"Feature Selection (Lựa chọn đặc trưng) là kỹ thuật xác định các đặc trưng liên quan nhất từ tập dữ liệu để xây dựng mô hình học máy hiệu quả. Kỹ thuật này giúp giảm số chiều dữ liệu, cải thiện hiệu suất mô hình, giảm overfitting, giảm thời gian huấn luyện và tăng khả năng diễn giải của mô hình.\"\n",
    "embedding1 = np.array(\n",
    "    litellm_service.embedding_ollama(\n",
    "        inputs=LiteLLMEmbeddingInput(\n",
    "            text=text1\n",
    "        )\n",
    "    ).embedding\n",
    ")\n",
    "embedding2 = np.array(\n",
    "    litellm_service.embedding_ollama(\n",
    "        inputs=LiteLLMEmbeddingInput(\n",
    "            text=text2\n",
    "        )\n",
    "    ).embedding\n",
    ")\n",
    "print(len(embedding1), len(embedding2))\n",
    "cosine = cosine_similarity(np.array(embedding1), np.array(embedding2))\n",
    "cosine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
